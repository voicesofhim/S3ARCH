# Permaweb Documentation Collection

Generated on: 2025-08-25T17:27:01.588Z
Total documents: 361
Total words: 186932

## Table of Contents

### Included Documents

1. [Glossary](https://glossary.arweave.net/glossary.txt)
2. [Arns viewer - Guides](https://docs.ar.io/guides/arns-viewer)
3. [Crossmint NFT Minting App](https://docs.ar.io/guides/example-apps/crossmint-app)
4. [Register an IP Asset on Arweave](https://docs.ar.io/guides/story)
5. [Minimal Svelte Starter Kit](https://cookbook.arweave.net/kits/svelte/minimal.html)
6. [Function Piping WAO](https://docs.wao.eco/api/function-piping)
7. [HyperBEAM WAO](https://docs.wao.eco/hyperbeam)
8. [Legacynet AOS WAO](https://docs.wao.eco/legacynet)
9. [Legacynet Compatible AOS WAO](https://docs.wao.eco/hyperbeam/legacynet-aos)
10. [Payment System WAO](https://docs.wao.eco/hyperbeam/payment-system)
11. [Processes and Scheduler WAO](https://docs.wao.eco/hyperbeam/processes-scheduler)
12. [wasm6410](https://hyperbeam.arweave.net/build/devices/wasm64-at-1-0.html)
13. [useWayfinder](https://docs.ar.io/wayfinder/react/use-wayfinder)
14. [Lua Serverless Functions](https://cookbook.arweave.net/fundamentals/decentralized-computing/hyperbeam/lua-serverless.html)
15. [Create React App Starter Kit](https://cookbook.arweave.net/kits/react/create-react-app.html)
16. [AO WAO](https://docs.wao.eco/api/ao)
17. [Hashpaths WAO](https://docs.wao.eco/hyperbeam/hashpaths)
18. [TEE Nodes](https://hyperbeam.arweave.net/run/tee-nodes.html)
19. [React Starter Kit wvite ArDrive](https://cookbook.arweave.net/kits/react/turbo.html)
20. [Legacynet AOS on HyperBEAM WAO](https://docs.wao.eco/tutorials/legacynet-aos)
21. [HyperBEAM Introduction](https://cookbook.arweave.net/fundamentals/decentralized-computing/hyperbeam/hyperbeam-introduction.html)
22. [Create Vue Starter Kit](https://cookbook.arweave.net/kits/vue/create-vue.html)
23. [GQL WAO](https://docs.wao.eco/api/gql)
24. [Core - Wayfinder](https://docs.ar.io/wayfinder/core)
25. [Overview](https://hyperbeam.arweave.net/build/devices/hyperbeam-devices.html)
26. [Module dev_p4erl](https://hyperbeam.arweave.net/build/devices/source-code/dev_p4.html)
27. [Module dev_routererl](https://hyperbeam.arweave.net/build/devices/source-code/dev_router.html)
28. [JoiningRunning a Router](https://hyperbeam.arweave.net/run/joining-running-a-router.html)
29. [Running a HyperBEAM Node](https://hyperbeam.arweave.net/run/running-a-hyperbeam-node.html)
30. [ARIO Network Testnet](https://docs.ar.io/guides/testnet)
31. [Getting started - Wayfinder](https://docs.ar.io/wayfinder/getting-started)
32. [GraphQL Queries](https://cookbook.arweave.net/fundamentals/accessing-arweave-data/graphql.html)
33. [What are AO Processes](https://cookbook.arweave.net/fundamentals/decentralized-computing/ao-processes/what-are-ao-processes.html)
34. [Exposing Process State to HyperBEAM](https://cookbook.arweave.net/fundamentals/decentralized-computing/hyperbeam/getting-ao-state.html)
35. [HyperBEAM Devices](https://cookbook.arweave.net/fundamentals/decentralized-computing/hyperbeam/hyperbeam-devices.html)
36. [Hello World (CLI)](https://cookbook.arweave.net/getting-started/quick-starts/hw-cli.html)
37. [Github Action](https://cookbook.arweave.net/tooling/deployment/github-action.html)
38. [ar-gql](https://cookbook.arweave.net/tooling/graphql/ar-gql.html)
39. [GraphQL Tools](https://cookbook.arweave.net/tooling/graphql/index.html)
40. [Querying Arweave with GraphQL](https://cookbook.arweave.net/tooling/querying-arweave.html)
41. [Process WAO](https://docs.wao.eco/api/process)
42. [HBSig WAO](https://docs.wao.eco/api/hbsig)
43. [Device Composition WAO](https://docs.wao.eco/hyperbeam/device-composition)
44. [Installing HyperBEAM and WAO](https://docs.wao.eco/hyperbeam/installing-hb-wao)
45. [Running LLMs on AOS (Highly Experimental) WAO](https://docs.wao.eco/tutorials/running-llms)
46. [Hyperbeam.arweave.net - Hyperbeam](https://hyperbeam.arweave.net/)
47. [meta10](https://hyperbeam.arweave.net/build/devices/meta-at-1-0.html)
48. [relay10](https://hyperbeam.arweave.net/build/devices/relay-at-1-0.html)
49. [scheduler10](https://hyperbeam.arweave.net/build/devices/scheduler-at-1-0.html)
50. [Intro to AO-Core](https://hyperbeam.arweave.net/build/introduction/what-is-ao-core.html)
51. [Core Capabilities](https://hyperbeam.arweave.net/build/hyperbeam-capabilities.html)
52. [Pathing in HyperBEAM](https://hyperbeam.arweave.net/build/pathing-in-hyperbeam.html)
53. [FAQ](https://hyperbeam.arweave.net/run/reference/faq.html)
54. [Configuring Your Machine](https://hyperbeam.arweave.net/run/configuring-your-machine.html)
55. [Troubleshooting](https://hyperbeam.arweave.net/run/reference/troubleshooting.html)
56. [A whistle stop tour of Lua](https://cookbook_ao.arweave.net/concepts/lua.html)
57. [Messages](https://cookbook_ao.arweave.net/concepts/messages.html)
58. [ao Specs](https://cookbook_ao.arweave.net/concepts/specs.html)
59. [aos Brief Tour](https://cookbook_ao.arweave.net/concepts/tour.html)
60. [DataItem Signers](https://cookbook_ao.arweave.net/guides/aoconnect/signers.html)
61. [Sending a Message to a Process](https://cookbook_ao.arweave.net/guides/aoconnect/sending-messages.html)
62. [CRED Utils Blueprint](https://cookbook_ao.arweave.net/guides/aos/blueprints/cred-utils.html)
63. [Spawning a Process](https://cookbook_ao.arweave.net/guides/aoconnect/spawning-processes.html)
64. [Staking Blueprint](https://cookbook_ao.arweave.net/guides/aos/blueprints/staking.html)
65. [Token Blueprint](https://cookbook_ao.arweave.net/guides/aos/blueprints/token.html)
66. [Editor setup](https://cookbook_ao.arweave.net/guides/aos/editor.html)
67. [FAQ](https://cookbook_ao.arweave.net/guides/aos/faq.html)
68. [CLI](https://cookbook_ao.arweave.net/guides/aos/cli.html)
69. [Voting Blueprint](https://cookbook_ao.arweave.net/guides/aos/blueprints/voting.html)
70. [aos AO Operating System](https://cookbook_ao.arweave.net/guides/aos/index.html)
71. [Building a Token in ao](https://cookbook_ao.arweave.net/guides/aos/token.html)
72. [Guides](https://cookbook_ao.arweave.net/guides/index.html)
73. [Troubleshooting using aolink](https://cookbook_ao.arweave.net/guides/aos/troubleshooting.html)
74. [HyperBEAM from AO Connect](https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/ao-connect.html)
75. [AO Dev-Cli 01](https://cookbook_ao.arweave.net/guides/dev-cli/index.html)
76. [Using WeaveDrive](https://cookbook_ao.arweave.net/guides/snacks/weavedrive.html)
77. [Reading Dynamic State](https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/reading-dynamic-state.html)
78. [Getting started with SQLite](https://cookbook_ao.arweave.net/guides/snacks/sqlite.html)
79. [Community Resources](https://cookbook_ao.arweave.net/references/community.html)
80. [Editor setup](https://cookbook_ao.arweave.net/references/editor-setup.html)
81. [Accessing Data from Arweave with ao](https://cookbook_ao.arweave.net/references/data.html)
82. [Meet Lua](https://cookbook_ao.arweave.net/references/lua.html)
83. [ao Token and Subledger Specification](https://cookbook_ao.arweave.net/references/token.html)
84. [Messaging Patterns in ao](https://cookbook_ao.arweave.net/references/messaging.html)
85. [Meet Web Assembly](https://cookbook_ao.arweave.net/references/wasm.html)
86. [Crafting a Token](https://cookbook_ao.arweave.net/tutorials/begin/token.html)
87. [Preparations](https://cookbook_ao.arweave.net/tutorials/begin/preparations.html)
88. [Automated Responses](https://cookbook_ao.arweave.net/tutorials/bots-and-games/attacking.html)
89. [Mechanics of the Arena](https://cookbook_ao.arweave.net/tutorials/bots-and-games/arena-mechanics.html)
90. [Lets Play A Game](https://cookbook_ao.arweave.net/tutorials/bots-and-games/ao-effect.html)
91. [Tokengating the Chatroom](https://cookbook_ao.arweave.net/tutorials/begin/tokengating.html)
92. [Bringing it Together](https://cookbook_ao.arweave.net/tutorials/bots-and-games/bringing-together.html)
93. [Strategic Decisions](https://cookbook_ao.arweave.net/tutorials/bots-and-games/decisions.html)
94. [Bots and Games](https://cookbook_ao.arweave.net/tutorials/bots-and-games/index.html)
95. [Introduction to AO-Core](https://cookbook_ao.arweave.net/welcome/ao-core-introduction.html)
96. [Get started in 5 minutes](https://cookbook_ao.arweave.net/welcome/getting-started.html)
97. [Ar io sdk - Docs](https://docs.ar.io/ar-io-sdk)
98. [ANTRegistry](https://docs.ar.io/ar-io-sdk/ant-registry)
99. [getAntsForAddress](https://docs.ar.io/ar-io-sdk/ant-registry/get-ants-for-address)
100. [ANTVersions](https://docs.ar.io/ar-io-sdk/ant-versions)
101. [ANT Configuration](https://docs.ar.io/ar-io-sdk/ants/configuration)
102. [Release name - Ants](https://docs.ar.io/ar-io-sdk/ants/release-name)
103. [setBaseNameRecord](https://docs.ar.io/ar-io-sdk/ants/set-base-name-record)
104. [setRecord](https://docs.ar.io/ar-io-sdk/ants/set-record)
105. [setUndernameRecord](https://docs.ar.io/ar-io-sdk/ants/set-undername-record)
106. [buyRecord](https://docs.ar.io/ar-io-sdk/ario/arns/buy-record)
107. [getArNSRecords](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-records)
108. [Extend lease - Arns](https://docs.ar.io/ar-io-sdk/ario/arns/extend-lease)
109. [getTokenCost](https://docs.ar.io/ar-io-sdk/ario/arns/get-token-cost)
110. [Get arns returned names - Arns](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-returned-names)
111. [getCostDetails](https://docs.ar.io/ar-io-sdk/ario/arns/get-cost-details)
112. [Increase undername limit - Arns](https://docs.ar.io/ar-io-sdk/ario/arns/increase-undername-limit)
113. [Get delegations - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/get-delegations)
114. [getGateways](https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateways)
115. [joinNetwork](https://docs.ar.io/ar-io-sdk/ario/gateways/join-network)
116. [Redelegate stake - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/redelegate-stake)
117. [Update gateway settings - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/update-gateway-settings)
118. [ARIO Smart Contract](https://docs.ar.io/ario-contract)
119. [Token Faucet](https://docs.ar.io/ar-io-sdk/faucet)
120. [Arweave Name System (ArNS)](https://docs.ar.io/arns)
121. [ARIO SDK Release Notes](https://docs.ar.io/ar-io-sdk/release-notes)
122. [Gateway Architecture](https://docs.ar.io/gateways)
123. [Advanced - Gateways](https://docs.ar.io/gateways/advanced)
124. [Advanced config - Ar io node](https://docs.ar.io/gateways/ar-io-node/advanced-config.html)
125. [Bundler - Gateways](https://docs.ar.io/gateways/bundler)
126. [ARIO Gateway Environment Variables](https://docs.ar.io/gateways/env)
127. [AO Compute Unit (CU)](https://docs.ar.io/gateways/cu)
128. [ARIO Node Filtering System](https://docs.ar.io/gateways/filters)
129. [ARIO Gateway Grafana](https://docs.ar.io/gateways/grafana)
130. [Gateway Network](https://docs.ar.io/gateways/gateway-network)
131. [Linux setup - Gateways](https://docs.ar.io/gateways/linux-setup)
132. [Join the Gateway Network](https://docs.ar.io/gateways/join-network)
133. [Observation and Incentives](https://docs.ar.io/gateways/observer)
134. [ARIO Node Release Notes](https://docs.ar.io/gateways/release-notes)
135. [Optimizing Data Handling in ARIO Gateway](https://docs.ar.io/gateways/optimize-data)
136. [Parquet and ClickHouse Usage Guide](https://docs.ar.io/gateways/parquet)
137. [Content Moderation](https://docs.ar.io/gateways/moderation)
138. [ARIO Node Release Notes](https://docs.ar.io/gateways/release-notes#)
139. [Upgrading - Gateways](https://docs.ar.io/gateways/upgrading)
140. [Windows setup - Gateways](https://docs.ar.io/gateways/windows-setup)
141. [Gateway Troubleshooting FAQ](https://docs.ar.io/gateways/troubleshooting)
142. [Importing SQLite Database Snapshots](https://docs.ar.io/gateways/snapshots)
143. [Deploy a dApp with ArDrive web](https://docs.ar.io/guides/ardrive-web)
144. [ANTs on Bazar](https://docs.ar.io/guides/ants-on-bazar)
145. [Glossary](https://docs.ar.io/glossary)
146. [Arlink Deploy](https://docs.ar.io/guides/arlink)
147. [Permaweb deploy - Guides](https://docs.ar.io/guides/permaweb-deploy)
148. [Managing Undernames](https://docs.ar.io/guides/managing-undernames)
149. [Gql - Guides](https://docs.ar.io/guides/gql)
150. [Uploading to Arweave](https://docs.ar.io/guides/uploading-to-arweave)
151. [Managing Primary Names](https://docs.ar.io/guides/primary-names)
152. [The ARIO Token](https://docs.ar.io/token)
153. [Wayfinder - Docs](https://docs.ar.io/wayfinder)
154. [Local storage - Gateway providers](https://docs.ar.io/wayfinder/core/gateway-providers/local-storage)
155. [Telemetry](https://docs.ar.io/wayfinder/core/telemetry)
156. [Signature Verification Strategy](https://docs.ar.io/wayfinder/core/verification-strategies/signature-verification)
157. [Wayfinder Core Release Notes](https://docs.ar.io/wayfinder/release-notes/core)
158. [ArNS - Arweave Name System](https://cookbook.arweave.net/fundamentals/accessing-arweave-data/arns.html)
159. [Accessing Arweave Data](https://cookbook.arweave.net/fundamentals/accessing-arweave-data/index.html)
160. [Gateways in the Arweave Network](https://cookbook.arweave.net/fundamentals/accessing-arweave-data/gateways.html)
161. [Path Manifests](https://cookbook.arweave.net/fundamentals/accessing-arweave-data/manifests.html)
162. [Decentralized Computing](https://cookbook.arweave.net/fundamentals/decentralized-computing/index.html)
163. [Transaction Metadata (Tags)](https://cookbook.arweave.net/fundamentals/transactions/tags.html)
164. [Posting Transactions](https://cookbook.arweave.net/fundamentals/transactions/post-transactions.html)
165. [Creating a wallet - Wallets and keyfiles](https://cookbook.arweave.net/fundamentals/wallets-and-keyfiles/creating-a-wallet.html)
166. [Wallets and Keys](https://cookbook.arweave.net/fundamentals/wallets-and-keyfiles/index.html)
167. [Developing on the Permaweb](https://cookbook.arweave.net/getting-started/welcome.html)
168. [Hello World (Code)](https://cookbook.arweave.net/getting-started/quick-starts/hw-code.html)
169. [Svelte Starter Kits](https://cookbook.arweave.net/kits/svelte/index.html)
170. [React Starter Kits](https://cookbook.arweave.net/kits/react/index.html)
171. [Vue Starter Kits](https://cookbook.arweave.net/kits/vue/index.html)
172. [SvelteVite Starter Kit](https://cookbook.arweave.net/kits/svelte/vite.html)
173. [arkb](https://cookbook.arweave.net/tooling/deployment/arkb.html)
174. [Permaweb Deploy](https://cookbook.arweave.net/tooling/deployment/permaweb-deploy.html)
175. [ANS-104 Bundled Data v20 - Binary Serialization](https://cookbook.arweave.net/tooling/specs/ans/ANS-104.html)
176. [ANS-102 Bundled Data - JSON Serialization](https://cookbook.arweave.net/tooling/specs/ans/ANS-102.html)
177. [ANS-103 Succinct Proofs of Random Access](https://cookbook.arweave.net/tooling/specs/ans/ANS-103.html)
178. [ArFS Protocol A Decentralized File System on Arweave](https://cookbook.arweave.net/tooling/specs/arfs/arfs.html)
179. [Privacy](https://cookbook.arweave.net/tooling/specs/arfs/privacy.html)
180. [Content Types](https://cookbook.arweave.net/tooling/specs/arfs/content-types.html)
181. [Data Model](https://cookbook.arweave.net/tooling/specs/arfs/data-model.html)
182. [Entity Types](https://cookbook.arweave.net/tooling/specs/arfs/entity-types.html)
183. [AR WAO](https://docs.wao.eco/api/ar)
184. [ArMem WAO](https://docs.wao.eco/api/armem)
185. [HyperBEAM WAO](https://docs.wao.eco/api/hyperbeam)
186. [Structured Codec WAO](https://docs.wao.eco/hyperbeam/codec-structured)
187. [Flat Codec WAO](https://docs.wao.eco/hyperbeam/codec-flat)
188. [Creating Custom HyperBEAM Devices WAO](https://docs.wao.eco/tutorials/creating-devices)
189. [Custom Devices in C WAO](https://docs.wao.eco/tutorials/devices-cpp)
190. [getArNSReservedNames](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-reserved-names)
191. [upgradeRecord](https://docs.ar.io/ar-io-sdk/ario/arns/upgrade-record)
192. [Get gateway delegates - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateway-delegates)
193. [Get allowed delegates - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/get-allowed-delegates)
194. [Hello World (No Code)](https://cookbook.arweave.net/getting-started/quick-starts/hw-no-code.html)
195. [createVault](https://docs.ar.io/ar-io-sdk/ario/vaults/create-vault)
196. [process10](https://hyperbeam.arweave.net/build/devices/process-at-1-0.html)
197. [BetterIDEa](https://cookbook_ao.arweave.net/references/betteridea/index.html)
198. [ARIO Configuration](https://docs.ar.io/ar-io-sdk/ario/configuration)
199. [Reassign name - Ants](https://docs.ar.io/ar-io-sdk/ants/reassign-name)
200. [Remove primary names - Ants](https://docs.ar.io/ar-io-sdk/ants/remove-primary-names)
201. [approvePrimaryNameRequest](https://docs.ar.io/ar-io-sdk/ants/approve-primary-name-request)
202. [Decrease delegate stake - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/decrease-delegate-stake)
203. [Instant withdrawal - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/instant-withdrawal)
204. [addVersion](https://docs.ar.io/ar-io-sdk/ant-versions/add-version)
205. [addController](https://docs.ar.io/ar-io-sdk/ants/add-controller)
206. [cancelWithdrawal](https://docs.ar.io/ar-io-sdk/ario/gateways/cancel-withdrawal)
207. [Leave network - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/leave-network)
208. [ariowayfinder-react](https://docs.ar.io/wayfinder/react)
209. [WAO Hub](https://docs.wao.eco/hub)
210. [AO The Web WAO](https://docs.wao.eco/web)
211. [Set logo - Ants](https://docs.ar.io/ar-io-sdk/ants/set-logo)
212. [Httpsig Codec WAO](https://docs.wao.eco/hyperbeam/codec-httpsig)
213. [Decrease operator stake - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/decrease-operator-stake)
214. [Normalized Addresses](https://docs.ar.io/concepts/normalized-addresses)
215. [removeRecord](https://docs.ar.io/ar-io-sdk/ants/remove-record)
216. [Network - Gateway providers](https://docs.ar.io/wayfinder/core/gateway-providers/network)
217. [Get redelegation fee - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/get-redelegation-fee)
218. [Module hb_singletonerl](https://hyperbeam.arweave.net/build/devices/source-code/hb_singleton.html)
219. [Remove controller - Ants](https://docs.ar.io/ar-io-sdk/ants/remove-controller)
220. [Increase operator stake - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/increase-operator-stake)
221. [removeUndernameRecord](https://docs.ar.io/ar-io-sdk/ants/remove-undername-record)
222. [Increase delegate stake - Gateways](https://docs.ar.io/ar-io-sdk/ario/gateways/increase-delegate-stake)
223. [Custom Devices and Codecs WAO](https://docs.wao.eco/hyperbeam/custom-devices-codecs)
224. [Get primary name - Primary names](https://docs.ar.io/ar-io-sdk/ario/primary-names/get-primary-name)
225. [setKeywords](https://docs.ar.io/ar-io-sdk/ants/set-keywords)
226. [transfer](https://docs.ar.io/ar-io-sdk/ants/transfer)
227. [HTTP Message Signatures WAO](https://docs.wao.eco/hyperbeam/http-message-signatures)
228. [Devices and Pathing WAO](https://docs.wao.eco/hyperbeam/devices-pathing)
229. [ANTgetLogo()](https://docs.ar.io/ar-io-sdk/ants/get-logo)
230. [Set ticker - Ants](https://docs.ar.io/ar-io-sdk/ants/set-ticker)
231. [Get demand factor - Arns](https://docs.ar.io/ar-io-sdk/ario/arns/get-demand-factor)
232. [resolveArNSName](https://docs.ar.io/ar-io-sdk/ario/arns/resolve-arns-name)
233. [Set name - Ants](https://docs.ar.io/ar-io-sdk/ants/set-name)
234. [Set description - Ants](https://docs.ar.io/ar-io-sdk/ants/set-description)
235. [Get arns returned name - Arns](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-returned-name)
236. [getArNSRecord](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-record)
237. [accessControlList](https://docs.ar.io/ar-io-sdk/ant-registry/access-control-list)
238. [getGatewayVaults](https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateway-vaults)
239. [getGateway](https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateway)
240. [Simple cache - Gateway providers](https://docs.ar.io/wayfinder/core/gateway-providers/simple-cache)
241. [getRecords](https://docs.ar.io/ar-io-sdk/ants/get-records)
242. [getInfo](https://docs.ar.io/ar-io-sdk/ario/general/get-info)
243. [getState](https://docs.ar.io/ar-io-sdk/ants/get-state)
244. [getInfo](https://docs.ar.io/ar-io-sdk/ants/get-info)
245. [getArNSNamesForAddress](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-names-for-address)
246. [getLatestANTVersion](https://docs.ar.io/ar-io-sdk/ant-versions/get-latest-ant-version)
247. [Module dev_metaerl](https://hyperbeam.arweave.net/build/devices/source-code/dev_meta.html)
248. [Module dev_snperl](https://hyperbeam.arweave.net/build/devices/source-code/dev_snp.html)
249. [Exposing Process State to HyperBEAM](https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/exposing-process-state.html)
250. [Installing aos](https://cookbook_ao.arweave.net/guides/aos/installing.html)
251. [getANTVersions](https://docs.ar.io/ar-io-sdk/ant-versions/get-ant-versions)
252. [getCurrentEpoch](https://docs.ar.io/ar-io-sdk/ario/epochs/get-current-epoch)
253. [getHandlers](https://docs.ar.io/ar-io-sdk/ants/get-handlers)
254. [register](https://docs.ar.io/ar-io-sdk/ant-registry/register)
255. [json10](https://hyperbeam.arweave.net/build/devices/json-at-1-0.html)
256. [Building Devices](https://hyperbeam.arweave.net/build/devices/building-devices.html)
257. [lua53a](https://hyperbeam.arweave.net/build/devices/lua-at-5-3a.html)
258. [message10](https://hyperbeam.arweave.net/build/devices/message-at-1-0.html)
259. [Module dev_cronerl](https://hyperbeam.arweave.net/build/devices/source-code/dev_cron.html)
260. [Module dev_fafferl](https://hyperbeam.arweave.net/build/devices/source-code/dev_faff.html)
261. [Module dev_codec_structurederl](https://hyperbeam.arweave.net/build/devices/source-code/dev_codec_structured.html)
262. [Module dev_patcherl](https://hyperbeam.arweave.net/build/devices/source-code/dev_patch.html)
263. [Module dev_stackerl](https://hyperbeam.arweave.net/build/devices/source-code/dev_stack.html)
264. [Intro to HyperBEAM](https://hyperbeam.arweave.net/build/introduction/what-is-hyperbeam.html)
265. [FAQ](https://hyperbeam.arweave.net/build/reference/faq.html)
266. [Fuel Your LLM](https://hyperbeam.arweave.net/llms.html)
267. [Glossary](https://hyperbeam.arweave.net/build/reference/glossary.html)
268. [Troubleshooting](https://hyperbeam.arweave.net/build/reference/troubleshooting.html)
269. [Glossary](https://hyperbeam.arweave.net/run/reference/glossary.html)
270. [How ao messaging works](https://cookbook_ao.arweave.net/concepts/how-it-works.html)
271. [Concepts](https://cookbook_ao.arweave.net/concepts/index.html)
272. [Processes](https://cookbook_ao.arweave.net/concepts/processes.html)
273. [Units](https://cookbook_ao.arweave.net/concepts/units.html)
274. [Sending an Assignment to a Process](https://cookbook_ao.arweave.net/guides/aoconnect/assign-data.html)
275. [Calling DryRun](https://cookbook_ao.arweave.net/guides/aoconnect/calling-dryrun.html)
276. [Connecting to specific ao nodes](https://cookbook_ao.arweave.net/guides/aoconnect/connecting.html)
277. [Reading results from an ao Process](https://cookbook_ao.arweave.net/guides/aoconnect/reading-results.html)
278. [Chatroom Blueprint](https://cookbook_ao.arweave.net/guides/aos/blueprints/chatroom.html)
279. [Introduction](https://cookbook_ao.arweave.net/guides/aos/intro.html)
280. [Understanding the Inbox](https://cookbook_ao.arweave.net/guides/aos/inbox-and-handlers.html)
281. [Load Lua Files with load filename](https://cookbook_ao.arweave.net/guides/aos/load.html)
282. [crypto](https://cookbook_ao.arweave.net/guides/aos/modules/crypto.html)
283. [Base64](https://cookbook_ao.arweave.net/guides/aos/modules/base64.html)
284. [Customizing the Prompt in aos](https://cookbook_ao.arweave.net/guides/aos/prompt.html)
285. [Creating a Pingpong Process in aos](https://cookbook_ao.arweave.net/guides/aos/pingpong.html)
286. [Utils](https://cookbook_ao.arweave.net/guides/aos/modules/utils.html)
287. [Connecting to HyperBEAM with aos](https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/aos-with-hyperbeam.html)
288. [Why Migrate to HyperBEAM](https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/why-migrate.html)
289. [Cron Messages](https://cookbook_ao.arweave.net/references/cron.html)
290. [ao Module](https://cookbook_ao.arweave.net/references/ao.html)
291. [Handlers (Version 005)](https://cookbook_ao.arweave.net/references/handlers.html)
292. [References](https://cookbook_ao.arweave.net/references/index.html)
293. [Release Notes](https://cookbook_ao.arweave.net/releasenotes/index.html)
294. [Lua Optimization Guide for AO Platform](https://cookbook_ao.arweave.net/references/lua-optimization.html)
295. [Begin An Interactive Tutorial](https://cookbook_ao.arweave.net/tutorials/begin/index.html)
296. [Messaging in ao](https://cookbook_ao.arweave.net/tutorials/begin/messaging.html)
297. [Building a Chatroom in aos](https://cookbook_ao.arweave.net/tutorials/begin/chatroom.html)
298. [Interpreting Announcements](https://cookbook_ao.arweave.net/tutorials/bots-and-games/announcements.html)
299. [Fetching Game State](https://cookbook_ao.arweave.net/tutorials/bots-and-games/game-state.html)
300. [Expanding the Arena](https://cookbook_ao.arweave.net/tutorials/bots-and-games/build-game.html)
301. [Welcome to ao](https://cookbook_ao.arweave.net/welcome/index.html)
302. [AO Processes](https://cookbook_ao.arweave.net/welcome/ao-processes.html)
303. [Legacynet HyperBEAM](https://cookbook_ao.arweave.net/welcome/legacynet-info/index.html)
304. [ARIO Documentation](https://docs.ar.io/)
305. [Getting started - Ar io sdk](https://docs.ar.io/ar-io-sdk/getting-started)
306. [Admin - Gateways](https://docs.ar.io/gateways/admin)
307. [Gateway Apex Domain Content Resolution](https://docs.ar.io/gateways/apex)
308. [Gateway ArNS Resolution](https://docs.ar.io/gateways/arns-resolution)
309. [Quick Start Guides](https://docs.ar.io/guides)
310. [Introduction](https://docs.ar.io/introduction)
311. [Staking](https://docs.ar.io/staking)
312. [ARIO Network Composition](https://docs.ar.io/network-composition)
313. [Random - Routing strategies](https://docs.ar.io/wayfinder/core/routing-strategies/random)
314. [Preferred with fallback - Routing strategies](https://docs.ar.io/wayfinder/core/routing-strategies/preferred-with-fallback)
315. [Fastest ping - Routing strategies](https://docs.ar.io/wayfinder/core/routing-strategies/fastest-ping)
316. [PingRoutingStrategy](https://docs.ar.io/wayfinder/core/routing-strategies/ping)
317. [Round robin - Routing strategies](https://docs.ar.io/wayfinder/core/routing-strategies/round-robin)
318. [Data Root Verification Strategy](https://docs.ar.io/wayfinder/core/verification-strategies/data-root-verification)
319. [Verification Strategies](https://docs.ar.io/wayfinder/core/verification-strategies)
320. [StaticRoutingStrategy](https://docs.ar.io/wayfinder/core/routing-strategies/static)
321. [Community](https://cookbook.arweave.net/community/index.html)
322. [Transaction Bundles](https://cookbook.arweave.net/fundamentals/transactions/bundles.html)
323. [Core Concepts](https://cookbook.arweave.net/fundamentals/index.html)
324. [Transactions](https://cookbook.arweave.net/fundamentals/transactions/transaction-types.html)
325. [Getting started - Cookbook](https://cookbook.arweave.net/getting-started/index.html)
326. [Contributing Workflow](https://cookbook.arweave.net/getting-started/contributing.html)
327. [Cooking with the Permaweb](https://cookbook.arweave.net/index.html)
328. [Guides](https://cookbook.arweave.net/guides/index.html)
329. [Bundling Services](https://cookbook.arweave.net/tooling/bundlers.html)
330. [Tooling](https://cookbook.arweave.net/tooling/index.html)
331. [Goldsky Search GraphQL Gateway](https://cookbook.arweave.net/tooling/graphql/search-indexing-service.html)
332. [ANS-101 Gateway Capabilities Endpoint](https://cookbook.arweave.net/tooling/specs/ans/ANS-101.html)
333. [ANS-106 Do-Not-Store Request](https://cookbook.arweave.net/tooling/specs/ans/ANS-106.html)
334. [ANS-105 License Tags](https://cookbook.arweave.net/tooling/specs/ans/ANS-105.html)
335. [ANS-110 Asset Discoverability](https://cookbook.arweave.net/tooling/specs/ans/ANS-110.html)
336. [ANS-109 Vouch-For (Assertion of Identity)](https://cookbook.arweave.net/tooling/specs/ans/ANS-109.html)
337. [Get started WAO](https://docs.wao.eco/getting-started)
338. [Decoding HyperBEAM from Scratch WAO](https://docs.wao.eco/hyperbeam/decoding-from-scratch)
339. [Custom Devices in Rust WAO](https://docs.wao.eco/tutorials/devices-rust)
340. [Hash Verification Strategy](https://docs.ar.io/wayfinder/core/verification-strategies/hash-verification)
341. [ao](https://cookbook_ao.arweave.net/guides/aos/modules/ao.html)
342. [Building ao Processes](https://hyperbeam.arweave.net/build/building-on-ao.html)
343. [JSON](https://cookbook_ao.arweave.net/guides/aos/modules/json.html)
344. [References](https://cookbook.arweave.net/references/index.html)
345. [LLMs Documentation](https://cookbook_ao.arweave.net/llms-explanation.html)
346. [Eval](https://cookbook_ao.arweave.net/concepts/eval.html)
347. [Monitoring Cron](https://cookbook_ao.arweave.net/guides/aoconnect/monitoring-cron.html)
348. [Routing Strategies](https://docs.ar.io/wayfinder/core/routing-strategies)
349. [Gateway providers - Core](https://docs.ar.io/wayfinder/core/gateway-providers)
350. [Static - Gateway providers](https://docs.ar.io/wayfinder/core/gateway-providers/static)
351. [Deployment Publishing Tools](https://cookbook.arweave.net/tooling/deployment.html)
352. [Schema diagrams - Arfs](https://cookbook.arweave.net/tooling/specs/arfs/schema-diagrams.html)
353. [Wayfinder Browser Extension Release Notes](https://docs.ar.io/wayfinder/release-notes/extension)
354. [Wayfinder React Release Notes](https://docs.ar.io/wayfinder/release-notes/react)
355. [Tutorials](https://cookbook_ao.arweave.net/tutorials/index.html)
356. [Pretty](https://cookbook_ao.arweave.net/guides/aos/modules/pretty.html)
357. [Installing ao connect](https://cookbook_ao.arweave.net/guides/aoconnect/installing-connect.html)
358. [aoconnect](https://cookbook_ao.arweave.net/guides/aoconnect/aoconnect.html)
359. [Blueprints](https://cookbook_ao.arweave.net/guides/aos/blueprints/index.html)
360. [LLMstxt](https://cookbook.arweave.net/references/llms-txt.html)
361. [Glossary](https://cookbook.arweave.net/references/glossary.html)

### Excluded Documents (Quality Filtered)

1. https://cookbook_ao.arweave.net/guides/aos/modules/index.html

---

# 1. Glossary

Document Number: 1
Source: https://glossary.arweave.net/glossary.txt
Words: 5
Extraction Method: plain-text

Permaweb Glossary Loading glossary data...

---

# 2. ARIO Docs

Document Number: 2
Source: https://docs.ar.io/guides/arns-viewer
Words: 2599
Extraction Method: html

ArNS Viewer Overview This guide will walk you through creating a project that uses the AR.IO SDK to interact with ArNS names in a web environment. It provides all the steps and context needed to help you get up and running smoothly, allowing you to effectively use these technologies.We will be using ARNext, a new framework based on Next.js, to simplify deployment to the Arweave permaweb. ARNext provides flexibility for deploying seamlessly to Arweave using an ArNS name, an Arweave transaction ID, or traditional services like Vercel—all without requiring major code modifications. This means you can deploy the same project across different environments with minimal effort.The guide will focus on the following core functionalities of the AR.IO SDK:Retrieving a List of All Active ArNS Names: Learn how to use the SDK to get and display a list of active ArNS names.Querying Detailed Records for a Specific ArNS Name: Learn how to access detailed records for a specific ArNS name using its ANT (Arweave Name Token).Updating and Creating Records on an ArNS Name: Learn how to modify and add records to an ArNS name, showcasing the capabilities of ANT for dynamic web content.By the end of this guide, you will have a complete, functional project that not only demonstrates how to use the AR.IO SDK but also shows the ease and flexibility of deploying applications to the Arweave permaweb. Whether you are an experienced developer or just starting out, this guide will help you understand the key aspects of building and deploying on Arweave.Getting Started Prerequisites Node v20.17 or greater git Install ARNext ARNext is a brand new framework that is still in development. It supports installation using npx, and you will need the proper Node version for the installation to be successful.You can then move your terminal into that newly created folder with:or open the folder in an IDE like VSCode, and open a new terminal inside that IDE in order to complete the next steps.Sanity Check It is good practice when starting a new project to view it in localhost without any changes, to make sure everything is installed and working correctly. To do this, run:or, if you prefer yarn:By default, the project will be served on port 3000, so you can access it by navigating to localhost:3000 in any browser. You should see something that looks like this: With this complete, you are ready to move on to customizing for your own project.Install AR.IO SDK Next, install the AR.IO SDK.or Polyfills Polyfills are used to provide missing functionality in certain environments. For example, browsers do not have direct access to a computer's file system, but many JavaScript libraries are designed to work in both browser and Node.js environments. These libraries might include references to fs, the module used by Node.js to interact with the file system. Since fs is not available in browsers, we need a polyfill to handle these references and ensure the application runs properly in a browser environment.Installation The below command will install several packages as development dependencies, which should be sufficient to handle most polyfill needs for projects that interact with Arweave.or Next Config With the polyfill packages installed, we need to tell our app how to use them. In NextJS, which ARNext is built on, this is done in the next.config.js file in the root of the project. The default config file will look like this:This configuration allows the app to determine if it is being served via an Arweave transaction Id, or through a more traditional method. From here, we need to add in the additional configurations for resolving our polyfills. The updated next.config.js will look like this:With that, you are ready to start customizing your app.Strip Default Content The first step in building your custom app is to remove the default content and create a clean slate. Follow these steps:Update the Home Page Navigate to pages > index.js, which serves as the main home page.Delete everything in this file and replace it with the following placeholder:Remove Unused Pages The folder pages > posts > [id].js will not be used in this project. Delete the entire posts folder to keep the project organized and free of unnecessary files.Create Header Create a new components folder Inside that, create a Header.js file, leave it blank for now.Create Routes Create a new file at components > ArweaveRoutes.js to handle routing between pages. Leave it simple for now.Your project is now a blank slate, ready for your own custom design and functionality. This clean setup will make it easier to build and maintain your application as you move forward.Add Utilities There are a few functions that we might end up wanting to use in multiple different pages in our finished product. So we can put these in a separate file and export them, so that other pages can import them to use. Start by creating a utils folder in the root of the project, then create 2 files inside of it:auth.js: This will contain the functions required for connecting an Arweave wallet using ArConnect arweave.js: This is where we will put most of our AR.IO SDK functions for interacting with Arweave import { ARIO, ANT, ArconnectSigner } from "@ar.io/sdk/web";

/**
 * Initialize ArIO and fetch all ArNS records.
 * @returns {Promise<Object>} All ArNS records.
 */
export const fetchArNSRecords = async () => {
  const ario = ARIO.init();
  let allRecords = [];
  let hasMore = true;
  let cursor;

  // Paginates through all records to get the full registry.
  while (hasMore) {
    const response = await ario.getArNSRecords({
      limit: 1000, // You can adjust the limit as needed, max is 1000
      sortBy: "name",
      sortOrder: "asc",
      cursor: cursor,
    });

    allRecords = [...allRecords, ...response.items];
    cursor = response.nextCursor;
    hasMore = response.hasMore;
  }

  // console.log(allRecords);
  return allRecords;
};

/**
 * Initialize ANT with the given processId.
 * @param {string} processId - The processId.
 * @returns {Object} ANT instance.
 */
export const initANT = (processId) => {
  return ANT.init({ processId });
};

/**
 * Fetch detailed records, owner, and controllers for a given processId.
 * @param {string} contractTxId - The processId.
 * @returns {Promise<Object>} Detailed records, owner, and controllers.
 */
export const fetchRecordDetails = async (processId) => {
  const ant = initANT(processId);
  const detailedRecords = await ant.getRecords();
  const owner = await ant.getOwner();
  const controllers = await ant.getControllers();
  return { detailedRecords, owner, controllers };
};

/**
 * Set a new record in the ANT process.
 * @param {string} processId - The processId.
 * @param {string} subDomain - The subdomain for the record.
 * @param {string} transactionId - The transaction ID the record should resolve to.
 * @param {number} ttlSeconds - The Time To Live (TTL) in seconds.
 * @returns {Promise<Object>} Result of the record update.
 */
export const setANTRecord = async (
  processId,
  name,
  transactionId,
  ttlSeconds
) => {
  console.log(`Pid: ${processId}`);
  console.log(`name: ${name}`);
  console.log(`txId: ${transactionId}`);
  const browserSigner = new ArconnectSigner(window.arweaveWallet);
  const ant = ANT.init({ processId, signer: browserSigner });
  const result = await ant.setRecord({
    undername: name,
    transactionId,
    ttlSeconds,
  });
  console.log(result);
  return result;
};Build Home Page Header We want the Header component to contain a button for users to connect their wallet to the site, and display their wallet address when Connected. To do this, we will use the functions we exported from the utils > auth.js file, and pass in a state and set state function from each page rendering the header:import React from "react";
import { connectWallet, truncateAddress } from "../utils/auth";

/**
 * Header component for displaying the connect wallet button and navigation.
 * @param {Object} props - Component props.
 * @param {string} props.address - The connected wallet address.
 * @param {function} props.setAddress - Function to set the connected wallet address.
 */
const Header = ({ address, setAddress }) => {
  const handleConnectWallet = async () => {
    try {
      const walletAddress = await connectWallet();
      setAddress(walletAddress);
    } catch (error) {
      console.error("Failed to connect wallet:", error);
    }
  };

  return (
    <div className="header">
      <button className="connect-wallet" onClick={handleConnectWallet}>
        {address ? `Connected: ${truncateAddress(address)}` : "Connect Wallet"}
      </button>
    </div>
  );
};

export default Header;Grid Component Our home page is going to fetch a list of all ArNS names and display them. To make this display cleaner and more organized, we are going to create a component to display the names as a grid.Create a new file in components named RecordsGrid.js This will take an individual ArNS record and display it as a button that logs the record name when clicked. We will update this later to make the button act as a link to the more detailed record page after we build that, which is why we are importing Link from arnext Home Page Go back to pages > index.js and lets build out our home page. We want to fetch the list of ArNS names when the page loads, and then feed the list into the grid component we just created. Because there are so many names, we also want to include a simple search bar to filter out displayed names. We will also need several states in order to manage all of this info:"use client";
import { useEffect, useState } from "react";
import Header from "@/components/Header";
import { fetchArNSRecords } from "@/utils/arweave";
import RecordsGrid from "@/components/RecordsGrid";

export default function Home() {
  const [arnsRecords, setArnsRecords] = useState(null); // State for storing all ArNS records
  const [isProcessing, setIsProcessing] = useState(true); // State for processing indicator
  const [searchTerm, setSearchTerm] = useState("") // used to filter displayed results by search input
  const [address, setAddress] = useState(null); // State for wallet address
  

  useEffect(() => {
    const fetchRecords = async () => {
      const allRecords = await fetchArNSRecords();
      setArnsRecords(allRecords);
      setIsProcessing(false);
    };

    fetchRecords();
  }, []);

  return (
    <div>
      <Header address={address} setAddress={setAddress} />
      {isProcessing ? (
        "processing"
      ) : (
        <div>
          <h2>Search</h2>
          <input 
          type="text"
          value={searchTerm}
          className ="search-bar"
          onChange = {(e) => {setSearchTerm(e.target.value)}}
          />
        <RecordsGrid
          keys={arnsRecords
            .map((r) => r.name)
            .filter((key) => key.toLowerCase().includes(searchTerm?.toLowerCase()))}
        /></div>
      )}
    </div>
  );
} Names Page NextJS, and ARNext by extension, supports dynamic routing, allowing us to create dedicated pages for any ArNS name without needing to use query strings, which makes the sharable urls much cleaner and more intuitive. We can do this by creating a page file with the naming convention [variable].js. Since we want to make a page for specific ArNS names we will create a new folder inside the pages folder named names, and then a new file pages > names > [name].js.This will be our largest file so far, including different logic for the displayed content depending on if the connected wallet is authorized to make changes the the name. We also need to make the page see what the name being looked at is, based on the url. We can do this using the custom useParams function from ARNext.The finished page will look like this:import Header from "@/components/Header";
import { useParams, Link } from "arnext"; // Import from ARNext, not NextJS
import { useEffect, useState } from "react";
import { ARIO } from "@ar.io/sdk/web";
import { fetchRecordDetails, setANTRecord } from "@/utils/arweave";

export async function getStaticPaths() {
  return { paths: [], fallback: "blocking" };
}

export async function getStaticProps({ params }) {
  const { name } = params;
  return { props: { name } }; // No initial record, just returning name
}

export default function NamePage() {
  const { name } = useParams();
  const [nameState, setNameState] = useState("");
  const [nameRecord, setNameRecord] = useState(null); // Initialize record to null
  const [arnsRecord, setArnsRecord] = useState(null);
  const [resultMessage, setResultMessage] = useState("");
  const [address, setAddress] = useState(null); // State for wallet address

  useEffect(() => {
    if (name && name !== nameState) {
      setNameState(name);

      // Fetch the record dynamically whenever routeName changes
      const fetchRecord = async () => {
        console.log("fetching records");
        try {
          const ario = ARIO.init();
          const newRecord = await ario.getArNSRecord({ name });
          console.log(newRecord);
          setNameRecord(newRecord);
        } catch (error) {
          console.error("Failed to fetch record:", error);
          setRecord(null);
        }
      };

      fetchRecord();
    }
    if (nameRecord && nameRecord.processId) {
      const fetchArnsRecord = async () => {
        try {
          const arnsRecord = await fetchRecordDetails(nameRecord.processId);
          console.log(arnsRecord);
          setArnsRecord(arnsRecord);
        } catch (error) {
          console.error(error);
        }
      };
      fetchArnsRecord();
    }
  }, [nameState, nameRecord]);

  const handleUpdateRecord = async (key, txId) => {
    const result = await setANTRecord(nameRecord.processId, key, txId, 900)
  console.log(`result Message: ${result}`)
  console.log(result)
    setResultMessage(result.id)
  };

  if (nameRecord === null) {
    return (
      <div>
        <Header address={address} setAddress={setAddress} />
        <p>Loading...</p>
      </div>
    );
  }

  const owner = arnsRecord?.owner || "N/A";
  const controllers = arnsRecord?.controllers || [];

  return (
    <div>
      <Header address={address} setAddress={setAddress} />
      <div className="record-details">
        <h3>Record Details for {nameState}</h3>
        <div>
          {arnsRecord?.detailedRecords &&
            Object.keys(arnsRecord.detailedRecords).map((recordKey, index) => (
              <div key={index} className="record-txid">
                <strong>{recordKey}:</strong>{" "}
                <a
                  href={`https://arweave.net/${arnsRecord.detailedRecords[recordKey].transactionId}`}
                  target="_blank"
                  rel="noopener noreferrer"
                >
                  {arnsRecord.detailedRecords[recordKey].transactionId}
                </a>
              </div>
            ))}
        </div>
        <p>Owner: {owner}</p>
        <p>
          Controllers: {controllers.length > 0 ? controllers.join(", ") : "N/A"}
        </p>
        {owner === address && ( 
          <>
            {arnsRecord?.detailedRecords &&
              Object.keys(arnsRecord.detailedRecords).map(
                (recordKey, index) => (
                  <div key={index} className="record-update">
                    <label>
                      {recordKey}:
                      <input
                        type="text"
                        placeholder="Enter new TxID"
                        id={`input-${index}`}
                      />
                      <button
                        onClick={() => {
                          const inputElement = document.getElementById(`input-${index}`);
                          const inputValue = inputElement ? inputElement.value : "";
                          handleUpdateRecord(
                            recordKey === "@" ? "@" : `${recordKey}`,
                            inputValue
                          );
                        }}
                      >
                        Update
                      </button>
                    </label>
                  </div>
                )
              )}
            <div className="new-record">
              <input
                type="text"
                placeholder="New Subdomain"
                id={`new-subdomain-input`}
              />
              <input
                type="text"
                placeholder="New TxID"
                id={`new-txid-input`}
              />
              <button
                onClick={() => {
                  const subdomainElement = document.getElementById("new-subdomain-input");
                  const txIdElement = document.getElementById("new-txid-input");
            
                  const newSubdomainValue = subdomainElement ? subdomainElement.value : "";
                  const newTxIdValue = txIdElement ? txIdElement.value : "";
            
                  console.log(newSubdomainValue)
                  console.log(newTxIdValue)
                  handleUpdateRecord(newSubdomainValue, newTxIdValue);
                }}
              >
                Set New Record
              </button>
            </div>
          </>
        )}
        <Link href="/">
          <button>Back to list</button>
        </Link>

        {resultMessage && <p>Successfully updated with message ID: {resultMessage}</p>}
      </div>
    </div>
  );
} When this page loads, it gets the name being queried by using useParams and our custom getStaticPaths and getStaticProps functions. It then uses the AR.IO sdk to get the process Id of the ANT that controls the name, and queries the ANT for its info and detailed records list.Once the page has that info, it renders the ArNS name, its owner address, any addresses authorized to make changes, and every record that name contains. If the user has connected a wallet authorized to make changes, the page also renders input fields for each record for making those updates. It also provides the option to create an entirely new undername record.Finish the Grid Component Now that we have a path for our main page displays to link to, we can update the components > RecordsGrid.js file to include that link when clicked.View Project The ArNS viewer should be fully functional now. You can view it locally in your browser using the same steps as the initial Sanity Check Run yarn dev in your terminal Navigate to localhost:3000 in a browser CSS You will likely notice that everything functions correctly, but it doesnt look very nice. This is because we havent updated our css at all.The primary css file for this project is css > App.css. You can make whatever css rules here that you like to make the page look the way you want. Deploy With Turbo Once your app is looking the way you want it, you can deploy it to the permaweb using Turbo. For this, you will need an Arweave wallet with some Turbo Credits. Make sure you don't place your keyfile for the wallet inside the project directory, or you risk it getting uploaded to Arweave by mistake.In your terminal, run the command:Make sure to replace <path-to-your-wallet> with the actual path to your Arweave wallet. This will create a static build of your entire project, upload it to Arweave, and print out in the terminal all of the details of the upload.Find the section in the print out manifestResponse which will have a key named id. That will be the Arweave transaction id for your project.You can view a permanently deployed version of your project at https://arweave.net/<transaction-id> References Completed Project example: github Deployed Project: transaction id

---

# 3. Crossmint NFT Minting App - ARIO Docs

Document Number: 3
Source: https://docs.ar.io/guides/example-apps/crossmint-app
Words: 5959
Extraction Method: html

Crossmint App Overview In today's web3 ecosystem, creating truly decentralized applications requires more than just smart contracts. You need decentralized storage, seamless user experiences, and simplified payment options. Unfortunately, more than 95% of all web3 dApps are deployed on centralized platforms like Vercel.This guide will walk you through building a completely decentralized NFT minting app that leverages the power of Arweave for permanent storage and Crossmint for simplified NFT creation. You'll learn how to store NFT content permanently, create and mint NFTs, build a frontend with authentication and payment options, and deploy your application to Arweave.By the end of this tutorial, you'll have created a fully functional dApp that allows users to:Log into your app easily through Crossmint's Auth Purchase semi-fungible tokens (SFTs) following the ERC-1155 standard Pay for SFTs using Crossmint with crypto or credit card Access your application through a human-readable ArNS domain You'll be well versed in building fully decentralized apps with the Crossmint SDK, ArDrive SDK, and the AR.IO SDK.Example Project Live Demo: https://crossmint_zerotoarweave.arweave.net GitHub Repository: https://github.com/ar-io/crossmint-arweave-example Here's a quick overview of what you'll learn:Storage Setup: You'll begin by storing an AI-generated image on Arweave using ArDrive.io, ensuring your NFT content is permanently preserved.Collection and Template Creation: You'll create an ERC-1155 collection and template on Crossmint where your semi-fungible tokens will be minted.SFT Minting: You'll use Crossmint's API to mint semi-fungible tokens from your template, following their straightforward API guide.Frontend Development: You'll clone the Zero-to-Arweave starter kit, which provides a solid foundation using Vite, React, and JavaScript along with the AR.IO SDK and ArDrive SDKs.Authentication Integration: You'll implement Crossmint's client-side authentication system, making it easy for users to interact with your application.Payment Integration: You'll add Crossmint's embedded checkout to enable users to purchase SFTs with both crypto and credit cards.Frontend Design Improvements: You'll update the frontend design to better showcase your NFTs and create a more intuitive user flow.Decentralized Deployment: You'll deploy your completed frontend to Arweave, ensuring your entire application stack is decentralized and permanently accessible.Domain Configuration: Finally, you'll configure your ArNS domain to point to your newly deployed application, providing users with a friendly URL to access your dApp.Let's get into the code!Step 1: Storage Setup First, you'll need to store your NFT image on Arweave using ardrive.io. This step ensures that your NFT's visual content is permanently preserved on the decentralized Arweave network.Generate an AI Image Start by creating an AI-generated image that will become your NFT:Visit ChatGPT (https://chat.openai.com/) or another AI image generation tool Use a prompt to generate an interesting image for your NFT Download the generated image to your local machine Make sure to save it in a common format like PNG or JPG Store the Image on ArDrive.io Now that you have your image, store it permanently on Arweave:Visit ArDrive.io and log in to your account (or create one if you're new) Fund your ArDrive wallet if needed (this requires AR tokens) Create a new folder for your NFT project Drag and drop your AI-generated image into this folder Wait for the upload to complete and for the transaction to be processed Retrieve the Arweave Transaction ID Once your image is successfully uploaded:Click on the uploaded image in your ArDrive folder Look for the "Transaction ID" or "TX ID" in the file details Copy this Transaction ID - it looks something like Abc123XYZ... (a long alphanumeric string) Save this Transaction ID somewhere safe - you'll need it later when creating your NFT metadata This Transaction ID is crucial as it's the permanent reference to your image on the Arweave network. When you create your SFT template in the next step, you'll include this ID to link the tokens to this permanently stored image.Step 2: Collection and Template Creation Next, you'll create an ERC-1155 collection and template using Crossmint's API. These will hold your semi-fungible tokens (SFTs).Create a Crossmint Developer Account Visit the Crossmint Staging Console to create a developer account Sign in and accept the dialog to continue Note that Crossmint provides two environments:Staging: For development and testing (what we'll use first) Production: For your final, live application Get a Server-Side API Key After logging in, navigate to the "Integrate" tab Click on "API Keys" at the top of the page In the "Server-side keys" section, click "Create new key" Select the following scopes under "Minting API":collections.create - Required for creating a new collection nfts.create - Required for minting NFTs nfts.read - Needed to read NFT information Create and save this API key securely - you'll need it for our API calls Create an ERC-1155 Collection Let's create a collection for your semi-fungible tokens using Crossmint's API:Create a new file called createCollection.js in your project directory Add the following code:const apiKey = "YOUR_API_KEY";
const env = "staging"; // Using staging environment for development

const url = `https://${env}.crossmint.com/api/2022-06-09/collections`;
const options = {
method: "POST",
headers: {
    "accept": "application/json",
    "content-type": "application/json",
    "x-api-key": apiKey,
},
body: JSON.stringify({
    chain: "ethereum-sepolia", // Using Ethereum testnet for development
    fungibility: "semi-fungible", // For ERC-1155 tokens
    metadata: {
        name: "lil dumdumz SFT Collection",
        imageUrl: "https://arweave.net/YOUR_ARWEAVE_TX_ID", // Optional collection image
        description: "A collection of semi-fungible tokens with images stored on Arweave"
    }
}),
};

fetch(url, options)
.then((res) => res.json())
.then((json) => {
    console.log(json);
    console.log("Collection created! Collection ID:", json.id);
    console.log("Save this Collection ID for the next steps");
})
.catch((err) => console.error("Error:", err));Replace the placeholders:YOUR_API_KEY with the API key you created in the previous step YOUR_ARWEAVE_TX_ID with your collection image's Arweave Transaction ID (optional) Run the script with: node createCollection.js After a few seconds, you'll receive a response with the Collection ID - save this ID for the next steps Create an SFT Template Now, let's create a template within your collection from which you can mint multiple identical SFTs:Create a new file called createTemplate.js in your project directory Add the following code:const apiKey = "YOUR_API_KEY";
const env = "staging"; // Using staging environment for development
const collectionId = "YOUR_COLLECTION_ID"; // From the previous step
const arweaveImageTxId = "YOUR_NFT_IMAGE_TX_ID"; // From Step 1

const url = `https://${env}.crossmint.com/api/2022-06-09/collections/${collectionId}/templates`;
const options = {
method: "POST",
headers: {
    "accept": "application/json",
    "content-type": "application/json",
    "x-api-key": apiKey,
},
body: JSON.stringify({
    onChain: {
        tokenId: "1" // You can assign any unique ID to this template
    },
    supply: {
        limit: 100 // Maximum number of tokens that can be minted from this template
    },
    metadata: {
        name: "lil dumdumz SFT",
        image: `https://arweave.net/${arweaveImageTxId}`, // Arweave gateway URL
        description: "Semi-fungible token with image permanently stored on Arweave",
        attributes: [
            {
                trait_type: "Storage",
                value: "Arweave"
            },
            {
                trait_type: "Permanence",
                value: "Forever"
            }
        ]
    }
}),
};

fetch(url, options)
.then((res) => res.json())
.then((json) => {
    console.log(json);
    console.log("Template created! Template ID:", json.id);
    console.log("Save this Template ID for minting SFTs");
})
.catch((err) => console.error("Error:", err));
</CodeGroup>

3. Replace the placeholders:

* `YOUR_API_KEY` with your API key
* `YOUR_COLLECTION_ID` with the Collection ID from the previous step
* `YOUR_NFT_IMAGE_TX_ID` with the Arweave Transaction ID of your NFT image from Step 1

4. Run the script with: `node createTemplate.js`

5. After a few seconds, you'll receive a response with the Template ID - save this ID for minting SFTs

## Step 3: SFT Minting

Now that you have your collection and template set up, let's mint an SFT from your template.

### Create a Minting Script

1. Create a new file called `mintSFT.js` in your project directory

2. Add the following code:

<CodeGroup>

```javascript {{ title: 'NodeJS' }}
const apiKey = "YOUR_API_KEY";
const env = "staging"; // Using staging environment for development
const collectionId = "YOUR_COLLECTION_ID"; // From Step 2
const templateId = "YOUR_TEMPLATE_ID"; // From Step 2
const recipientEmail = "YOUR_EMAIL_ADDRESS"; // Replace with your email

const url = `https://${env}.crossmint.com/api/2022-06-09/collections/${collectionId}/sfts`;
const options = {
method: "POST",
headers: {
    "accept": "application/json",
    "content-type": "application/json",
    "x-api-key": apiKey,
},
body: JSON.stringify({
    templateId: templateId,
    recipient: `email:${recipientEmail}:ethereum-sepolia`, // Using email as recipient
    amount: 1 // Number of tokens to mint
}),
};

fetch(url, options)
.then((res) => res.json())
.then((json) => {
    console.log(json);
    console.log("Minting initiated! Action ID:", json.actionId);
    console.log("Save this Action ID to check the minting status");
})
.catch((err) => console.error("Error:", err));Replace the placeholders:YOUR_API_KEY with your API key YOUR_COLLECTION_ID with the Collection ID from Step 2 YOUR_TEMPLATE_ID with the Template ID from Step 2 YOUR_EMAIL_ADDRESS with your email for testing Run the script with: node mintSFT.js After a few seconds, you'll receive a response with an Action ID - save this ID for checking the minting status Check Minting Status Since blockchain transactions take time to confirm, you need to check the status of your mint:Create a new file called checkMintStatus.js with the following code:Replace the placeholders with your API key and the Action ID from the previous step Run the script with: node checkMintStatus.js Keep checking until the status field returns "success" View Your Newly Minted SFT Once the minting is successful, you can view your SFT by:Logging into your wallet from Crossmint's staging website Looking for your newly minted SFT with the Arweave-stored image Verify that all the metadata appears correctly, including the image from Arweave Congratulations! You've now successfully created a collection, defined a template, and minted an SFT with its image permanently stored on Arweave. This combination provides true decentralization for your token's content.In the next step, you'll set up your frontend application by cloning the Zero-to-Arweave starter kit.Step 4: Frontend Development Now that you have your image stored on Arweave and your NFT contract deployed with Crossmint, let's set up the frontend application. You'll use the Zero-to-Arweave Starter Kit, which provides a solid foundation for building decentralized applications on Arweave.Clone the Starter Kit Open your terminal and clone the starter kit repository:Install Dependencies Install the required dependencies using your preferred package manager:Configure Your Arweave Wallet Place your Arweave wallet file in the project root as wallet.json. This will be required for deploying the application later, but you'll need it now to test the app functionality.Important Never commit your wallet file to version control. The starter kit includes wallet.json in its .gitignore file by default.Explore the Project Structure Take a moment to understand the project structure:Customize the Application Let's customize the application for your NFT minting project. First, update the title and description in index.html:Update the homepage in src/pages/Home.jsx to include information about your NFT minting application:import React from 'react';
import { ConnectButton } from '@arweave-wallet-kit/react';

const Home = () => {
  return (
    <div className="container mx-auto px-4 py-8">
      <h1 className="text-4xl font-bold mb-6">Decentralized NFT Minting</h1>
      
      <div className="bg-gray-100 p-6 rounded-lg mb-8">
        <h2 className="text-2xl font-semibold mb-4">About This Project</h2>
        <p className="mb-4">
          This is a fully decentralized NFT minting application that leverages:
        </p>
        <ul className="list-disc pl-6 mb-4">
          <li>Arweave for permanent image storage</li>
          <li>Crossmint for simplified NFT minting</li>
          <li>AR.IO for decentralized domain names</li>
        </ul>
        <p>
          Connect your wallet to get started with minting your own NFTs.
        </p>
      </div>
      
      <div className="text-center mb-8">
        <ConnectButton className="bg-purple-600 hover:bg-purple-700 text-white font-bold py-2 px-4 rounded" />
      </div>
    </div>
  );
};

export default Home;Start the Development Server Run the development server to make sure everything is working properly:Open your browser to the URL shown in the terminal (typically http://localhost:5173) to see your application.At this point, you should see your customized frontend with the Arweave wallet integration working. The application is still missing your Crossmint integration, which you'll add in the next steps.The Zero-to-Arweave Starter Kit provides all the necessary SDKs and configurations to work with Arweave, including:Arweave Wallet Kit for wallet connections AR.IO SDK for domain name management Turbo SDK for efficient data uploads In the next step, you'll add Crossmint's client-side authentication to allow users to interact with your NFT minting functionality.Step 5: Authentication Integration Now you need to integrate Crossmint's client-side authentication into your app. This will allow users to log in and interact with your NFT minting functionality seamlessly. Let's implement Crossmint Auth in your Zero-to-Arweave starter kit.Create and Configure a Crossmint Project If you haven't already, go to the Crossmint Staging Console and log in or create an account Get a Client-Side API Key Navigate to the "Integrate" section in the left navigation bar Click on the "API Keys" tab In the "Client-side keys" section, click "Create new key" Add your development URL as an authorized origin:For local development: http://localhost:5173 (Vite's default port) Select the following scopes:users.create - Required for authentication functionality users.read - Needed to read user information Enable "JWT Auth" by checking the box Create the key and save it for the next steps Set Up Environment Variables Create a .env file in the root of your project (or modify the existing one) and add your Crossmint API key:VITE_CROSSMINT_API_KEY="YOUR_CLIENT_SIDE_API_KEY" Install the Crossmint SDK Install the Crossmint React SDK:Create a Providers Component Create a new file at src/components/CrossmintProviders.jsx:Update Main Application Entry Point Modify your src/main.jsx file to include the Crossmint providers:Create an Authentication Component Create a new component at src/components/AuthButton.jsx:import { useAuth } from "@crossmint/client-sdk-react-ui";

function AuthButton() {
  const { login, logout, user } = useAuth();

  return (
    <div className="flex flex-col items-center gap-4">
      {user == null ? (
        <button
          type="button"
          onClick={login}
          className="bg-blue-600 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded"
        >
          Sign in with Crossmint
        </button>
      ) : (
        <div className="flex flex-col items-center gap-4">
          <div className="bg-white p-4 rounded-lg shadow-md">
            <h3 className="font-bold text-lg mb-2">User Info</h3>
            <p><span className="font-medium">User ID:</span> {user?.userId}</p>
            <p><span className="font-medium">Email:</span> {user?.email || "Not available"}</p>
            {user?.google && (
              <p><span className="font-medium">Google:</span> {user.google.displayName}</p>
            )}
            {user?.farcaster && (
              <p><span className="font-medium">Farcaster:</span> {user.farcaster.username}</p>
            )}
          </div>
          
          <button
            type="button"
            onClick={logout}
            className="bg-gray-800 hover:bg-gray-900 text-white font-bold py-2 px-4 rounded"
          >
            Sign Out
          </button>
        </div>
      )}
    </div>
  );
}

export default AuthButton;Simplify the NavBar Component We'll create a simplified NavBar component that only includes the AuthButton:Create a Protected Route Component Create a component to protect routes that require authentication at src/components/ProtectedRoute.jsx:Test the Authentication Flow Start your development server:Visit your application in the browser and test these features:Sign in button should open the Crossmint authentication modal After signing in, you should see your user information displayed Sign out button should work correctly Customization Notes In our implementation, we made the following changes to improve the user experience:Simplified the NavBar - We removed the Arweave wallet connection and ARIO SDK functionality to focus solely on Crossmint authentication, creating a cleaner interface.Streamlined User Interface - The app now shows a single "Sign in with Crossmint" button in the navbar, which provides a more intuitive authentication flow.Removed Redundant Components - We eliminated unused state variables and effects from the NavBar component, making the code more maintainable.Enhanced Brand Identity - Changed all references to "lil dumdumz NFT" to strengthen the brand presence in the application.With Crossmint's authentication now integrated, your users can easily sign up and log in to your decentralized NFT minting application. In the next step, you'll add the Crossmint payment button to enable users to mint NFTs directly from your application.Step 6: Payment Integration (Crypto Payments) For your fully decentralized NFT minting application, you've implemented a client-side approach for payment integration using Crossmint's headless API for crypto payments. This allows users to purchase NFTs directly from your application using cryptocurrency, maintaining the decentralized nature of your platform.Environment Variables Setup You've configured your application with the necessary environment variables:Both the API key and collection ID are from the staging environment, ensuring compatibility between the two.Purchase Page Implementation You've created a Purchase.jsx component that implements the complete crypto payment flow:Wallet Connection: Allows users to connect their MetaMask wallet to the application Order Creation: Creates an order with Crossmint's API using the connected wallet address Recipient Assignment: Assigns the authenticated user as the recipient of the NFT Payment Processing: Prompts the user to sign a transaction in their wallet Status Tracking: Polls the order status until completion Here's an overview of how your Purchase component is structured:// Purchase.jsx
import React, { useState, useEffect } from 'react';
import { useAuth } from "@crossmint/client-sdk-react-ui";

const Purchase = () => {
  const { user } = useAuth();
  const apiKey = import.meta.env.VITE_CROSSMINT_API_KEY;
  const collectionId = import.meta.env.VITE_CROSSMINT_COLLECTION_ID;
  
  // State variables for the payment flow
  const [payerAddress, setPayerAddress] = useState('');
  const [orderId, setOrderId] = useState(null);
  const [orderStatus, setOrderStatus] = useState('');
  const [serializedTransaction, setSerializedTransaction] = useState(null);
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState(null);

  // Functions for wallet connection, order creation, order updating,
  // transaction sending, and status polling...
  
  return (
    <div className="container mx-auto px-4 py-8">
      {/* Component UI with conditional rendering based on state */}
    </div>
  );
};

export default Purchase;Crypto Payment Flow The crypto payment flow in your application follows these steps:Connect Wallet: User connects their MetaMask wallet to the application Create Order: Application creates an order with Crossmint's API Update Order with Recipient: Application assigns the NFT to the authenticated user Send Transaction: User signs and sends the transaction from their wallet Poll for Status: Application checks the order status until completion User Interface Elements Your Purchase page includes several key UI elements:NFT Display: Shows the NFT image and details Connect Wallet Button: Allows users to connect their cryptocurrency wallet Create Order Button: Initiates the purchase process Pay with Crypto Button: Appears when ready for payment Status Messages: Keeps users informed about the current state of their purchase Success/Failure Messages: Provides feedback on the outcome of the purchase Authentication Integration Your payment flow is integrated with Crossmint's authentication system:Only authenticated users can initiate the purchase process The user's email from their Crossmint account is used as the recipient for the NFT Protected routes ensure only authenticated users can access the Purchase page App Routes Update You've updated the application routes to include the Purchase page:NavBar Update You've also updated the NavBar component to include a link to the Purchase page:Testing the Crypto Payment Flow To test the complete purchase flow:Start the development server with pnpm run dev Sign in with Crossmint using the AuthButton Navigate to the Purchase page Connect your MetaMask wallet to the application Create an order by clicking the Create Order button When prompted, sign the transaction in your wallet Monitor the order status until completion Verify that the NFT appears in your Crossmint wallet Benefits of the Crypto Payment Approach This crypto payment implementation offers several advantages:Fully Decentralized: The entire payment process happens client-side, aligning with your goal of a decentralized application Native Blockchain Experience: Users interact directly with their wallets for a true web3 experience No Server-Side Requirements: All API calls are made from the client, making the app compatible with Arweave deployment Transparent Process: Users can see and verify each step of the transaction process Cross-Chain Compatibility: The same approach can be used to accept payments on various blockchains By implementing this payment flow, you've created a seamless way for users to purchase NFTs using cryptocurrency while maintaining the decentralized nature of your application. This sets the stage for the next step: deploying your complete application to Arweave.Step 7: Frontend Design Improvements After implementing the core functionality of your application, it's important to enhance the user interface to create a more engaging and professional experience. A well-designed UI not only improves usability but also increases user trust in your NFT platform. Let's update your frontend design to better showcase your NFTs and create a more intuitive user flow.Redesigning the NavBar Component First, let's enhance your NavBar with a modern design and responsive functionality:import { useState } from 'react';
import { Link, useLocation } from 'react-router-dom';
import { useAuth } from "@crossmint/client-sdk-react-ui";
import AuthButton from './AuthButton';

function NavBar() {
  const { user } = useAuth();
  const [isMenuOpen, setIsMenuOpen] = useState(false);
  const location = useLocation();

  const isActive = (path) => {
    return location.pathname === path;
  };

  return (
    <nav className="bg-white shadow-lg sticky top-0 z-50">
      <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div className="flex justify-between h-16 items-center">
          {/* Logo/Brand with gradient text effect */}
          <div className="flex-shrink-0">
            <Link to="/" className="flex items-center">
              <span className="text-2xl font-bold bg-clip-text text-transparent bg-gradient-to-r from-indigo-600 to-purple-600">
                lil dumdumz NFT
              </span>
            </Link>
          </div>

          {/* Desktop Navigation with active state indicators */}
          <div className="hidden md:flex items-center space-x-6">
            <Link 
              to="/" 
              className={`px-3 py-2 rounded-md text-sm font-medium transition-colors ${
                isActive('/') 
                  ? 'text-indigo-600 bg-indigo-50' 
                  : 'text-gray-700 hover:text-indigo-600 hover:bg-gray-50'
              }`}
            >
              Home
            </Link>
            
            {user && (
              <>
                <Link 
                  to="/purchase" 
                  className={`px-3 py-2 rounded-md text-sm font-medium transition-colors ${
                    isActive('/purchase') 
                      ? 'text-indigo-600 bg-indigo-50' 
                      : 'text-gray-700 hover:text-indigo-600 hover:bg-gray-50'
                  }`}
                >
                  Purchase NFT
                </Link>
                <Link 
                  to="/dashboard" 
                  className={`px-3 py-2 rounded-md text-sm font-medium transition-colors ${
                    isActive('/dashboard') 
                      ? 'text-indigo-600 bg-indigo-50' 
                      : 'text-gray-700 hover:text-indigo-600 hover:bg-gray-50'
                  }`}
                >
                  Dashboard
                </Link>
              </>
            )}
          </div>

          {/* Authentication Button - Desktop */}
          <div className="hidden md:block">
            <AuthButton />
          </div>

          {/* Mobile menu button with animation */}
          <div className="md:hidden flex items-center">
            <button 
              onClick={() => setIsMenuOpen(!isMenuOpen)}
              className="inline-flex items-center justify-center p-2 rounded-md text-gray-700 hover:text-indigo-600 hover:bg-gray-50 focus:outline-none"
              aria-expanded="false"
            >
              <span className="sr-only">Open main menu</span>
              {/* Icon when menu is closed */}
              <svg
                className={`${isMenuOpen ? 'hidden' : 'block'} h-6 w-6`}
                xmlns="http://www.w3.org/2000/svg"
                fill="none"
                viewBox="0 0 24 24"
                stroke="currentColor"
                aria-hidden="true"
              >
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth="2" d="M4 6h16M4 12h16M4 18h16" />
              </svg>
              {/* Icon when menu is open */}
              <svg
                className={`${isMenuOpen ? 'block' : 'hidden'} h-6 w-6`}
                xmlns="http://www.w3.org/2000/svg"
                fill="none"
                viewBox="0 0 24 24"
                stroke="currentColor"
                aria-hidden="true"
              >
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth="2" d="M6 18L18 6M6 6l12 12" />
              </svg>
            </button>
          </div>
        </div>
      </div>

      {/* Mobile menu, show/hide based on menu state */}
      <div className={`${isMenuOpen ? 'block' : 'hidden'} md:hidden`}>
        <div className="px-2 pt-2 pb-3 space-y-1 sm:px-3 border-t border-gray-200">
          <Link
            to="/"
            className={`block px-3 py-2 rounded-md text-base font-medium ${
              isActive('/') 
                ? 'text-indigo-600 bg-indigo-50' 
                : 'text-gray-700 hover:text-indigo-600 hover:bg-gray-50'
            }`}
            onClick={() => setIsMenuOpen(false)}
          >
            Home
          </Link>
          
          {user && (
            <>
              <Link
                to="/purchase"
                className={`block px-3 py-2 rounded-md text-base font-medium ${
                  isActive('/purchase') 
                    ? 'text-indigo-600 bg-indigo-50' 
                    : 'text-gray-700 hover:text-indigo-600 hover:bg-gray-50'
                }`}
                onClick={() => setIsMenuOpen(false)}
              >
                Purchase NFT
              </Link>
              <Link
                to="/dashboard"
                className={`block px-3 py-2 rounded-md text-base font-medium ${
                  isActive('/dashboard') 
                    ? 'text-indigo-600 bg-indigo-50' 
                    : 'text-gray-700 hover:text-indigo-600 hover:bg-gray-50'
                }`}
                onClick={() => setIsMenuOpen(false)}
              >
                Dashboard
              </Link>
            </>
          )}
        </div>
        
        {/* Mobile Authentication Button */}
        <div className="pt-4 pb-3 border-t border-gray-200">
          <div className="px-2">
            <AuthButton />
          </div>
        </div>
      </div>
    </nav>
  );
}

export default NavBar;This updated NavBar includes:Modern Design Elements:Gradient text for the brand name Subtle hover and active state animations Clean, consistent spacing Sticky positioning so it remains visible while scrolling Responsive Features:Collapsible mobile menu with toggle button Different layouts for mobile and desktop Automatic menu closing when a link is clicked User Experience Improvements:Visual feedback for the current active page Conditional navigation links based on authentication status Accessible design with proper ARIA attributes Enhanced Brand Identity:Consistent color scheme, typography, and styling elements Strengthened "lil dumdumz NFT" brand presence Improved Conversion Rates:Strategic placement of call-to-action buttons Clear value propositions for the platform's benefits Enhancing the App Layout Next, you'll update your App.jsx to use a more structured layout that provides consistency across all pages:The new App.jsx layout includes:A full-height flexible layout with a subtle gradient background Consistent page structure with appropriate margins and padding A placeholder dashboard with skeleton loading states for future NFT displays Proper routing with protected routes for authenticated content Creating an Engaging Home Page A good landing page is crucial for NFT platforms. You've redesigned your Home component to create a more engaging and informative experience:import React from 'react';
import { Link } from 'react-router-dom';
import { useAuth } from "@crossmint/client-sdk-react-ui";

const Home = () => {
  const { user, login } = useAuth();
  const arweaveImageUrl = "https://btruuwgkero6dqsk6y2w72kgbtfbncafhdch3bepa33cdpxxdhfa.arweave.net/DONKWMokXeHCSvY1b-lGDMoWiAU4xH2Ejwb2Ib73Gco";

  return (
    <>
      {/* Hero Section */}
      <section className="bg-gradient-to-r from-indigo-600 to-purple-600 text-white">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-20 md:py-28">
          <div className="grid md:grid-cols-2 gap-12 items-center">
            <div className="text-center md:text-left">
              <h1 className="text-4xl md:text-5xl font-bold mb-6 leading-tight">
                Collect Unique Digital Art from lil dumdumz
              </h1>
              <p className="text-xl mb-8 text-indigo-100">
                Fully decentralized NFTs with permanent storage on Arweave. Own a piece of digital history that lasts forever.
              </p>
              <div className="flex flex-col sm:flex-row justify-center md:justify-start gap-4">
                {user ? (
                  <Link 
                    to="/purchase" 
                    className="px-8 py-3 bg-white text-indigo-600 rounded-full font-bold text-lg shadow-lg hover:bg-indigo-50 transition duration-300"
                  >
                    Purchase NFT
                  </Link>
                ) : (
                  <button
                    onClick={login}
                    className="px-8 py-3 bg-white text-indigo-600 rounded-full font-bold text-lg shadow-lg hover:bg-indigo-50 transition duration-300"
                  >
                    Connect Wallet
                  </button>
                )}
                <a 
                  href="#learn-more" 
                  className="px-8 py-3 bg-transparent border-2 border-white rounded-full font-bold text-lg hover:bg-white hover:bg-opacity-10 transition duration-300"
                >
                  Learn More
                </a>
              </div>
            </div>
            <div className="relative">
              <div className="relative z-10 overflow-hidden rounded-2xl shadow-2xl transform rotate-2 hover:rotate-0 transition-transform duration-500">
                <img 
                  src={arweaveImageUrl} 
                  alt="Featured NFT" 
                  className="w-full h-auto"
                />
                <div className="absolute bottom-0 left-0 right-0 bg-gradient-to-t from-black/70 to-transparent p-6">
                  <p className="text-white text-xl font-semibold">lil dumdumz #1</p>
                  <div className="flex justify-between items-center mt-2">
                    <span className="text-indigo-200">Price: 0.01 ETH</span>
                    <span className="bg-indigo-500 text-white px-3 py-1 rounded-full text-sm">Limited Edition</span>
                  </div>
                </div>
              </div>
              <div className="absolute -bottom-5 -right-5 h-48 w-48 bg-yellow-400 rounded-full opacity-70 blur-3xl -z-10"></div>
              <div className="absolute -top-5 -left-5 h-36 w-36 bg-purple-500 rounded-full opacity-70 blur-3xl -z-10"></div>
            </div>
          </div>
        </div>
      </section>

      {/* Features Section */}
      <section id="learn-more" className="py-20 bg-white">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
          <h2 className="text-3xl font-bold text-center mb-16 text-gray-800">
            A Truly <span className="text-indigo-600">Decentralized</span> NFT Platform
          </h2>
          
          <div className="grid md:grid-cols-3 gap-12">
            <div className="bg-gray-50 rounded-xl p-8 shadow-md transform hover:scale-105 transition duration-300">
              <div className="h-14 w-14 bg-indigo-100 text-indigo-600 rounded-full flex items-center justify-center mb-6">
                <svg xmlns="http://www.w3.org/2000/svg" className="h-8 w-8" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                  <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M4 7v10c0 2.21 3.582 4 8 4s8-1.79 8-4V7M4 7c0 2.21 3.582 4 8 4s8-1.79 8-4M4 7c0-2.21 3.582-4 8-4s8 1.79 8 4m0 5c0 2.21-3.582 4-8 4s-8-1.79-8-4" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold mb-3 text-gray-800">Permanent Storage</h3>
              <p className="text-gray-600">
                Your NFT images are stored on Arweave, ensuring they'll be accessible forever. No more broken NFTs due to centralized servers going offline.
              </p>
            </div>
            
            <div className="bg-gray-50 rounded-xl p-8 shadow-md transform hover:scale-105 transition duration-300">
              <div className="h-14 w-14 bg-indigo-100 text-indigo-600 rounded-full flex items-center justify-center mb-6">
                <svg xmlns="http://www.w3.org/2000/svg" className="h-8 w-8" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                  <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M9 12l2 2 4-4m5.618-4.016A11.955 11.955 0 0112 2.944a11.955 11.955 0 01-8.618 3.04A12.02 12.02 0 003 9c0 5.591 3.824 10.29 9 11.622 5.176-1.332 9-6.03 9-11.622 0-1.042-.133-2.052-.382-3.016z" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold mb-3 text-gray-800">Simplified Purchasing</h3>
              <p className="text-gray-600">
                Crossmint integration allows for easy NFT purchasing with cryptocurrency, making blockchain technology accessible to everyone.
              </p>
            </div>
            
            <div className="bg-gray-50 rounded-xl p-8 shadow-md transform hover:scale-105 transition duration-300">
              <div className="h-14 w-14 bg-indigo-100 text-indigo-600 rounded-full flex items-center justify-center mb-6">
                <svg xmlns="http://www.w3.org/2000/svg" className="h-8 w-8" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                  <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M21 12a9 9 0 01-9 9m9-9a9 9 0 00-9-9m9 9H3m9 9a9 9 0 01-9-9m9 9c1.657 0 3-4.03 3-9s-1.343-9-3-9m0 18c-1.657 0-3-4.03-3-9s1.343-9 3-9m-9 9a9 9 0 019-9" />
                </svg>
              </div>
              <h3 className="text-xl font-semibold mb-3 text-gray-800">Decentralized Domain</h3>
              <p className="text-gray-600">
                Your app is hosted on the AR.IO network, ensuring it will remain accessible indefinitely with a human-readable domain name.
              </p>
            </div>
          </div>
        </div>
      </section>

      {/* NFT Preview Section */}
      <section className="py-20 bg-gray-50">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
          <div className="text-center mb-16">
            <h2 className="text-3xl font-bold text-gray-800 mb-4">Featured NFT</h2>
            <p className="text-lg text-gray-600 max-w-3xl mx-auto">
              A limited edition artwork with permanent storage on the Arweave network, and verification on the Base Sepolia blockchain.
            </p>
          </div>
          
          <div className="bg-white rounded-2xl shadow-xl overflow-hidden max-w-4xl mx-auto">
            <div className="md:flex">
              <div className="md:w-1/2">
                <img 
                  src={arweaveImageUrl} 
                  alt="Featured NFT" 
                  className="w-full h-full object-cover"
                />
              </div>
              <div className="md:w-1/2 p-8">
                <h3 className="text-2xl font-bold text-gray-800 mb-4">lil dumdumz NFT #1</h3>
                <div className="flex items-center mb-6">
                  <span className="bg-green-100 text-green-800 px-3 py-1 rounded-full text-sm mr-3">Available</span>
                  <span className="text-gray-500 text-sm">Limited Edition of 100</span>
                </div>
                <p className="text-gray-600 mb-6">
                  This unique NFT features stunning artwork permanently stored on the Arweave network, 
                  ensuring its availability for generations to come. Own a piece of digital history that can't be altered or deleted.
                </p>
                <div className="border-t border-gray-200 pt-6 mb-6">
                  <div className="flex justify-between mb-2">
                    <span className="text-gray-500">Storage</span>
                    <span className="font-medium">Arweave</span>
                  </div>
                  <div className="flex justify-between mb-2">
                    <span className="text-gray-500">Blockchain</span>
                    <span className="font-medium">Base Sepolia</span>
                  </div>
                  <div className="flex justify-between mb-2">
                    <span className="text-gray-500">Price</span>
                    <span className="font-medium">0.01 ETH</span>
                  </div>
                </div>
                {user ? (
                  <Link 
                    to="/purchase" 
                    className="block w-full py-3 px-4 bg-indigo-600 text-white text-center font-medium rounded-lg hover:bg-indigo-700 transition duration-300"
                  >
                    Purchase This NFT
                  </Link>
                ) : (
                  <button
                    onClick={login}
                    className="block w-full py-3 px-4 bg-indigo-600 text-white text-center font-medium rounded-lg hover:bg-indigo-700 transition duration-300"
                  >
                    Connect Wallet to Purchase
                  </button>
                )}
              </div>
            </div>
          </div>
        </div>
      </section>

      {/* CTA Section */}
      <section className="bg-indigo-600 text-white py-16 pb-24">
        <div className="max-w-5xl mx-auto px-4 sm:px-6 lg:px-8 text-center">
          <h2 className="text-3xl font-bold mb-6">Ready to own your piece of digital history?</h2>
          <p className="text-xl text-indigo-100 mb-8 max-w-3xl mx-auto">
            Join the movement for truly permanent digital ownership with our decentralized NFT platform.
          </p>
          {user ? (
            <Link 
              to="/purchase" 
              className="inline-block px-8 py-4 bg-white text-indigo-600 rounded-full font-bold text-lg shadow-lg hover:bg-indigo-50 transition duration-300"
            >
              Purchase Your NFT Now
            </Link>
          ) : (
            <button
              onClick={login}
              className="inline-block px-8 py-4 bg-white text-indigo-600 rounded-full font-bold text-lg shadow-lg hover:bg-indigo-50 transition duration-300"
            >
              Connect Wallet to Get Started
            </button>
          )}
        </div>
      </section>
    </>
  );
};

export default Home;This enhanced Home page includes several key elements:Engaging Hero Section:A bold color gradient background with a two-column layout Prominent headline and subheading that clearly communicates the value proposition Featured NFT display with hover effects and overlay information Call-to-action buttons that adapt based on user authentication status Features Explanation:Three-column layout highlighting the key benefits of the platform Custom icons with consistent styling Interactive hover effects to increase engagement Clear, concise descriptions of the advantages of using your platform NFT Showcase Section:Detailed card displaying the NFT with its specifications Responsive design that adapts to different screen sizes Clear visual hierarchy with appropriate spacing Contextual call-to-action based on authentication state Compelling Call-to-Action Section:Bold color background to draw attention Clear, benefit-focused headline Prominent action button to drive conversions Design Enhancement Benefits These frontend improvements offer several advantages for your NFT platform:Increased User Engagement: The visually appealing design with interactive elements keeps users interested and encourages exploration.Better User Experience: The responsive design ensures a consistent experience across all devices, while the intuitive navigation makes the platform easy to use.Enhanced Brand Identity: The consistent color scheme, typography, and styling elements strengthen your "lil dumdumz NFT" brand identity.Improved Conversion Rates: Strategic placement of call-to-action buttons and clear value propositions help guide users toward purchasing NFTs.Accessibility Improvements: Proper contrast ratios, ARIA attributes, and semantic HTML make the platform more accessible to all users.By implementing these design improvements, you've transformed your functional NFT platform into a visually appealing and user-friendly experience that better showcases your NFTs and encourages user interaction.Step 8: Decentralized Deployment Now that you have built your complete NFT minting application with Arweave storage and Crossmint integration, let's deploy it to the Arweave network. This will make your entire application permanently available and truly decentralized.Prepare for Deployment Before deploying, make sure you have:Your Arweave wallet file (wallet.json) in the project root Sufficient AR tokens in your wallet for the deployment transaction Turbo credits for fast uploads (available from turbo-topup.com) Build the Production Version Build your application for production:This will create optimized production files in the dist directory.Deploy to Arweave The starter kit comes with a pre-configured deployment script. Run it to deploy your application:This script will:Take your built application from the dist directory Upload all assets to Arweave using Turbo for faster deployment Generate a manifest file that binds all your application files together Provide you with a deployment URL in the format: https://arweave.net/{manifestId} Once the deployment completes, you'll see output similar to:Important: Save this manifest ID as you'll need it for the next step.Test Your Deployed Application Visit the provided URL (https://arweave.net/YOUR_MANIFEST_ID) to verify your application is working correctly.Test all functionality:Arweave wallet connection Crossmint authentication NFT minting with the payment button Since your application is now deployed to Arweave, it will remain accessible at this URL forever! Unlike traditional web hosting, there are no recurring fees or servers to maintain.Benefits of Arweave Deployment By deploying to Arweave, your NFT minting application gains several advantages:Permanence: The application is stored permanently on the blockchain Censorship resistance: No central authority can remove your application No server maintenance: No need to manage servers or renew domains True decentralization: Both the app's data (NFT images) and the application itself are decentralized In the next and final step, you'll connect your deployed application to a human-readable domain name, making it easier for users to access.Step 9: Domain Configuration Now that your application is deployed to Arweave, the final step is to connect it to a human-readable domain name using the Arweave Name System (ArNS). This will make your dApp easily accessible with a memorable URL.Prerequisites Before configuring your domain, ensure you have:Successfully deployed your application to Arweave (from Step 8) The manifest ID from your deployment An ARNS name (purchased from arns.app) $ARIO tokens for transaction fees Purchase an ARNS Name (if needed) If you don't already have an ARNS name:Visit arns.app Connect your Arweave wallet Search for an available name Purchase it with $ARIO tokens Get Your Process ID To update your ARNS name:Visit arns.app Connect your Arweave wallet Click "Manage Assets" in the top-right Find your ARNS name and click on the settings icon Copy the Process ID displayed in the management interface Update the Base Record Configuration Open the /scripts/setBaseArns.js file in your project Update the processId in the configuration:Update the dataLink value with your deployment's manifest ID:Set the Base Record Run the command to update your ARNS name with your application's manifest ID:The script will connect to AR.IO, submit the transaction, and update your name to point to your application. When successful, you'll see output similar to:Optional: Configure Undernames If you want to create subdomains for different parts of your application, you can set up undernames:Open /scripts/setUndername.js Update the processId with your Process ID Configure the undername and data link Run:Verify Your Domain Configuration To verify all records associated with your ARNS name:Open /scripts/getRecords.js Update the processId with your Process ID Run:This will display all the records for your ARNS name, including the base record that should now point to your application.Access Your Application Your application is now accessible at:https://YOUR-NAME.ar.io Share this user-friendly URL with your users - they can now access your fully decentralized NFT minting application using a memorable domain name.Important Notes ARNS name record updates may take a few minutes to propagate through the network The default TTL (Time-to-Live) for name records is 15 minutes If users experience issues accessing your domain, they can always use the direct Arweave URL: https://arweave.net/YOUR_MANIFEST_ID Congratulations! 🎉 You've successfully built and deployed a fully decentralized NFT minting application that:Stores NFT images permanently on Arweave Mints NFTs on Ethereum using Crossmint Provides seamless authentication for users Offers multiple payment options for NFT purchases Is deployed in a truly decentralized manner on Arweave Is accessible via a human-readable domain name Your application represents the future of Web3: decentralized storage, simplified user experiences, and permanent availability. Users can now mint NFTs with confidence, knowing that their digital assets will truly last forever on the permaweb supported by the permanent cloud solution, AR.IO.Conclusion Congratulations! You've successfully built an end-to-end decentralized NFT minting application that combines the permanent storage capabilities of Arweave with the user-friendly minting experience of Crossmint. This powerful combination creates a truly decentralized application that overcomes many of the traditional barriers to NFT adoption.What You've Accomplished In this tutorial, you've:Stored Content Permanently - Used ArDrive.io to store your NFT images on Arweave, ensuring they'll remain accessible forever Simplified NFT Creation - Integrated Crossmint's SDK to mint NFTs without requiring users to understand complex blockchain interactions Built a User-Friendly Frontend - Created a React application with wallet connection and authentication Added Payment Options - Implemented Crossmint's payment button to allow purchases with credit cards and cryptocurrencies Deployed Decentrally - Published your entire application to Arweave, ensuring it will remain available indefinitely Created a Memorable Address - Connected your application to a human-readable domain name The Power of This Approach This architecture solves several key problems in the NFT space:True Permanence: Unlike NFTs that reference assets on centralized servers, your NFT images are stored permanently on Arweave Accessibility: By offering credit card payments, you've made NFTs accessible to mainstream users Complete Decentralization: Both your application and your assets are stored on decentralized networks User-Friendly Experience: The combination of Arweave and Crossmint creates a seamless experience for both creators and collectors Next Steps To continue building on this foundation, consider:Adding a gallery feature to display owned NFTs Implementing NFT rarity traits and metadata Creating a multi-creator marketplace Adding social features like comments or likes Building analytics to track sales and engagement Resources For more information on the technologies you've used:Arweave Documentation ArDrive Developer Docs AR.IO SDK Documentation Crossmint Developer Docs By combining these powerful tools, you've built an application that represents the best of what Web3 has to offer: true ownership, permanence, permissionlessness, and accessibility. Your users can now mint NFTs with confidence, knowing that their digital assets will truly last forever on the permaweb supported by the permanent cloud solution, AR.IO.

---

# 4. Register an IP Asset on Arweave - ARIO Docs

Document Number: 4
Source: https://docs.ar.io/guides/story
Words: 1932
Extraction Method: html

Registering Story Protocol IP Assets with Arweave Metadata using Turbo Utilize the speed and reliability of ArDrive Turbo to store metadata for Story Protocol IP Assets permanently on Arweave.Story Protocol enables the registration and management of intellectual property (IP) on-chain. A crucial part of this process involves linking metadata to your IP Assets. While various storage solutions exist, Arweave offers permanent, decentralized storage, making it an ideal choice for valuable IP metadata.This guide demonstrates how to use the ArDrive Turbo SDK to efficiently upload IP Asset metadata to Arweave and register it with the Story Protocol TypeScript SDK.Prerequisites Before you begin, ensure you have the following:Node.js: Version 18 or later. Download from nodejs.org.npm/pnpm/yarn: A compatible package manager.Arweave Wallet: A wallet.json file. Generate one using tools like the Wander browser extension. Keep this file secure and do not commit it to version control.Turbo Credits: Your Arweave wallet must be funded with Turbo credits to pay for uploads. Top up at https://turbo-topup.com.Story Protocol Account: An Ethereum-compatible private key (WALLET_PRIVATE_KEY) and an RPC Provider URL (RPC_PROVIDER_URL) for the desired Story Protocol network (e.g., Aeneid testnet) stored in a .env file.TypeScript Environment: You'll need to execute TypeScript code, so make sure you have ts-node installed globally (npm install -g ts-node) or as a dev dependency.Setup 1. Install Dependencies First, set up a new project directory and install the necessary SDKs:Then install the required dependencies:2. Project Setup Create the following files in your project:.env file (in the project root):Place your Arweave wallet.json file in the project root.Create a tsconfig.json file in the project root:3. Initialize SDK Clients Create a configuration file to set up and export both the Turbo and Story clients:import { TurboFactory, TurboAuthenticatedClient } from "@ardrive/turbo-sdk";
import { StoryClient, StoryConfig } from "@story-protocol/core-sdk";
import { http } from "viem";
import { Account, privateKeyToAccount, Address } from "viem/accounts";
import fs from 'fs';
import path from 'path';
import 'dotenv/config';

// --- Environment Variable Loading ---
const privateKeyEnv = process.env.WALLET_PRIVATE_KEY;
const rpcProviderUrlEnv = process.env.RPC_PROVIDER_URL;
const walletPath = path.resolve(process.cwd(), 'wallet.json'); // Assumes wallet.json is in the project root

// --- Validations ---
if (!privateKeyEnv) {
  throw new Error("WALLET_PRIVATE_KEY is not set in the .env file");
}
if (!rpcProviderUrlEnv) {
  throw new Error("RPC_PROVIDER_URL is not set in the .env file");
}
if (!fs.existsSync(walletPath)) {
    throw new Error(`Arweave wallet file not found at ${walletPath}. Please ensure wallet.json exists in the project root.`);
}

// --- ArDrive Turbo Client Setup ---
function parseWallet(filePath: string): any {
    try {
        const walletData = fs.readFileSync(filePath, 'utf8');
        return JSON.parse(walletData);
    } catch (error) {
        console.error(`Error reading or parsing wallet file at ${filePath}:`, error);
        throw new Error(`Failed to load Arweave wallet. Ensure ${filePath} exists and is valid JSON.`);
    }
}

const arweaveWallet = parseWallet(walletPath);

export const turboClient: TurboAuthenticatedClient = TurboFactory.authenticated({
    privateKey: arweaveWallet,
});
console.log("ArDrive Turbo Client initialized.");

// --- Story Protocol Client Setup ---
const storyPrivateKey: Address = `0x${privateKeyEnv}`;
const storyAccount: Account = privateKeyToAccount(storyPrivateKey);

const storyConfig: StoryConfig = {
  account: storyAccount,
  transport: http(rpcProviderUrlEnv),
  chainId: "aeneid", // Adjust chainId if necessary
};

export const storyClient = StoryClient.newClient(storyConfig);
console.log("Story Client initialized.");Make sure to create the utils directory first:Now, let's create a script to register an IP asset. This involves three steps:Define metadata for the IP itself and the NFT representing ownership Upload metadata to Arweave using Turbo Register the IP on Story Protocol Create the following script file:import { storyClient, turboClient } from "./utils/clients";
import { createHash } from "crypto";
import { Address } from "viem";
import type { UploadResult } from "@ardrive/turbo-sdk";

// Helper function to upload JSON to Arweave via Turbo
async function uploadJSONToArweave(jsonData: any, description: string): Promise<UploadResult> {
    const dataBuffer = Buffer.from(JSON.stringify(jsonData));
    console.log(`Uploading ${description} (${dataBuffer.byteLength} bytes) to Arweave via Turbo...`);

    const tags = [
        { name: "Content-Type", value: "application/json" },
        { name: "App-Name", value: "ArDrive-Story-Tutorial" } // Example tag
    ];

    try {
        // Use Turbo to upload the file buffer
        const result = await turboClient.uploadFile(dataBuffer, { tags });
        console.log(`${description} uploaded successfully: Transaction ID ${result.id}`);
        return result;
    } catch (error) {
        console.error(`Error uploading ${description} to Arweave:`, error);
        throw new Error(`Arweave upload failed for ${description}.`);
    }
}

async function register() {
  // --- Step 1: Define IP Metadata ---
  const ipMetadata = {
    title: "My Arweave-Powered IP",
    description: "An example IP asset with metadata stored permanently on Arweave via Turbo.",
    // Add other required fields like image, creators, etc.
    // Example creator:
    creators: [
      { name: "Your Name/Org", address: storyClient.account.address, contributionPercent: 100 },
    ],
  };
  console.log("IP Metadata defined.");

  const nftMetadata = {
    name: "Ownership NFT for My Arweave IP",
    description: "This NFT represents ownership of the IP Asset whose metadata is on Arweave.",
    // Add other fields like image
  };
  console.log("NFT Metadata defined.");

  // --- Step 2: Upload Metadata to Arweave ---
  const ipUploadResult = await uploadJSONToArweave(ipMetadata, "IP Metadata");
  const nftUploadResult = await uploadJSONToArweave(nftMetadata, "NFT Metadata");

  // Use arweave.net URLs instead of ar:// protocol
  const ipMetadataArweaveURI = `https://arweave.net/${ipUploadResult.id}`;
  const nftMetadataArweaveURI = `https://arweave.net/${nftUploadResult.id}`;

  console.log(`IP Metadata Arweave URI: ${ipMetadataArweaveURI}`);
  console.log(`NFT Metadata Arweave URI: ${nftMetadataArweaveURI}`);

  // Calculate metadata hashes (required by Story Protocol)
  const ipMetadataHash = `0x${createHash("sha256")
    .update(JSON.stringify(ipMetadata))
    .digest("hex")}`;
  const nftMetadataHash = `0x${createHash("sha256")
    .update(JSON.stringify(nftMetadata))
    .digest("hex")}`;

  console.log(`IP Metadata Hash: ${ipMetadataHash}`);
  console.log(`NFT Metadata Hash: ${nftMetadataHash}`);

  // --- Step 3: Register IP on Story Protocol ---
  console.log("Registering IP Asset on Story Protocol...");

  // Choose an SPG NFT contract (Story Protocol Governed NFT)
  // Use a public testnet one or create your own (see Story docs)
  const spgNftContract: Address = "0xc32A8a0FF3beDDDa58393d022aF433e78739FAbc"; // Aeneid testnet example

  try {
    const response = await storyClient.ipAsset.mintAndRegisterIp({
      spgNftContract: spgNftContract,
      ipMetadata: {
        ipMetadataURI: ipMetadataArweaveURI,      // URI pointing to Arweave
        ipMetadataHash: ipMetadataHash as Address, // Content hash
        nftMetadataURI: nftMetadataArweaveURI,     // URI pointing to Arweave
        nftMetadataHash: nftMetadataHash as Address // Content hash
      },
      txOptions: { waitForTransaction: true }, // Wait for confirmation
    });

    console.log(
      `Successfully registered IP Asset!`
    );
    console.log(`  Transaction Hash: ${response.txHash}`);
    console.log(`  IP ID: ${response.ipId}`);
    console.log(`  Story Explorer Link: https://aeneid.explorer.story.foundation/ipa/${response.ipId}`); // Adjust explorer link for different networks
    console.log(`  IP Metadata (Arweave): ${ipMetadataArweaveURI}`);
    console.log(`  NFT Metadata (Arweave): ${nftMetadataArweaveURI}`);

  } catch (error) {
    console.error("Error registering IP Asset on Story Protocol:", error);
  }
}

// Execute the register function
register().catch(console.error);Run the Registration Script To execute the script and register your IP Asset:This will:Upload your IP metadata to Arweave permanently Upload your NFT metadata to Arweave permanently Register an IP Asset on Story Protocol pointing to these Arweave URLs Once an IP Asset is registered, you can attach license terms and allow others to mint license tokens. Create a new script for this:import { storyClient } from "./utils/clients";
import { Address } from "viem";

// Assume these values are known for the IP Asset you want to license
const LICENSOR_IP_ID: Address = "0x..."; // Replace with the actual IP ID of the asset
const LICENSE_TERMS_ID: string = "..."; // Replace with the specific terms ID attached to the IP Asset
const RECEIVER_ADDRESS: Address = "0x..."; // Address to receive the license token(s)

async function mintLicense() {
  console.log(`Minting license token(s) for IP ID ${LICENSOR_IP_ID} under terms ${LICENSE_TERMS_ID}...`);

  try {
    const response = await storyClient.license.mintLicenseTokens({
      licenseTermsId: LICENSE_TERMS_ID,
      licensorIpId: LICENSOR_IP_ID,
      receiver: RECEIVER_ADDRESS,
      amount: 1, // Number of license tokens to mint
      // Optional parameters:
      // maxMintingFee: BigInt(0), // Set if the terms have a fee; 0 disables check if no fee expected
      // maxRevenueShare: 100, // Default check for revenue share percentage
      txOptions: { waitForTransaction: true },
    });

    console.log(
      `Successfully minted license token(s)!`
    );
    console.log(`  Transaction Hash: ${response.txHash}`);
    console.log(`  License Token ID(s): ${response.licenseTokenIds}`);

  } catch (error) {
    console.error("Error minting license token(s):", error);
  }
}

// Execute the function (after updating the constants above)
// mintLicense().catch(console.error);Before running this script:Replace LICENSOR_IP_ID with the actual IP ID obtained from your registration Replace LICENSE_TERMS_ID with the ID of license terms attached to that IP Replace RECEIVER_ADDRESS with the address to receive the license token Uncomment the function call at the bottom Then run:Finally, let's create a script to register a derivative work based on an existing IP, also using Arweave for metadata storage:import { storyClient, turboClient } from "./utils/clients";
import { createHash } from "crypto";
import { Address } from "viem";
import type { UploadResult } from "@ardrive/turbo-sdk";
import { DerivativeData } from "@story-protocol/core-sdk";

// Helper function to upload JSON to Arweave via Turbo (same as in registerIpWithArweave.ts)
async function uploadJSONToArweave(jsonData: any, description: string): Promise<UploadResult> {
    const dataBuffer = Buffer.from(JSON.stringify(jsonData));
    console.log(`Uploading ${description} (${dataBuffer.byteLength} bytes) to Arweave via Turbo...`);

    const tags = [
        { name: "Content-Type", value: "application/json" },
        { name: "App-Name", value: "ArDrive-Story-Tutorial" }
    ];

    try {
        const result = await turboClient.uploadFile(dataBuffer, { tags });
        console.log(`${description} uploaded successfully: Transaction ID ${result.id}`);
        return result;
    } catch (error) {
        console.error(`Error uploading ${description} to Arweave:`, error);
        throw new Error(`Arweave upload failed for ${description}.`);
    }
}

// --- Information about the Parent IP and License ---
const PARENT_IP_ID: Address = "0x..."; // Replace with the actual Parent IP ID
const LICENSE_TERMS_ID: string = "..."; // Replace with the License Terms ID to derive under

async function registerDerivative() {
  // --- Step 1: Define Derivative Metadata ---
  const derivativeIpMetadata = {
    title: "My Derivative Work (Arweave Metadata)",
    description: "A remix/adaptation based on a parent IP, metadata on Arweave.",
    // Add other required fields (image, creators matching the derivative creator, etc.)
  };

  const derivativeNftMetadata = {
    name: "Ownership NFT for My Derivative Work",
    description: "NFT for the derivative IP, metadata on Arweave.",
    // Add other fields
  };

  // --- Step 2: Upload Derivative Metadata to Arweave ---
  console.log("Uploading derivative metadata to Arweave via Turbo...");
  const derivIpUploadResult = await uploadJSONToArweave(derivativeIpMetadata, "Derivative IP Metadata");
  const derivNftUploadResult = await uploadJSONToArweave(derivativeNftMetadata, "Derivative NFT Metadata");

  // Use arweave.net URLs instead of ar:// protocol
  const derivIpMetadataArweaveURI = `https://arweave.net/${derivIpUploadResult.id}`;
  const derivNftMetadataArweaveURI = `https://arweave.net/${derivNftUploadResult.id}`;

  const derivIpMetadataHash = `0x${createHash("sha256")
    .update(JSON.stringify(derivativeIpMetadata))
    .digest("hex")}`;
  const derivNftMetadataHash = `0x${createHash("sha256")
    .update(JSON.stringify(derivativeNftMetadata))
    .digest("hex")}`;

  console.log(`Derivative IP Metadata Arweave URI: ${derivIpMetadataArweaveURI}`);
  console.log(`Derivative NFT Metadata Arweave URI: ${derivNftMetadataArweaveURI}`);

  // --- Step 3: Register Derivative on Story Protocol ---
  // Prepare Derivative Data for Story Protocol
  const derivData: DerivativeData = {
    parentIpIds: [PARENT_IP_ID],
    licenseTermsIds: [LICENSE_TERMS_ID],
  };

  console.log("Registering Derivative IP Asset on Story Protocol...");

  // Use the same SPG NFT contract or your own
  const spgNftContract: Address = "0xc32A8a0FF3beDDDa58393d022aF433e78739FAbc"; // Aeneid testnet example

  try {
    const response = await storyClient.ipAsset.mintAndRegisterIpAndMakeDerivative({
      spgNftContract: spgNftContract,
      derivData: derivData, // Link to parent IP and license terms
      ipMetadata: { // Metadata for the *new* derivative IP
        ipMetadataURI: derivIpMetadataArweaveURI,      // Arweave URI
        ipMetadataHash: derivIpMetadataHash as Address, // Content hash
        nftMetadataURI: derivNftMetadataArweaveURI,     // Arweave URI
        nftMetadataHash: derivNftMetadataHash as Address // Content hash
      },
      txOptions: { waitForTransaction: true },
    });

    console.log(
      `Successfully registered Derivative IP Asset!`
    );
    console.log(`  Transaction Hash: ${response.txHash}`);
    console.log(`  Derivative IP ID: ${response.ipId}`);
    console.log(`  Derivative Token ID: ${response.tokenId}`);
    console.log(`  Story Explorer Link: https://aeneid.explorer.story.foundation/ipa/${response.ipId}`);
    console.log(`  Derivative Metadata (Arweave): ${derivIpMetadataArweaveURI}`);

  } catch (error) {
    console.error("Error registering derivative IP Asset on Story Protocol:", error);
  }
}

// Before running this script:
// 1. Replace PARENT_IP_ID with a real IP ID you have access to
// 2. Replace LICENSE_TERMS_ID with the actual license terms ID
// Then uncomment the line below to execute
// registerDerivative().catch(console.error);Before running this script:Replace PARENT_IP_ID with the actual parent IP ID Replace LICENSE_TERMS_ID with the license terms ID that permits derivatives Uncomment the function execution at the bottom Run:Conclusion By leveraging the ArDrive Turbo SDK, you can seamlessly integrate permanent Arweave storage into your Story Protocol workflow. Uploading metadata with Turbo ensures fast, reliable, and cost-effective data persistence for your valuable IP Assets, whether they are root IPs or complex derivatives with licensing relationships.This tutorial demonstrated a complete workflow:Setting up a project structure with all required dependencies Creating a utility module for client initialization Registering original IP Assets with metadata stored on Arweave Minting license tokens for IP Assets Creating and registering derivative works For further details on Story Protocol concepts like licensing, derivatives, or specific SDK functions, refer to the Story Protocol Documentation.

---

# 5. Minimal Svelte Starter Kit  Cooking with the Permaweb

Document Number: 5
Source: https://cookbook.arweave.net/kits/svelte/minimal.html
Words: 566
Extraction Method: html

Minimal Svelte Starter Kit This guide will walk you through in a step by step flow to configure your development environment to build and deploy a permaweb application.Prerequisites Know typescript NodeJS v18 or greater Know Svelte - https://svelte.dev Know git and common terminal commands Development Dependencies TypeScript esbuild w3 Steps Create Project mkdir myproject
cd myproject
npm init -y
npm install -D svelte esbuild typescript esbuild-svelte tinro svelte-preprocess mkdir myproject
cd myproject
yarn init -y
yarn add -D svelte esbuild typescript esbuild-svelte tinro svelte-preprocess Create buildscript.js import fs from "fs";
import esbuild from "esbuild";
import esbuildSvelte from "esbuild-svelte";
import sveltePreprocess from "svelte-preprocess";

//make sure the directoy exists before stuff gets put into it
if (!fs.existsSync("./dist/")) {
    fs.mkdirSync("./dist/");
}
esbuild
    .build({
        entryPoints: [`./src/main.ts`],
        bundle: true,
        outdir: `./dist`,
        mainFields: ["svelte", "browser", "module", "main"],
        // logLevel: `info`,
        splitting: true,
        write: true,
        format: `esm`,
        plugins: [
            esbuildSvelte({
                preprocess: sveltePreprocess(),
            }),
        ],
    })
    .catch((error, location) => {
        console.warn(`Errors: `, error, location);
        process.exit(1);
    });

//use a basic html file to test with
fs.copyFileSync("./index.html", "./dist/index.html");Modify package.json Set type to module, add a build script {
  "type": "module"
  ...
  "scripts": {
    "build": "node buildscript.js"
  }
} Create src directory and some src files mkdir src
touch src/main.ts
touch src/app.svelte
touch src/counter.svelte
touch src/about.svelte Main.ts import App from "./app.svelte";

new App({
    target: document.body,
});app.svelte Hash Routing You will notice the router.mode.hash() setting in the script session, this is important to configure your application to use hash based routing, which will enable url support when running that application on a path, like https://[gateway]/[TX] counter.svelte <script lang="ts">
    let count = 0;

    function inc() {
        count += 1;
    }
</script>
<h1>Hello Permaweb</h1>
<button on:click="{inc}">Inc</button>
<p>Count: {count}</p> about.svelte <h1>About Page</h1>
<p>Minimal About Page</p>
<a href="/">Home</a> Add index.html <!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Vite + Svelte + TS</title>
  </head>
  <body>
    <div id="app"></div>
    <script type="module" src="./main.js"></script>
  </body>
</html> Deploy Permanently Generate Wallet We need the arweave package to generate a wallet npm install --save arweave yarn add arweave -D then run this command in the terminal node -e "require('arweave').init({}).wallets.generate().then(JSON.stringify).then(console.log.bind(console))" > wallet.json Fund Wallet You will need to fund your wallet with ArDrive Turbo credits. To do this, enter ArDrive and import your wallet. Then, you can purchase turbo credits for your wallet.Setup Permaweb-Deploy npm install --global permaweb-deploy yarn global add permaweb-deploy Update vite.config.ts import { defineConfig } from 'vite'
import { svelte } from '@sveltejs/vite-plugin-svelte'

export default defineConfig({
  plugins: [svelte()],
  base: './'
}) Update package.json {
  ...
  "scripts": {
    ...
    "deploy": "DEPLOY_KEY=$(base64 -i wallet.json) permaweb-deploy --ant-process << ANT-PROCESS >> --deploy-folder build"
  }
  ...
} Replace << ANT-PROCESS >> with your ANT process id.Run build Now it is time to generate a build, run npm run build yarn build Run deploy Finally we are good to deploy our first Permaweb Application npm run deploy yarn deploy ERROR If you receive an error Insufficient funds, make sure you remembered to fund your deployment wallet with ArDrive Turbo credits.Response You should see a response similar to the following:Deployed TxId [<<tx-id>>] to ANT [<<ant-process>>] using undername [<<undername>>] Your Svelte app can be found at https://arweave.net/<< tx-id >>.SUCCESS You should now have a Svelte Application on the Permaweb! Great Job!Summary This is a minimal version of publishing a Svelte application on the permaweb, but you may want more features, like hot-reloading and tailwind, etc. Check out hypar for a turnkey starter kit. HypAR

---

# 6. Function Piping  WAO

Document Number: 6
Source: https://docs.wao.eco/api/function-piping
Words: 528
Extraction Method: html

pipe Most functions return in the format of { err, res, out, pid, mid, id }, and these function can be chained with pipe, which makes executing multiple messages a breeze.For example, the following is how deploy uses pipe internally. The execution will be immediately aborted if any of the functions in fns produces an error.let fns = [

  {

    fn: "spwn",

    args: { module, scheduler, tags, data },

    then: { "args.pid": "pid" },

   },

   { fn: "wait", then: { "args.pid": "pid" } },

   { fn: "load", args: { src, fills }, then: { "args.pid": "pid" } }

]

const { err, res, out, pid } = await this.pipe({ jwk, fns }) bind If the function comes from other instances rather than AO, use bind.const fns = [{ fn: "post", bind: this.ar, args: { data, tags }}] then You can pass values between functions with then. For instance, passing the result from the previous functions to the next function's arguments is a common operation.const fns = [

  { fn: "post", bind: ao.ar, args: { data, tags }, then: ({ id, args, out })=>{

    args.tags.TxId = id // adding TxId tag to `msg` args

    out.txid = id // `out` will be returned at last with `pipe`

  }},

  { fn: "msg", args: { pid, tags }},

]

const { out: { txid } } = await ao.pipe({ fns, jwk }) If then returns a value, pipe will immediately return with that single value. You can also use err to abort pipe with an error.const fns = [

  { fn: "msg", args: { pid, tags }, then: ({ inp })=>{

     if(inp.done) return inp.val

  }},

  { fn: "msg", args: { pid, tags }, err: ({ inp })=>{

     if(!inp.done) return "something went wrong"

  }},

]

const val = await ao.pipe({ jwk, fns }) then has many useful parameters.res: res from the previous result args: args for the next function out: the final out result from the pipe sequence inp: out from the previous result _: if values are assigned to the _ fields, pipe returns them as top-level fields in the end pid: pid will be passed if any previous functions return pid ( e.g. deploy ) mid: mid will be passed if any previous functions return mid ( e.g. msg ) id: id will be passed if any previous functions return id ( e.g. post ) then can be a simplified hashmap object.let fns = [

  {

    fn: "msg",

    args: { tags },

    then: { "args.mid": "mid", "out.key": "inp.a", "_.val": "inp.b" },

   }, 

   { fn: "some_func", args: {} } // args.mid will be set from the previous `then`

]

const { out: { key }, val } = await ao.pipe({ jwk, fns }) err err has the same signature as then. If err returns a value, pipe will throw an Error with that value.const fns = [

  { fn: "msg", args: { pid, tags }, err: ({ inp })=>{

     if(!inp.done) return "something went wrong!"

  }}

]

const val = await ao.pipe({ jwk, fns }) cb cb can report the current progress of pipe after every function execution.await ao.pipe({ jwk, fns, cb: ({ i, fns, inp })=>{

  console.log(`${i} / ${fns.length} functions executed`)

}})

---

# 7. HyperBEAM  WAO

Document Number: 7
Source: https://docs.wao.eco/hyperbeam
Words: 1082
Extraction Method: html

Install WAO & HyperBEAM yarn add wao In addition to the WAO SDK, you will need to install HyperBEAM on your local computer (recommended) or a cloud server. The required systems are not the only ones you can install HyperBEAM on. I was, for example, able to install HyperBEAM on Arch Linux by installing the necessary dependencies. You could throw installation errors at LLMs like ChatGPT and Claude and likely be able to figure it out.If you want to test Mainnet AOS modules, you need to install the WAO fork of HyperBEAM, which includes a custom helper device for testing.git clone --branch wao https://github.com/weavedb/HyperBEAM.git Make sure you compiled it and are able to run rebar3 shell, but you don't need to have it running yet. You might need to add environment variables depending on how you installed it on your system. WAO SDK handles that too.CMAKE_POLICY_VERSION_MINIMUM=3.5 CC=gcc-12 CXX=g++-12 rebar3 shell Create a Project To create a project, you could use npx wao create APP_NAME,npx wao create myapp && cd myapp or you could manually install wao into your project.mkdir myapp && cd myapp && yarn init && yarn add wao

mkdir test && touch test/test.js Make sure your package.json looks something like the following to enable ES6 and test commands with wasm64. The wasm64 flag is unnecessary for NodeJS v24 and later. Also, you need to disable concurrency so the test won't try to run multiple HyperBEAM nodes on duplicate ports.File  /package.json {

  "name": "myapp",

  "version": "1.0.0",

  "type": "module",

  "scripts": {

    "test": "node --experimental-wasm-memory64 --test --test-concurrency=1",

    "test-only": "node --experimental-wasm-memory64 --test-only --test-concurrency=1",

    "test-all": "node --experimental-wasm-memory64 --test --test-concurrency=1 test/**/*.test.js"

  },

  "dependencies": {

    "wao": "^0.22.1"

  }

} If your HyperBEAM installation requires environment variables, define them in .env.hyperbeam.File  /.env.hyperbeam CC=gcc-12

CXX=g++-12

CMAKE_POLICY_VERSION_MINIMUM=3.5 Write Tests HyperBEAM class let you create a HyperBEAM sandbox by starting a fresh HyperBEAM node before testing and shutting it down when tests are complete.
You can interact with the HyperBEAM node with HB (hbeam.hb).File  /test/hyperbeam.test.js import assert from "assert"

import { describe, it, before, after, beforeEach } from "node:test"

import { HyperBEAM } from "wao/test"

 

/*

  The link to your HyperBEAM node directory.

  It's relative to your app root folder, not the test folder.

*/

const cwd = "../HyperBEAM"

 

describe("HyperBEAM", function () {

  let hbeam, hb

 

  // start a hyperbeam node and wait till it's ready, reset storage for test

  before(async () => {

    hbeam = await new HyperBEAM({ cwd, reset: true }).ready()

  })

 

  beforeEach(async () => (hb = hbeam.hb))

 

  // kill the node after testing

  after(async () => hbeam.kill())

 

  it("should run a HyperBEAM node", async () => {

    // change config

    await hb.post({ path: "/~meta@1.0/info", test_config: "abc" })

 

    // get config

    const { out } = await hb.get({ path: "/~meta@1.0/info" })

    assert.equal(out.test_config, "abc")

  })

}) You can also skip the HyperBEAM class and connect with an already running remote node by specifying the node url to HB.File  /test/hb.test.js import assert from "assert"

import { describe, it, before, after, beforeEach } from "node:test"

import { acc } from "wao/test"

import { HB } from "wao"

 

const cwd = "../HyperBEAM"

 

describe("HyperBEAM", function () {

  let hb

 

  // using one of the pre-generated accounts from acc for test

  beforeEach(async () => {

    hb = new HB({ jwk: acc[0].jwk, url: "http://localhost:10001" })

  })

 

  it("should connect to a HyperBEAM node", async () => {

    // get build info

    const build = await hb.g("/~meta@1.0/build")

    assert.equal(build.node, "HyperBEAM")

  })

}) HyperBEAM has a test device to test basic features with the HB methods.spawn: spawn a process schedule: schedule a message to a process compute: compute the result of a slot messages: list messages in a process it("should test test-device@1.0", async () => {

  const { pid } = await hb.spawn({ "execution-device": "test-device@1.0" })

  const { slot } = await hb.schedule({ pid })

  const res = await hb.compute({ pid, slot })

  assert.equal(res.results["assignment-slot"], 1)

  const {

      edges: [ edge0, { node: { assignment, message } }]

  } = await hb.messages({ pid, from: 0, to: 1 })

  assert.equal(message.Target, pid)

}) AO-Core Pathing and HTTP Message Signatures It is extremely important to understand the pathing scheme of AO-Core protocol and HTTP Message Signature (RFC 9421) when interacting with HyperBEAM nodes.get and post help you to construct complex messages and send signed requests to HyperBEAM nodes.it("should interact with meta@1.0", async () => {

  // change node configuration

  await hb.post({ path: "/~meta@1.0/info", test_config: 123 })

  

  const { out } = await hb.get({ path: "/~meta@1.0/info" })

  assert.equal(out.test_config, 123)

}) g and p are the shortcut methods for get and post.it("should interact with meta@1.0 #2", async () => {

  await hb.p("/~meta@1.0/info", { test_config2: "abc" })

  

  const out = await hb.g("/~meta@1.0/info")

  assert.equal(out.test_config2, "abc")

}) Spawn AOS Processes You can also spawn and interact with AOS processes on HyperBEAM.const data = `

local count = 0

Handlers.add("Inc", "Inc", function (msg)

  count = count + 1

  msg.reply({ Data = "Count: "..tostring(count) })

end)

 

Handlers.add("Get", "Get", function (msg)

  msg.reply({ Data = "Count: "..tostring(count) })

end)`

 

it("should spawn a legacynet compatible AOS process", async () => {

  const { pid } = await hb.spawnLegacy()

  const { slot } = await hb.scheduleLegacy({ pid, data })

  const r = await hb.computeLegacy({ pid, slot })

  const { slot: slot2 } = await hb.scheduleLegacy({ pid, action: "Inc" })

  const r2 = await hb.computeLegacy({ pid, slot: slot2 })

  assert.equal(r2.Messages[0].Data, "Count: 1")

  const { slot: slot3 } = await hb.scheduleLegacy({ pid, action: "Inc" })

  const r4 = await hb.computeLegacy({ pid, slot: slot3 })

  const r3 = await hb.dryrun({ pid, action: "Get" })

  assert.equal(r3.Messages[0].Data, "Count: 2")

}) Create Custom Devices You can create your own custom devices in Erlang, Rust, and C++.Create your device under /HyperBEAM/src.File  /HyperBEAM/src/dev_foo.erl const data = `

local count = 0

Handlers.add("Inc", "Inc", function (msg)

  count = count + 1

  msg.reply({ Data = "Count: "..tostring(count) })

end)

 

Handlers.add("Get", "Get", function (msg)

  msg.reply({ Data = "Count: "..tostring(count) })

end)`

``

`

Define it in `preloaded_devices` in `/HyperBEAM/src/hb_opts.erl`

 

```erlang [/HyperBEAM/src/hb_opts.erl]

preloaded_devices => [

  #{<<"name">> => <<"ans104@1.0">>, <<"module">> => dev_codec_ans104},

  #{<<"name">> => <<"compute@1.0">>, <<"module">> => dev_cu},

  ...

  #{<<"name">> => <<"foo@1.0">>, <<"module">> => dev_foo}

],Test the device with WAO.import assert from "assert"

import { describe, it, before, after, beforeEach } from "node:test"

import { HyperBEAM } from "wao"

 

const cwd = "../HyperBEAM" // HyperBEAM directory

 

describe("Hyperbeam Custom Devices", function () {

  let hbeam, hb

  before(async () => {

    hbeam = await new HyperBEAM({ cwd, reset: true }).ready()

  })

  beforeEach(async () => (hb = hbeam.hb))

  after(async () => hbeam.kill())

 

  it("should query a custom device", async () => {

    const { version } = await hb.g("/~foo@1.0/info")

    assert.equal("1.0", version)

  })

})

---

# 8. Legacynet AOS  WAO

Document Number: 8
Source: https://docs.wao.eco/legacynet
Words: 1804
Extraction Method: html

WAO is still actively being developed; please use it at your discretion.Installation yarn add wao Drop-in aoconnect Replacement for Tests By replacing aoconnect with WAO connect, everything runs in memory with zero latency and your tests are executed 1000x faster. The APIs are identical. So, there's no need to change anything else in your code.//import { spawn, message, dryrun, assign, result } from "@permaweb/aoconnect"

import { connect, acc } from "wao/test"

const { spawn, message, dryrun, assign, result } = connect() Setting up a Project It's super easy to set up a test AO project manually.mkdir wao-test && cd wao-test

yarn init && yarn add wao Add test and test-only commands to your package.json.{

  "scripts": {

    "test": "node --experimental-wasm-memory64",

    "test-only": "node --experimental-wasm-memory64 --test-only"

  }

} Create test directory and test.js file.mkdir test && touch test/test.js Writing Tests Write a simple test in test.js.import assert from "assert"

import { describe, it } from "node:test"

import { connect, acc } from "wao/test"

const { spawn, message, dryrun } = connect()

const signer = acc[0].signer

const src_data = `

Handlers.add("Hello", "Hello", function (msg)

  msg.reply({ Data = "Hello, World!" })

end)

`

describe("WAO", function () {

  it("should spawn a process and send messages", async () => {

    const pid = await spawn({ 

      signer,

      module: "Do_Uc2Sju_ffp6Ev0AnLVdPtot15rvMjP-a9VVaA5fM",

      scheduler: "_GQ33BkPtZrqxA84vM8Zk-N2aO0toNNu_C-l-rawrBA"

    })

 

    // on mainnet, you need to wait till the process becomes available.

    // WAO automatically handles it. No need with in-memory tests.

    // await wait({ pid })

 

    await message({

      process: pid,

      tags: [{ name: "Action", value: "Eval" }],

      data: src_data,

      signer,

    })

    const res = await dryrun({

      process: pid,

      tags: [{ name: "Action", value: "Hello" }],

      signer,

    })

    assert.equal(res.Messages[0].Data, "Hello, World!")

  })

}) Note that generating random Arweave wallets for every test takes time and slows down your test executions, so Wao connect provides pre-generated accounts for your tests, which saves hours if you are to run your tests thousands of times.acc[0] = { jwk, addr, signer } Run the test.yarn test test/test.js Using WAO SDK WAO comes with elegant syntactic sugar and makes writing AO projects an absolute joy.The same test can be written as follows.import assert from "assert"

import { describe, it } from "node:test"

import { AO, acc } from "wao/test"

 

const src_data = `

Handlers.add("Hello", "Hello", function (msg)

  msg.reply({ Data = "Hello, World!" })

end)

`

describe("WAO", function () {

  it("should spawn a process and send messages", async () => {

    const ao = await new AO().init(acc[0])

    const { p } = await ao.deploy({ src_data })

    assert.equal(await p.d("Hello", false), "Hello, World!")

  })

}) The AO class is not only for in-memory tests, but also for production code. You just need to import from a different path.import { AR, AO, GQL } from "wao" Cherry-Picking Outputs You often need to pick a specific piece of data from returned results with multiple spawned messages. You need to go through all the returned messages and further go through tags and data to find it. That's too much code to write. AO comes with get parameter to simplify it.Consider the following Lua handlers.local json = require('json')

 

Handlers.add("Hello", "Hello", function (msg)

  msg.reply({ Data = json.encode({ Name = "Bob" })})

end)

 

Handlers.add("Hello2", "Hello2", function (msg)

  msg.reply({ Data = "Hello, World!", Name = "Bob", Age = "30" })

end)

 

Handlers.add("Hello3", "Hello3", function (msg)

  msg.reply({ Profile = json.encode({ Name = "Bob", Age = "30" })})

end) // by default it extracts string Data

const out = await p.d("Hello")

assert.deepEqual(out, { Name: "Bob" })

 

// equivalent

const out2 = await p.d("Hello", { get: false })

assert.deepEqual(out2, { Name: "Bob" })

 

// get JSON decoded Data

const out3 = await p.d("Hello2", { get: true })

assert.equal(out3, "Hello, World!")

 

// get a tag

const out4 = await p.d("Hello2", { get: "Age" })

assert.equal(out4, "30")

 

// get multiple tags

const out5 = await p.d("Hello2", { get: { obj: { firstname: "Name", age: "Age" }}})

assert.deepEqual(out5, { firstname: "Bob", age: "30" })

 

// shortcut if keys don't include name, data, from, json

const out6 = await p.d("Hello2", { get: { firstname: "Name", age: "Age" }})

assert.deepEqual(out6, { firstname: "Bob", age: "30" })

 

// await p.d("Hello2", { get: { name: "Name", age: "Age" } }) doesn't work

 

// handle tag as json

const out7 = await p.d("Hello3", { get: { prof: { name: "Profile", json: true }}})

assert.deepEqual(out7, { prof: { Name: "Bob", Age: "30" }}) Determining Message Success To determine if your message is successful, you often need to track down a chain of asynchronous messages and examine resulted tags and data. This is actually a fairy complex operation and too much code to write. Luckily for you, AO comes with check parameter to extremely simplify it. check tracks down messages and lazy-evaluates if your check conditions are met.// check if Data exists

await p.m("Hello2", { check: true })

 

// check if Data is a certain value

await p.m("Hello2", { check: "Hello, World! })

 

// check if a tag exists

await p.m("Hello2", { check: "Name" })

 

// check if tags are certain values

await p.m("Hello2", { check: { Name: "Bob", Age: "30" } })

 

// it throws an Error if the conditions are not met

try{

  await p.m("Hello2", { check: { Name: "Bob", Age: "20" } })

}catch(e){

  console.log("something went wrong!")

}

 

// check if Name is Bob and Age exists, then get Age

const age = await p.m("Hello2", { check: { Name: "Bob", Age: true }, get : "Age" })

assert.equal(age, "30", "Bob is not 30 yo!") Async Message Tracking with receive() AOS2 introduced a handy function receive() to send a message to another process and receive a reply in the same handler.Handlers.add("Hello3", "Hello3", function (msg)

  msg.reply({ Data = "How old are you?" })

  local age = Send({

    Target = msg.To, Action = "Get-Age", Name = msg.Who

  }).receive().Data

  msg.reply({ Data = "got your age!", Name = msg.Who, Age = age })

end) Since the second reply will be a part of another message triggerd by the Target process reply, you cannot get the final reply simply with the arconnect result function. You need to keep pinging the process results or track down the chain of messages to examine what went wrong. The AO get and check automatically handle this complex operation in a lazy short-circuit manner in the background for you. A proper timeout (ms) should be specified.const age = await p.m(

  "Hello3", 

  { Who: "Bob", To: DB_PROCESS_ID }, // second argument can be tags

  { get: "Age", check: "got your age!", timeout: 5000 }

)

assert.equal(age, "30") There are so many more powerful tricks you can utilize to make complex AO development easier.Read on to the API reference section to find out!Logging WAO hot-patches the core AOS module code so ao.log automatically is forwarded to JS console.log and whatever you log will be directly displayed in your terminal. Lua tables will be auto-converted to JSON objects. It doesn't affect your production code, it only hot-paches the module during testing. This makes complex debugging so easy.Handlers.add("Hello4", "Hello4", function (msg)

  ao.log("Hello, Wordl!") -- will be displayed in the terminal

  ao.log({ Hello = "World!" }) -- will be auto-converted to JSON

  

  -- passing multiple values 

  ao.log("Hi", 3, true, [ 1, 2, 3 ], { Hello = "World!" })

end) You can get logs even when an error occurs in the handler, which is extremely handy to identify the error causes.Fork Wasm Memory You can fork wasm memory to a new process. This could come in handy to create checkpoints for tests.It only works with in-memory testing.const src_counter = `

local count = 0

Handlers.add("Add", "Add", function (msg)

  count = count + tonumber(msg.Plus)

end)

Handlers.add("Get", "Get", function (msg)

  msg.reply({ Data = tostring(count) })

end)

`

const ao = await new AO().init(acc[0])

const { p, pid } = await ao.deploy({ boot: true, src_data: src_counter })

await p.m("Add", { Plus: 3 })

assert.equal(await p.d("Get"), "3")

 

const ao2 = await new AO().init(acc[0])

// pass the exisiting wasm memory to a new process

const { p: p2 } = await ao2.spwn({ memory: ao.mem.env[pid].memory })

assert.equal(await p2.d("Get"), "3")

await p2.m("Add", { Plus: 2 })

assert.equal(await p2.d("Get"), "5") You can also get mainnet process memory from the CU endpoint (GET /state/{pid}) and fork it for tests.WeaveDrive The WeaveDrive extension is fully emulated with WAO. You can use attest and avail functions from AO.import { blueprint, AO, acc } from "wao/test"

const attestor = acc[0]

const handler = `

apm.install('@rakis/WeaveDrive')

Drive = require('@rakis/WeaveDrive')

Handlers.add("Get", "Get", function (msg)

  msg.reply({ Data = Drive.getData(msg.id) })

end)`

 

describe("WeaveDrive", () => {

  it("should load Arweave tx data", async () => {

    const ao = await new AO().init(attestor)

    

    const { p } = await ao.deploy({

      tags: { Extension: "WeaveDrive", Attestor: attestor.addr },

      loads: [ await blueprint("apm"), handler ],

    })

    

    const { id } = await ao.ar.post({ data: "Hello" })

    await ao.attest({ id })

    

    assert.equal(await p.d("Get", { id }), "Hello")

  })

}) Local Persistent Server You can run a local WAO server with persistent storage, which enables connections with outside components such as frontend apps.npx wao port: Arweave port, the ports of AO units are based on this port (default to 4000) AR: localhost:4000 MU: localhost:4002 SU: localhost:4003 CU: localhost:4004 db: a directory to store data (default to .cache) reset: to reset the database npx wao --port 5000 --db .custom_cache_dir --reset In this case, the ports will be, AR => 5000, MU => 5002, SU => 5003, CU => 5004.You can use WAO SDK or AOConnect to connect with the WAO units, but the following tags will be automatically set with WAO SDK.AOS2.0.1 Module: Do_Uc2Sju_ffp6Ev0AnLVdPtot15rvMjP-a9VVaA5fM Scheduler: _GQ33BkPtZrqxA84vM8Zk-N2aO0toNNu_C-l-rawrBA Authority: eNaLJLsMiWCSWvQKNbk_YT-9ydeWl9lrWwXxLVp9kcg import { describe, it } from "node:test"

import assert from "assert"

import { AO } from "wao"

 

const src_data = `

Handlers.add("Hello", "Hello", function (msg)

  msg.reply({ Data = "Hello, World!" })

end)`

 

describe("WAO Server", ()=>{

  it("should connect with WAO SDK", async ()=>{

    const ao = await new AO(4000).init(YOUR_JWK)

    const { p } = await ao.deploy({ src_data })

    assert.equal(await p.d("Hello"), "Hello, World!")

  })

}) With AOConnect,import { describe, it } from "node:test"

import assert from "assert"

import { connect, createDataItemSigner } from "@permaweb/aoconnect"

const { spawn, message, dryrun, assign, result } = connect({

  MU_URL: `http://localhost:4002`,

  CU_URL: `http://localhost:4003`,

  GATEWAY_URL: `http://localhost:4000`

})

 

const src_data = `

Handlers.add("Hello", "Hello", function (msg)

  msg.reply({ Data = "Hello, World!" })

end)`

 

describe("WAO Server", () => {

  it("should connect with WAO SDK", async () => {

    const pid = await spawn({

      module: "Do_Uc2Sju_ffp6Ev0AnLVdPtot15rvMjP-a9VVaA5fM",

      scheduler: "_GQ33BkPtZrqxA84vM8Zk-N2aO0toNNu_C-l-rawrBA",

      tags: [

        {

          name: "Authority",

          value: "eNaLJLsMiWCSWvQKNbk_YT-9ydeWl9lrWwXxLVp9kcg",

        },

      ],

      signer: createDataItemSigner(YOUR_JWK),

    })

 

    // wait till the process becomes available

 

    const mid = await message({

      process: pid,

      tags: [{ name: "Action", value: "Eval" }],

      data: src_data,

      signer: createDataItemSigner(acc[0].jwk),

    })

 

    console.log(await result({ process: pid, message: mid }))

 

    const res = await dryrun({

      process: pid,

      data: "",

      tags: [{ name: "Action", value: "Hello" }],

      anchor: "1234",

    })

 

    assert.equal(res.Messages[0].Data, "Hello, World!")

  })

}) Connecting with the AOS terminal,aos \

  --gateway-url http://localhost:4000 \

  --cu-url http://localhost:4004 \

  --mu-url http://localhost:4002 \

  --tag-name Authority \

  --tag-value eNaLJLsMiWCSWvQKNbk_YT-9ydeWl9lrWwXxLVp9kcg

---

# 9. Legacynet Compatible AOS  WAO

Document Number: 9
Source: https://docs.wao.eco/hyperbeam/legacynet-aos
Words: 586
Extraction Method: html

Legacynet compatible AOS uses genesis-wasm@1.0 to delegate compute to an external local CU. You can use wasm modules stored on the Arweave Mainnet storage, or you could create a helper method to locally store wasm modules for testing. You should also pass as = ["genesis_wasm"] to the test HyperBEAM node to auto-start a local CU server with HyperBEAM.Let's use the production AOS2.0.6 module stored at ISShJH1ij-hPPt9St5UFFr_8Ys3Kj5cyg7zrMGt7H9s for now.File  /test/legacynet-aos.test.js import assert from "assert"

import { describe, it, before, after } from "node:test"

import { HyperBEAM } from "wao/test"

 

const seed = num => {

  const array = new Array(num)

  for (let i = 0; i < num; i++) array[i] = Math.floor(Math.random() * 256)

  return Buffer.from(array).toString("base64")

}

 

const data = `

local count = 0

Handlers.add("Inc", "Inc", function (msg)

  count = count + 1

  msg.reply({ Data = "Count: "..tostring(count) })

end)

 

Handlers.add("Get", "Get", function (msg)

  msg.reply({ Data = "Count: "..tostring(count) })

end)`

 

describe("Processes and Scheduler", function () {

  let hbeam, hb

  before(async () => {

    hbeam = await new HyperBEAM({ 

      reset: true, 

      as: ["genesis_wasm"]

    }).ready()

    hb = hbeam.hb

  })

  after(async () => hbeam.kill())

 

  it("should spawn a legacynet AOS process", async () => {

    const { process: pid } = await hb.p(

      "/schedule",

      {

        device: "process@1.0",

        type: "Process",

        "data-protocol": "ao",

        variant: "ao.TN.1",

        scheduler: hb.addr,

        "scheduler-location": hb.addr,

        authority: hb.addr,

        "random-seed": seed(16),

        module: "ISShJH1ij-hPPt9St5UFFr_8Ys3Kj5cyg7zrMGt7H9s",

        "scheduler-device": "scheduler@1.0",

        "execution-device": "stack@1.0",

        "device-stack": ["genesis-wasm@1.0", "patch@1.0"],

        "push-device": "push@1.0",

        "patch-from": "/results/outbox",

      },

      { path: false }

    )

 

    await hb.p(

      `/${pid}/schedule`,

      { type: "Message", target: pid, action: "Eval", data },

      { path: false }

    )

 

    await hb.p(

      `/${pid}/schedule`,

      { type: "Message", target: pid, action: "Inc" },

      { path: false }

    )

    const { slot } = await hb.p(

      `/${pid}/schedule`,

      { type: "Message", target: pid, action: "Inc" },

      { path: false }

    )

    const { results } = await hb.g(`/${pid}/compute`, { slot })

    assert.equal("Count: 2", results.outbox["1"].data)

 

    const { body } = await hb.post({

      path: "/~relay@1.0/call",

      method: "POST",

      "relay-path": `http://localhost:6363/dry-run?process-id=${pid}`,

      "content-type": "application/json",

      "relay-body": JSON.stringify({

        Tags: [{ name: "Action", value: "Get" }],

        Owner: hb.addr,

      }),

    })

    assert.equal(JSON.parse(body).Messages[0].Data, "Count: 2")

  })

}) Dryruns HyperBEAM introduces the patch@1.0 device and disables the traditional dryruns for performance reasons, but we can use the call method on the relay@1.0 device to access the http://localhost:6363/dry-run endpoint on the local CU server.File  /test/legacynet-aos.test.js const { body } = await hb.post({

  path: "/~relay@1.0/call",

  method: "POST",

  "relay-path": `http://localhost:6363/dry-run?process-id=${pid}`,

  "content-type": "application/json",

  "relay-body": JSON.stringify({

    Tags: [{ name: "Action", value: "Get" }],

    Owner: addr,

  }),

})

assert.equal(JSON.parse(body).Messages[0].Data, "Count: 2") WAO SDK The HB class has convenient methods for legacynet AOS. To write the same tests:File  /test/legacynet-aos.test.js const { pid } = await hb.spawnLegacy()

await hb.scheduleLegacy({ pid, data })

const { slot } = await hb.scheduleLegacy({ pid, action: "Inc" })

const res = await hb.computeLegacy({ pid, slot })

assert.equal(res.Messages[0].Data, "Count: 1")

const { res: res2 } = await hb.messageLegacy({ pid, action: "Inc" })

assert.equal(res2.Messages[0].Data, "Count: 2")

const res3 = await hb.dryrun({ pid, action: "Get" })

assert.equal(res3.Messages[0].Data, "Count: 2") The AO class makes the code even more concise.File  /test/legacynet-aos.test.js import { AO } from "wao"

 

const ao = await new AO({ module_type: "mainnet", hb: hbeam.url }).init(

  hbeam.jwk

)

const { p } = await ao.deploy({ src_data: data })

await p.m("Inc")

assert.equal(await p.d("Get"), "Count: 1")

await p.m("Inc")

assert.equal(await p.d("Get"), "Count: 2") Running Tests You can find the working test file for this chapter here:legacynet-aos.test.js Run tests:Terminal   Terminal yarn test test/legacynet-aos.test.js References General Building ao Processes AO Cookbook Device Docs Device: ~relay@1.0 Device API dev_stack.erl dev_message.erl dev_process.erl dev_scheduler.erl dev_patch.erl dev_relay.erl dev_genesis_wasm.erl WAO API AO Class API Process Class API HyperBEAM Class API HB Class API

---

# 10. Payment System  WAO

Document Number: 10
Source: https://docs.wao.eco/hyperbeam/payment-system
Words: 2209
Extraction Method: html

faff@1.0 faff@1.0 restricts node access to whitelisted accounts. You can pass a list of allowed accounts to your test HyperBEAM node. It only restricts POST requests. GET still works for all accounts.The node operator can update faff_allow_list via /~meta@1.0/info to manage the allowed accounts.File  /test/payment-system.test.js import assert from "assert"

import { describe, it, before, after } from "node:test"

import { HyperBEAM, acc } from "wao/test"

import HB from "wao"

import { rsaid, hmacid } from "hbsig"

 

describe("Payment System faff@1.0", function () {

  let hbeam, hb, operator

  let allowed_user = acc[0]

  let disallowed_user = acc[1]

 

  before(async () => {

    hbeam = await new HyperBEAM({

      reset: true,

      faff: [HyperBEAM.OPERATOR, allowed_user.addr],

    }).ready()

    operator = hbeam

    allowed_user.hb = new HB({ jwk: allowed_user.jwk })

    disallowed_user.hb = new HB({ jwk: disallowed_user.jwk })

  })

  after(async () => hbeam.kill())

 

  it("should test faff@1.0", async () => {

    const msg = ["/~message@1.0/set/hello", { hello: "world" }]

 

    // GET

    assert(await operator.hb.g(...msg))

    assert(await allowed_user.hb.g(...msg))

    assert(await disallowed_user.hb.g(...msg))

 

    // POST

    assert(await operator.hb.p(...msg))

    assert(await allowed_user.hb.p(...msg))

    await assert.rejects(disallowed_user.hb.p(...msg))

 

    const info = await operator.hb.g("/~meta@1.0/info")

    assert.deepEqual(info.faff_allow_list, [operator.addr, allowed_user.addr])

 

    // remove allowed_user

    await operator.hb.p("/~meta@1.0/info", { faff_allow_list: [operator.addr] })

    const info2 = await operator.hb.g("/~meta@1.0/info")

    assert.deepEqual(info2.faff_allow_list, [operator.addr])

 

    // now previously allowed_user fails too

    await assert.rejects(allowed_user.hb.p(...msg))

  })

}) simple-pay@1.0 simple-pay@1.0 allows you to set the base price for all requests.You can set simple_pay_price and simple_pay=true on your test HyperBEAM node.You also need to explicitly set the payment operator address who can change the payment settings.The node operator can topup users and change the simple_pay_price via /~meta@1.0/info.Users can view their own balances at /~simple-pay@1.0/balance with POST.simple-pay@1.0 uses p4@1.0 underneath, which charges for all POST access except for the endpoints on the p4_non_chargable_routes list.The HyperBEAM SDK automatically puts the following paths onto the p4_non_chargable_routes list, but you can also set it explicitly./~meta@1.0/* /~simple-pay@1.0/topup /~simple-pay@1.0/balance File  /test/payment-system.test.js describe("Payment System simple-pay@1.0", function () {

  let hbeam, hb, operator

  let user = acc[0]

  before(async () => {

    hbeam = await new HyperBEAM({

      reset: true,

      operator: HyperBEAM.OPERATOR,

      simple_pay: true,

      simple_pay_price: 2,

    }).ready()

    operator = hbeam

    user.hb = await new HB({}).init(user.jwk)

  })

  after(async () => hbeam.kill())

 

  it("should test simple-pay@1.0", async () => {

    // cost = simplePayPrice * 3

    const msg = ["/~message@1.0/set/hello", { hello: "world" }]

 

    // balance is non_chargable

    const balance = "/~simple-pay@1.0/balance"

 

    // topup user

    await operator.hb.p("/~simple-pay@1.0/topup", {

      amount: 15,

      recipient: user.addr,

    })

    assert.equal(await user.hb.p(balance), "15")

    assert(await user.hb.p(...msg)) // cost = 2 * 3 = 6

    assert.equal(await user.hb.p(balance), "9")

 

    const info1 = await operator.hb.g("/~meta@1.0/info")

    assert.equal(info1.simple_pay_price, 2)

 

    // change simple_pay_price

    assert(await operator.hb.p("/~meta@1.0/info", { simple_pay_price: 3 }))

 

    const info2 = await operator.hb.g("/~meta@1.0/info")

    assert.equal(info2.simple_pay_price, 3)

 

    assert(await user.hb.p(...msg)) // cost = 3 * 3 = 9

    assert.equal(await user.hb.p(balance), "0")

 

    // this should fail for insufficient fund

    await assert.rejects(user.hb.p(...msg)) // cost = 3 * 3 = 9

  })

}) p4@1.0 p4@1.0 allows you to use Lua scripts with node-process@1.0 to manage node access.p4@1.0 requires a complex setup with a few hacks to test externally with JS code, so we'll go over it step by step.Required Configurations p4@1.0 requires a handful of configurations when starting up a HyperBEAM node with hb:start_mainnet.on: defines hooks p4_non_chargable_routes: defines free-of-charge endpoints node_processes: defines Lua scripts to be executed with hooks Basically, we need to create 2 Lua scripts, one for the processor and the other for the clients, store them on Arweave or a local store to get the message IDs, then pass the IDs to the on settings of the hook device.Caching Lua Scripts There are 3 ways to cache Lua scripts to use with node-process@1.0:upload to the production Arweave storage create a custom device method to internally cache with hb_cache on HyperBEAM create a process and upload with a message We don't want to upload our test scripts to Arweave. So we're going with the 3rd hack since it produces the least conflict and the most flexibility without creating a custom HyperBEAM device. This only works with text-based scripts like Lua due to how messages are processed internally, but won't work for binary scripts like Wasm. For wasm modules for AOS processes, we need to go with the 2nd hack.We need 2 Lua scripts (processor and client) for p4@1.0 to work. For now, we can use the test Lua scripts HyperBEAM uses in their GitHub repo.p4-payment-process.lua --- A ledger that allows account balances to be debited and credited by a

--- specified address.

 

-- Check if the request is a valid debit/credit request by checking if one of

-- the committers is the operator.

local function is_valid_request(base, assignment)

    -- First, validate that the assignment is signed by the scheduler.

    local scheduler = base.scheduler

    local status, res = ao.resolve(assignment, "committers")

    ao.event({

        "assignment committers resp:",

        { status = status, res = res, scheduler = scheduler }

    })

    

    if status ~= "ok" then

        return false

    end

 

    local valid = false

    for _, committer in ipairs(res) do

        if committer == scheduler then

            valid = true

        end

    end

 

    if not valid then

        return false

    end

 

    -- Next, validate that the request is signed by the operator.

    local operator = base.operator

    status, res = ao.resolve(assignment.body, "committers")

    ao.event({

        "request committers resp:",

        { status = status, res = res, operator = operator }

    })

 

    if status ~= "ok" then

        return false

    end

 

    for _, committer in ipairs(res) do

        if committer == operator then

            return true

        end

    end

 

    return false

end

 

-- Debit the specified account by the given amount.

function debit(base, assignment)

    ao.event({ "process debit starting", { assignment = assignment } })

    if not is_valid_request(base, assignment) then

        base.result = { status = "error", error = "Operator signature required." }

        ao.event({ "debit error", base.result })

        return "ok", base

    end

    ao.event({ "process debit valid", { assignment = assignment } })

    base.balance = base.balance or {}

    base.balance[assignment.body.account] =

        (base.balance[assignment.body.account] or 0) - assignment.body.quantity

    

    ao.event({ "process debit success", { balances = base.balance } })

    return "ok", base

end

 

-- Credit the specified account by the given amount.

_G["credit-notice"] = function (base, assignment)

    ao.event({ "credit-notice", { assignment = assignment }, { balances = base.balance } })

    if not is_valid_request(base, assignment) then

        base.result = { status = "error", error = "Operator signature required." }

        return "ok", base

    end

    ao.event({ "is valid", { req = assignment.body } })

    base.balance = base.balance or {}

    base.balance[assignment.body.recipient] =

        (base.balance[assignment.body.recipient] or 0) + assignment.body.quantity

    ao.event({ "credit", { ["new balances"] = base.balance } })

    return "ok", base

end

 

--- Index function, called by the `~process@1.0` device for scheduled messages.

--- We route each to the appropriate function based on the request path.

function compute(base, assignment, opts)

    ao.event({ "compute", { assignment = assignment }, { balances = base.balance } })

    if assignment.body.path == "debit" then

        return debit(base, assignment.body)

    elseif assignment.body.path == "credit-notice" then

        return _G["credit-notice"](base, assignment.body)

    elseif assignment.body.path == "balance" then

        return balance(base, assignment.body)

    elseif assignment.slot == 0 then

        base.balance = base.balance or {}

        return "ok", base

    end

end p4-payment-client.lua --- A simple script that can be used as a `~p4@1.0` ledger device, marshalling

--- requests to a local process.

 

-- Find the user's balance in the current ledger state.

function balance(base, request)

    local status, res = ao.resolve({

        path =

            base["ledger-path"]

            .. "/now/balance/"

            .. request["target"]

    })

    ao.event({ "client received balance response", 

        { status = status, res = res, target = request["target"] } }

    )

    -- If the balance request fails (most likely because the user has no balance),

    -- return a balance of 0.

    if status ~= "ok" then

        return "ok", 0

    end

 

    -- We have successfully retrieved the balance, so return it.

    return "ok", res

end

 

-- Debit the user's balance in the current ledger state.

function debit(base, request)

    ao.event({ "client starting debit", { request = request, base = base } })

    local status, res = ao.resolve({

        path = "(" .. base["ledger-path"] .. ")/schedule",

        method = "POST",

        body = request

    })

    ao.event({ "client received schedule response", { status = status, res = res } })

    status, res = ao.resolve({

        path = base["ledger-path"] .. "/compute/balance/" .. request["account"],

        slot = res.slot

    })

    ao.event({ "confirmed balance", { status = status, res = res } })

    return "ok"

end

 

--- Poll an external ledger for credit events. If new credit noticess have been

--- sent by the external ledger, push them to the local ledger.

function poll(base, req)

    local status, local_last_credit = ao.resolve({

        path = base["ledger-path"] .. "/now/last-credit"

    })

    if status ~= "ok" then

        ao.event(

            { "error getting local last credit",

                { status = status, res = local_last_credit } }

        )

        return "error", base

    end

 

    local status, external_last_credit = ao.resolve({

        path = base["external-ledger"] .. "/now/last-credit"

    })

    if status ~= "ok" then

        ao.event({ "error getting external last credit",

            { status = status, res = external_last_credit } })

        return "error", base

    end

 

    ao.event({ "Retreived sync data. Last credit info:",

        {

            local_last_credit = local_last_credit,

            external_last_credit = external_last_credit }

        }

    )

    while local_last_credit < external_last_credit do

        status, res = ao.resolve({

            path = base["external-ledger"] .. "/push",

            slot = local_last_credit + 1

        })

        if status ~= "ok" then

            ao.event({ "error pushing slot", { status = status, res = res } })

            return "error", base

        end

        local_last_credit = local_last_credit + 1

    end

 

    return "ok", base

end You can spawn a process and upload these 2 scripts as messages.schedule only returns slot and not the message ID. You can get message IDs via /~scheduler@1.0/schedule, which returns a list of scheduled messages for the target process and contains IDs.File  /test/payment-system.test.js const process = hbeam.file("scripts/p4-payment-process.lua")

const { pid: cache_pid } = await hb.spawn({})

const { slot } = await hb.schedule({

  pid: cache_pid,

  data: process,

  "content-type": "application/lua",

})

const { body } = await hb.g("/~scheduler@1.0/schedule", {

  target: cache_pid,

  from: slot,

  accept: "application/aos-2",

})

const {

  edges: [msg],

} = JSON.parse(body)

const pid = msg.node.message.Id

assert(pid)

const client = hbeam.file("scripts/p4-payment-client.lua")

const { slot: slot2 } = await hb.schedule({

  pid: cache_pid,

  data: client,

  "content-type": "application/lua",

})

const { body: body2 } = await hb.g("/~scheduler@1.0/schedule", {

  target: cache_pid,

  from: slot2,

  accept: "application/aos-2",

})

const {

  edges: [msg2],

} = JSON.parse(body2)

const cid = msg2.node.message.Id

assert(cid) HB has a convenient method for /~scheduler@1.0/schedule.File  /test/payment-system.test.js const msgs = await this.messages({ pid, from: slot, to: slot })

const pid = msgs.edges[0].node.message.Id

 

const msgs2 = await this.messages({ pid, from: slot2, to: slot2 })

const cid = msgs2.edges[0].node.message.Id Indeed, it has a convenient method to cache scripts.File  /test/payment-system.test.js const process = readFileSync(`${hb_dir}/scripts/p4-payment-process.lua`)

const pid = await hb.cacheScript(process)

 

const client = readFileSync(`${hb_dir}/scripts/p4-payment-client.lua`)

const cid = await hb.cacheScript(client) Starting Another Node with p4@1.0 Once you get script IDs, you need to start another HyperBEAM node with the same store configurations. We can do this by simply instantiating another HyperBEAM with a different port, and without clearing the cache storage.For p4@1.0 to work, we need to pass the payment operator address and p4_lua, and the complex settings will be handled for you.File  /test/payment-system.test.js // comment out reset to use the same store where we cached Lua scripts

const hbeam2 = await new HyperBEAM({

  //reset: true,    

  port: 10002,

  operator: addr,

  p4_lua: { processor: pid, client: cid },

}).ready() Now we have a new HyperBEAM node with the p4@1.0 Lua scripts running at http://localhost:10002.Let's set up 2 new HyperBEAM clients for the operator and a new user for the new node.File  /test/payment-system.test.js const operator = hbeam2

const user = acc[0]

user.hb = await new HB({ url: hbeam2.url }).init(user.jwk) Now to topup the user account, we need to send credit-notice to the Lua script with the operator account. But this gets extra tricky since HyperBEAM first verifies the whole message sent to the node, then the Lua script extracts and verifies a nested message placed in body. So we need to somehow create a signed message with the correct commitment format internally used in HyperBEAM, then wrap that message in body and sign the parent message again. This is indeed extra complex, and the crux of this tutorial series where you need to put everything you learned so far together.Create Commitments If you recall from the previous chapter, HyperBEAM internally signs and creates 2 commitments with sha256 hash of the signature and hmac-sha256 hash of the signed content. We can first construct this internal message to be passed to the Lua script.File  /test/payment-system.test.js const obj = {

  path: "credit-notice",

  quantity: 100,

  recipient: user.addr,

}

const lua_msg = await operator.hb.sign(obj) Now we got a signed message with path included in signature-input.We can get the 2 hashes required to construct commitments.Now, we can construct commitments and the wrapped message.WAO has, of course, a convenient method to create commitment.const committed_lua_msg = await operator.hb.commit(obj, { path: true }) Finally, we can send it to /ledger~node-process@1.0/schedule.File  /test/payment-system.test.js await operator.hb.post({

  path: "/ledger~node-process@1.0/schedule",

  body: committed_lua_msg,

}) Let's check the balance of the user.File  /test/payment-system.test.js const { out: balance } = await operator.hb.get({

    path: `/ledger~node-process@1.0/now/balance/${user.addr}`,

})

assert.equal(balance, 100) Now try executing some messages with the user.File  /test/payment-system.test.js // this costs 3

const hello = { path: "/~message@1.0/set/hello", hello: "world" }

assert(await user.hb.post(hello))

 

const { out: balance2 } = await operator.hb.get({

    path: `/ledger~node-process@1.0/now/balance/${user.addr}`,

})

assert.equal(balance2, 97) It works!!Congratulations on having come this far!
The p4@1.0 payment system with internal Lua scripts using node-processes@1.0 is one of the most advanced usages of HyperBEAM to be tested externally. If you got this to work, most other things are less complex, so you should be ready to build anything on top of HyperBEAM now.Running Tests You can find the working test file for this chapter here:payment-system.test.js Run tests:Terminal   Terminal yarn test test/payment-system.test.js References Device Docs Device: ~lua@5.3a Device API dev_faff.erl dev_simple_pay.erl dev_p4.erl dev_node_process.erl dev_lua.erl WAO API HyperBEAM Class API HB Class API HBSig API

---

# 11. Processes and Scheduler  WAO

Document Number: 11
Source: https://docs.wao.eco/hyperbeam/processes-scheduler
Words: 1042
Extraction Method: html

Most things we've learned so far to do manually, such as device method composition, message caching, and commitments for message verification, are handled automatically by message@1.0, process@1.0, and scheduler@1.0.You can spawn a process, schedule messages to process slots, and compute the process state going through the allocated messages using multiple execution devices with stack@1.0.Let's create a new custom device dev_inc.erl. The minimum viable device to be compatible with process@1.0 requires 4 methods:init, normalize, compute, and snapshot.File  /HyperBEAM/src/dev_inc.erl -module(dev_inc).

-export([ compute/3, init/3, snapshot/3, normalize/3 ]).

-include_lib("eunit/include/eunit.hrl").

-include("include/hb.hrl").

 

compute(Msg1, Msg2, Opts) ->

  Num = maps:get(<<"num">>, Msg1),

  {ok, hb_ao:set( Msg1, #{ <<"num">> => Num + 1 }, Opts )}.

 

init(Msg, Msg2, Opts) -> 

  {ok, hb_ao:set(Msg, #{ <<"num">> => 0 }, Opts)}.

 

snapshot(Msg, _Msg2, _Opts) -> {ok, Msg}.

 

normalize(Msg, _Msg2, _Opts) -> {ok, Msg}.Don't forget to add it to preloaded_devices in hb_opts.erl.File  /HyperBEAM/src/hb_opts.erl preloaded_devices => [

  ...

  #{ <<"name">> => <<"inc@1.0">>, <<"module">> => dev_inc }

],Add message@1.0, process@1.0, scheduler@1.0, and inc@1.0 to our HyperBEAM node in the test file.You'll also need a random seed generation function, since message IDs are deterministic content hashes and it generates the same message ID if you send the same initialization message.To avoid this, you need to include a randomized value in the message content to spawn a new process.Also you need to nest the message in body without specifying path in the inner message.You can use the following paths to spawn, schedule, and compute:/~process@1.0/schedule: to spawn a process, requires scheduler=[operator_wallet_address] body device="process@1.0" scheduler=[operator_wallet_address] type="Process" execution-device /[pid]/schedule: to schedule a message to the pid body type="Message" /[pid]/compute?slot=[slot]: to compute the pid state up to the slot pid is the process message ID returned by the spawn message.At this point, you could simply remove the devices parameter and preload all existing devices in our test file.File  /test/processes-scheduler.test.js import assert from "assert"

import { describe, it, before, after } from "node:test"

import { HyperBEAM } from "wao/test"

 

const seed = num => {

  const array = new Array(num)

  for (let i = 0; i < num; i++) array[i] = Math.floor(Math.random() * 256)

  return Buffer.from(array).toString("base64")

}

 

describe("Processes and Scheduler", function () {

  let hbeam, hb

  before(async () => {

    hbeam = await new HyperBEAM({ reset: true }).ready()

    hb = hbeam.hb

  })

  after(async () => hbeam.kill())

 

  it("should spawn a process", async () => {

    const { process: pid } = await hb.p("/~process@1.0/schedule", {

      scheduler: hb.addr,

      body: {

        device: "process@1.0",

        type: "Process",

        scheduler: hb.addr,

        "random-seed": seed(16),

        "execution-device": "inc@1.0",

      },

    })

    console.log(`Process ID: ${pid}`)

    const { slot } = await hb.p(`/${pid}/schedule`, {

      body: { type: "Message" },

    })

    console.log(`Allocated Slot: ${slot}`)

 

    const out = await hb.g(`/${pid}/compute`, { slot })

    assert.equal(out.num, 2)

 

    const { slot: slot2 } = await hb.p(`/${pid}/schedule`, {

      body: { type: "Message" },

    })

    console.log(`Allocated Slot: ${slot2}`)

 

    const out2 = await hb.g(`/${pid}/compute`, { slot: slot2 })

    assert.equal(out2.num, 3)

  })

}) now /[pid]/now gives you the latest process state.File  /test/processes-scheduler.test.js const { slot: slot3 } = await hb.p(`/${pid}/schedule`, {

  body: { type: "Message" },

})

console.log(`Allocated Slot: ${slot3}`)

 

const out3 = await hb.g(`/${pid}/now`)

assert.equal(out3.num, 4) WAO SDK WAO has convenient APIs for process management.File  /test/processes-scheduler.test.js const { pid } = await hb.spawn({ "execution-device": "inc@1.0" })

const { slot } = await hb.schedule({ pid })

const { num } = await hb.compute({ pid, slot })

assert.equal(num, 2)

 

const {

  res: { num: num2 },

} = await hb.message({ pid }) // schedule + compute

assert.equal(num2, 3)

 

const { num: num3 } = await hb.now({ pid })

assert.equal(num3, 3) stack@1.0 Just like the previous chapter, you can stack multiple devices and let the state transition go through each compute method.Let's create double@1.0 and square@1.0 devices.File  /HyperBEAM/src/dev_double.erl -module(dev_double).

-export([ compute/3, init/3, snapshot/3, normalize/3 ]).

-include_lib("eunit/include/eunit.hrl").

-include("include/hb.hrl").

 

compute(Msg1, Msg2, Opts) ->

  Num = maps:get(<<"num">>, Msg1),

  {ok, hb_ao:set( Msg1, #{ <<"num">> => Num * 2 }, Opts )}.

 

init(Msg, Msg2, Opts) -> 

  {ok, hb_ao:set(Msg, #{ <<"num">> => 0 }, Opts)}.

 

snapshot(Msg, _Msg2, _Opts) -> {ok, Msg}.

 

normalize(Msg, _Msg2, _Opts) -> {ok, Msg}.File  /HyperBEAM/src/dev_square.erl -module(dev_square).

-export([ compute/3, init/3, snapshot/3, normalize/3 ]).

-include_lib("eunit/include/eunit.hrl").

-include("include/hb.hrl").

 

compute(Msg1, Msg2, Opts) ->

  Num = maps:get(<<"num">>, Msg1),

  {ok, hb_ao:set( Msg1, #{ <<"num">> => Num * Num }, Opts )}.

 

init(Msg, Msg2, Opts) -> 

  {ok, hb_ao:set(Msg, #{ <<"num">> => 0 }, Opts)}.

 

snapshot(Msg, _Msg2, _Opts) -> {ok, Msg}.

 

normalize(Msg, _Msg2, _Opts) -> {ok, Msg}.Don't forget to add double@1.0 and square@1.0 to hb_opts.erl.Then test the stack@1.0 process with multiple devices.File  /test/processes-scheduler.test.js const { pid } = await hb.spawn({

  "execution-device": "stack@1.0",

  "device-stack": ["inc@1.0", "double@1.0", "square@1.0"],

})

 

const { num } = await hb.now({ pid })

assert.equal(num, 4) // ((0 + 1) * 2) * ((0 + 1) * 2)

 

const { res: { num: num2 } } = await hb.message({ pid })

assert.equal(num2, 100) // ((4 + 1) * 2) * ((4 + 1) * 2) patch@1.0 patch@1.0 allows you to cache any pieces of data to arbitrary URLs. You can pass patch-from to specify where the data to patch comes from in the resulting messages, and patch-to to specify a URL endpoint to expand the cache to. So let's set patch-from="/results" and patch-to="/cache".First, let's create a modified version of the inc device and call it inc2@1.0, which increments num and caches double and square values of num. We return these caches under results:1.File  /HyperBEAM/src/dev_inc2.erl -module(dev_inc2).

-export([ compute/3, init/3, snapshot/3, normalize/3 ]).

-include_lib("eunit/include/eunit.hrl").

-include("include/hb.hrl").

 

compute(Msg1, Msg2, Opts) ->

  Num = maps:get(<<"num">>, Msg1) + 1,

  {ok, hb_ao:set( 

    Msg1,

    #{ 

      <<"num">> => Num,

      <<"results">> => #{ 

        <<"1">> => #{ 

          <<"method">> => <<"PATCH">>, 

          <<"double">> => Num * 2,

          <<"square">> => Num * Num 

        }

      } 

    }, 

    Opts

  )}.

 

init(Msg, Msg2, Opts) -> 

  {ok, hb_ao:set(Msg, #{ <<"num">> => 0 }, Opts)}.

 

snapshot(Msg, _Msg2, _Opts) -> {ok, Msg}.

 

normalize(Msg, _Msg2, _Opts) -> {ok, Msg}.Now /now/cache/double and /now/cache/square will be accessible with the cached latest values.File  /test/processes-scheduler.test.js const { pid } = await hb.spawn({

  "execution-device": "stack@1.0",

  "device-stack": ["inc2@1.0", "patch@1.0"],

  "patch-from": "/results",

  "patch-to": "/cache",

})

await hb.schedule({ pid })

await hb.schedule({ pid })

const square = (await hb.now({ pid, path: "/cache/square" })).body

const double = (await hb.now({ pid, path: "/cache/double" })).body

assert.equal(square, 9)

assert.equal(double, 6) Running Tests You can find the working test file for this chapter here:processes-scheduler.test.js Run tests:Terminal   Terminal yarn test test/processes-scheduler.test.js References Device Docs Device: ~message@1.0 Device: ~process@1.0 Device: ~scheduler@1.0 Device API dev_stack.erl dev_message.erl dev_process.erl dev_scheduler.erl dev_patch.erl WAO API HyperBEAM Class API HB Class API

---

# 12. wasm6410 - HyperBEAM - Documentation

Document Number: 12
Source: https://hyperbeam.arweave.net/build/devices/wasm64-at-1-0.html
Words: 538
Extraction Method: html

Device: ~wasm64@1.0 Overview The ~wasm64@1.0 device enables the execution of 64-bit WebAssembly (WASM) code within the HyperBEAM environment. It provides a sandboxed environment for running compiled code from various languages (like Rust, C++, Go) that target WASM.Core Concept: WASM Execution This device allows AO processes to perform complex computations defined in WASM modules, which can be written in languages like Rust, C++, C, Go, etc., and compiled to WASM.The device manages the lifecycle of a WASM instance associated with the process state.Key Functions (Keys) These keys are typically used within an execution stack (managed by dev_stack) for an AO process.init Action: Initializes the WASM environment for the process. It locates the WASM image (binary), starts a WAMR instance, and stores the instance handle and helper functions (for reading/writing WASM memory) in the process's private state (priv/...).Inputs (Expected in Process Definition or init Message):[Prefix]/image: The Arweave Transaction ID of the WASM binary, or the WASM binary itself, or a message containing the WASM binary in its body.[Prefix]/Mode: (Optional) Specifies execution mode (WASM (default) or AOT if allowed by node config).Outputs (Stored in priv/):[Prefix]/instance: The handle to the running WAMR instance.[Prefix]/write: A function to write data into the WASM instance's memory.[Prefix]/read: A function to read data from the WASM instance's memory.[Prefix]/import-resolver: A function used to handle calls from the WASM module back to the AO environment (imports).compute Action: Executes a function within the initialized WASM instance. It retrieves the target function name and parameters from the incoming message or process definition and calls the WASM instance via hb_beamr.Inputs (Expected in Process State or Incoming Message):priv/[Prefix]/instance: The handle obtained during init.function or body/function: The name of the WASM function to call.parameters or body/parameters: A list of parameters to pass to the WASM function.Outputs (Stored in results/):results/[Prefix]/type: The result type returned by the WASM function.results/[Prefix]/output: The actual result value returned by the WASM function.import Action: Handles calls originating from the WASM module (imports). The default implementation (default_import_resolver) resolves these calls by treating them as sub-calls within the AO environment, allowing WASM code to invoke other AO device functions or access process state via the hb_ao:resolve mechanism.Inputs (Provided by hb_beamr): Module name, function name, arguments, signature.Response: Returns the result of the resolved AO call back to the WASM instance.snapshot Action: Captures the current memory state of the running WASM instance. This is used for checkpointing and restoring process state.Inputs:priv/[Prefix]/instance.Outputs: A message containing the raw binary snapshot of the WASM memory state, typically tagged with [Prefix]/State.normalize (Internal Helper) Action: Ensures a consistent state representation for computation, primarily by loading a WASM instance from a snapshot ([Prefix]/State) if a live instance (priv/[Prefix]/instance) isn't already present. This allows resuming execution from a cached state.terminate Action: Stops and cleans up the running WASM instance associated with the process.Inputs:priv/[Prefix]/instance.Usage within dev_stack The ~wasm64@1.0 device is almost always used as part of an execution stack configured in the Process Definition Message and managed by dev_stack. dev_stack ensures that init is called on the first pass, compute on subsequent passes, and potentially snapshot or terminate as needed.# Example Process Definition Snippet
Execution-Device: [`stack@1.0`](./source-code/dev_stack.md)
Execution-Stack: "[`scheduler@1.0`](./source-code/dev_scheduler.md)", "wasm64@1.0"
WASM-Image: <WASMImageTxID> This setup allows AO processes to leverage the computational power and language flexibility offered by WebAssembly in a decentralized, verifiable manner.wasm module

---

# 13. useWayfinder - ARIO Docs

Document Number: 13
Source: https://docs.ar.io/wayfinder/react/use-wayfinder
Words: 891
Extraction Method: html

Overview The useWayfinder hook provides access to the complete Wayfinder instance from the React context. This hook gives you full control over all Wayfinder methods and is ideal for advanced usage scenarios where you need access to multiple Wayfinder capabilities.Signature Usage Basic Usage Advanced Usage with Event Listeners import { useWayfinder } from '@ar.io/wayfinder-react'
import { useEffect, useState } from 'react'

function AdvancedComponent() {
  const { wayfinder } = useWayfinder()
  const [events, setEvents] = useState([])

  useEffect(() => {
    // Listen to routing events
    const handleRoutingSuccess = (event) => {
      setEvents((prev) => [
        ...prev,
        {
          type: 'routing-success',
          data: event,
          timestamp: Date.now(),
        },
      ])
    }

    const handleRoutingFailed = (error) => {
      setEvents((prev) => [
        ...prev,
        {
          type: 'routing-failed',
          data: error,
          timestamp: Date.now(),
        },
      ])
    }

    const handleVerificationSuccess = (event) => {
      setEvents((prev) => [
        ...prev,
        {
          type: 'verification-success',
          data: event,
          timestamp: Date.now(),
        },
      ])
    }

    // Subscribe to events
    wayfinder.emitter.on('routing-succeeded', handleRoutingSuccess)
    wayfinder.emitter.on('routing-failed', handleRoutingFailed)
    wayfinder.emitter.on('verification-succeeded', handleVerificationSuccess)

    // Cleanup
    return () => {
      wayfinder.emitter.off('routing-succeeded', handleRoutingSuccess)
      wayfinder.emitter.off('routing-failed', handleRoutingFailed)
      wayfinder.emitter.off('verification-succeeded', handleVerificationSuccess)
    }
  }, [wayfinder])

  return (
    <div>
      <h3>Wayfinder Events</h3>
      <ul>
        {events.map((event, index) => (
          <li key={index}>
            {event.type} - {new Date(event.timestamp).toLocaleTimeString()}
          </li>
        ))}
      </ul>
    </div>
  )
} Custom Request with Overrides import { useWayfinder } from '@ar.io/wayfinder-react'
import { useState } from 'react'
import {
  StaticRoutingStrategy,
  HashVerificationStrategy,
} from '@ar.io/wayfinder-core'

function CustomRequestComponent({ txId }) {
  const { wayfinder } = useWayfinder()
  const [data, setData] = useState(null)
  const [loading, setLoading] = useState(false)
  const [error, setError] = useState(null)

  const fetchWithCustomSettings = async () => {
    setLoading(true)
    setError(null)

    try {
      // Override routing and verification for this specific request
      const response = await wayfinder.request(`ar://${txId}`, {
        routingSettings: {
          strategy: new StaticRoutingStrategy({
            gateway: 'https://arweave.net',
          }),
        },
        verificationSettings: {
          enabled: true,
          strict: true,
          strategy: new HashVerificationStrategy({
            trustedGateways: ['https://arweave.net'],
          }),
        },
      })

      const text = await response.text()
      setData(text)
    } catch (err) {
      setError(err)
    } finally {
      setLoading(false)
    }
  }

  return (
    <div>
      <button onClick={fetchWithCustomSettings} disabled={loading}>
        {loading
          ? 'Fetching with custom settings...'
          : 'Fetch with Custom Settings'}
      </button>

      {error && <div className="error">Error: {error.message}</div>}
      {data && <pre>{data}</pre>}
    </div>
  )
} Provider Context Error The hook throws an error if used outside of a WayfinderProvider:Proper Error Handling import { useWayfinder } from '@ar.io/wayfinder-react'
import { useState, useCallback } from 'react'

function RobustComponent() {
  const { wayfinder } = useWayfinder()
  const [error, setError] = useState(null)
  const [data, setData] = useState(null)
  const [loading, setLoading] = useState(false)

  const handleRequest = useCallback(
    async (txId) => {
      setLoading(true)
      setError(null)

      try {
        const response = await wayfinder.request(`ar://${txId}`)
        const result = await response.text()
        setData(result)
      } catch (err) {
        setError(err)

        // Log different error types
        if (err.name === 'TimeoutError') {
          console.error('Request timed out')
        } else if (err.name === 'VerificationError') {
          console.error('Data verification failed')
        } else if (err.name === 'NetworkError') {
          console.error('Network error occurred')
        } else {
          console.error('Unknown error:', err)
        }
      } finally {
        setLoading(false)
      }
    },
    [wayfinder],
  )

  const clearError = useCallback(() => {
    setError(null)
  }, [])

  return (
    <div>
      {error && (
        <div className="error-banner">
          <p>Error: {error.message}</p>
          <button onClick={clearError}>Dismiss</button>
          <button onClick={() => handleRequest('retry')}>Retry</button>
        </div>
      )}

      {loading && <div>Loading...</div>}
      {data && <pre>{data}</pre>}
    </div>
  )
} Performance Considerations Memoization The Wayfinder instance is automatically memoized in the provider, but you should memoize callbacks that use it:Event Listener Cleanup Always clean up event listeners to prevent memory leaks:TypeScript Support Typed Usage import { useWayfinder } from '@ar.io/wayfinder-react'
import { WayfinderContextValue } from '@ar.io/wayfinder-react'
import { Wayfinder, WayfinderEvent } from '@ar.io/wayfinder-core'

interface ComponentProps {
  txId: string
  onSuccess?: (data: string) => void
  onError?: (error: Error) => void
}

const TypedComponent: React.FC<ComponentProps> = ({
  txId,
  onSuccess,
  onError,
}) => {
  const context: WayfinderContextValue = useWayfinder()
  const wayfinder: Wayfinder = context.wayfinder

  const handleFetch = async (): Promise<void> => {
    try {
      const response = await wayfinder.request(`ar://${txId}`)
      const data = await response.text()
      onSuccess?.(data)
    } catch (error) {
      onError?.(error as Error)
    }
  }

  // Type-safe event handling
  const handleRoutingEvent = (
    event: WayfinderEvent['routing-succeeded'],
  ): void => {
    console.log('Selected gateway:', event.selectedGateway)
  }

  return <button onClick={handleFetch}>Fetch Data</button>
} Custom Hook with TypeScript import { useWayfinder } from '@ar.io/wayfinder-react'
import { useState, useCallback } from 'react'

interface UseWayfinderDataResult {
  data: string | null
  loading: boolean
  error: Error | null
  fetchData: (txId: string) => Promise<void>
  clearData: () => void
}

function useWayfinderData(): UseWayfinderDataResult {
  const { wayfinder } = useWayfinder()
  const [data, setData] = useState<string | null>(null)
  const [loading, setLoading] = useState<boolean>(false)
  const [error, setError] = useState<Error | null>(null)

  const fetchData = useCallback(
    async (txId: string): Promise<void> => {
      setLoading(true)
      setError(null)

      try {
        const response = await wayfinder.request(`ar://${txId}`)
        const result = await response.text()
        setData(result)
      } catch (err) {
        setError(err as Error)
      } finally {
        setLoading(false)
      }
    },
    [wayfinder],
  )

  const clearData = useCallback((): void => {
    setData(null)
    setError(null)
  }, [])

  return {
    data,
    loading,
    error,
    fetchData,
    clearData,
  }
}

// Usage
function MyComponent() {
  const { data, loading, error, fetchData, clearData } = useWayfinderData()

  return (
    <div>
      <button onClick={() => fetchData('transaction-id')}>Fetch</button>
      <button onClick={clearData}>Clear</button>
      {loading && <div>Loading...</div>}
      {error && <div>Error: {error.message}</div>}
      {data && <pre>{data}</pre>}
    </div>
  )
} Testing Mocking the Hook When to Use Use useWayfinder when you need:Full Wayfinder API access: Access to all methods like request(), resolveUrl(), and event emitters Event monitoring: Listening to routing, verification, or other Wayfinder events Custom request configurations: Overriding routing or verification settings per request Advanced integrations: Building complex components that need multiple Wayfinder capabilities Custom abstractions: Creating your own hooks or utilities that wrap Wayfinder functionality For simpler use cases, consider:useWayfinderRequest for basic data fetching useWayfinderUrl for URL resolution with loading states

---

# 14. Lua Serverless Functions  Cooking with the Permaweb

Document Number: 14
Source: https://cookbook.arweave.net/fundamentals/decentralized-computing/hyperbeam/lua-serverless.html
Words: 541
Extraction Method: html

Lua Serverless Functions HyperBEAM's ~lua@5.3a device enables you to run serverless Lua functions with permanent availability, instant execution, and cryptographic verification. Unlike traditional serverless platforms, your functions are deployed permanently to Arweave and executed on the decentralized HyperBEAM network.How HyperBEAM Lua Works HyperBEAM executes Lua functions through HTTP requests using a specific URL structure:https://forward.computer/~lua@5.3a/{function_name}/~json@1.0/serialize?param1=value1&module={ARWEAVE_TX_ID} URL Components:~lua@5.3a - The Lua execution device {function_name} - The function to call from your deployed module ~json@1.0/serialize - Output formatting set to JSON Query parameters - Function arguments with optional type casting module - Arweave transaction ID of your deployed Lua code Function Parameters HyperBEAM calls your Lua functions with three parameters:Example Functions 1. Simple Calculator -- calculator.lua
function add(base, req, opts)
    base = base or {}
    
    local a = tonumber(req.a) or 0
    local b = tonumber(req.b) or 0
    
    base.a = a
    base.b = b
    base.result = a + b
    base.operation = "addition"
    
    return base
end 2. Text Analysis function analyze_text(base, req, opts)
    base = base or {}
    
    local text = req.text or ""
    if text == "" then
        base.error = "Text parameter required"
        return base
    end
    
    -- Word count
    local word_count = 0
    for word in string.gmatch(text, "%S+") do
        word_count = word_count + 1
    end
    
    -- Character counts
    local char_count = string.len(text)
    local char_count_no_spaces = string.len(string.gsub(text, "%s", ""))
    
    base.text_length = char_count
    base.text_length_no_spaces = char_count_no_spaces
    base.word_count = word_count
    
    return base
end 3. Data Processing function process_numbers(base, req, opts)
    base = base or {}
    
    local numbers_str = req.numbers or ""
    local numbers = {}
    
    -- Parse comma-separated numbers
    for num_str in string.gmatch(numbers_str, "[^,]+") do
        local num = tonumber(num_str:gsub("^%s*(.-)%s*$", "%1"))
        if num then
            table.insert(numbers, num)
        end
    end
    
    if #numbers == 0 then
        base.error = "No valid numbers found"
        return base
    end
    
    -- Calculate basic statistics
    local sum = 0
    local min_val = numbers[1]
    local max_val = numbers[1]
    
    for i, num in ipairs(numbers) do
        sum = sum + num
        if num < min_val then min_val = num end
        if num > max_val then max_val = num end
    end
    
    base.count = #numbers
    base.sum = sum
    base.average = sum / #numbers
    base.min = min_val
    base.max = max_val
    base.numbers = numbers
    
    return base
end Deployment 1. Deploy to Arweave Propagation Delay After deployment, it can take up to 20 minutes for your Lua module to fully propagate across the Arweave network. You may not be able to access it via HyperBEAM until propagation is complete.2. Execute Your Function # Simple addition
curl 'https://forward.computer/~lua@5.3a/add/~json@1.0/serialize?a+integer=10&b+integer=20&module=YOUR_MODULE_ID'

# Text analysis
curl 'https://forward.computer/~lua@5.3a/analyze_text/~json@1.0/serialize?text=Hello%20world%20example&module=YOUR_MODULE_ID'

# Number processing
curl 'https://forward.computer/~lua@5.3a/process_numbers/~json@1.0/serialize?numbers=1,2,3,4,5&module=YOUR_MODULE_ID' Type Casting Parameters Use type casting to convert parameters from strings:Available Type Casts:+integer - Convert to integer +float - Convert to floating point number +boolean - Convert to boolean +list - Convert comma-separated values to array Example:curl 'https://forward.computer/~lua@5.3a/add/~json@1.0/serialize?a+integer=10&b+integer=20&module=YOUR_MODULE_ID' JavaScript Integration async function callLuaFunction(moduleId, functionName, params = {}) {
    const queryParams = new URLSearchParams();
    queryParams.append('module', moduleId);
    
    for (const [key, value] of Object.entries(params)) {
        if (typeof value === 'number') {
            queryParams.append(`${key}+${Number.isInteger(value) ? 'integer' : 'float'}`, value.toString());
        } else {
            queryParams.append(key, value.toString());
        }
    }
    
    const url = `https://forward.computer/~lua@5.3a/${functionName}/~json@1.0/serialize?${queryParams}`;
    const response = await fetch(url);
    
    if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
    }
    
    return await response.json();
}

// Usage
const result = await callLuaFunction('YOUR_MODULE_ID', 'add', { a: 10, b: 20 });
console.log(result); // { a: 10, b: 20, result: 30, operation: "addition" }

---

# 15. Create React App Starter Kit  Cooking with the Permaweb

Document Number: 15
Source: https://cookbook.arweave.net/kits/react/create-react-app.html
Words: 701
Extraction Method: html

Create React App Starter Kit This guide will walk you through in a step by step flow to configure your development environment to build and deploy a permaweb react application.Prerequisites Basic Typescript Knowledge (Not Mandatory) - [https://www.typescriptlang.org/docs/](Learn Typescript) NodeJS v16.15.0 or greater - [https://nodejs.org/en/download/](Download NodeJS) Knowledge of ReactJS - [https://reactjs.org/](Learn ReactJS) Know git and common terminal commands Development Dependencies TypeScript NPM or Yarn Package Manager Steps Create Project If you are not familiar with typescript you can exclude the extra check --template typescript npx create-react-app permaweb-create-react-app --template typescript yarn create react-app permaweb-create-react-app --template typescript Change into the Project Directory cd permaweb-create-react-app Install react-router-dom You have to install this package to manage routing between different pages npm install react-router-dom --save yarn add react-router-dom -D Run the App Now we need to check if everything is working before jumping into next step, run npm start yarn start This will start a new development server locally on your machine. By default it uses `PORT 3000`, if this PORT is already in use it may ask you to switch to another available PORT in Terminal Modify the package.json to contain the following config Setup Routing Now modify the application and add a new route such as an about page, first create 2 more.tsx files. (if you have exluceded the extra check --template typescript, then your component file extension should be .jsx or .js) HomePage.tsx About.tsx import { Link } from "react-router-dom";

function About() {
    return (
    <div>
            Welcome to the About page!
            <Link to={"/"}>
                <div>Home</div>
            </Link>
        </div>
    );
}

export default About;Modify App.tsx We need to update the App.tsx to manage the different pages Hash Routing Note that we are wrapping the routes in a HashRouter and using the react-router-dom Link component to build links. This is important on the permaweb in its current state, it will ensure the routes work properly because applications are served on a path like https://[gateway]/[TX] Deploy Permanently Generate Wallet Existing Wallet This step will generate a new, empty, Arweave wallet. If you already have an existing Arweave wallet you may provide its keyfile and skip this step.We need the arweave package to generate a wallet npm install --save arweave yarn add arweave -D then run this command in the terminal node -e "require('arweave').init({}).wallets.generate().then(JSON.stringify).then(console.log.bind(console))" > wallet.json It is very important to make sure that your wallet file is not included in any folder you want uploaded to Arweave.Setup Turbo We need Turbo to deploy our app to the Permaweb.Fund Wallet You will need to fund your wallet with ArDrive Turbo credits. To do this, enter ArDrive and import your wallet. Then, you can purchase turbo credits for your wallet.Setup Permaweb-Deploy npm install --global permaweb-deploy yarn global add permaweb-deploy Fund Your Wallet Turbo uses Turbo Credits to upload data to Arweave. You can purchase Turbo Credits with a variety of fiat currencies or crypto tokens. Below is an example for funding your wallet with 10 USD. It will open a browser window to complete the purchase using Stripe.npm install @ardrive/turbo-sdk
turbo top-up --wallet-file wallet.json --currency USD --value 10 Be sure to replace wallet.json with the path to your Arweave wallet.Update package.json {
  ...
  "scripts": {
    ...
    "deploy": "turbo upload-folder --folder-path ./build --wallet-file wallet.json > latest-manifest.json"
  }
  ...
} This will upload your build folder to the permaweb, and save all of the details of the upload to a file named "latest-manifest.json". That way, you'll have a reference for the manifest TxId to use later.Run build Now it is time to generate a build, run npm run build yarn build Run deploy Finally we are good to deploy our first Permaweb Application npm run deploy yarn deploy ERROR If you receive an error Insufficient funds, make sure you remembered to fund your deployment wallet with ArDrive Turbo credits.Response You should see a response similar to the following:Deployed TxId [<<tx-id>>] to ANT [<<ant-process>>] using undername [<<undername>>] Your React app can be found at https://arweave.net/<< tx-id >>.SUCCESS You should now have a React Application on the Permaweb! Great Job!Summary This is a Create React App version of publishing a React app on the permaweb. You may discover new ways to deploy an app on the permaweb or checkout other starter kits in this guide!

---

# 16. AO  WAO

Document Number: 16
Source: https://docs.wao.eco/api/ao
Words: 1354
Extraction Method: html

Instantiate You can initialize AO in the same way as AR.import { AO } from "wao"

const ao = await new AO().init(jwk || arweaveWallet) If you need to pass AR settings, use ar. ao.ar will be automatically available.const ao = await new AO({ ar: { port: 4000 }}).init(jwk || arweaveWallet)

const addr = ao.ar.addr

await ao.ar.post({ data, tags }) AO Core Functions deploy Spawn a process, get a Lua source, and eval the script. src is an Arweave txid of the Lua script.const { err, res, pid, p } = await ao.deploy({ data, tags, src, fills }) You can directly pass the Lua script with src_data instead of src.const { err, res, pid, p } = await ao.deploy({ data, tags, src_data, fills }) boot will use On-Boot tag to initialize the process instead of Eval action. You can set either true to use src_data, or set a txid of an existing script. In case of true, data should be undefined so the src_data can fill it with spawn.const { err, res, pid, p } = await ao.deploy({ boot: true, tags, src_data, fills }) fills replace the Lua source script from src.local replace_me = '<REPLACE_ME>'

local replace_me_again = '<REPLACE_ME_AGAIN>'

local replace_me_with_hello_again = '<REPLACE_ME>' const fills = { REPLACE_ME: "hello", REPLACE_ME_AGAIN: "world" } This will end up in the following lua script.local replace_me = 'hello'

local replace_me_again = 'world'

local replace_me_with_hello_again = 'hello' In case you have multiple scripts, use loads and pass src and fills respectively.await ao.deploy({ tags, loads: [ { src, fills }, { src: src2, fills: fills2 } ] }) You can also pass an array of string data to loads.const num = `num = 0`

const inc = `Handlers.add("Inc", "Inc", function () num = num + 1 end)`

const get = `Handlers.add("Get", "Get", function (m) m.reply({ Data = num }) end)`

 

const { p } = await ao.deploy({ tags, loads: [ num, inc, get ] })

await p.m("Inc")

assert.equal(await p.d("Get"), 1) msg Send a message.const { err, mid, res, out } = await ao.msg({

  pid, data, act, tags, check, get, mode, limit

}) check determins if the message call is successful by checking through Tags in Messages in res.When using from either in check or get, mode needs to be set gql. mode defaults to aoconnect which uses the aoconnect.results function to track down results, which cannot tell where results come from. gql mode doesn't sometimes catch all results if used with AO/Arweave mainnet since there are lags due to the block finality time. limit specifies how many transactions or results to fetch for the check.const check = { Status : "Success" } // succeeds if Status tag is "Success"

const check2 = { Status : true } // succeeds if Status tag exists Assigning either a string or boolean value checks Data field instead of Tags.const check3 = "Success"  // succeeds if Data field is "Success"

const check4 = true // succeeds if Data field exists

const check5 = /ok/ // succeeds if Data field is string containing "ok"

const check6 = (n)=> +n > 10 // succeeds if Data field is bigger than 10

const check7 = { json: { a: 3 } } // succeeds if Data field is JSON and a is 3

const check8 = { json: { a: 3, b: 4 }, eq: true } // deep equal JSON

const check9 = { data: true, tags: { Status: true, Balance: (n)=> +n > 10 } }

const check10 = { data: true, from: PID } // specify message sender process Use an array to check multiple conditions.const check11 = ["Success", { Age: "30" }] // Data is Success and Age tag is 30

const check12 = [{ data: "Success", from: PID }, { Age: "30", from: PID2 }] get will return specified data via out.const get = "ID" // returns the value of "ID" tag

const get2 = { name: "Profile", json: true } // "Profile" tag with JSON.parse()

const get3 = { data: true, json: true } // returns Data field with JSON.parse()

const get4 = true // => { data: true, json: true }

const get5 = false // => { data: true, json: false }

const get6 = { obj: { age: "Age", who: "Name" }} // => { age: 30, who: "Bob" }

const get7 = { age: "Age", who: "Name" } // same as get6

const get8 = { name: "Profile", json: true, from: PID } // specify sender process

const get9 = { age: { name: "Age", from: PID }, who: "Name" } // another example

const get10 = { data: true, json: true, match: (val, index, res)=> val.Age < 10 } check and get lazy-evaluate tags and data by tracking down async messages. As soon as the conditions are met, they won't track further messages. With receive() added with AOS 2.0, you can only get spawned messages up to the receive() function from result. But WAO automatically tracks down further messages and determines if check conditions are met beyond the receive() function.For example, consider the following handler.Handlers.add("Hello", "Hello", function (msg)

  msg.reply({ Data = "Hello, World!" })

  local name = Send({ Target = msg.to, Action = "Reply" }).receive().Data

  msg.reply({ Data = "Hello, " .. name .. "!" })

end) you can only get the first Hello, World!, but not the second "Hello, " .. name .. "!" from the aoconnect result function.dry Dryrun a message without writing to Arweave.const { err, res, out } = await ao.dry({ pid, data, action, tags, check, get }) res res does the same thing as msg but for an existing result with mid.const { err, res, out } = await ao.res({ pid, mid, check, get }) ress ress gets multiple results from a process.next() will be returned for pagenation if there are more messages.const { err, out: msgs, res, next } = await ao.ress({ pid, limit, asc, from })

 

if(next){

  const { out: msgs2 } = await next()

} pid: process ID limit: how many to get asc: messages are sorted descendingly by default, set asc=true to reverse from: cursor to get from asgn Assign an existing message to a process.const { err, mid, res, out } = await ao.asgn({ pid, mid, check, get }) load Get a Lua source script from Arweave and eval it on a process.const { err, res, mid } = await ao.load({ src, fills, pid }) eval Eval a Lua script on a process.const { err, res, mid } = await ao.eval({ pid, data }) spwn Spawn a process. module and scheduler are auto-set if omitted.const { err, res, pid } = await ao.spwn({ module, scheduler, tags, data }) aoconnect Functions The original aoconnect functions message | spawn | result | assign | dryrun  are also available. createDataItemSigner is available as toSigner.const signer = ao.toSigner(jwk)

const process = await ao.spawn({ module, scheduler, signer, tags, data })

const message = await ao.message({ process, signer, tags, data })

const result = await ao.result({ process, message }) Advanced Functions postModule data should be wasm binary. overwrite to replace the default module set to the AO instance.const { err, id: module } = await ao.postModule({ data, jwk, tags, overwrite }) postScheduler This will post Scheduler-Location with the jwk address as the returning scheduler.const { err, scheduler } = await ao.postScheduler({ url, jwk, tags, overwrite }) attest Attest Arweave transactions for WeaveDrive.const { err, res, id } = await ao.attest({ id, tags, jwk }) avail Make Arweave transactions available for WeaveDrive.const { err, res, id } = await ao.avail({ ids, tags, jwk }) wait wait until the process becomes available after spwn. This is mostly used internally with deploy.const { err } = await ao.wait({ pid }) var var reads a Lua variable from the current state with dryrun.const { pid } = await ao.deploy({

  src_data: `Table = { String = "Hello", Array = { "str", 3, true } }`,

})

 

const table = await ao.var({ pid, data: "Table" }) It strips off pretty tags from the output and auto-converts Lua tables to JSON, you can disable it with json and pretty.const table = await ao.var({ pid, data: "Table", json: false, pretty: true })

---

# 17. Hashpaths  WAO

Document Number: 17
Source: https://docs.wao.eco/hyperbeam/hashpaths
Words: 1136
Extraction Method: html

Hashpath is a mechanism to make compute steps verifiable with chained hashes.Message ID Each message has an ID.For signed messages, the ID is the sha256 hash of all commitment IDs except for hmac joined with , .For unsigned messages, the ID is the hmac-sha256 hash of the message content with ao as the key.You can use hb_message:id on HyperBEAM.ID = hb_message:id(Msg) Or you can use id from hbsig.import { id } from "hbisg"

const msg_id = id(msg) Message Resolving As we've learned so far, URLs like http://localhost:10001/~mydev@1.0/forward are automatically resolved to the forward method of the mydev@1.0 device with 3 arguments (Msg1, Msg2, Opts).We can internally do the same with hb_ao:resolve(Msg1, Msg2, Opts) to result in a new message Msg3.But recall from the previous chapter, Msg1 needs to contain device, and Msg2 has to contain path to be resolved.Let's create an add/3 method, which takes a message with num and plus, then executes num = num + plus. It returns device in addition to the new num since this will be chained and the first message to hb_ao:resolve needs to contain device.File  /HyperBEAM/src/dev_mydev.erl -export([ add/3 ]).

 

add(Msg1, Msg2, Opts)->

  Num = maps:get(<<"num">>, Msg1),

  Plus = maps:get(<<"plus">>, Msg2),

  {ok, #{ <<"device">> => <<"mydev@1.0">>, <<"num">> => Num + Plus }}.Also, create a resolve method that chains messages and resolves to add 3 times with incremental plus.File  /HyperBEAM/src/dev_mydev.erl -export([ resolve/3 ]).

 

resolve(_, _, Opts)->

  Msg1 = #{ <<"device">> => <<"mydev@1.0">>, <<"num">> => 0 },

  io:format("Msg1 ID: ~p~n", [hb_message:id(Msg1)]),

 

  Msg2 = #{ <<"path">> => <<"add">>, <<"plus">> => 1 },

  io:format("Msg2 ID: ~p~n", [hb_message:id(Msg2)]),

  {ok, Msg3} = hb_ao:resolve(Msg1, Msg2, Opts),

  io:format("Msg3: ~p~n", [Msg3]),

  io:format("Msg3 ID: ~p~n", [hb_message:id(Msg3)]),

 

  Msg4 = #{ <<"path">> => <<"add">>, <<"plus">> => 2 },

  io:format("Msg4 ID: ~p~n", [hb_message:id(Msg4)]),

 

  {ok, Msg5} = hb_ao:resolve(Msg3, Msg4, Opts),

  io:format("Msg5: ~p~n", [Msg5]),

  io:format("Msg5 ID: ~p~n", [hb_message:id(Msg5)]),

 

  Msg6 = #{ <<"path">> => <<"add">>, <<"plus">> => 3 },

  io:format("Msg6 ID: ~p~n", [Msg6]),

 

  {ok, Msg7} = hb_ao:resolve(Msg5, Msg6, Opts),

  io:format("Msg7: ~p~n", [Msg7]),

  io:format("Msg7 ID: ~p~n", [Msg7]),

 

  {ok, Msg7}.It's supposed to go...Msg1: { device: "mydev@1.0", num: 0 } Msg2: { path: "add", plus: 1 } Msg3 = resolve(Msg1, Msg2, Opts): { device: "mydev@1.0", num: 1 } Msg4: { path: "add", plus: 2 } Msg5 = resolve(Msg3, Msg4, Opts): { device: "mydev@1.0", num: 3 } Msg6: { path: "add", plus: 3 } Msg7 = resolve(Msg5, Msg6, Opts): { device: "mydev@1.0", num: 6 } Let's execute it:File  /test/hashpaths.test.js await hb.p("/~mydev@1.0/resolve") and we get the logs.Msg1 ID: <<"M3yMP4CqvdUkLXMM2tWQN8PnbT8iy0y70Pf3Mv_x2oA">>

 

Msg2 ID: <<"MFHRUaRJM96_-rtuJ5fEvQXZB5upG1FEQTnWAVoSLOc">>

 

Msg3: #{<<"device">> => <<"mydev@1.0">>,<<"num">> => 1,

        <<"priv">> =>

            #{<<"hashpath">> =>

                  <<"M3yMP4CqvdUkLXMM2tWQN8PnbT8iy0y70Pf3Mv_x2oA/MFHRUaRJM96_-rtuJ5fEvQXZB5upG1FEQTnWAVoSLOc">>}}

 

Msg3 ID: <<"SGXsgupRFDL40G5-rQQBIVRQ9eIJUoxx6g3xfMbASqE">>

 

Msg4 ID: <<"Yf4umWKkjUe4MaBN_ya7DOixRCUrSTG2jqE0DlicC2Q">>

 

Msg5: #{<<"device">> => <<"mydev@1.0">>,<<"num">> => 3,

        <<"priv">> =>

            #{<<"hashpath">> =>

                  <<"20vIiC-SsCktGvR3UeU6zrYkBS8GALoL5jRWKcB1QTo/Yf4umWKkjUe4MaBN_ya7DOixRCUrSTG2jqE0DlicC2Q">>}}

 

Msg5 ID: <<"IEb0vP4sXNGmsTgL1cvN-uo4ulD9uAz7y9cSUiD7Yxw">>

 

Msg6 ID: #{<<"path">> => <<"add">>,<<"plus">> => 3}

 

Msg7: #{<<"device">> => <<"mydev@1.0">>,<<"num">> => 6,

        <<"priv">> =>

            #{<<"hashpath">> =>

                  <<"McDwn8fpdVA4UORmdqvhzYxIn3sEytmK4IrNeE-aDrk/02-Vjx59gbI2vdl5fJNfCSlKDmmj9p0KKGZGn0fi7V4">>}}

 

Msg7 ID: #{<<"device">> => <<"mydev@1.0">>,<<"num">> => 6,

           <<"priv">> =>

               #{<<"hashpath">> =>

                     <<"McDwn8fpdVA4UORmdqvhzYxIn3sEytmK4IrNeE-aDrk/02-Vjx59gbI2vdl5fJNfCSlKDmmj9p0KKGZGn0fi7V4">>}} Msg7 gets num = 6, so it's working as expected, but what's interesting is each resolved message got hashpath under priv. These hashpaths are the core of the AO Core protocol and HyperBEAM, which keep track of compute steps and make execution verifiable.You can observe the pattern with hashpaths.Msg3_Hashpath: Msg1_ID + / + Msg2_ID Msg5_Hashpath: 20vIiC-SsCktGvR3UeU6zrYkBS8GALoL5jRWKcB1QTo + / + Msg4_ID Msg7_Hashpath: McDwn8fpdVA4UORmdqvhzYxIn3sEytmK4IrNeE-aDrk + / + Msg6_ID 20vIiC-SsCktGvR3UeU6zrYkBS8GALoL5jRWKcB1QTo is actually the sha256 hash of Msg1_ID/Msg2_ID (Msg3_Hashpath), and McDwn8fpdVA4UORmdqvhzYxIn3sEytmK4IrNeE-aDrk is the sha256 hash of Msg3_hashpath/Msg5_ID (Msg5_Hashpath).So the hashpaths evolve like the following.Msg3_Hashpath: Msg1_ID + / + Msg2_ID Msg5_Hashpath: hash(Msg3_Hashpath) + / + Msg4_ID Msg7_Hashpath: hash(Msg5_Hashpath) + / + Msg6_ID The formula is:New_Hashpath = hash(Prev_Hashpath)/New_Msg_ID Now, if you sign the 2nd messages to hb_ao:resolve, a new hashpath contains the previous hashpath and the new message ID, which usually contains commitments with the sha256 hash of the signatures, which verifies the message content.You can sign a message with the operator wallet using hb_message:commit(Msg, Opts).File  /HyperBEAM/src/dev_mydev.erl -export([ resolve2/3 ]).

 

resolve2(_, _, Opts)->

  Msg1 = #{ <<"device">> => <<"mydev@1.0">>, <<"num">> => 0 },

  io:format("Msg1 ID: ~p~n", [hb_message:id(Msg1)]),

 

  Msg2 = hb_message:commit(#{ <<"path">> => <<"add">>, <<"plus">> => 1 }, Opts),

  io:format("Msg2 ID: ~p~n", [hb_message:id(Msg2)]),

  

  {ok, Msg3} = hb_ao:resolve(Msg1, Msg2, Opts),

  io:format("Msg3: ~p~n", [Msg3]),

  io:format("Msg3 ID: ~p~n", [hb_message:id(Msg3)]),

 

  Msg4 = hb_message:commit(#{ <<"path">> => <<"add">>, <<"plus">> => 2 }, Opts),

  io:format("Msg4 ID: ~p~n", [hb_message:id(Msg4)]),

 

  {ok, Msg5} = hb_ao:resolve(Msg3, Msg4, Opts),

  io:format("Msg5: ~p~n", [Msg5]),

  io:format("Msg5 ID: ~p~n", [hb_message:id(Msg5)]),

 

  Msg6 = hb_message:commit(#{ <<"path">> => <<"add">>, <<"plus">> => 3 }, Opts),

  io:format("Msg6 ID: ~p~n", [Msg6]),

 

  {ok, Msg7} = hb_ao:resolve(Msg5, Msg6, Opts),

  io:format("Msg7: ~p~n", [Msg7]),

  io:format("Msg7 ID: ~p~n", [Msg7]),

 

  {ok, Msg7}.External messages passed via URLs are automatically cached with their IDs and readable with hb_cache:read(ID).You can also internally cache messages with their hashpaths using hb_cache:write_hashpath(Msg, Opts) or with their IDs using hb_cache:write(Msg, Opts).File  /HyperBEAM/src/dev_mydev.erl -export([ resolve3/3 ]).

 

resolve3(_, _, Opts)->

  Msg1 = #{ <<"device">> => <<"mydev@1.0">>, <<"num">> => 0 },

  io:format("Msg1 ID: ~p~n", [hb_message:id(Msg1)]),

 

  Msg2 = hb_message:commit(#{ <<"path">> => <<"add">>, <<"plus">> => 1 }, Opts),

  io:format("Msg2 ID: ~p~n", [hb_message:id(Msg2)]),

  

  {ok, Msg3} = hb_ao:resolve(Msg1, Msg2, Opts),

  io:format("Msg3: ~p~n", [Msg3]),

  io:format("Msg3 ID: ~p~n", [hb_message:id(Msg3)]),

  hb_cache:write_hashpath(Msg3, Opts),

 

  Msg4 = hb_message:commit(#{ <<"path">> => <<"add">>, <<"plus">> => 2 }, Opts),

  io:format("Msg4 ID: ~p~n", [hb_message:id(Msg4)]),

 

  {ok, Msg5} = hb_ao:resolve(Msg3, Msg4, Opts),

  io:format("Msg5: ~p~n", [Msg5]),

  io:format("Msg5 ID: ~p~n", [hb_message:id(Msg5)]),

  hb_cache:write_hashpath(Msg5, Opts),

  

  Msg6 = hb_message:commit(#{ <<"path">> => <<"add">>, <<"plus">> => 3 }, Opts),

  io:format("Msg6 ID: ~p~n", [Msg6]),

 

  {ok, Msg7} = hb_ao:resolve(Msg5, Msg6, Opts),

  io:format("Msg7: ~p~n", [Msg7]),

  io:format("Msg7 ID: ~p~n", [Msg7]),

  hb_cache:write_hashpath(Msg7, Opts),

  

  {ok, Msg7#{ 

    <<"hashpath_3">> => maps:get(<<"hashpath">>, maps:get(<<"priv">>, Msg3)),

    <<"hashpath_5">> => maps:get(<<"hashpath">>, maps:get(<<"priv">>, Msg5)),

    <<"hashpath_7">> => maps:get(<<"hashpath">>, maps:get(<<"priv">>, Msg7))

  }}.Reading Cached Messages with ID / Hashpath You can internally read any cached messages with hb_cache:read(ID, Opts) or hb_cache:read(Hashpath, Opts).But also, HyperBEAM makes cached messages accessible from external URL paths with /[msg_id] and /[hashpath] format.File  /test/hashpaths.test.js const out = await hb.p("/~mydev@1.0/resolve3")

const msg3 = await hb.g(`/${out.hashpath_3}`)

const msg5 = await hb.g(`/${out.hashpath_5}`)

const msg7 = await hb.g(`/${out.hashpath_7}`)

 

assert.deepEqual({ device: "mydev@1.0", num: 1 }, msg3)

assert.deepEqual({ device: "mydev@1.0", num: 3 }, msg5)

assert.deepEqual({ device: "mydev@1.0", num: 6 }, msg7) You can chain compute steps using this URL schema, but we'll talk about it in the next chapter.Hashpath of Signed Requests Any signed request to a HyperBEAM node returns a hashpath as tag in signature-input in the HTTP headers, which is the hashpath of the passed messages to the resolved device method.hb.post automatically extracts it from the response for you.File  /test/hashpaths.test.js import { id } from "hbsig"

 

const { out, hashpath } = await hb.post({ path: "/~mydev@1.0/forward" })

const { msg1, msg2 } = JSON.parse(out)

assert.equal(`${id(msg1)}/${id(msg2)}`, hashpath) This hashpath contains the IDs of the messages passed to our forward method in this case.hashpath: _msg1_id/_msg2_id This would be extremely useful if the hashpaths were automatically cached as they contain the compute results, but unfortunately, this doesn't seem to be the case with the current HyperBEAM implementation.Running Tests You can find the working test file for this chapter here:hashpaths.test.js Run tests:Terminal   Terminal yarn test test/hashpaths.test.js References WAO API HyperBEAM Class API HB Class API HBSig API

---

# 18. TEE Nodes - HyperBEAM - Documentation

Document Number: 18
Source: https://hyperbeam.arweave.net/run/tee-nodes.html
Words: 681
Extraction Method: html

Trusted Execution Environment (TEE) Recommended Setup Use HyperBEAM OS for the easiest TEE deployment with pre-configured AMD SEV-SNP support. Note: HB-OS is typically used for TEE operations, but is not necessary for router registration.Overview HyperBEAM supports Trusted Execution Environments (TEEs) through the ~snp@1.0 device, enabling secure, verifiable computation on remote machines. TEEs provide hardware-level isolation and cryptographic attestation that allows users to verify their code is running in a protected environment exactly as intended, even on untrusted hardware.The ~snp@1.0 device generates and validates attestation reports that prove:Code is running inside a genuine AMD SEV-SNP TEE The execution environment hasn't been tampered with Specific software components (firmware, kernel, initramfs) match trusted hashes Debug mode is disabled for security Configuration Files Configuration can be set in either config.json (JSON) or config.flat (flat) format. For full details and examples of both formats, see Configuration Reference.The examples below use JSON for clarity.When to use HB-OS Operation Use HB-OS?Purpose TEE Node (SNP) Recommended Secure, attested computation (hardware isolation) Router Registration Optional Registering/joining a router (TEE not required) If you are registering or running a router, you can do so without HB-OS.If you want to run a TEE node, HB-OS or an equivalent TEE setup is recommended for convenience and security.Quick Start: TEE Node with HyperBEAM OS Prerequisites AMD EPYC processor with SEV-SNP support (Milan generation or newer) Host system with SEV-SNP enabled in BIOS Setup TEE Node # Clone and build TEE-enabled HyperBEAM
# (Only needed for TEE nodes if you choose HB-OS)
git clone https://github.com/permaweb/hb-os.git && cd hb-os
./run init && ./run setup_host && ./run build_base_image && ./run build_guest_image

# Launch TEE-protected node
./run start The VM boots with dm-verity protection, measured boot, and automatic attestation report generation.Using the SNP Device Generate Attestation Report Request an attestation report from a TEE node:curl https://your-tee-node.com/~snp@1.0/generate Returns a signed attestation report containing:
- Nonce: Unique identifier preventing replay attacks
- Address: Node's ephemeral public key (only exists inside TEE)
- Measurement: Cryptographic hash of the execution environment
- Report: AMD SEV-SNP hardware attestation with certificate chain Verify Attestation Report The verification process validates:
1. Nonce integrity: Ensures report freshness and prevents replay
2. Signature validity: Confirms the report was signed by the claimed address
3. Address authenticity: Verifies the signing key exists only in the TEE
4. Debug disabled: Ensures no debugging capabilities that could compromise security
5. Trusted software: Validates firmware, kernel, and initramfs hashes match approved versions
6. Measurement accuracy: Confirms the reported environment matches actual execution
7. Hardware attestation: Verifies AMD's cryptographic signature on the report Configuration Trusted Software Hashes (config.json example) Configure which software components are trusted by setting snp_trusted in your node options:"snp_trusted": [
  // Trusted software hashes here
] Custom Trust Validation Implement custom trust policies by specifying an is-trusted-device:curl -X POST https://your-node.com/~snp@1.0/verify \
  -H "is-trusted-device: my-custom-validator@1.0" \
  -d '{"report": "...", "target": "self"}' Security Considerations SEV-SNP capable CPU: AMD EPYC Milan or newer Firmware support: Recent AMD firmware with SEV-SNP enabled Memory encryption: SME (Secure Memory Encryption) recommended RMP table: Sufficient memory reserved for Reverse Map Page Table Attestation Tools HyperBEAM OS includes several attestation utilities:get_report: Generate attestation reports with custom data verify_report: Validate attestation report signatures sev_feature_info: Check host SEV-SNP capabilities idblock_generator: Create signed VM configuration blocks Integration Examples Router Registration with TEE (Advanced, config.json example) If you want to register a TEE-protected router node, use the following configuration (see also the router registration guide):{
  "operator": "trustless",
  "initialized": "permanent",
  "snp_trusted": [ /* ... */ ],
  "on": {
    "request": {
      "device": "p4@1.0",
      "ledger-device": "lua@5.3a",
      "pricing-device": "simple-pay@1.0",
      "ledger-path": "/ledger~node-process@1.0",
      "module": ""
    },
    "response": {
      "device": "p4@1.0",
      "ledger-device": "lua@5.3a",
      "pricing-device": "simple-pay@1.0",
      "ledger-path": "/ledger~node-process@1.0",
      "module": ""
    }
  },
  "p4_non_chargable_routes": [
    {"template": "/.*~node-process@1.0/.*"},
    {"template": "/.*~greenzone@1.0/.*"},
    {"template": "/.*~router@1.0/.*"},
    {"template": "/.*~meta@1.0/.*"},
    {"template": "/schedule"},
    {"template": "/push"},
    {"template": "/~hyperbuddy@1.0/.*"}
  ],
  "node_process_spawn_codec": "ans104@1.0",
  "node_processes": {
    "ledger": {
      "device": "process@1.0",
      "execution-device": "lua@5.3a",
      "scheduler-device": "scheduler@1.0",
      "authority-match": 1,
      "admin": "",
      "token": "",
      "module": "",
      "authority": ""
    }
  },
  "router_opts": {
    "offered": [ /* ... */ ]
  },
  "green_zone_peer_location": "",
  "green_zone_peer_id": "",
  "p4_recipient": ""
} TEE attestation TEE-protected computation Trusted software validation HyperBEAM OS Repository See the router registration guide for non-TEE router setup.Configuration Reference

---

# 19. React Starter Kit wvite  ArDrive  Cooking with the Permaweb

Document Number: 19
Source: https://cookbook.arweave.net/kits/react/turbo.html
Words: 563
Extraction Method: html

React Starter Kit w/vite & ArDrive This guide will walk you through in a step by step flow to configure your development environment to build and deploy a permaweb react application.Prerequisites Basic Typescript Knowledge (Not Mandatory) - [https://www.typescriptlang.org/docs/](Learn Typescript) NodeJS v16.15.0 or greater - [https://nodejs.org/en/download/](Download NodeJS) Knowledge of ReactJS - [https://reactjs.org/](Learn ReactJS) Know git and common terminal commands Development Dependencies TypeScript NPM or Yarn Package Manager Steps Create React App npm create vite my-arweave-app --template react-ts
cd my-arweave-app
npm install yarn create vite my-arweave-app --template react-ts
cd my-arweave-app
yarn Add React Router DOM npm install react-router-dom yarn add react-router-dom We need to use the hash-router to create a working app on arweave.Page Components touch src/Home.tsx src/About.tsx src/Home.tsx import { Link } from "react-router-dom";

function Home() {
    return (
        <div>
            Welcome to the Permaweb!
            <Link to={"/about/"}>
                <div>About</div>
            </Link>
        </div>
    );
}

export default Home;src/About.tsx import { Link } from "react-router-dom";

function About() {
    return (
        <div>
            Welcome to the About page!
            <Link to={"/"}>
                <div>Home</div>
            </Link>
        </div>
    );
}

export default About;Modify App.tsx We need to update the App.tsx to manage different pages import { HashRouter } from "react-router-dom";
import { Routes, Route } from "react-router-dom";

import Home from "./Home";
import About from "./About";

function App() {
    return (
        <HashRouter>
            <Routes>
                <Route path={"/"} element={<Home />} />
                <Route path={"/about/"} element={<About />} />
            </Routes>
        </HashRouter>
    );
}

export default App;Modify index.css Alter the body selector body {
  margin: 0;
  padding-top: 200px;
  display: flex;
  flex-direction: column;
  place-items: center;
  min-width: 100%;
  min-height: 100vh;
} Run the project npm run dev yarn dev Building React App Modify vite.config.ts import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

// https://vitejs.dev/config/
export default defineConfig({
  base: "",
  plugins: [react()],
}) Build App yarn build Deploy Permanently Generate Wallet We need the arweave package to generate a wallet npm install --save arweave yarn add arweave -D then run this command in the terminal node -e "require('arweave').init({}).wallets.generate().then(JSON.stringify).then(console.log.bind(console))" > wallet.json Fund Wallet You will need to fund your wallet with ArDrive Turbo credits. To do this, enter ArDrive and import your wallet. Then, you can purchase turbo credits for your wallet.Setup Permaweb-Deploy npm install --save-dev permaweb-deploy yarn add permaweb-deploy --dev --ignore-engines You will need to add AR to your wallet and fund it with Turbo credits to be able to upload this app. See Turbo SDK for more information.Update package.json {
  ...
  "scripts": {
    ...
    "deploy": "npm run build && permaweb-deploy --arns-name my-react-app"
  }
  ...
} Replace my-react-app with your actual ArNS name. You can also add additional options like --undername staging for staging deployments.Run build Now it is time to generate a build, run npm run build yarn build Run deploy Finally we are good to deploy our first Permaweb Application npm run deploy yarn deploy Insufficient Funds If you receive an error Insufficient funds, make sure you remembered to fund your deployment wallet with Turbo credits. See Turbo SDK for more information.Response You should see a response similar to the following:-------------------- DEPLOY DETAILS --------------------
Tx ID: abc123def456ghi789jkl012mno345pqr678stu901v
ArNS Name: my-react-app
Undername: @
ANT: xyz789abc012def345ghi678jkl901mno234pqr567s
AR IO Process: bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM
TTL Seconds: 3600
--------------------------------------------------------
Deployed TxId [abc123def456ghi789jkl012mno345pqr678stu901v] to name [my-react-app] for ANT [xyz789abc012def345ghi678jkl901mno234pqr567s] using undername [@] Your React app can be found at https://my-react-app.arweave.net (if using ArNS) or https://arweave.net/abc123def456ghi789jkl012mno345pqr678stu901v.SUCCESS You should now have a React Application on the Permaweb! Great Job!Congrats!You just published a react application on the Permaweb! This app will be hosted forever!

---

# 20. Legacynet AOS on HyperBEAM  WAO

Document Number: 20
Source: https://docs.wao.eco/tutorials/legacynet-aos
Words: 253
Extraction Method: html

Currently, testing legacynet aos processes works much better without HyperBEAM.You can check out Legacynet AOS guide to test with legacy AO units.Legacynet AOS on HyperBEAM uses genesis-wasm@1.0 device and an external CU for computation. The standalone WAO server works as a local CU.spawnLegacy, schedule, and computeLegacy manages the process for you.import assert from "assert"

import { describe, it, before, after, beforeEach } from "node:test"

import { HyperBEAM } from "wao"

 

const cwd = "../HyperBEAM" // HyperBEAM directory

 

const lua = `

local count = 0

Handlers.add("Inc", "Inc", function (msg)

  count = count + 1

  msg.reply({ Data = "Count: "..tostring(count) })

end)

 

Handlers.add("Get", "Get", function (msg)

  msg.reply({ Data = "Count: "..tostring(count) })

end)`

 

describe("Hyperbeam Legacynet", function () {

  let hbeam, hb

  before(async () => {

    hbeam = await new HyperBEAM({

      cwd,

      reset: true,

      as: ["genesis_wasm"],

    }).ready()

  })

 

  beforeEach(async () => (hb = hbeam.hb))

  after(async () => hbeam.kill())

 

  it("should run a legacynet AOS process with a local CU", async () => {

    const { pid } = await hb.spawnLegacy()

    const { slot } = await hb.schedule({

      pid,

      data: lua,

      tags: { Action: "Eval" },

    })

    const res = await hb.computeLegacy({ pid, slot })

    

    let i = 0

    while (i < 10) {

      const { slot } = await hb.schedule({

        pid,

        tags: { Action: "Inc" },

      })

      const res2 = await hb.computeLegacy({ pid, slot })

      assert.equal(res2.Messages[0].Data, `Count: ${++i}`)

    }

    

    const res3 = await ao.hb.dryrun({ pid, action: "Get" })

    assert.equal(res3.Messages[0].Data, `Count: ${i}`)

 

    const res4 = await hb.messages({ pid, from: 0 })

    assert.equal(res4.edges.length, i + 2)

  })

})

---

# 21. HyperBEAM Introduction  Cooking with the Permaweb

Document Number: 21
Source: https://cookbook.arweave.net/fundamentals/decentralized-computing/hyperbeam/hyperbeam-introduction.html
Words: 1473
Extraction Method: html

HyperBEAM Introduction HyperBEAM is the primary, production-ready implementation of the AO-Core protocol, built on the robust Erlang/OTP framework. It serves as a decentralized operating system, powering the AO Computer—a scalable, trust-minimized, distributed supercomputer built on the permanent storage of Arweave.For the most current technical specifications and implementation details, refer to the official HyperBEAM documentation.What is HyperBEAM?Think of HyperBEAM as your "Swiss Army knife" for decentralized development. It's not a single-purpose application, but rather a powerful, extensible engine that transforms the abstract concepts of AO-Core into a concrete, operational system.HyperBEAM provides the runtime environment and essential services to execute computations across a network of distributed nodes, making the AO Computer accessible through familiar web technologies like HTTP.HyperBEAM serves as the bridge between standard HTTP interfaces and the AO Computer, managing message routing, device execution, state queries, and cryptographic verification.Core AO-Core Concepts in HyperBEAM HyperBEAM implements the three fundamental components of AO-Core:Messages: Modular Data Packets In HyperBEAM, every interaction within the AO Computer is handled as a message. Messages are cryptographically-linked data units that form the foundation for communication, allowing processes to trigger computations, query state, and transfer value.// Example message structure in HyperBEAM
const message = {
  Id: "MESSAGE_TX_ID",
  Process: "TARGET_PROCESS_ID", 
  Owner: "SENDER_ADDRESS",
  Data: "Hello, AO Computer!",
  Tags: [
    { name: "Action", value: "Greet" },
    { name: "Device", value: "~lua@5.3a" }
  ]
};HyperBEAM nodes are responsible for routing and processing these messages according to the rules of the AO-Core protocol, ensuring reliable delivery and execution.Devices: Extensible Execution Engines HyperBEAM introduces a uniquely modular architecture centered around Devices. These pluggable components are Erlang modules that define specific computational logic—like running WASM, managing state, or relaying data—allowing for unprecedented flexibility.Common HyperBEAM Devices:~process@1.0: Manages persistent, shared computational states (like smart contracts) ~lua@5.3a: Executes Lua scripts for serverless functions ~wasm64@1.0: Executes WebAssembly code for high-performance computing ~json@1.0: Provides JSON data structure manipulation ~relay@1.0: Forwards messages between nodes or external HTTP endpoints ~scheduler@1.0: Handles message ordering and execution timing For a complete list of devices and their latest specifications, see the HyperBEAM Devices documentation.# Using the Lua device to execute a calculation
GET /~lua@5.3a&script=return 2 + 2/result
# Response: 4

# Using the process device to query state
GET /PROCESS_ID~process@1.0/now/balance
# Response: Current process balance Paths: Composable Pipelines HyperBEAM exposes a powerful HTTP API that uses structured URL patterns to interact with processes and data. This pathing mechanism allows developers to create verifiable data pipelines, composing functionality from multiple devices into a single, atomic request.The URL bar becomes a command-line interface for AO's trustless compute environment.# Complex pipeline example: Calculate token supply and format as JSON
GET /TOKEN_PROCESS~process@1.0/now/cache/~lua@5.3a&module=CALC_MODULE/sum/serialize~json@1.0

# This path:
# 1. Reads latest state of TOKEN_PROCESS on the cache variable
# 2. Pipes state to Lua script 
# 3. Calls 'sum' function to calculate total supply
# 4. Formats result as JSON Four Key Principles of HyperBEAM Building with HyperBEAM can be simplified to four core principles:1. Everything is a Message You can compute on any message by calling its keys by name. The device specified in the message determines how these keys are resolved.# Direct message access
GET /~message@1.0&greeting="Hello"&count+integer=42/count
# Response: 42

# Process message
GET /PROCESS_ID~process@1.0/compute/userCount  
# Response: Number of users in the process 2. Paths are Pipelines of Messages A path defines a sequence of 'request' messages to be executed. You can set a key in a message directly within the path using the &key=value syntax.# Pipeline: Get process data → Transform with Lua → Format as JSON
GET /PROCESS~process@1.0/now/~lua@5.3a&script=transform_data/result~json@1.0 3. Device-Specific Requests with ~x@y The ~device@version syntax allows you to apply a request as if the base message had a different device, providing powerful compute and storage logic.# Execute Lua on process state
GET /PROCESS~process@1.0/now/~lua@5.3a&func=calculateMetrics/metrics

# Execute WASM for high-performance computing  
GET /DATA~message@1.0/~wasm64@1.0&module=WASM_MODULE/compute 4. Signed Responses over HTTP The final message in a pipeline is returned as an HTTP response. This response is signed against the hashpath that generated it, ensuring integrity and verifiability of the computation.# Every response includes cryptographic proof
HTTP/1.1 200 OK
X-HyperBEAM-Signature: 0x123abc...
X-HyperBEAM-HashPath: path_to_computation_verification
Content-Type: application/json

{"result": 42, "verified": true} HyperBEAM Architecture Built on Erlang/OTP Framework HyperBEAM leverages the battle-tested Erlang/OTP platform, providing:Exceptional Concurrency Handle millions of lightweight processes simultaneously Actor model naturally maps to AO processes Message passing between isolated processes Fault Tolerance "Let it crash" philosophy with supervised restarts System continues operating even if individual components fail Automatic recovery and error isolation Hot Code Swapping Update running code without stopping the system Zero-downtime deployments and upgrades Live system maintenance and improvements Distributed Systems Built-in clustering and node discovery Network partitioning tolerance Transparent inter-node communication %% Example of HyperBEAM device implementation
-module(my_custom_device).
-export([handle_message/2]).

handle_message(Message, State) ->
    case maps:get(<<"action">>, Message, null) of
        <<"calculate">> ->
            Result = perform_calculation(Message),
            {ok, Result, State};
        _ ->
            {error, unknown_action, State}
    end.Hardware Abstraction HyperBEAM abstracts away underlying hardware differences, allowing diverse nodes to contribute resources without compatibility issues. Whether running on:Consumer laptops Enterprise servers Cloud instances Edge devices Specialized hardware (GPUs, TPUs) All nodes can participate in the AO Computer through the common HyperBEAM interface.Use Cases and Applications Serverless Computing with Trustless Guarantees Replace traditional cloud functions with permanently available, cryptographically verifiable compute:// Traditional serverless function
exports.handler = async (event) => {
    return { statusCode: 200, body: "Hello World" };
};

// HyperBEAM equivalent - permanently available on AO
// Accessible via: /PROCESS~process@1.0/hello
Handlers.add(
    "hello", 
    function() { return true; },
    function(msg) {
        ao.send({
            Target = msg.From,
            Data = "Hello from the permanent web!"
        })
    }
) Hybrid Smart Contract + Serverless Applications Combine persistent state management with on-demand compute:# Smart contract state management
GET /TOKEN_CONTRACT~process@1.0/now/balance

# + Serverless data processing  
GET /DATA_PROCESSOR~lua@5.3a&module=ANALYTICS/process/result

# + External API integration
GET /~relay@1.0/call?method=GET&path=https://api.external.com/data Custom Execution Environments Build specialized computational environments through custom devices:AI/ML Inference: Custom devices for model serving and GPU acceleration Scientific Computing: Devices for mathematical libraries and simulations Cryptographic Applications: Specialized devices for zero-knowledge proofs Cross-chain Bridges: Devices for interacting with other blockchain networks Composable Data Pipelines Create complex, verifiable data processing workflows:# Data pipeline: Fetch → Validate → Transform → Store → Notify
GET /DATA_SOURCE/fetch/
    ~validator@1.0&schema=SCHEMA/validate/
    ~transformer@1.0&rules=RULES/transform/
    ~storage@1.0&destination=DEST/store/
    ~notifier@1.0&webhook=WEBHOOK/notify Getting Started with HyperBEAM Accessing HyperBEAM Nodes HyperBEAM nodes are accessible via HTTP. You can use any node while maintaining trustless guarantees:// Example HyperBEAM node URLs (verify node availability before use)
// Note: Node availability can change. Always verify endpoints are operational.
const EXAMPLE_NODES = [
  'https://forward.computer',  // Community HyperBEAM node
  // Additional nodes can be found in the AO ecosystem
  // Use official node directories for current endpoints
];

// Example: Query process state from a HyperBEAM node
const processId = 'YOUR_PROCESS_ID';
const nodeUrl = 'YOUR_HYPERBEAM_NODE_URL'; // Replace with verified node URL
const response = await fetch(`${nodeUrl}/${processId}~process@1.0/now`);
const state = await response.json();Basic Operations Query Process State:GET /PROCESS_ID~process@1.0/compute
# Returns the latest known state (faster)

GET /PROCESS_ID~process@1.0/now  
# Returns real-time state (slower, more accurate) Execute Lua Code:GET /~lua@5.3a&script=return os.time()/result
# Returns current timestamp Send Messages to Processes:import { connect, createDataItemSigner } from "@permaweb/aoconnect";

const ao = connect({
  MU_URL: "YOUR_HYPERBEAM_NODE_URL"  // Replace with verified HyperBEAM node
});

await ao.message({
  process: "PROCESS_ID",
  tags: [
    { name: "Action", value: "Transfer" },
    { name: "Recipient", value: "RECIPIENT_ADDRESS" }, 
    { name: "Quantity", value: "100" }
  ],
  signer: createDataItemSigner(wallet)
});Security and Trust Model Cryptographic Verification Every HyperBEAM response includes cryptographic proofs:Signatures: All responses signed by the executing node HashPaths: Verifiable computation trails State Hashes: Merkle proofs of state integrity Message IDs: Tamper-evident message identification Trusted Execution Environments (TEEs) Many HyperBEAM nodes run in TEEs for additional security:# Verify node is running in genuine TEE
GET /~snp@1.0/attestation
# Returns cryptographic attestation report Decentralized Trust No single point of failure or control:Computation can be verified independently Multiple nodes can execute the same request Consensus mechanisms for critical operations Open network - anyone can run a node Performance Characteristics Concurrency Millions of concurrent processes per node Lightweight message passing between processes Non-blocking I/O for network operations Parallel device execution for complex pipelines Scalability Horizontal scaling through additional nodes Load balancing across node network Caching layers for frequently accessed data Eventual consistency model for global state Efficiency Compiled code execution via BEAM VM Memory management with garbage collection Hot code reloading for zero-downtime updates Optimized message serialization for network transport Network Effects As more nodes join the HyperBEAM network:Increased Resilience: More redundancy and fault tolerance Better Performance: Load distribution and edge computing Enhanced Security: More verification nodes and cryptographic diversity Expanded Capabilities: New devices and specialized services Lower Costs: Competition drives down execution costs Explore HyperBEAM's capabilities in detail:Learn Querying: Querying AO Process State Build Serverless Functions: Lua Serverless Functions Understand Devices: HyperBEAM Devices Start Building: Builder's Journey Resources HyperBEAM Official Documentation: HyperBEAM Docs AO Cookbook: AO Documentation HyperBEAM Migration Guide: Migration from AO Connect Note: Node endpoints and availability may change over time. Always verify node status and select reliable endpoints for production use. Consider running your own HyperBEAM node for maximum reliability and control.

---

# 22. Create Vue Starter Kit  Cooking with the Permaweb

Document Number: 22
Source: https://cookbook.arweave.net/kits/vue/create-vue.html
Words: 707
Extraction Method: html

Create Vue Starter Kit This guide will provide step-by-step instructions to configure your development environment and build a permaweb Vue application.Prerequisites Basic Typescript Knowledge (Not Mandatory) - Learn Typescript NodeJS v16.15.0 or greater - Download NodeJS Knowledge of Vue.js (preferably Vue 3) - Learn Vue.js Know git and common terminal commands Development Dependencies TypeScript (Optional) NPM or Yarn Package Manager Steps Create Project The following command installs and launches create-vue, the official scaffolding tool for Vue projects.npm init vue@latest yarn create vue During the process, you'll be prompted to select optional features such as TypeScript and testing support. I recommend selecting the Vue Router with yes, the rest can be selected as per your preference.✔ Project name: … <your-project-name>
✔ Add TypeScript? … No / Yes
✔ Add JSX Support? … No / Yes
✔ Add Vue Router for Single Page Application development? … No / *Yes*
✔ Add Pinia for state management? … No / Yes
✔ Add Vitest for Unit testing? … No / Yes
✔ Add Cypress for both Unit and End-to-End testing? … No / Yes
✔ Add ESLint for code quality? … No / Yes
✔ Add Prettier for code formatting? … No / Yes Change into the Project Directory cd <your-project-name> Install Dependencies npm install yarn Setup Router Vue Router is the official router for Vue.js and seamlessly integrates with Vue. To make it work with Permaweb, switch from a browser history router to a hash router as the URL cannot be sent to the server. Change createWebHistory to createWebHashHistory in your src/router/index.ts or src/router/index.js file.import { createRouter, createWebHashHistory } from "vue-router";
import HomeView from "../views/HomeView.vue";

const router = createRouter({
    history: createWebHashHistory(import.meta.env.BASE_URL),
    routes: [
        {
            path: "/",
            name: "home",
            component: HomeView,
        },
        {
            path: "/about",
            name: "about",
            component: () => import("../views/AboutView.vue"),
        },
    ],
});

export default router;Setup Build Configure the build process in the vite.config.ts or vite.config.js file. To serve Permaweb apps from a sub-path (https://[gateway]/[TX_ID]), update the base property to./ in the config file.export default defineConfig({
  base: './',
  ...
}) Run the App Before moving forward, it is crucial to verify that everything is working correctly. Run a check to ensure smooth progress.npm run dev yarn dev it will start a new development server locally on your machine by default it uses `PORT 5173`. If this PORT is already in use it may increase the PORT number by 1 (`PORT 5174`) and try again. Deploy Permanently Generate Wallet We need the arweave package to generate a wallet npm install --save arweave yarn add arweave -D then run this command in the terminal node -e "require('arweave').init({}).wallets.generate().then(JSON.stringify).then(console.log.bind(console))" > wallet.json Fund Wallet You will need to fund your wallet with ArDrive Turbo credits. To do this, enter ArDrive and import your wallet. Then, you can purchase turbo credits for your wallet.Setup Permaweb-Deploy npm install --global permaweb-deploy yarn global add permaweb-deploy Fund Your Wallet Turbo uses Turbo Credits to upload data to Arweave. You can purchase Turbo Credits with a variety of fiat currencies or crypto tokens. Below is an example for funding your wallet with 10 USD. It will open a browser window to complete the purchase using Stripe.npm install @ardrive/turbo-sdk
turbo top-up --wallet-file wallet.json --currency USD --value 10 Be sure to replace wallet.json with the path to your Arweave wallet.Update package.json {
  ...
  "scripts": {
    ...
    "deploy": "DEPLOY_KEY=$(base64 -i wallet.json) permaweb-deploy --ant-process << ANT-PROCESS >> --deploy-folder build"
  }
  ...
} Replace << ANT-PROCESS >> with your ANT process id.Run build Now it is time to generate a build, run npm run build yarn build Run deploy Finally we are good to deploy our first Permaweb Application npm run deploy yarn deploy ERROR If you receive an error Insufficient funds, make sure you remembered to fund your deployment wallet with ArDrive Turbo credits.Response You should see a response similar to the following:Deployed TxId [<<tx-id>>] to ANT [<<ant-process>>] using undername [<<undername>>] Your Vue app can be found at https://arweave.net/<< tx-id >>.SUCCESS You should now have a Vue Application on the Permaweb! Great Job!Summary This guide provides a simple step-by-step method to publish a Vue.js app on the Permaweb using Create Vue. If you need additional features Tailwind, consider exploring alternative starter kits listed in the guide to find a suitable solution for your requirements.

---

# 23. GQL  WAO

Document Number: 23
Source: https://docs.wao.eco/api/gql
Words: 682
Extraction Method: html

GQL simplifies the Arwave GraphQL operations to query blocks and transactions.Instantiate You can instantiate the GQL class with an endpoint url.import { GQL } from "wao"

const gql = new GQL({ url: "https://arweave.net/graphql" }) // the default url AR class auto-instantiates GQL internally.import { AO } from "wao"

const ao = new AO()

const gql = ao.ar.gql import { AR } from "wao"

const ar = new AR()

const gql = ar.gql Txs Get latest transactions.const txs = await gql.txs() asc Get transactions in ascending order.const txs = await gql.txs({ asc: true }) first Get the firxt X transactions.const txs = await gql.txs({ first: 3 }) after Get transactions after a specific one to paginate. Pass a cursor.const txs = await gql.txs({ first: 3 })

const txs2 = await gql.txs({ first: 3, after: txs[2].cursor }) next Easier pagination with next.const { next, data: txs0_2 } = await gql.txs({ first: 3, next: true })

const { next: next2, data: txs3_5 } = await next()

const { next: next3, data: txs6_8 } = await next2() res.next will be null if there's no more transactions to paginate.block Get transactions within a block height range.const txs = await gql.txs({ block: { min: 0, max: 10 } }) or const txs = await gql.txs({ block: [0, 10] }) You can also specify only min or max.by Transaction IDs Get transactions by transaction ids.const txs = await gql.txs({ id: TXID }) or const txs = await gql.txs({ ids: [ TXID1, TXID2, TXID3 ] }) by Recipients Get transactions by recipients.const txs = await gql.txs({ recipient: ADDR }) or const txs = await gql.txs({ recipients: [ ADDR1, ADDR2, ADDR3 ] }) by Owners Get transactions by owners.const txs = await gql.txs({ owner: ADDR }) or const txs = await gql.txs({ owners: [ ADDR1, ADDR2, ADDR3 ] }) by Tags Get transactions that match tags.const txs = await gql.txs({ tags: { Name: "Bob", Age: "30" } }) fields Choose fields to be returned.const txs = await gql.txs({ fields: ["id", "recipient"] }) For nested objects,const txs = await gql.txs({ fields: ["id", { owner: ["address", "key"] }] }) You can use a hashmap to specify fields too.const txs = await gql.txs({ 

  fields: { id: true, { owner: { address: true, key: true } } } 

}) If you assign false, the other fields will be returned.const txs = await gql.txs({ 

  fields: { id: true, { block: { previous: false } } } 

}) For example, the above will exclude previous from block and return id, timestamp and height.The entire available fields for transactions as in a graphql query are as follows.const tx_fields = `{

  id 

  anchor 

  signature 

  recipient 

  owner { address key } 

  fee { winston ar } 

  quantity { winston ar } 

  data { size type } 

  tags { name value } 

  block { id timestamp height previous } 

  parent { id }

  bundledIn { id }

}` Blocks Get latest blocks.const blocks = await gql.blocks() asc Get blocks in ascending order.const blocks = await gql.blocks({ asc: true }) first Get the firxt X blocks.const blocks = await gql.blocks({ first: 3 }) after Get blocks after a specific one to paginate. Pass a cursor.const blocks = await gql.blocks({ first: 3 })

const blocks2 = await gql.blocks({ first: 3, after: blocks[2].cursor }) by Block IDs Get blocks by block ids.const blocks = await gql.blocks({ id: BLCID }) or const blocks = await gql.blocks({ ids: [ BLKID1, BLKID2, BLKID3 ] }) height Get blocks within a block height range.const blocks = await gql.blocks({ height: { min: 0, max: 10 } }) or const blocks = await gql.blocks({ height: [0, 10] }) next Easier pagination with next.const { next, data: blocks0_2 } = await gql.blocks({ first: 3, next: true })

const { next: next2, data: blocks3_5 } = await next() 

const { next: next3, data: blocks6_8 } = await next2() res.next will be null if there's no more blocks to paginate.fields const blocks = await gql.blocks({ 

  fields: ["id", "timestamp", "height", "previous"]

}) The entire available fields for blocks as in a graphql query are as follows.const block_fields = `{ id timestamp height previous }`

---

# 24. ARIO Docs

Document Number: 24
Source: https://docs.ar.io/wayfinder/core
Words: 298
Extraction Method: html

Wayfinder The @ar.io/wayfinder-core library provides intelligent gateway routing and data verification for accessing Arweave data through the AR.IO network. It's the foundational package that powers all other Wayfinder tools.What is Wayfinder?Wayfinder Core is a JavaScript/TypeScript library that:Intelligently Routes Requests: Automatically selects the best AR.IO gateway for each request Verifies Data Integrity: Cryptographically verifies that data hasn't been tampered with Handles Failures Gracefully: Automatically retries with different gateways when requests fail Provides Observability: Emits events and telemetry for monitoring and debugging Works Everywhere: Compatible with browsers, Node.js, and edge environments Installation Basic Configuration Advanced Configuration With Routing Strategy With Data Verification Full Configuration Example import {
  Wayfinder,
  NetworkGatewaysProvider,
  FastestPingRoutingStrategy,
  HashVerificationStrategy,
} from '@ar.io/wayfinder-core'
import { ARIO } from '@ar.io/sdk'

const wayfinder = new Wayfinder({
  // Gateway discovery
  gatewaysProvider: new NetworkGatewaysProvider({
    ario: ARIO.mainnet(),
    limit: 10,
    sortBy: 'operatorStake',
  }),

  // Routing configuration
  routingSettings: {
    strategy: new FastestPingRoutingStrategy({
      timeoutMs: 500,
      cacheResultsMs: 30000,
    }),
    events: {
      onRoutingSucceeded: (event) => {
        console.log('Selected gateway:', event.selectedGateway)
      },
      onRoutingFailed: (error) => {
        console.error('Routing failed:', error.message)
      },
    },
  },

  // Data verification
  verificationSettings: {
    enabled: true,
    strategy: new HashVerificationStrategy({
      trustedGateways: ['https://arweave.net'],
    }),
    strict: false,
    events: {
      onVerificationSucceeded: (event) => {
        console.log('Verification passed:', event.txId)
      },
      onVerificationFailed: (error) => {
        console.warn('Verification failed:', error.message)
      },
    },
  },

  // Telemetry (optional)
  telemetrySettings: {
    enabled: true,
    serviceName: 'my-application',
    clientName: 'my-app',
    clientVersion: '1.0.0',
    sampleRate: 0.1,
  },

  // Custom logger (optional)
  logger: {
    debug: (message, ...args) =>
      console.debug(`[WAYFINDER] ${message}`, ...args),
    info: (message, ...args) => console.info(`[WAYFINDER] ${message}`, ...args),
    warn: (message, ...args) => console.warn(`[WAYFINDER] ${message}`, ...args),
    error: (message, ...args) =>
      console.error(`[WAYFINDER] ${message}`, ...args),
  },
}) request(): How to fetch Arweave data using Wayfinder resolveUrl(): Use dynamic URLs for transaction IDs, ArNS names, etc.Gateway Providers: Understand gateway discovery options Routing Strategies: Explore different routing algorithms Verification Strategies: Learn about data integrity verification Telemetry: Set up monitoring and observability

---

# 25. Overview - HyperBEAM - Documentation

Document Number: 25
Source: https://hyperbeam.arweave.net/build/devices/hyperbeam-devices.html
Words: 687
Extraction Method: html

HyperBEAM Devices In AO-Core and its implementation HyperBEAM, Devices are modular components responsible for processing and interpreting Messages. They define the specific logic for how computations are performed, data is handled, or interactions occur within the AO ecosystem.Think of Devices as specialized engines or services that can be plugged into the AO framework. This modularity is key to AO's flexibility and extensibility.Purpose of Devices Define Computation: Devices dictate how a message's instructions are executed. One device might run WASM code, another might manage process state, and yet another might simply relay data.Enable Specialization: Nodes running HyperBEAM can choose which Devices to support, allowing them to specialize in certain tasks (e.g., high-compute tasks, storage-focused tasks, secure TEE operations).Promote Modularity: New functionalities can be added to AO by creating new Devices, without altering the core protocol.Distribute Workload: Different Devices can handle different parts of a complex task, enabling parallel processing and efficient resource utilization across the network.Device Naming and Versioning Devices are typically referenced using a name and version, like ~<name>@<version> (e.g., ~process@1.0). The tilde (~) often indicates a primary, user-facing device, while internal or utility devices might use a dev_ prefix in the source code (e.g., dev_router).Versioning indicates the specific interface and behavior of the device. Changes to a device that break backward compatibility usually result in a version increment.Familiar Examples HyperBEAM includes many preloaded devices that provide core functionality. Some key examples include:~meta@1.0: Configures the node itself (hardware specs, supported devices, payment info).~process@1.0: Manages persistent, shared computational states (like traditional smart contracts, but more flexible).~scheduler@1.0: Handles the ordering and execution of messages within a process.~wasm64@1.0: Executes WebAssembly (WASM) code, allowing for complex computations written in languages like Rust, C++, etc.~lua@5.3a: Executes Lua scripts.~relay@1.0: Forwards messages between AO nodes or to external HTTP endpoints.~json@1.0: Provides access to JSON data structures.~message@1.0: Manages message state and processing.~patch@1.0: Applies state updates directly to a process, often used for migrating or managing process data.Beyond the Basics Devices aren't limited to just computation or state management. They can represent more abstract concepts:Security Devices (~snp@1.0, dev_codec_httpsig): Handle tasks related to Trusted Execution Environments (TEEs) or message signing, adding layers of security and verification.Payment/Access Control Devices (~p4@1.0, ~faff@1.0): Manage metering, billing, or access control for node services.Workflow/Utility Devices (dev_cron, dev_stack, dev_monitor): Coordinate complex execution flows, schedule tasks, or monitor process activity.Using Devices Devices are typically invoked via GET requests. The path specifies which Device should interpret the subsequent parts of the path or the request body.# Example: Execute the 'now' key on the process device for a specific process
/<procId>~process@1.0/now

# Example: Relay a GET request via the relay device
/~relay@1.0/call?method=GET&path=https://example.com The specific functions or 'keys' available for each Device are documented individually. See the Devices section for details on specific built-in devices. The Potential of Devices The modular nature of AO Devices opens up vast possibilities for future expansion and innovation. The current set of preloaded and community devices is just the beginning. As the AO ecosystem evolves, we can anticipate the development of new devices catering to increasingly specialized needs:Specialized Hardware Integration: Devices could be created to interface directly with specialized hardware accelerators like GPUs (for AI/ML tasks such as running large language models), TPUs, or FPGAs, allowing AO processes to leverage high-performance computing resources securely and verifiably.Advanced Cryptography: New devices could implement cutting-edge cryptographic techniques, such as zero-knowledge proofs (ZKPs) or fully homomorphic encryption (FHE), enabling enhanced privacy and complex computations on encrypted data.Cross-Chain & Off-Chain Bridges: Devices could act as secure bridges to other blockchain networks or traditional Web2 APIs, facilitating seamless interoperability and data exchange between AO and the wider digital world.AI/ML Specific Devices: Beyond raw GPU access, specialized devices could offer higher-level AI/ML functionalities, like optimized model inference engines or distributed training frameworks.Domain-Specific Logic: Communities or organizations could develop devices tailored to specific industries or use cases, such as decentralized finance (DeFi) primitives, scientific computing libraries, or decentralized identity management systems.The Device framework ensures that AO can adapt and grow, incorporating new technologies and computational paradigms without requiring fundamental changes to the core protocol. This extensibility is key to AO's long-term vision of becoming a truly global, decentralized computer.

---

# 26. Module dev_p4erl - HyperBEAM - Documentation

Document Number: 26
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_p4.html
Words: 550
Extraction Method: html

Module dev_p4.erl The HyperBEAM core payment ledger.Description This module allows the operator to
specify another device that can act as a pricing mechanism for transactions
on the node, as well as orchestrating a payment ledger to calculate whether
the node should fulfil services for users.The device requires the following node message settings in order to function:p4_pricing-device: The device that will estimate the cost of a request.p4_ledger-device: The device that will act as a payment ledger.The pricing device should implement the following keys:<code>GET /estimate?type=pre|post&body=[...]&request=RequestMessage</code><code>GET /price?type=pre|post&body=[...]&request=RequestMessage</code> The body key is used to pass either the request or response messages to the
device. The type key is used to specify whether the inquiry is for a request
(pre) or a response (post) object. Requests carry lists of messages that will
be executed, while responses carry the results of the execution. The price key may return infinity if the node will not serve a user under any
circumstances. Else, the value returned by the price key will be passed to
the ledger device as the amount key.A ledger device should implement the following keys:<code>POST /credit?message=PaymentMessage&request=RequestMessage</code><code>POST /charge?amount=PriceMessage&request=RequestMessage</code><code>GET /balance?request=RequestMessage</code> The type key is optional and defaults to pre. If type is set to post,
the charge must be applied to the ledger, whereas the pre type is used to
check whether the charge would succeed before execution.Function Index balance/3 Get the balance of a user in the ledger.faff_test/0* Simple test of p4's capabilities with the faff@1.0 device.hyper_token_ledger/0*  hyper_token_ledger_test_/0* Ensure that Lua scripts can be used as pricing and ledger devices.is_chargable_req/2* The node operator may elect to make certain routes non-chargable, using
the routes syntax also used to declare routes in router@1.0.non_chargable_route_test/0* Test that a non-chargable route is not charged for.request/3 Estimate the cost of a transaction and decide whether to proceed with
a request.response/3 Postprocess the request after it has been fulfilled.test_opts/1*  test_opts/2*  test_opts/3*  Function Details balance/3 balance(X1, Req, NodeMsg) -> any() Get the balance of a user in the ledger.faff_test/0 * faff_test() -> any() Simple test of p4's capabilities with the faff@1.0 device.hyper_token_ledger/0 * hyper_token_ledger() -> any() hyper_token_ledger_test_/0 * hyper_token_ledger_test_() -> any() Ensure that Lua scripts can be used as pricing and ledger devices. Our
scripts come in two components:
1. A process script which is executed as a persistent local-process on the
node, and which maintains the state of the ledger. This process runs hyper-token.lua as its base, then adds the logic of hyper-token-p4.lua to it. This secondary script implements the charge function that p4@1.0 will call to charge a user's account.
2. A client script, which is executed as a p4@1.0 ledger device, which
uses ~push@1.0 to send requests to the ledger process.is_chargable_req/2 * is_chargable_req(Req, NodeMsg) -> any() The node operator may elect to make certain routes non-chargable, using
the routes syntax also used to declare routes in router@1.0.non_chargable_route_test/0 * non_chargable_route_test() -> any() Test that a non-chargable route is not charged for.request/3 request(State, Raw, NodeMsg) -> any() Estimate the cost of a transaction and decide whether to proceed with
a request. The default behavior if pricing-device or p4_balances are
not set is to proceed, so it is important that a user initialize them.response/3 response(State, RawResponse, NodeMsg) -> any() Postprocess the request after it has been fulfilled.test_opts/1 * test_opts(Opts) -> any() test_opts/2 * test_opts(Opts, PricingDev) -> any() test_opts/3 * test_opts(Opts, PricingDev, LedgerDev) -> any()

---

# 27. Module dev_routererl - HyperBEAM - Documentation

Document Number: 27
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_router.html
Words: 1355
Extraction Method: html

Module dev_router.erl A device that routes outbound messages from the node to their
appropriate network recipients via HTTP.Description All messages are initially
routed to a single process per node, which then load-balances them
between downstream workers that perform the actual requests.The routes for the router are defined in the routes key of the Opts,
as a precidence-ordered list of maps. The first map that matches the
message will be used to determine the route.Multiple nodes can be specified as viable for a single route, with the Choose key determining how many nodes to choose from the list (defaulting
to 1). The Strategy key determines the load distribution strategy,
which can be one of Random, By-Base, or Nearest. The route may also
define additional parallel execution parameters, which are used by the hb_http module to manage control of requests.The structure of the routes should be as follows:Node?: The node to route the message to.
       Nodes?: A list of nodes to route the message to.
       Strategy?: The load distribution strategy to use.
       Choose?: The number of nodes to choose from the list.
       Template?: A message template to match the message against, either as a
                  map or a path regex.Function Index add_route_test/0*  apply_route/3* Apply a node map's rules for transforming the path of the message.apply_routes/3* Generate a uri key for each node in a route.binary_to_bignum/1* Cast a human-readable or native-encoded ID to a big integer.by_base_determinism_test/0* Ensure that By-Base always chooses the same node for the same
hashpath.choose/5* Implements the load distribution strategies if given a cluster.choose_1_test/1*  choose_n_test/1*  device_call_from_singleton_test/0*  do_apply_route/3*  dynamic_route_provider_test/0*  dynamic_router/0*  dynamic_router_test_/0* Example of a Lua module being used as the route_provider for a
HyperBEAM node.dynamic_routing_by_performance/0*  dynamic_routing_by_performance_test_/0* Demonstrates routing tables being dynamically created and adjusted
according to the real-time performance of nodes.explicit_route_test/0*  extract_base/2* Extract the base message ID from a request message.field_distance/2* Calculate the minimum distance between two numbers
(either progressing backwards or forwards), assuming a
256-bit field.find_target_path/2* Find the target path to route for a request message.generate_hashpaths/1*  generate_nodes/1*  get_routes_test/0*  info/1 Exported function for getting device info, controls which functions are
exposed via the device API.info/3 HTTP info response providing information about this device.load_routes/1* Load the current routes for the node.local_dynamic_router/0*  local_dynamic_router_test_/0* Example of a Lua module being used as the route_provider for a
HyperBEAM node.local_process_route_provider/0*  local_process_route_provider_test_/0*  lowest_distance/1* Find the node with the lowest distance to the given hashpath.lowest_distance/2*  match/3 Find the first matching template in a list of known routes.match_routes/3*  match_routes/4*  preprocess/3 Preprocess a request to check if it should be relayed to a different node.register/3 Register function that allows telling the current node to register
a new route with a remote router node.request_hook_reroute_to_nearest_test/0* Test that the preprocess/3 function re-routes a request to remote
peers via ~relay@1.0, according to the node's routing table.route/2 Find the appropriate route for the given message.route/3  route_provider_test/0*  route_regex_matches_test/0*  route_template_message_matches_test/0*  routes/3 Device function that returns all known routes.simulate/4*  simulation_distribution/2*  simulation_occurences/2*  strategy_suite_test_/0*  template_matches/3* Check if a message matches a message template or path regex.unique_nodes/1*  unique_test/1*  weighted_random_strategy_test/0*  within_norms/3*  Function Details add_route_test/0 * add_route_test() -> any() apply_route/3 * apply_route(Msg, Route, Opts) -> any() Apply a node map's rules for transforming the path of the message.
Supports the following keys:
- opts: A map of options to pass to the request.
- prefix: The prefix to add to the path.
- suffix: The suffix to add to the path.
- replace: A regex to replace in the path.apply_routes/3 * apply_routes(Msg, R, Opts) -> any() Generate a uri key for each node in a route.binary_to_bignum/1 * binary_to_bignum(Bin) -> any() Cast a human-readable or native-encoded ID to a big integer.by_base_determinism_test/0 * by_base_determinism_test() -> any() Ensure that By-Base always chooses the same node for the same
hashpath.choose/5 * choose(N, X2, Hashpath, Nodes, Opts) -> any() Implements the load distribution strategies if given a cluster.choose_1_test/1 * choose_1_test(Strategy) -> any() choose_n_test/1 * choose_n_test(Strategy) -> any() device_call_from_singleton_test/0 * device_call_from_singleton_test() -> any() do_apply_route/3 * do_apply_route(X1, R, Opts) -> any() dynamic_route_provider_test/0 * dynamic_route_provider_test() -> any() dynamic_router/0 * dynamic_router() -> any() dynamic_router_test_/0 * dynamic_router_test_() -> any() Example of a Lua module being used as the route_provider for a
HyperBEAM node. The module utilized in this example dynamically adjusts the
likelihood of routing to a given node, depending upon price and performance.
also include preprocessing support for routing dynamic_routing_by_performance/0 * dynamic_routing_by_performance() -> any() dynamic_routing_by_performance_test_/0 * dynamic_routing_by_performance_test_() -> any() Demonstrates routing tables being dynamically created and adjusted
according to the real-time performance of nodes. This test utilizes the dynamic-router script to manage routes and recalculate weights based on the
reported performance.explicit_route_test/0 * explicit_route_test() -> any() extract_base/2 * extract_base(RawPath, Opts) -> any() Extract the base message ID from a request message. Produces a single
binary ID that can be used for routing decisions.field_distance/2 * field_distance(A, B) -> any() Calculate the minimum distance between two numbers
(either progressing backwards or forwards), assuming a
256-bit field.find_target_path/2 * find_target_path(Msg, Opts) -> any() Find the target path to route for a request message.generate_hashpaths/1 * generate_hashpaths(Runs) -> any() generate_nodes/1 * generate_nodes(N) -> any() get_routes_test/0 * get_routes_test() -> any() info/1 info(X1) -> any() Exported function for getting device info, controls which functions are
exposed via the device API.info/3 info(Msg1, Msg2, Opts) -> any() HTTP info response providing information about this device load_routes/1 * load_routes(Opts) -> any() Load the current routes for the node. Allows either explicit routes from
the node message's routes key, or dynamic routes generated by resolving the route_provider message.local_dynamic_router/0 * local_dynamic_router() -> any() local_dynamic_router_test_/0 * local_dynamic_router_test_() -> any() Example of a Lua module being used as the route_provider for a
HyperBEAM node. The module utilized in this example dynamically adjusts the
likelihood of routing to a given node, depending upon price and performance.local_process_route_provider/0 * local_process_route_provider() -> any() local_process_route_provider_test_/0 * local_process_route_provider_test_() -> any() lowest_distance/1 * lowest_distance(Nodes) -> any() Find the node with the lowest distance to the given hashpath.lowest_distance/2 * lowest_distance(Nodes, X) -> any() match/3 match(Base, Req, Opts) -> any() Find the first matching template in a list of known routes. Allows the
path to be specified by either the explicit path (for internal use by this
module), or route-path for use by external devices and users.match_routes/3 * match_routes(ToMatch, Routes, Opts) -> any() match_routes/4 * match_routes(ToMatch, Routes, Keys, Opts) -> any() preprocess/3 preprocess(Msg1, Msg2, Opts) -> any() Preprocess a request to check if it should be relayed to a different node.register(M1, M2, Opts) -> any() Register function that allows telling the current node to register
a new route with a remote router node. This function should also be idempotent.
so that it can be called only once.request_hook_reroute_to_nearest_test/0 * request_hook_reroute_to_nearest_test() -> any() Test that the preprocess/3 function re-routes a request to remote
peers via ~relay@1.0, according to the node's routing table.route/2 route(Msg, Opts) -> any() Find the appropriate route for the given message. If we are able to
resolve to a single host+path, we return that directly. Otherwise, we return
the matching route (including a list of nodes under nodes) from the list of
routes.If we have a route that has multiple resolving nodes, check
the load distribution strategy and choose a node. Supported strategies:All: Return all nodes (default).
         Random: Distribute load evenly across all nodes, non-deterministically.
        By-Base: According to the base message's hashpath.
      By-Weight: According to the node's <code>weight</code> key.
        Nearest: According to the distance of the node's wallet address to the
                 base message's hashpath.By-Base will ensure that all traffic for the same hashpath is routed to the
same node, minimizing work duplication, while Random ensures a more even
distribution of the requests.Can operate as a ~router@1.0 device, which will ignore the base message,
routing based on the Opts and request message provided, or as a standalone
function, taking only the request message and the Opts map.route/3 route(X1, Msg, Opts) -> any() route_provider_test/0 * route_provider_test() -> any() route_regex_matches_test/0 * route_regex_matches_test() -> any() route_template_message_matches_test/0 * route_template_message_matches_test() -> any() routes/3 routes(M1, M2, Opts) -> any() Device function that returns all known routes.simulate/4 * simulate(Runs, ChooseN, Nodes, Strategy) -> any() simulation_distribution/2 * simulation_distribution(SimRes, Nodes) -> any() simulation_occurences/2 * simulation_occurences(SimRes, Nodes) -> any() strategy_suite_test_/0 * strategy_suite_test_() -> any() template_matches/3 * template_matches(ToMatch, Template, Opts) -> any() Check if a message matches a message template or path regex.unique_nodes/1 * unique_nodes(Simulation) -> any() unique_test/1 * unique_test(Strategy) -> any() weighted_random_strategy_test/0 * weighted_random_strategy_test() -> any() within_norms/3 * within_norms(SimRes, Nodes, TestSize) -> any()

---

# 28. JoiningRunning a Router - HyperBEAM - Documentation

Document Number: 28
Source: https://hyperbeam.arweave.net/run/joining-running-a-router.html
Words: 906
Extraction Method: html

Router Networks: Joining vs Running Router networks in HyperBEAM have two distinct roles that are often confused:Two Different Concepts Joining a router = Registering your worker node with an existing router to receive work Running a router = Operating a router that manages and distributes work to other nodes When to use HB-OS Operation Use HB-OS?Purpose TEE Node (SNP) Recommended Secure, attested computation (hardware isolation) Router Registration Optional Registering/joining a router (TEE not required) You can join or run a router without HB-OS.If you want to run a TEE node, HB-OS or an equivalent TEE setup is recommended for convenience and security.Configuration Files: config.json vs config.flat Configuration can be set in either config.json (JSON syntax) or config.flat (flat syntax). The examples below use JSON for clarity, but you can use either format depending on your deployment. The syntax differs:config.json uses standard JSON structure (see examples below) config.flat uses key-value pairs Joining a Router Network (Worker Node) Most users want to join an existing router to offer computational services. This does NOT require HB-OS or TEE unless you specifically want TEE security.1. Prepare Your Configuration (config.json example) Use the following configuration as a template for your worker node:{
    // ─── Initial Configuration ─────────────────────────────────────────────────
    // Lock this configuration so it cannot be changed again
    "operator": "trustless",
    "initialized": "permanent",

    // ─── SNP-Based TEE Attestation Parameters ──────────────────────────────────
    // These values let the TEE verify its own environment—and any other VM
    // instantiated from the same image—before granting access.
    "snp_trusted": [],

    // ─── Request/Response Processing Configuration ─────────────────────────────
    // Defines how requests and responses are processed through the p4 device
    "on": {
        "request": {
            "device": "p4@1.0",
            "ledger-device": "lua@5.3a",
            "pricing-device": "simple-pay@1.0",
            "ledger-path": "/ledger~node-process@1.0",
            "module": ""        // Automatically injected
        },
        "response": {
            "device": "p4@1.0",
            "ledger-device": "lua@5.3a",
            "pricing-device": "simple-pay@1.0",
            "ledger-path": "/ledger~node-process@1.0",
            "module": ""        // Automatically injected
        }
    },

    // ─── Non-Chargeable Routes Configuration ──────────────────────────────────
    // Routes that should not incur charges when accessed through p4
    "p4_non_chargable_routes": [
        { "template": "/.*~node-process@1.0/.*" },
        { "template": "/.*~greenzone@1.0/.*" },
        { "template": "/.*~router@1.0/.*" },
        { "template": "/.*~meta@1.0/.*" },
        { "template": "/schedule" },
        { "template": "/push" },
        { "template": "/~hyperbuddy@1.0/.*" }
    ],

    // ─── Node Process Spawn Configuration ─────────────────────────────────────
    // Codec used for spawning new node processes
    "node_process_spawn_codec": "ans104@1.0",

    // ─── Node Process Definitions ─────────────────────────────────────────────
    // Configuration for individual node processes
    "node_processes": {
        "ledger": {
            "device": "process@1.0",
            "execution-device": "lua@5.3a",
            "scheduler-device": "scheduler@1.0",
            "authority-match": 1,
            "admin": "",                   // Automatically injected
            "token": "",                   // Automatically injected
            "module": "",                  // Automatically injected
            "authority": ""                // Automatically injected
        }
    },

    // ─── Router Registration Options ──────────────────────────────────────────
    // Configuration for how processes register with the router
    "router_opts": {
        "offered": [
            // {
            //     "registration-peer": {},            // Automatically injected
            //     "template": "/*~process@1.0/*",   // The routes that the node will register with
            //     "prefix": "",                       // Automatically injected
            //     "price": 4500000                    // Registration fee in smallest units
            // }
        ]
    },

    // ─── Greenzone Registration Options ────────────────────────────────────────
    // Configuration for how processes register with the greenzone
    "green_zone_peer_location": "",         // Automatically injected
    "green_zone_peer_id": "",               // Automatically injected

    // ─── P4 Recipient ──────────────────────────────────────────────────────────
    // The Address of the node that will receive the P4 messages
    "p4_recipient": ""                      // Automatically injected
} Perform the following API calls in order:Meta Info Post:Endpoint: ~meta@1.0/info POST Example:
Join Green Zone:Endpoint: ~greenzone@1.0/join GET Become Green Zone Member:Endpoint: ~greenzone@1.0/become GET Register as Router:Endpoint: ~router@1.0/register GET 3. Verify Registration Check your node's status in the network Confirm green zone membership Test routing functionality 4. Troubleshooting If registration fails:
1. Verify all configuration parameters are correct
2. Check network connectivity to the node URL
3. Ensure proper headers are set in API requests
4. Review logs for specific error messages
5. Confirm green zone availability and accessibility Running Your Own Router (Advanced) If you want to operate a router that manages other worker nodes:Deploy the dynamic router Lua process to handle registrations Configure trusted software hashes for TEE validation (if using TEE) Set up load balancing and performance monitoring Manage worker node admissibility policies Example Router Configuration (config.json example) {
    // ─── Router Node Preprocessing Settings ───────────────────────────────────
    // Defines the router process and how it preprocesses incoming requests
    "on": {
        "request": {
            "device": "router@1.0",
            "path": "preprocess",
            "commit-request": true         // Enable request commitment for routing
        }
    },

    // ─── Route Provider Configuration ─────────────────────────────────────────
    // Specifies where to get routing information from the router node process
    "router_opts": {
        "provider": {
            "path": "/router~node-process@1.0/compute/routes~message@1.0"
        },
        "registrar": {
            "path": "/router~node-process@1.0"
        },
        "registrar-path": "schedule"
    },

    // ─── Relay Configuration ──────────────────────────────────────────────────
    // Allow the relay to commit requests when forwarding
    "relay_allow_commit_request": true,

    // ─── Router Node Process Configuration ────────────────────────────────────
    // Specifies the Lua-based router logic, weights for scoring, and admission check
    "node_processes": {
        "router": {
            "type": "Process",
            "device": "process@1.0",
            "execution-device": "lua@5.3a",
            "scheduler-device": "scheduler@1.0",
            "pricing-weight": 9,           // Weight for pricing in routing decisions
            "performance-weight": 1,       // Weight for performance in routing decisions
            "score-preference": 4,         // Preference scoring for route selection
            "performance-period": 2,       // Period for performance measurement
            "initial-performance": 1000,   // Initial performance score
            // Default admission policy (currently set to false)
            "is-admissible": {
                "path": "default",
                "default": "false"
            },
            "module": "",                  // Automatically injected
            "trusted-peer": "",            // Automatically injected
            "trusted": ""                  // Automatically injected
        }
    }
} Advanced Configuration Running a production router requires careful consideration of security, performance, and economic incentives. Most users should join existing routers rather than run their own.Further Exploration Examine the dev_router.erl source code for detailed implementation.Review the scripts/dynamic-router.lua for router-side logic.Review the available configuration options in hb_opts.erl related to routing (routes, strategies, etc.).Consult community channels for best practices on deploying production routers.

---

# 29. Running a HyperBEAM Node - HyperBEAM - Documentation

Document Number: 29
Source: https://hyperbeam.arweave.net/run/running-a-hyperbeam-node.html
Words: 841
Extraction Method: html

Running a HyperBEAM Node This guide provides the basics for running your own HyperBEAM node, installing dependencies, and connecting to the AO network.System Dependencies To successfully build and run a HyperBEAM node, your system needs several software dependencies installed.Install core dependencies using Homebrew:brew install cmake git pkg-config openssl ncurses Install core dependencies using apt:sudo apt-get update && sudo apt-get install -y --no-install-recommends \
    build-essential \
    cmake \
    git \
    pkg-config \
    ncurses-dev \
    libssl-dev \
    sudo \
    curl \
    ca-certificates Erlang/OTP HyperBEAM is built on Erlang/OTP. You need version OTP 27 installed (check the rebar.config or project documentation for specific version requirements, typically OTP 27).Installation methods:brew install erlang@27 sudo apt install erlang=1:27.* Download from erlang.org and follow the build instructions for your platform.Rebar3 Rebar3 is the build tool for Erlang projects.Installation methods:brew install rebar3 Get the rebar3 binary from the official website. Place the downloaded rebar3 file in your system's PATH (e.g., /usr/local/bin) and make it executable (chmod +x rebar3).Node.js Node.js might be required for certain JavaScript-related tools or dependencies. Node version 22+ is required.Installation methods:brew install node Rust Rust is needed if you intend to work with or build components involving WebAssembly (WASM) or certain Native Implemented Functions (NIFs) used by some devices (like ~snp@1.0).The recommended way to install Rust on all platforms is via rustup:curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source "$HOME/.cargo/env" # Or follow the instructions provided by rustup Prerequisites for Running Before starting a node, ensure you have:Installed the system dependencies mentioned above.Cloned the HyperBEAM repository (git clone ...).Compiled the source code (rebar3 compile in the repo directory).An Arweave wallet keyfile (e.g., generated via Wander). The path to this file is typically set via the hb_key configuration option (see Configuring Your HyperBEAM Node).Starting a Basic Node The simplest way to start a HyperBEAM node for development or testing is using rebar3 from the repository's root directory:rebar3 shell This command:Starts the Erlang Virtual Machine (BEAM) with all HyperBEAM modules loaded.Initializes the node with default settings (from hb_opts.erl).Starts the default HTTP server (typically on port 8734), making the node accessible.Drops you into an interactive Erlang shell where you can interact with the running node.This basic setup is suitable for local development and exploring HyperBEAM's functionalities.HyperBEAM uses build profiles to enable optional features, often requiring extra dependencies. To run a node with specific profiles enabled, use rebar3 as ... shell:Available Profiles (Examples):genesis_wasm: Enables Genesis WebAssembly support.rocksdb: Enables the RocksDB storage backend.http3: Enables HTTP/3 support.Example Usage:# Start with RocksDB profile
rebar3 as rocksdb shell

# Start with RocksDB and Genesis WASM profiles
rebar3 as rocksdb, genesis_wasm shell Note: Choose profiles before starting the shell, as they affect compile-time options.Node Configuration HyperBEAM offers various configuration options (port, key file, data storage, logging, etc.). These are primarily set using a config.flat file and can be overridden by environment variables or command-line arguments.See the dedicated Configuring Your HyperBEAM Node guide for detailed information on all configuration methods and options.Verify Installation To quickly check if your node is running and accessible, you can send a request to its ~meta@1.0 device (assuming default port 8734):curl http://localhost:8734/~meta@1.0/info A JSON response containing node information indicates success.Running for Production (Mainnet) While you can connect to the main AO network using the rebar3 shell for testing purposes (potentially using specific configurations or helper functions like hb:start_mainnet/1 if available and applicable), the standard and recommended method for a stable production deployment (like running on the mainnet) is to build and run a release.1. Build the Release:From the root of the HyperBEAM repository, build the release package. You might include specific profiles needed for your mainnet setup (e.g., rocksdb if you intend to use it):# Build release with default profile
rebar3 release

# Or, build with specific profiles (example)
# rebar3 as rocksdb release This command compiles the project and packages it along with the Erlang Runtime System (ERTS) and all dependencies into a directory, typically _build/default/rel/hb.2. Configure the Release:Navigate into the release directory (e.g., cd _build/default/rel/hb). Ensure you have a correctly configured config.flat file here. See the configuration guide for details on setting mainnet parameters (port, key file location, store path, specific peers, etc.). Environment variables can also be used to override settings in the release's config.flat when starting the node.3. Start the Node:Use the generated start script (bin/hb) to run the node:# Start the node in the foreground (logs to console)
./bin/hb console

# Start the node as a background daemon
./bin/hb start

# Check the status
./bin/hb ping
./bin/hb status

# Stop the node
./bin/hb stop Consult the generated bin/hb script or Erlang/OTP documentation for more advanced start-up options (e.g., attaching a remote shell).Running as a release provides a more robust, isolated, and manageable way to operate a node compared to running directly from the rebar3 shell.Stopping the Node (rebar3 shell) To stop the node running within the rebar3 shell, press Ctrl+C twice or use the Erlang command q()..Configure Your Node: Deep dive into configuration options.TEE Nodes: Learn about running nodes in Trusted Execution Environments for enhanced security.Routers: Understand how to configure and run a router node.

---

# 30. ARIO Network Testnet - ARIO Docs

Document Number: 30
Source: https://docs.ar.io/guides/testnet
Words: 885
Extraction Method: html

Testnet The AR.IO Network Testnet allows developers to test their applications and workflows using ARIO Network features such as ArNS Names before deploying to the mainnet. The ARIO Network Testnet offers a faucet for requesting testnet ARIO tokens (tARIO). The initial version of testnet only supports registering and resolving temporary ArNS names; however, enhancements such as temporary data uploads will be added in the future. We welcome feedback for improvements and other feature requests.Faucet Browser UI The ARIO Network Testnet Faucet is a service that allows developers to request testnet ARIO tokens (tARIO). It can be accessed in a browser by visiting ar://faucet.This is the recommended way to use the faucet. To use it:Select Testnet from the network dropdown Enter your wallet address Enter the an amount of tARIO tokens (max 10000) Complete the captcha challenge Click the "Request Tokens" button Onece complete, tARIO tokens will automatically be sent to your wallet Using Testnet Using the testnet is similar to using the mainnet, with a few key differences:Using the ARIO SDK When using the ARIO SDK, to interact with the AR.IO testnet - you can create your ARIO instance in one of two ways;Using the ARIO.tesntet() API By default, this instance will leverage cu.ardrive.io for process evaluation and the recommended way to interact with testnet.Using process with ARIO_TESTNET_PROCESS_ID By default, this instance will leverage community CUs managed by forward.Note: ANTs are network-agnostic, so no additional configuration is needed when working with them.Once configured, all SDK methods will operate on testnet instead of mainnet. For more details on configuration, see the ARIO Configuration documentation.Accessing ArNS Names To access ArNS names on testnet in a browser, you must use a gateway that is configured to operate on testnet instead of mainnet.The gateway ar-io.dev is configured to operate on the ARIO Network Testnet.Using arns.app with Testnet arns.app is the primary graphical dApp for purchasing and managing ArNS names. To configure arns.app to operate on testnet:Click the Connect button in the top right corner to connect your wallet After connecting, click on your user profile button (which replaces the Connect button) Go to Settings  Click on ArNS Registry Settings  On the right side of the screen, you'll see three buttons: Devnet, Testnet, and Mainnet Click on Testnet to switch the app to operate on the testnet  The app will now operate on testnet, allowing you to purchase and manage ArNS names using testnet tokens.Running your own Gateway with testnet In addition to ar-io.dev - you can also elect to run your own ARIO gateway that resolves names against testnet. To do so, you need to setup your gateway by following the steps in the Linux Setup Guide or the Windows Setup Guide.Once running, modify the .env to point ARIO testnet process id.Once set, restart your gateway and navigate to <your-gateway-url>/ar-io/info - you should see agYcCFJtrMG6cqMuZfskIkFTGvUPddICmtQSBIoPdiA as the process id. Your gateway will now resolve arns names stored on the ARIO tesntet process.Restrictions Testnet has a few primary purposes: to mimic mainnet functionality as close as possible, to provide a testing bed for upcoming network upgrades, and to provide a playground for users and developers to experiment. It is NOT intended for production purposes and should not be used as such.
Test ARIO (tARIO) tokens are just that - test tokens. They have no external value, may break, and have no guarantee of continued support. tARIO tokens have no relation to mainnet $ARIO and are not a proxy for any rewards. There is no supply cap on tARIO tokens.
While advanced notice will be provided whenever possible, testnet may go offline for maintenance. Likewise, test token balances and test ArNS names may be reset/nullified at any point to clean up the contract state or prepare for an upgrade.Advanced Integrating AR.IO Testnet in your client-side applications If you'd like to incorporate the AR.IO faucet into your application, you can programmatically retrieve access tokens - which allow your application to request testnet tokens for your users.To integrate:import { ARIO, ARIOToken } from '@ar.io/sdk'

// setup testnet client;
const testnet = ARIO.testnet()

// request the captcha URL for the token, which will require a human to solve
const captchaURL = await testnet.faucet.captchaURL()

// open the captcha URL in a browser;
const captchaWindow = window.open(
  captchaUrl.captchaUrl,
  '_blank',
  'width=600,height=600',
)

// The captcha URL includes a window.parent.postMessage event that is used to send the auth token to the parent window.
// You can store the auth token in localStorage and use it to claim tokens for the duration of the auth token's expiration (default 1 hour).
window.parent.addEventListener('message', async (event) => {
  if (event.data.type === 'ario-jwt-success') {
    localStorage.setItem('ario-jwt', event.data.token)
    localStorage.setItem('ario-jwt-expires-at', event.data.expiresAt)
    // close our captcha window
    captchaWindow?.close()
    // claim the tokens using the JWT token,
    const res = await testnet.faucet
      .claimWithAuthToken({
        authToken: event.data.token,
        recipient: await window.arweaveWallet.getActiveAddress(),
        quantity: new ARIOToken(100).toMARIO().valueOf(), // 100 ARIO
      })
      .then((res) => {
        alert('Successfully claimed 100 ARIO tokens! Transaction ID: ' + res.id)
      })
      .catch((err) => {
        alert(`Failed to claim tokens: ${err}`)
      })
  }
})

// you can re-use the JWT for up to 1 hour, allowing you to request tokens for multiple wallets without having to satisfy the catpcha multiple times
if (
  localStorage.getItem('ario-jwt-expires-at') &&
  Date.now() < parseInt(localStorage.getItem('ario-jwt-expires-at') ?? '0')
) {
  const res = await testnet.faucet.claimWithAuthToken({
    authToken: localStorage.getItem('ario-jwt') ?? '',
    recipient: await window.arweaveWallet.getActiveAddress(),
    quantity: new ARIOToken(100).toMARIO().valueOf(), // 100 ARIO
  })
}

---

# 31. ARIO Docs

Document Number: 31
Source: https://docs.ar.io/wayfinder/getting-started
Words: 604
Extraction Method: html

Getting Started with Wayfinder Wayfinder provides decentralized and verified access to data stored on Arweave. This guide will help you get started with the core concepts and basic usage.Installation Choose the package that fits your project:Core Library (JavaScript/TypeScript) React Components Quick Start Basic Usage The simplest way to get started is with the default configuration:React Integration For React applications, use the wayfinder-react package:Automatic Configuration Wayfinder React automatically configures LocalStorageGatewaysProvider with NetworkGatewaysProvider to avoid rate limits and provide optimal performance
for web applications.function YourComponent() {
  const txId = 'your-transaction-id'; // Replace with actual txId

  // Use custom hooks for URL resolution and data fetching
  const request = useWayfinderRequest();

  // store the fetched data
  const [data, setData] = useState<any>(null);
  const [dataLoading, setDataLoading] = useState(false);
  const [dataError, setDataError] = useState<Error | null>(null);

  useEffect(() => {
    (async () => {
      try {
        setDataLoading(true);
        setDataError(null);
        // fetch the data for the txId using wayfinder
        const response = await request(`ar://${txId}`, {
          verificationSettings: {
            enabled: true, // enable verification on the request
            strict: true, // don't use the data if it's not verified
          },
        });
        const data = await response.arrayBuffer(); // or response.json() if you want to parse the data as JSON
        setData(data);
      } catch (error) {
        setDataError(error as Error);
      } finally {
        setDataLoading(false);
      }
    })();
  }, [request, txId]);

  return (
    <div>
      {dataLoading && <p>Loading data...</p>}
      {dataError && <p>Error loading data: {dataError.message}</p>}
      <pre>{data}</pre>
    </div>
  );
} Available Strategies Routing Strategies Strategy Description Use Case FastestPingRoutingStrategy Selects gateway with lowest latency Performance-critical applications PreferredWithFallbackRoutingStrategy Tries preferred gateway first, falls back to others When you have a trusted primary gateway RoundRobinRoutingStrategy Distributes requests evenly across gateways Load balancing and fair distribution RandomRoutingStrategy Randomly selects from available gateways Simple load distribution PingRoutingStrategy Wraps other strategies with health checks Adding reliability to any routing strategy Verification Strategies Verification strategies may be dependent on the gateway being used having the
data indexed locally. A gateway cannot verify data it doesn't have access to
or hasn't indexed yet.Strategy Description Use Case HashVerificationStrategy Verifies data against trusted gateway hashes Fast verification with trusted sources SignatureVerificationStrategy Validates Arweave transaction signatures Cryptographic proof of authenticity DataRootVerificationStrategy Verifies against transaction data roots Block-level verification Advanced Configuration For production applications, you'll want to configure gateway providers, routing strategies, and verification:import {
  Wayfinder,
  NetworkGatewaysProvider,
  FastestPingRoutingStrategy,
  HashVerificationStrategy,
} from '@ar.io/wayfinder-core'
import { ARIO } from '@ar.io/sdk'

const wayfinder = new Wayfinder({
  // Discover gateways from the AR.IO Network
  gatewaysProvider: new SimpleCacheGatewaysProvider({
    gatewaysProvider: new NetworkGatewaysProvider({
      ario: ARIO.mainnet(),
      limit: 10,
      sortBy: 'operatorStake',
      sortOrder: 'desc',
    }),
  }),

  // Use fastest ping routing strategy
  routingSettings: {
    strategy: new FastestPingRoutingStrategy({
      timeoutMs: 500,
    }),
    events: {
      onRoutingSucceeded: (event) => {
        console.log('Selected gateway:', event.selectedGateway)
      },
    },
  },

  // Enable data verification
  verificationSettings: {
    enabled: true,
    strategy: new HashVerificationStrategy({
      trustedGateways: ['https://arweave.net'],
    }),
    events: {
      onVerificationSucceeded: (event) => {
        console.log('Verification passed for:', event.txId)
      },
      onVerificationFailed: (event) => {
        console.log('Verification failed for:', event.txId)
      },
    },
  },

  // Enable telemetry
  telemetrySettings: {
    enabled: true,
    clientName: 'my-app',
    clientVersion: '1.0.0',
    sampleRate: 0.1, // 10% sampling
  },
}) Core Concepts Gateway Providers Gateway providers discover and manage the list of available AR.IO gateways:NetworkGatewaysProvider: Fetches gateways from the AR.IO Network StaticGatewaysProvider: Uses a predefined list of gateways SimpleCacheGatewaysProvider: Caches gateway lists for performance in-memory LocalStorageGatewaysProvider: Caches gateway lists for performance in window.localStorage (default for React applications) Routing Strategies Routing strategies determine which gateway to use for each request:FastestPingRoutingStrategy: Selects the gateway with lowest latency PingRoutingStrategy: Wraps other strategies with health checks PreferredWithFallbackRoutingStrategy: Tries a preferred gateway first RoundRobinRoutingStrategy: Distributes requests evenly RandomRoutingStrategy: Randomly selects gateways Verification Strategies Verification strategies ensure data integrity:HashVerificationStrategy: Verifies data against trusted gateway hashes SignatureVerificationStrategy: Validates Arweave transaction signatures DataRootVerificationStrategy: Verifies against transaction data roots

---

# 32. GraphQL Queries  Cooking with the Permaweb

Document Number: 32
Source: https://cookbook.arweave.net/fundamentals/accessing-arweave-data/graphql.html
Words: 248
Extraction Method: html

GraphQL Queries Overview Over time, indexing services that implement a GraphQL interface have became the preferred method for querying transaction data on Arweave. An indexing service reads transaction and block headers as they are added to the network (usually from a full Arweave node which the service operates). Once read, the header info is inserted into a database where it can be indexed and efficiently queried. The indexing service uses this database to provide a GraphQL endpoint for clients to query.GraphQL has a few advantages that make it ideal for retrieving query data sets. It enables indexing services to create a single endpoint that can then be used to query all types data. The service is able to return multiple resources in a single request as opposed to making an HTTP request for each resource (like one would with a REST API). With GraphQL, clients can batch multiple requests in a single round-trip and specify exactly what data is needed which increases performance.Basic Query Example The following GraphQL example queries all the transaction ids from a given owners wallet address that have a "Type" tag with a value of "manifest". For more information about tags, read the guide on Transaction Tags.const queryObject = {
    query:
    `{
        transactions (
            owners:["${address}"],
            tags: [
              {
                    name: "Type",
                    values: ["manifest"]
                }
            ]
        ) {
            edges {
                node {
                    id
                }
            }
        }
    }`
};
const results = await arweave.api.post('/graphql', queryObject);Public Indexing Services https://arweave.net/graphql https://arweave-search.goldsky.com/graphql Resources Querying Arweave Guide ar-gql package GraphQL Reference

---

# 33. What are AO Processes  Cooking with the Permaweb

Document Number: 33
Source: https://cookbook.arweave.net/fundamentals/decentralized-computing/ao-processes/what-are-ao-processes.html
Words: 1399
Extraction Method: html

What are AO Processes AO processes are autonomous compute units that run on the Arweave network, enabling decentralized applications to execute complex logic permanently and trustlessly. Think of them as serverless functions that never go down and can maintain state across invocations.Core Architecture AO processes represent a paradigm shift from traditional smart contracts. Unlike Ethereum's synchronous execution model, AO processes operate asynchronously, communicating through message passing in a distributed network.AO Process Architecture:User/Application
       ↓ (Message)
   AO Process ←→ Another Process
       ↓ ↑         (Messages)
   State Update/Read
       ↓ ↑
 Arweave Storage This architecture demonstrates how AO processes communicate through asynchronous message passing while maintaining persistent state on Arweave. Each process operates independently while being able to interact with other processes in the network.Key Components Process Instance Unique process ID (43-character string) Lua-based execution environment Persistent state storage on Arweave Message inbox for receiving communications Message System Asynchronous message passing Tagged messages for routing and filtering Cryptographic signatures for authentication Permanent message history on Arweave State Management Deterministic state transitions Immutable state snapshots Conflict-free replicated data types (CRDTs) Rollback and replay capabilities Process Lifecycle 1. Process Creation Creating an AO process involves deploying Lua code to the network:import { connect } from "@permaweb/aoconnect";

const ao = connect();

// Deploy a new process
const processId = await ao.spawn({
  module: "MODULE_TX_ID", // Pre-compiled Lua module
  scheduler: "SCHEDULER_ADDRESS", // Network scheduler
  signer: createDataItemSigner(wallet), // Wallet signer
  tags: [
    { name: "App-Name", value: "MyApp" },
    { name: "App-Version", value: "1.0.0" }
  ]
});

console.log("Process created:", processId);2. Process Initialization Once spawned, the process can be initialized with initial state:// Send initialization message
await ao.message({
  process: processId,
  tags: [
    { name: "Action", value: "Initialize" }
  ],
  data: JSON.stringify({
    owner: "USER_ADDRESS",
    name: "My Process",
    version: "1.0.0"
  }),
  signer: createDataItemSigner(wallet)
});3. Message Processing Processes receive and handle messages according to their Lua handlers:-- Example Lua handler in the process
Handlers.add(
  "Initialize",
  Handlers.utils.hasMatchingTag("Action", "Initialize"),
  function(msg)
    local data = json.decode(msg.Data)
    State.owner = data.owner
    State.name = data.name
    State.initialized = true
    
    ao.send({
      Target = msg.From,
      Data = "Process initialized successfully"
    })
  end
) State Management Patterns Deterministic State Updates AO processes maintain deterministic state through ordered message processing:-- State variables
Balance = Balance or 0
Transactions = Transactions or {}

-- Handler for balance updates
Handlers.add(
  "UpdateBalance",
  Handlers.utils.hasMatchingTag("Action", "UpdateBalance"),
  function(msg)
    local amount = tonumber(msg.Tags.Amount)
    local operation = msg.Tags.Operation
    
    if operation == "credit" then
      Balance = Balance + amount
    elseif operation == "debit" and Balance >= amount then
      Balance = Balance - amount
    else
      ao.send({
        Target = msg.From,
        Data = "Insufficient balance"
      })
      return
    end
    
    -- Record transaction
    table.insert(Transactions, {
      id = msg.Id,
      from = msg.From,
      amount = amount,
      operation = operation,
      timestamp = msg.Timestamp,
      balance = Balance
    })
    
    ao.send({
      Target = msg.From,
      Data = json.encode({
        success = true,
        balance = Balance,
        transactionId = msg.Id
      })
    })
  end
) State Persistence State is automatically persisted to Arweave through the process lifecycle:-- State checkpoint handler
Handlers.add(
  "SaveCheckpoint",
  Handlers.utils.hasMatchingTag("Action", "SaveCheckpoint"),
  function(msg)
    local checkpoint = {
      balance = Balance,
      transactions = Transactions,
      lastUpdate = msg.Timestamp,
      version = "1.0.0"
    }
    
    -- State is automatically persisted
    ao.send({
      Target = msg.From,
      Data = "Checkpoint saved",
      Tags = {
        { name = "Checkpoint-Data", value = json.encode(checkpoint) }
      }
    })
  end
) Common Use Cases 1. Token Contracts AO processes excel at implementing token logic:-- Token contract implementation
Name = "MyToken"
Ticker = "MTK"
Denomination = 12
TotalSupply = 1000000 * 10^Denomination
Balances = { [Owner] = TotalSupply }

Handlers.add(
  "Transfer",
  Handlers.utils.hasMatchingTag("Action", "Transfer"),
  function(msg)
    local target = msg.Tags.Recipient
    local quantity = tonumber(msg.Tags.Quantity)
    
    if Balances[msg.From] and Balances[msg.From] >= quantity then
      Balances[msg.From] = Balances[msg.From] - quantity
      Balances[target] = (Balances[target] or 0) + quantity
      
      -- Emit events
      ao.send({ Target = msg.From, Data = "Transfer successful" })
      ao.send({ Target = target, Data = "Tokens received" })
    else
      ao.send({ Target = msg.From, Data = "Insufficient balance" })
    end
  end
) 2. Decentralized Applications Build complex dApps with multiple interacting processes:-- DAO voting process
Proposals = Proposals or {}
Votes = Votes or {}

Handlers.add(
  "CreateProposal",
  Handlers.utils.hasMatchingTag("Action", "CreateProposal"),
  function(msg)
    local proposalId = msg.Id
    Proposals[proposalId] = {
      title = msg.Tags.Title,
      description = msg.Data,
      creator = msg.From,
      created = msg.Timestamp,
      status = "active",
      votesFor = 0,
      votesAgainst = 0
    }
    
    ao.send({
      Target = msg.From,
      Data = "Proposal created: " .. proposalId
    })
  end
) 3. Data Processing Pipelines Chain processes together for complex workflows:-- Data processing handler
Handlers.add(
  "ProcessData",
  Handlers.utils.hasMatchingTag("Action", "ProcessData"),
  function(msg)
    local data = json.decode(msg.Data)
    
    -- Process the data
    local processed = transformData(data)
    
    -- Send to next process in pipeline
    ao.send({
      Target = msg.Tags.NextProcess,
      Data = json.encode(processed),
      Tags = {
        { name = "Action", value = "ReceiveProcessedData" },
        { name = "Source", value = ao.id }
      }
    })
  end
) Process Communication Patterns Direct Messaging Processes communicate directly through tagged messages:// Send message to specific process
await ao.message({
  process: targetProcessId,
  tags: [
    { name: "Action", value: "GetBalance" },
    { name: "Account", value: userAddress }
  ],
  signer: createDataItemSigner(wallet)
});

// Receive response
const result = await ao.result({
  message: messageId,
  process: targetProcessId
});Pub/Sub Patterns Implement publish-subscribe messaging:Development Best Practices Implement robust error handling in your processes:-- Comprehensive error handling
Handlers.add(
  "SafeOperation",
  Handlers.utils.hasMatchingTag("Action", "SafeOperation"),
  function(msg)
    local success, result = pcall(function()
      -- Your operation logic here
      local data = json.decode(msg.Data)
      if not data.required_field then
        error("Missing required field")
      end
      
      return processData(data)
    end)
    
    if success then
      ao.send({
        Target = msg.From,
        Data = json.encode({ success = true, result = result })
      })
    else
      ao.send({
        Target = msg.From,
        Data = json.encode({ 
          success = false, 
          error = result,
          timestamp = msg.Timestamp 
        })
      })
    end
  end
) Access Control Implement proper authorization:-- Role-based access control
Roles = {
  [Owner] = "admin",
  -- Add other role assignments
}

local function hasRole(address, requiredRole)
  return Roles[address] == requiredRole
end

Handlers.add(
  "AdminOnly",
  Handlers.utils.hasMatchingTag("Action", "AdminOnly"),
  function(msg)
    if not hasRole(msg.From, "admin") then
      ao.send({
        Target = msg.From,
        Data = "Access denied: Admin role required"
      })
      return
    end
    
    -- Admin logic here
  end
) Testing Strategies Use AOS (AO Studio) for local development and testing:# Install AOS for local testing
npm install -g https://get_ao.g8way.io

# Start AOS REPL
aos

# Load your process code
.load process.lua

# Test message handling
Send({ Action = "Test", Data = "test data" }) Performance Considerations Message Optimization Structure messages for efficient processing:-- Batch operations for efficiency
Handlers.add(
  "BatchTransfer",
  Handlers.utils.hasMatchingTag("Action", "BatchTransfer"),
  function(msg)
    local transfers = json.decode(msg.Data)
    local results = {}
    
    for i, transfer in ipairs(transfers) do
      local success = executeTransfer(transfer.to, transfer.amount)
      table.insert(results, {
        index = i,
        success = success,
        to = transfer.to,
        amount = transfer.amount
      })
    end
    
    ao.send({
      Target = msg.From,
      Data = json.encode(results)
    })
  end
) State Management Optimization Keep state lean and efficient:-- Use efficient data structures
-- Instead of storing full transaction history:
-- Transactions = {} -- Can grow very large

-- Use rolling window or summary data:
RecentTransactions = {} -- Last 100 transactions
TransactionSummary = {
  total_count = 0,
  total_volume = 0,
  last_updated = 0
} Security Considerations Input Validation Always validate incoming data:local function validateTransfer(msg)
  local recipient = msg.Tags.Recipient
  local quantity = tonumber(msg.Tags.Quantity)
  
  if not recipient or recipient == "" then
    return false, "Invalid recipient"
  end
  
  if not quantity or quantity <= 0 then
    return false, "Invalid quantity"
  end
  
  if not Balances[msg.From] or Balances[msg.From] < quantity then
    return false, "Insufficient balance"
  end
  
  return true, "Valid"
end Reentrancy Protection Protect against message replay attacks:ProcessedMessages = ProcessedMessages or {}

local function isProcessed(messageId)
  return ProcessedMessages[messageId] ~= nil
end

local function markProcessed(messageId)
  ProcessedMessages[messageId] = true
end

Handlers.add(
  "IdempotentHandler",
  Handlers.utils.hasMatchingTag("Action", "IdempotentHandler"),
  function(msg)
    if isProcessed(msg.Id) then
      return -- Already processed
    end
    
    -- Process the message
    -- ... handler logic ...
    
    markProcessed(msg.Id)
  end
) Monitoring and Debugging Process Health Checks Implement health monitoring:Handlers.add(
  "HealthCheck",
  Handlers.utils.hasMatchingTag("Action", "HealthCheck"),
  function(msg)
    local health = {
      status = "healthy",
      uptime = msg.Timestamp - (StartTime or 0),
      balance = Balance,
      message_count = #ProcessedMessages,
      last_activity = LastActivity or StartTime
    }
    
    ao.send({
      Target = msg.From,
      Data = json.encode(health)
    })
  end
) Debugging Tools Use logging for debugging:-- Debug logging handler
local DEBUG_MODE = true

local function debugLog(message, data)
  if DEBUG_MODE then
    print("DEBUG [" .. os.date() .. "]: " .. message)
    if data then
      print("Data: " .. json.encode(data))
    end
  end
end

Handlers.add(
  "DebugHandler",
  function() return DEBUG_MODE end,
  function(msg)
    debugLog("Received message", {
      action = msg.Tags.Action,
      from = msg.From,
      id = msg.Id
    })
  end
) Now that you understand AO processes fundamentals:Learn process communication - Process Communication Master state management - State Management Explore HyperBEAM - HyperBEAM Introduction Build your first process - Builder's Journey Resources AO Documentation: Official AO Docs AOS (AO Studio): Development Environment Code Examples: AO Cookbook Repository Community: AO Discord Channel

---

# 34. Exposing Process State to HyperBEAM  Cooking with the Permaweb

Document Number: 34
Source: https://cookbook.arweave.net/fundamentals/decentralized-computing/hyperbeam/getting-ao-state.html
Words: 447
Extraction Method: html

Exposing Process State to HyperBEAM HyperBEAM introduces a powerful feature for exposing parts of a process's state for immediate reading over HTTP. This improves performance for web frontends and data services by replacing the need for dryrun calls.The Patch Device The ~patch@1.0 device is the mechanism that allows AO processes to make parts of their internal state readable via direct HTTP GET requests.How it Works Exposing state is a four-step process:Process Logic: Send an outbound message to the ~patch@1.0 device from your process.Patch Message Format: The message must include device and cache tags.Send({ Target = ao.id, device = 'patch@1.0', cache = { mydatakey = MyValue } }) HyperBEAM Execution: HyperBEAM's dev_patch module processes this message, mapping the key-value pairs from the cache table to a URL path.HTTP Access: The exposed data is then immediately available via a standard HTTP GET request:GET /<process-id>~process@1.0/now/cache/<mydatakey> Initial State Sync (Optional) To make data available immediately on process creation:-- Place this logic at the top level of your process script

Balances = { token1 = 100, token2 = 200 } -- A table of balances
TotalSupply = 1984 -- A single total supply value

-- 1. Initialize Flag:
InitialSync = InitialSync or 'INCOMPLETE'

-- 2. Check Flag:
if InitialSync == 'INCOMPLETE' then
  -- 3. Patch State:
  Send({ device = 'patch@1.0', cache = { balances = Balances, totalsupply = TotalSupply } })
  
  -- 4. Update Flag:
  InitialSync = 'COMPLETE'
end Practical Example Here's a complete example of a token contract that exposes its balance state:-- Token Process with State Exposure

Balances = Balances or { [ao.id] = 1000000 }
TotalSupply = TotalSupply or 1000000

-- Initial state sync
InitialSync = InitialSync or 'INCOMPLETE'
if InitialSync == 'INCOMPLETE' then
  Send({ device = 'patch@1.0', cache = { 
    balances = Balances, 
    totalsupply = TotalSupply 
  }})
  InitialSync = 'COMPLETE'
end

-- Transfer handler
Handlers.add("transfer", "Transfer", function(msg)
  local from = msg.From
  local to = msg.Tags.Recipient or msg.Recipient
  local amount = tonumber(msg.Tags.Quantity or msg.Quantity)
  
  if Balances[from] and Balances[from] >= amount then
    Balances[from] = Balances[from] - amount
    Balances[to] = (Balances[to] or 0) + amount
    
    -- Update exposed state after transfer
    Send({ device = 'patch@1.0', cache = { 
      balances = Balances 
    }})
    
    ao.send({ Target = from, Data = "Transfer successful" })
  else
    ao.send({ Target = from, Data = "Insufficient balance" })
  end
end) Accessing Exposed Data Once state is exposed via the patch device, you can query it directly over HTTP:# Get all balances
curl https://hyperbeam-node.arweave.net/<process-id>~process@1.0/compute/cache/balances

# Get total supply
curl https://hyperbeam-node.arweave.net/<process-id>~process@1.0/compute/cache/totalsupply Benefits Performance: Direct HTTP access is significantly faster than traditional dryrun calls.Simplicity: Standard REST-like patterns instead of complex message handling.Real-time Updates: State changes are immediately reflected in HTTP responses.Caching: HyperBEAM can cache frequently accessed data for even better performance.

---

# 35. HyperBEAM Devices  Cooking with the Permaweb

Document Number: 35
Source: https://cookbook.arweave.net/fundamentals/decentralized-computing/hyperbeam/hyperbeam-devices.html
Words: 1713
Extraction Method: html

HyperBEAM Devices HyperBEAM devices are the modular building blocks that power the AO Computer. Think of them as specialized engines or services that can be plugged into the AO framework to provide specific computational capabilities. This modularity is key to AO's flexibility, extensibility, and ability to evolve with new technologies.Understanding the Device Architecture What Are Devices?In AO-Core and HyperBEAM, Devices are modular components responsible for processing and interpreting Messages. They define the specific logic for how computations are performed, data is handled, or interactions occur within the AO ecosystem.Each device is essentially an Erlang module that implements a specific interface, allowing it to:Define computation logic - Dictate how message instructions are executed Enable specialization - Allow nodes to focus on specific computational tasks Promote modularity - Add new functionality without altering the core protocol Distribute workload - Handle different parts of complex tasks in parallel HyperBEAM Device Architecture:HTTP Request
     ↓
HyperBEAM Router
     ↓
Device Selection
     ↓
┌─────────────────────────────────────────┐
│ Device Types:                           │
│ • ~process@1.0  → Process State Mgmt    │
│ • ~lua@5.3a     → Lua Script Execution │
│ • ~wasm64@1.0   → WebAssembly Execution│
│ • ~json@1.0     → JSON Processing      │
└─────────────────────────────────────────┘
     ↓
Processing Results
     ↓
HTTP Response

Device Ecosystem:
┌─────────────────────────────────────────┐
│ • Security Devices (authentication)     │
│ • Utility Devices (routing, caching)   │
│ • Custom Devices (domain-specific)     │
│ • Communication Devices (relays)       │
│ • Storage Devices (state management)   │
└─────────────────────────────────────────┘ This modular architecture allows HyperBEAM to handle diverse computational tasks by routing requests to specialized devices, each optimized for specific types of processing.Device Naming and Versioning Devices follow a consistent naming convention that makes them easy to identify and use:Format:~name@version or dev_name (for internal devices) Examples:~process@1.0 - Primary process management device ~lua@5.3a - Lua 5.3 execution device ~wasm64@1.0 - WebAssembly 64-bit execution device dev_router - Internal routing device (development prefix) The tilde (~) indicates a primary, user-facing device, while the dev_ prefix is used for internal or utility devices in the source code.Versioning Strategy Versioning indicates the specific interface and behavior of the device:Semantic versioning - Major.minor.patch format Backward compatibility - Breaking changes increment major version Feature additions - New features increment minor version Bug fixes - Patches increment patch version Core HyperBEAM Devices Process Management Devices ~process@1.0 - Process State Manager The process device manages persistent, shared computational states similar to traditional smart contracts, but with greater flexibility.# Access current process state
GET /PROCESS_ID~process@1.0/now

# Get cached state (faster)
GET /PROCESS_ID~process@1.0/compute

# Access specific state fields
GET /PROCESS_ID~process@1.0/now/balance
GET /PROCESS_ID~process@1.0/compute/users/USER_ADDRESS Key Functions:now - Calculates real-time process state by processing all messages compute - Returns the latest known state without checking for new messages State persistence - Automatic state snapshots to Arweave Message ordering - Ensures deterministic state transitions Use Cases:Token contracts and DeFi applications Voting and governance systems Game state management Decentralized databases ~scheduler@1.0 - Message Scheduling Handles the ordering and execution timing of messages within processes.# Query scheduler status
GET /PROCESS_ID~scheduler@1.0/status

# Get message queue information
GET /PROCESS_ID~scheduler@1.0/queue/pending Responsibilities:Message ordering and consensus Execution timing coordination Load balancing across compute units Fault tolerance and recovery Execution Devices ~lua@5.3a - Lua Script Execution Executes Lua scripts for serverless functions and data processing.# Simple calculation
GET /~lua@5.3a&script=return 2 + 3 * 4/result

# With parameters
GET /~lua@5.3a&script=return Args.name .. " is " .. Args.age .. " years old"&name="Alice"&age+integer=25/result

# Using modules
GET /~lua@5.3a&module=MODULE_TX_ID&script=return math_utils.factorial(Args.n)&n+integer=5/result Capabilities:Full Lua 5.3 language support Module loading from Arweave transactions JSON processing and manipulation String processing and regex Mathematical computations HTTP client functionality (via libraries) Performance Characteristics:Lightweight execution overhead Fast startup time (no cold starts) Memory efficient for small to medium computations Excellent for data transformation and business logic ~wasm64@1.0 - WebAssembly Execution Executes WebAssembly code for high-performance computations written in languages like Rust, C++, Go, and others.# Execute WASM module
GET /~wasm64@1.0&module=WASM_MODULE_TX_ID/function_name

# With parameters
GET /~wasm64@1.0&module=WASM_MODULE_TX_ID&arg1+integer=100&arg2="test"/compute Advantages:High performance - Near-native execution speed Multiple languages - Support for Rust, C++, Go, AssemblyScript Sandboxed execution - Secure isolated environment Predictable performance - No garbage collection pauses Use Cases:Cryptographic operations (hashing, signatures, ZK proofs) Image and video processing Machine learning inference Scientific computing Game engines and simulations Example WASM Module (Rust):// Compile to WASM and deploy to Arweave
use wasm_bindgen::prelude::*;

#[wasm_bindgen]
pub fn fibonacci(n: u32) -> u32 {
    match n {
        0 | 1 => n,
        _ => fibonacci(n - 1) + fibonacci(n - 2),
    }
}

#[wasm_bindgen]
pub fn hash_data(data: &str) -> String {
    use sha2::{Sha256, Digest};
    let mut hasher = Sha256::new();
    hasher.update(data);
    format!("{:x}", hasher.finalize())
} Data Processing Devices ~json@1.0 - JSON Manipulation Provides JSON data structure access and manipulation capabilities.# Format process state as JSON
GET /PROCESS_ID~process@1.0/now~json@1.0

# Pretty-print JSON
GET /PROCESS_ID~process@1.0/compute~json@1.0&pretty=true

# Extract specific JSON fields
GET /~json@1.0&data={"users":{"alice":{"balance":100}}}/users/alice/balance Features:JSON serialization and deserialization Path-based field access Pretty printing and formatting Schema validation (when configured) Type conversion and casting ~message@1.0 - Message Processing The default device that resolves keys to their literal values within messages.# Create temporary message with data
GET /~message@1.0&greeting="Hello"&count+integer=42/count
# Response: 42

# Complex data structures
GET /~message@1.0&config+map=host="localhost";port+integer=3000&items+list="a","b","c"/config/port
# Response: 3000 Type Casting Support:+integer - Convert to integer +float - Convert to floating point +boolean - Convert to boolean +list - Parse comma-separated values +map - Parse key-value pairs +binary - Treat as binary string (default) Communication Devices ~relay@1.0 - Message Relay Forwards messages between AO nodes or to external HTTP endpoints.# Relay GET request to external API
GET /~relay@1.0/call?method=GET&path=https://api.example.com/data

# Relay POST with data
POST /~relay@1.0/call?method=POST&path=https://webhook.site/your-webhook
Content-Type: application/json

{"message": "Hello from AO"}

# Relay to another AO process
GET /~relay@1.0/process/TARGET_PROCESS_ID?action=GetBalance&user=ALICE Use Cases:Cross-chain bridges - Connect to other blockchain networks External API integration - Fetch data from Web2 services Inter-process communication - Route messages between AO processes Webhook delivery - Send notifications to external services Security and Verification Devices ~snp@1.0 - Secure Enclave Verification Handles Trusted Execution Environment (TEE) attestation and verification.# Get TEE attestation report
GET /~snp@1.0/attestation

# Verify node is running in genuine TEE
GET /~snp@1.0/verify Security Features:AMD SEV-SNP attestation Intel TXT support Hardware security verification Remote attestation protocols Cryptographic proof generation dev_codec_httpsig - HTTP Signature Processing Manages HTTP message signing and verification for authentication.Capabilities:HTTP signature generation and verification Multiple signature algorithms (RSA, ECDSA, EdDSA) Request/response integrity verification Authentication and authorization Utility and System Devices ~meta@1.0 - Node Configuration Configures the HyperBEAM node itself including hardware specs, supported devices, and payment information.# Get node capabilities
GET /~meta@1.0/capabilities

# Get supported devices
GET /~meta@1.0/devices

# Get node status
GET /~meta@1.0/status Configuration Options:Hardware specifications Available compute resources Supported device list Payment and billing information Network connectivity options dev_cron - Task Scheduling Coordinates scheduled task execution and workflow management.Features:Cron-like task scheduling Recurring job management Event-driven automation Workflow orchestration dev_monitor - System Monitoring Monitors process activity, performance metrics, and system health.Monitoring Capabilities:Process execution metrics Resource utilization tracking Error rate monitoring Performance benchmarking Alert generation Financial and Access Control Devices ~p4@1.0 - Payment Processing Manages metering, billing, and micropayments for node services.# Check payment status
GET /~p4@1.0/balance/USER_ADDRESS

# Get pricing information
GET /~p4@1.0/pricing/compute Payment Features:Micropayment processing Usage-based billing Multi-token support Payment channel management Revenue sharing protocols ~faff@1.0 - Access Control Handles authorization and access control for protected resources.Access Control Features:Role-based access control (RBAC) Attribute-based access control (ABAC) Token-based authentication Multi-signature authorization Temporary access grants Data Storage and Management ~patch@1.0 - State Management Applies state updates directly to processes, often used for data migration and management.# Apply state patch to process
POST /PROCESS_ID~patch@1.0/apply
Content-Type: application/json
{
  "operation": "update",
  "path": "/users/alice/balance",
  "value": 1500
}

# Get patch history
GET /PROCESS_ID~patch@1.0/history Patch Operations:State updates and migrations Data consistency maintenance Version control for process state Rollback and recovery operations Advanced Device Concepts Device Composition and Pipelines Devices can be chained together to create sophisticated processing pipelines:# Multi-device pipeline:
# 1. Get process state
# 2. Transform with Lua
# 3. Format as JSON
# 4. Apply template
GET /TOKEN_PROCESS~process@1.0/now/~lua@5.3a&module=ANALYTICS_MODULE/calculateMetrics/~json@1.0/format~template@1.0&type=dashboard Device Specialization Nodes can choose which devices to support, allowing for specialization:Compute-Optimized Nodes:Focus on ~wasm64@1.0 and ~lua@5.3a devices High-performance processors and memory Optimized for CPU-intensive workloads Storage-Optimized Nodes:Specialize in ~process@1.0 and ~patch@1.0 devices Large storage capacity and fast I/O Optimized for state management and data persistence Security-Focused Nodes:Run ~snp@1.0 and security-related devices Hardware security modules (HSMs) Trusted Execution Environments (TEEs) Custom Device Development While HyperBEAM comes with a comprehensive set of built-in devices, you can create custom devices in Erlang to extend functionality for specialized use cases. This is an advanced topic that allows you to build domain-specific functionality tailored to your exact needs.For detailed guidance on building custom devices, see the HyperBEAM Device Development Guide.Device Discovery and Routing HyperBEAM automatically routes requests to the appropriate devices based on the URL path. You can discover available devices on any node:# List all available devices
GET /~meta@1.0/devices

# Get information about a specific device
GET /~meta@1.0/device/~lua@5.3a Devices are automatically load-balanced across available instances, with HyperBEAM handling routing optimization internally.Performance Considerations Different devices have varying performance characteristics:~lua@5.3a - Fast startup, low resource usage, ideal for simple logic ~wasm64@1.0 - Higher performance for complex computations ~process@1.0 - Use /compute for cached state, /now for real-time updates ~json@1.0 - Very lightweight for data serialization Optimization Tips:Use device pipelines to chain operations in a single request Cache frequently accessed data at the application level Choose the right device for your workload (Lua for simple logic, WASM for computation) Extensible Device Ecosystem The modular nature of HyperBEAM devices enables endless possibilities for expansion. The community and ecosystem are continuously developing new devices for:Specialized Hardware - GPU computing, AI/ML acceleration, quantum computing Domain-Specific Logic - DeFi protocols, scientific computing, media processing Cross-Chain Integration - Bridges to other blockchain networks Industry Solutions - Custom devices for specific business needs This extensibility ensures HyperBEAM can adapt to new technologies and use cases without requiring changes to the core protocol.Security Considerations HyperBEAM devices run in isolated environments with built-in security features:Sandboxing - Each device operates in its own isolated environment Resource Limits - Automatic memory and execution time constraints Verification - Device signatures and integrity checking Access Control - Permission-based device access Best Practices:Always specify device versions (e.g., ~lua@5.3a not just ~lua) Validate inputs when building applications that use devices Use TEE-enabled nodes (~snp@1.0) for sensitive computations Explore the broader HyperBEAM ecosystem:Build Custom Devices: Device Development Guide Lua Programming: Lua Serverless Functions Process Integration: AO Process Development Production Deployment: Builder's Journey Resources HyperBEAM Device Documentation: Official Device Docs Erlang/OTP Documentation: Erlang Reference

---

# 36. Hello World (CLI)  Cooking with the Permaweb

Document Number: 36
Source: https://cookbook.arweave.net/getting-started/quick-starts/hw-cli.html
Words: 140
Extraction Method: html

Hello World (CLI) This guide walks you through the most simple way to get data on to the Permaweb using a command-line interface (CLI).Requirements NodeJS open in new window LTS or greater Description Using a terminal/console window create a new folder called hw-permaweb-1.Setup cd hw-permaweb-1
npm init -y
npm install arweave ardrive-cli Generate a wallet npx -y @permaweb/wallet > ~/.demo-arweave-wallet.json Create a web page echo "<h1>Hello Permaweb</h1>" > index.html Upload using Ardrive CLI # Create a Drive
FOLDER_ID=$(npx ardrive create-drive -n public -w ~/.demo-arweave-wallet.json --turbo | jq -r '.created[] | select(.type == "folder") | .entityId')
# Upload file
TX_ID=$(npx ardrive upload-file -l index.html --content-type text/html -w ~/.demo-arweave-wallet.json --turbo -F ${FOLDER_ID} | jq -r '.created[] | select(.type == "file
") | .dataTxId')
# open file from ar.io gateway
open https://arweave.net/${TX_ID} Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 37. Github Action  Cooking with the Permaweb

Document Number: 37
Source: https://cookbook.arweave.net/tooling/deployment/github-action.html
Words: 618
Extraction Method: html

Github Action WARNING This guide is for educational purposes only, and you should use to learn options of how you might want to deploy your application. In this guide, we are trusting a 3rd party resource github owned by microsoft to protect our secret information, in their documentation they encrypt secrets in their store using libsodium sealed box, you can find more information about their security practices here. https://docs.github.com/en/actions/security-guides/encrypted-secrets Github Actions are CI/CD pipelines that allows developers to trigger automated tasks via events generated from the github workflow system. These tasks can be just about anything, in this guide we will show how you can use github actions to deploy your permaweb application to the permaweb using permaweb-deploy and ArNS.TIP This guide requires understanding of github actions, and you must have some Turbo Credits and an ArNS name. Go to https://ar.io/arns/ for more details on acquiring an ArNS name.WARNING This guide does not include testing or any other checks you may want to add to your production workflow.Prerequisites Before setting up GitHub Actions deployment, you'll need:An Arweave wallet with sufficient Turbo Credits for deployment An ArNS name that you own A built application (e.g., in a ./dist folder) Install permaweb-deploy Add permaweb-deploy as a development dependency to your project:npm install --save-dev permaweb-deploy Configure Deployment Script Add a deployment script to your package.json that builds your application and deploys it using permaweb-deploy:{
  "scripts": {
    "dev": "vuepress dev src",
    "build": "vuepress build src",
    "deploy": "npm run build && permaweb-deploy --arns-name YOUR_ARNS_NAME"
  }
} Replace YOUR_ARNS_NAME with your actual ArNS name (e.g., my-app).Advanced Configuration You can customize the deployment with additional options:{
  "scripts": {
    "deploy": "npm run build && permaweb-deploy --arns-name my-app --deploy-folder ./dist --undername @"
  }
} Available options:--arns-name (required): Your ArNS name --deploy-folder: Folder to deploy (default: ./dist) --undername: ANT undername to update (default: @) --ario-process: ARIO process (default: mainnet) Create GitHub Action Create a .github/workflows/deploy.yml file in your repository:name: Deploy to Permaweb

on:
  push:
    branches:
      - "main"

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 20.x
      - run: npm install
      - run: npm run deploy
        env:
          DEPLOY_KEY: ${{ secrets.DEPLOY_KEY }} Setup GitHub Secrets 1. Prepare Your Wallet First, encode your Arweave wallet as base64:base64 -i wallet.json Copy the output (it will be a long base64 string).2. Add Secret to GitHub Go to your repository on GitHub Navigate to Settings → Secrets and variables → Actions Click New repository secret Name: DEPLOY_KEY Value: Paste the base64 encoded wallet string Click Add secret Fund Your Wallet Ensure your deployment wallet has sufficient Turbo Credits. You can fund it using:# Check current balance
npx @ardrive/turbo-cli balance --wallet-file wallet.json

# Add credits (amount in Winston - 1 AR = 1,000,000,000,000 Winston)
npx @ardrive/turbo-cli top-up --value 500000000000 --wallet-file wallet.json Security Best Practices Use a dedicated wallet solely for deployments Keep minimal funds in the deployment wallet Never commit wallet files to your repository Regularly rotate deployment keys Test Your Deployment Local Testing Test your deployment locally before pushing:DEPLOY_KEY=$(base64 -i wallet.json) npm run deploy Verify Deployment After a successful GitHub Action run:Check the action logs for the deployment transaction ID Wait 10-20 minutes for ArNS propagation Visit your ArNS name: https://YOUR_ARNS_NAME.arweave.net Troubleshooting Common Issues:Insufficient Credits: Ensure your wallet has enough Turbo Credits ArNS Propagation: Wait 10-20 minutes after deployment for changes to appear Build Failures: Ensure your build command works locally first Secret Issues: Verify the DEPLOY_KEY secret is properly set and base64 encoded Check Deployment Status:Monitor your deployments through:GitHub Actions logs ArNS resolver: https://arns.arweave.net/resolve/YOUR_ARNS_NAME 🎉 You now have automated permaweb deployment with GitHub Actions!Your application will automatically deploy to the permaweb whenever you push to the main branch, and your ArNS name will point to the latest version.

---

# 38. ar-gql  Cooking with the Permaweb

Document Number: 38
Source: https://cookbook.arweave.net/tooling/graphql/ar-gql.html
Words: 127
Extraction Method: html

ar-gql This package is a minimal layer on top of GraphQL, it supports parameterized queries with query variables. It also implements management of paged results.Installation To install `ar-gql run npm i ar-gql yarn add ar-gql Example import { arGql } from "ar-gql"

const argql = arGql()

(async () => {
    let results = await argql.run(`query( $count: Int ){
    transactions(
      first: $count, 
      tags: [
        {
          name: "App-Name",
          values: ["PublicSquare"]
        },
        {
          name: "Content-Type",
          values: ["text/plain"]
        },
      ]
    ) {
      edges {
        node {
          id
          owner {
            address
          }
          data {
            size
          }
          block {
            height
            timestamp
          }
          tags {
            name,
            value
          }
        }
      }
    }
  }`, {count: 1});
  console.log(results);
})();Resources ar-gql github page open in new window Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 39. GraphQL Tools  Cooking with the Permaweb

Document Number: 39
Source: https://cookbook.arweave.net/tooling/graphql/index.html
Words: 253
Extraction Method: html

GraphQL Tools This section covers the tools and libraries available for querying Arweave data using GraphQL. GraphQL provides a powerful and flexible way to retrieve exactly the data you need from the Arweave network.Core GraphQL Tools ar-gql - JavaScript Library Lightweight GraphQL client for Arweave TypeScript support Easy integration with web applications Comprehensive query building Querying Arweave - Comprehensive Guide Complete overview of Arweave querying methods GraphQL query examples and patterns Best practices for data retrieval Advanced Querying Goldsky Search Gateway - Search & Indexing Advanced search capabilities Full-text search across Arweave data Indexing and aggregation features High-performance querying Getting Started Basic GraphQL Query import { gql } from 'ar-gql'

const query = gql`
  query {
    transactions(
      owners: ["YOUR_WALLET_ADDRESS"]
      first: 10
    ) {
      edges {
        node {
          id
          block {
            height
          }
          tags {
            name
            value
          }
        }
      }
    }
  }
` Query Patterns Transaction Queries Filter by owner, recipient, or tags Retrieve transaction metadata and content Search across time ranges Block Queries Get block information and statistics Query network state at specific heights Analyze network activity Bundle Queries Access bundled transactions Query bundle metadata Retrieve nested transaction data Best Practices Use Specific Queries: Request only the data you need Implement Pagination: Handle large result sets efficiently Cache Results: Store frequently accessed data locally Error Handling: Implement robust error handling for network issues Rate Limiting: Respect API rate limits and implement backoff strategies Start with ar-gql: ar-gql Library Learn Querying: Querying Arweave Advanced Search: Goldsky Search Gateway Explore Examples: Zero to Deployed App

---

# 40. Querying Arweave with GraphQL  Cooking with the Permaweb

Document Number: 40
Source: https://cookbook.arweave.net/tooling/querying-arweave.html
Words: 828
Extraction Method: html

Querying Arweave with GraphQL Arweave provides a simple way of querying for transactions and filtering them by tags. Arweave GraphQL-compatible indexing services provide endpoints users can post GraphQL queries to, and also provide a playground for trying queries.GraphQL is a flexible query language that services can use to build a customized data schema for clients to query. GraphQL also allows clients to specify which elements of the available data structure they would like to see in the results.Public Indexing Services arweave.net graphql the original graphql endpoint, managed by ar.io goldsky search service a public service specifically optimized for search using a superset of the graphql syntax, managed by goldsky ar.io decentralized indexing A decentralized network for indexing services. Currently in testing with L1 transactions available.Executing a GraphQL Query To query arweave we’ll need to access it through an indexing service that supports GraphQL. Use one of the GraphQL playgrounds listed above to get started!Copy and paste in the following query query {
  transactions(tags: [{
    name: "App-Name",
    values: ["PublicSquare"]
  }]) 
  {
    edges {
      node {
        id
        tags {
          name
          value
        }
      }
    }
  }
} If you’re not familiar with GraphQL it can seem a little overwhelming at first but once you know the structure, it’s fairly easy to read and understand.query { <schema type> ( <filter criteria> ) { <data structure of the results> } } In the example query we pasted our <schema type> is transactions but we could also query for blocks. A full description of Arweave's GraphQL schema is written up in the Arweave GraphQL Guide. The guide refers to the filter criteria as “Query Structures” and the complete data structure definition of transactions and blocks as “Data Structures”.When it comes to the <data structure of the results>, the thing to note is that you can specify a subset of the complete data structure you’re interested in. For example, the complete data structure for a transactions schema is listed here.In our case we’re interested in the id and complete list of tags for any transaction matching our filter criteria.Hit the big “Play” button in the middle of the playground to run the query. You’ll notice we get back a list of transactions in the results data structure we specified in our original query.If you’re new to blockchains this is unexpected, we haven’t built anything, why do these results exist? It turns out, the “PublicSquare”: “App-Name” tag we’ve filtered for has been in use for a while.Arweave protocol's founder, Sam Williams, proposed the transaction format a few years ago in a github code snippet. Since then builders in the ecosystem have been building on and around it, experimenting, posting transactions with those tags.Back to querying Arweave. You’ll notice in the GraphQL results that there are no readable post messages, just tags and information about posts.This is because the GraphQL indexing service is concerned with indexing and retrieving header data for transactions and blocks but not their associated data.To get the data of a transaction we need to look it up using another HTTP endpoint.https://arweave.net/<transaction id> Copy and paste one of the id’s in your query results and modify the above link, appending the id. It should look something like this… https://arweave.net/eaUAvulzZPrdh6_cHwUYV473OhvCumqT3K7eWI8tArk The result of navigating to that URL in the browser (HTTP GET) would be retrieving the content of the post (stored in the transactions data). In this example it’s… Woah that's pretty cool 😎 (For a complete listing arweave HTTP endpoints visit the HTTP API documentation.) Posting a Query From JavaScript Posting a GraphQL query from javascript isn't much different than posting it in the playground.First install the arweave-js package for easy access to a GraphQL endpoint.npm install --save arweave Then enter a slightly more advanced version of the example query from above and await the results of posting it.import Arweave from 'arweave';

// initialize an arweave instance
const arweave = Arweave.init({});

// create a query that selects tx data the first 100 tx with specific tags
const queryObject = {
    query:
    `{
        transactions(
            first:100,
            tags: [
                {
                    name: "App-Name",
                    values: ["PublicSquare"]
                },
                {
                    name: "Content-Type",
                    values: ["text/plain"]
                }
            ]
        ) 
        {
            edges {
                node {
                    id
                    tags {
                        name
                        value
                    }
                }
            }
        }
    }`
};
const results = await arweave.api.post('/graphql', queryObject);Multiple Queries It is possible to post multiple queries in a single round-trip to the GraphQL endpoint. This example queries the name transaction (each as a separate query) for two wallet addresses using the now obsolete (replaced by ar-profile) but still permanent arweave-id protocol.query {
    account1: transactions(first: 1, owners:["89tR0-C1m3_sCWCoVCChg4gFYKdiH5_ZDyZpdJ2DDRw"],
        tags: [
            {
                name: "App-Name",
                values: ["arweave-id"]
            },
            {
                name: "Type",
                values: ["name"]
            }
        ]
    ) {
        edges {
            node {
                id
                    owner {
                    address
                }
            }
        }
    }
    account2: transactions(first: 1, owners:["kLx41ALBpTVpCAgymxPaooBgMyk9hsdijSF2T-lZ_Bg"],
        tags: [
            {
                name: "App-Name",
                values: ["arweave-id"]
            },
            {
                name: "Type",
                values: ["name"]
            }
        ]
    ) {
        edges {
            node {
                id
                    owner {
                    address
                }
            }
        }
    }
} Resources Arweave GQL Reference ArDB package ar-gql package Search Indexing Service

---

# 41. Process  WAO

Document Number: 41
Source: https://docs.wao.eco/api/process
Words: 369
Extraction Method: html

You can go for even more concise syntax with Process class.Instantiate const p = ao.p(pid) or const { p, pid } = await ao.deploy({ data, tags, src, fills }) msg The first argument is Action, the second argument is Tags, and the third argument is the rest of the options.const { mid, res, out, err } = await p.msg(

  "Action", 

  { Tag1: "value1", Tag2: "value2" }, 

  { get: true, check: { TagA: "valueA" }, jwk }

) The default third argument is { get: false } to return the text Data.const { mid, out } = await p.msg("Action", { Tag1: "value1", Tag2: "value2" }) The third parameter defaults to get if it's not an object.const { mid, out } = await p.msg("Action", { Tag1: "value1" }, "TagA") is equivalent to const { mid, out } = await p.msg("Action", { Tag1: "value1" }, "TagA") You can omit the second argument if there is no tag to pass to.const { mid, out } = await p.msg("Action", { check: "success!" }} m You can only get out with m. This is the most extreme form.const out = await p.m("Action", { Tag1: "value1", Tag2: "value2" }) This is a quite common pattern during testing. Doing the same with aoconnect requires an enormous amount of code, especially if it involves async/await receive().const { p } = await ao.deploy({ tags, src_data, fills })

const out = await p.m("Action", { Tag1: "value1", Tag2: "value2" }) // get Data

assert.equal(out, EXPECTED_JSON) dry const { mid, out } = await p.dry("Action", { Tag1: "value1", Tag2: "value2" }) d const out = await p.d("Action", { Tag1: "value1", Tag2: "value2" }) res const { err, res, out } = await p.res({ mid, check, get }) r const out = await p.r({ mid, check, get }) v v is a shortcut for var to get a Lua variable with dryrun.const { p } = await ao.deploy({

  src_data: `Table = { String = "Hello", Array = { "str", 3, true } }`,

})

const table = await p.v("Table") // { String: "Hello", Array: [ "str", 3, true ] } To disable the auto JSON conversion and enable pretty print, use the 2nd and the 3rd arguments.const table = await p.v("Table", false, true)

---

# 42. HBSig  WAO

Document Number: 42
Source: https://docs.wao.eco/api/hbsig
Words: 887
Extraction Method: html

HyperBEAM / AO-Core requires complex encoding and signing involving multiple codecs and HTTP Message Signatures. However, it is not 100% compatible with the standard http-message-signatures libraries, and aoconnect only provides basic encoding for AOS messages.hbsig handles encoding of arbitrarily complex objects, which works on HyperBEAM. It is built by emulating the HyperBEAM codec devices as well as creating workaround encoding strategies using a custom device for extensive tests with LLMs.Installation hbsig is a standalone package providing utility methods for HyperBEAM codecs, signatures, encoding and hash algorithms.yarn add hbsig Sign Message createSigner import { createSigner } from "hbsig"

 

const hyperbeam_url = "http://localhost:10001"

const sign = createSigner(jwk, hyperbeam_url) Sign and Send Message hbsig can encode and sign almost any complex objects combining layers of HyperBEAM codecs and http-message-signatures. While there are a few edge cases where HyperBEAM's decoder has limitations, hbsig intelligently provides workarounds for many of these scenarios. The encoding strategies were refined through extensive battle-testing with LLMs.import { send } from "hbsig"

 

const msg = { str: "abc", num: 123, bin: Buffer.from([1,2,3]) }

const signed = await sign({ path: "/~hbsig@1.0/msg2", ...msg })

const { out } = await send(signed) Exclude @path @path is automatically included in the signed components. To exclude @path, set path=false in the 2nd argument. There are some cases you want to exclude @path due to HyperBEAM's non-standard handing of the path field. HyperBEAM strips off the leading / from path, and also removes @ from the field name in the signed body, which might not be compatible with the Http Message Signatures standard, and invalidates signatures in some scenarios.const msg = await sign(

  { path: "/~hbsig@1.0/msg2", key: "value" }, 

  { path: false }

) Verify Message You can verify signed messages while decoding signature-input.import { verify } from "hbsig"

 

const { 

  valid, // should be true

  verified,

  signatureName, 

  keyId, 

  algorithm, 

  decodedSignatureInput : { components, params: { alg, keyid, tag }, raw }

} = await verify(signed) Commitments You can sign a message and create commitments with the signature.import { commit } from "hbsig"

const committed = await commit({path, ...msg}, { signer: sign }) Commit IDs A commitment needs to include two IDs, which are sha256 hash of the signature hmac-sha256 hash of the signed components You can explicitly get these IDs from a signed message.Message ID You can calculate the ID from committed message.import { id } from "hbsig"

 

const msg_id = id(committed) Hashpath You can calculate the next hashpath from the current hashpath and a new message.import { hashpath } from "hbsig"

 

const next_hashpath = hashpath(current_hashpath, committed) base A hashpath consists of the hash of the current hashpath and the new message ID joined by /. You can independently calculate the base hash with base.import { base, id } from "hbsig"

const next_hashpath = `${base(current_hashpath)}/${id(committed)}` Utilities toAddr Synchronously calculate Arweave address from a public key. arweave.js provides only asynchronous method for this. You can extract a public key from jwk.n as well as verify(signed), which gives you keyId from signature-input.import { toAddr } from "hbsig"

const address = toAddr(jwk.n) // toAddr(jwk) works too Codecs hbsig internally handles many different representations of the same object using multiple codecs from HyperBEAM (flat structured httpsig) and 2 added codecs to achieve seamless data exchange between JS and Erlang (erljson erlstr).hbsig@1.0 Device hbsig comes with an accompanying device (hbsig@1.0) on HyperBEAM to validate various encoding strategies.git clone https://github.com/weavedb/wao.git && cd wao

git submodule update --init --recursive json_to_erl/3: convert stringified JSON to an Erlang object to_erl/1: convert stringified JSON in body to an Erlang object to_str/1: convert an Erlang object to ErlStr structured_from/3: expose structured@1.0:from structured_to/3: expose structured@1.0:to httpsig_from/3: expose httpsig@1.0:from httpsig_to/3: expose httpsig@1.0:to flat_from/3: expose flat@1.0:from flat_to/3: expose flat@1.0:to msg2/3: return Msg2 as ErlStr so we can check the decoded message ErlJSON JSON and erlang objects have different types such as null, boolean, and atom. ErlJSON normalized JSON objects to Erlang compatible structures.import { normalize, erl_json_from, erl_json_to } from "hbsig" ErlStr Erlang doesn't differentiate between strings and buffers, and the built-in format method loses precision when converting binary data containing non-standard characters. To address this, ErlStr maintains both binary and stringified formats  for accurate Erlang-to-JSON conversion.import { erl_str_from, erl_str_to } from "hbsig" ErlJSON and ErlStr enable seamless conversions between JSON and Erlang objects, allowing precise encoding tests across both environments.flat import { flat_from, flat_to } from "hbsig" structured import { structured_from, structured_to } from "hbsig" httpsig import { httpsig_from, httpsig_to } from "hbsig" Example Test You can find comprehensive encoding tests here.import { structured_from, structured_to } from "../src/structured.js"

import { normalize, erl_json_to } from "../src/erl_json.js"

import { httpsig_from, httpsig_to } from "../src/httpsig.js"

 

describe(desc, function () {

  let hbeam, sign

  before(async () => {

    hbeam = await new HyperBEAM({ reset: true }).ready()

    sign = createSigner(hbeam.jwk, hbeam.url)

  })

  after(async () => hbeam.kill())

  it("should validate", async ()=>{

    const msg = { str: "abc", num: 123, bin: Buffer.from([1, 2, 3]) }

    const structured = structured_from(normalize(msg))

    const json = erl_json_to(structured)

    const signed = await sign({

      path: "/~hbsig@1.0/httpsig_to", 

      body: JSON.stringify(json)

    })

    const { out } = await send(signed)

    const input = httpsig_to(normalize(structured))

    const output = erl_str_from(out)

    const expected = normalize(input, true)

    const output_b = erl_str_from(out, true) // true for binary format

    assert.deepEqual(expected, output_b) // compare in binary format

  })

}) To run all tests, you need to clone wao branch of HyperBEAM, compile it, add .wallet.json, and set CWD in .env.hyperbeam.yarn test-all

---

# 43. Device Composition  WAO

Document Number: 43
Source: https://docs.wao.eco/hyperbeam/device-composition
Words: 843
Extraction Method: html

So far, we've learned about HyperBEAM devices and URL pathing, the core codecs, HTTP message signatures, and hashpaths. You already know the fundamentals of how HyperBEAM works.Chaining Device Methods with URL Path Let's play around with device composition to build something powerful. We can access any cached messages with an ID or a hashpath at /[id | hashpath]. And we can also chain device methods like /~meta@1.0/info/~json@1.0/serialize.Could we chain our own device methods like the following?/[hashpath]/~mydev@1.0/inc/~mydev@1.0/double/~mydev@1.0/square Let's find out!Our goal is to pass an existing message with num, and compute num through the device method chaining. So if the initial message with a hashpath has num=6,=> /~mydev@1.0/inc => 6 + 1 => num=7 => /~mydev@1.0/double => 7 * 2 => num=14 => /~mydev@1.0/square => 14 * 14 => num=196 is what we need to end up with.File  /HyperBEAM/src/dev_mydev.erl -export([ inc/3, double/3, square/3 ]).

 

inc(Msg1, Msg2, Opts)->

  Num = maps:get(<<"num">>, Msg1),

  {ok, #{ <<"num">> => Num + 1 }}.

 

double(Msg1, Msg2, Opts)->

  Num = maps:get(<<"num">>, Msg1),

  {ok, #{ <<"num">> => Num * 2 }}.

 

square(Msg1, Msg2, Opts)->

  Num = maps:get(<<"num">>, Msg1),

  {ok, #{ <<"num">> => Num * Num }}.We can use the resolve3 method from the previous chapter to create the base num with the hashpath cached. /~mydev@1.0/resolve3 returns num=6 with out.hashpath_7.File  /test/device-composition.test.js const out = await hb.p("/~mydev@1.0/resolve3")

const { num } = await hb.g(

  `/${out.hashpath_7}/~mydev@1.0/inc/~mydev@1.0/double/~mydev@1.0/square`

)

assert.equal(num, 196) Voila! It works! But there are 3 caveats to this.First of all, during this pipeline, the Msg2 passed to each device method of inc/3, double/3, and square/3 stays the same and is the original committed Msg2 to the first method in the chain, which in this case is deviceless since we start the pipeline with /${out.hashpath_7}.If we were to start the chain with /~mydev@1.0/inc/~mydev@1.0/double/~mydev@1.0/square, the Msg2 would always be the same as what is passed to /~mydev@1.0/inc. So to evolve the state, you need to use the values from Msg1.Secondly, as we learned in an earlier chapter, Msg1 contains inter-decoded values, and not the final decoded values, which means even if you pass integer, Msg1 will have stringified num. You need to take the initial values from Msg2.Lastly, during the pipeline, you cannot overwrite the fields initially passed to Msg2. So you cannot pass num and update num during the pipeline. The initial Msg2 always overwrites the updated num and it ends up unchanged. So you need to pass something other than num, then update num during the pipeline. The case with /[hashpath]/~mydev@1.0/inc/~mydev@1.0/double/~mydev@1.0/square works since we're not passing num to the initial /[hashpath] execution.One way to solve this is to create an entry method like calc to take a different field such as init_num from Msg2, then pass it down to the pipeline as num.File  /HyperBEAM/src/dev_mydev.erl -export([ calc/3 ]).

 

calc(Msg1, Msg2, Opts)->

  Num = maps:get(<<"init_num">>, Msg2),

  {ok, #{ <<"num">> => Num}}.Now we can POST to /~mydev@1.0/calc/~mydev@1.0/inc/~mydev@1.0/double/~mydev@1.0/square, and get the correct output.File  /test/device-composition.test.js const { num } = await hb.p(

  "/~mydev@1.0/calc/~mydev@1.0/inc/~mydev@1.0/double/~mydev@1.0/square",

  { init_num: 1 }

)

assert.equal(num, 16) Stacking Devices There is a built-in device called stack@1.0 to make device composition easy. It's supposed to be used with process@1.0, so it's limited in a certain way, but we can still use it without processes.Let's modify our methods to make them compatible with stack@1.0. We just need to forward device-stack from Msg1.File  /HyperBEAM/src/dev_mydev.erl -export([ inc2/3, double2/3, square2/3 ]).

 

inc2(Msg1, Msg2, Opts)->

  io:format("Inc: ~p~n", [Msg1]),

  Num = maps:get(<<"num">>, Msg1),

  {ok, #{ 

    <<"num">> => Num + 1, 

    <<"device-stack">> => maps:get(<<"device-stack">>, Msg1)

  }}.

 

double2(Msg1, Msg2, Opts)->

  Num = maps:get(<<"num">>, Msg1),

  {ok, #{ 

    <<"num">> => Num * 2,

    <<"device-stack">> => maps:get(<<"device-stack">>, Msg1)

  }}.

 

square2(Msg1, Msg2, Opts)->

  Num = maps:get(<<"num">>, Msg1),

  {ok, #{ 

    <<"num">> => Num * Num,

    <<"device-stack">> => maps:get(<<"device-stack">>, Msg1)

   }}.Add the stack@1.0 device to the HyperBEAM class in our test file.File  /test/device-composition.test.js import assert from "assert"

import { describe, it, before, after } from "node:test"

import { HyperBEAM } from "wao/test"

import { id } from "hbsig"

 

const devices = [

  "json",

  "structured",

  "httpsig",

  "flat",

  "meta",

  "stack",

  { name: "mydev@1.0", module: "dev_mydev" },

]

 

describe("Device Composition", function () {

  let hbeam, hb

  before(async () => {

    hbeam = await new HyperBEAM({ devices, reset: true }).ready()

    hb = hbeam.hb

  })

  after(async () => hbeam.kill())

  

  it("should stack devices", async () => {

    const msg_base = {

      device: "stack@1.0",

      "device-stack": { 1: "mydev@1.0", 2: "mydev@1.0", 3: "mydev@1.0" },

      mode: "Fold",

      num: 3,

    }

 

    const out = await hb.p("inc2", msg_base)

    assert.equal(out.num, 6) // 3 + 1 + 1 + 1

 

    const out2 = await hb.p("double2", msg_base)

    assert.equal(out2.num, 24) // 3 * 2 * 2 * 2

 

    const out3 = await hb.p("square2", msg_base)

    assert.equal(out3.num, 6561) // 3 * 3 * 9 * 81

  })

}) You can stack multiple devices in device-stack, but the limitation is it executes the same method on each device specified in path. With process@1.0, it executes the compute method, which we'll talk about in the next chapter.Running Tests You can find the working test file for this chapter here:device-composition.test.js Run tests:Terminal   Terminal yarn test test/device-composition.test.js References Device API dev_stack.erl WAO API HyperBEAM Class API HB Class API HBSig API

---

# 44. Installing HyperBEAM and WAO

Document Number: 44
Source: https://docs.wao.eco/hyperbeam/installing-hb-wao
Words: 514
Extraction Method: html

Installing HyperBEAM Follow the HyperBEAM docs and install HyperBEAM on your local machine. You could use one of the existing remote nodes, but you'll miss many important details in these tutorials since we'll literally crack open the internals.Installing WAO Create a WAO project that comes with the wao SDK and testing framework:Terminal   Terminal npx wao create myapp && cd myapp You can also create an empty directory and install wao and hbsig:Terminal   Terminal mkdir myapp && cd myapp && yarn init && yarn add wao hbsig

mkdir test && touch test/hyperbeam.js Edit package.json to enable ESM and test commands with the --experimental-wasm-memory64 flag and disable concurrency so the test won't try running multiple HyperBEAM nodes:File  /package.json {

  "name": "myapp",

  "version": "0.0.1",

  "type": "module",

  "scripts": {

    "test": "node --experimental-wasm-memory64 --test --test-concurrency=1",

    "test-only": "node --experimental-wasm-memory64 --test-only --test-concurrency=1",

    "test-all": "node --experimental-wasm-memory64 --test --test-concurrency=1 test/**/*.test.js"

  },

  "dependencies": {

    "hbsig": "^0.0.7",

    "wao": "^0.33.3"

  }

} Writing Tests Import the HyperBEAM and HB classes from wao to interact with your HyperBEAM node.Make sure you have an Arweave wallet JWK at HyperBEAM/.wallet.json for the node operator account.Also, set CWD in .env.hyperbeam, which should be the HyperBEAM node directory path relative to the root directory of your app.File  /.env.hyperbeam CWD=../HyperBEAM Here's the minimum viable test code. The HyperBEAM class starts up a HyperBEAM node and kills it once your tests complete, creating a sandbox environment for each test suite.File  /test/hyperbeam.test.js import assert from "assert"

import { describe, it, before, after, beforeEach } from "node:test"

import { HyperBEAM } from "wao/test"

 

describe("HyperBEAM", function () {

  let hbeam, hb

 

  // start a hyperbeam node and wait till it's ready, reset node storage

  before(async () => {

    hbeam = await new HyperBEAM({ reset: true }).ready()

    hb = hbeam.hb

  })

 

  // kill the node after testing

  after(async () => hbeam.kill())

 

  it("should run a HyperBEAM node", async () => {

    // change config

    await hb.post({ path: "/~meta@1.0/info", test_config: "abc" })

 

    // get config

    const { out } = await hb.get({ path: "/~meta@1.0/info" })

    assert.equal(out.test_config, "abc")

  })

}) You can interact with any HyperBEAM node from JS using the HB class.With these two classes, you can write complete test suites for your HyperBEAM node, devices, processes, and modules running on top (such as AOS) using only JavaScript.If you can't run HyperBEAM on your local machine, skip the HyperBEAM class and pass the remote node url to HB:File  /test/hb.test.js import assert from "assert"

import { describe, it, before, after } from "node:test"

import { acc } from "wao/test"

import { HB } from "wao"

 

describe("HyperBEAM", function () {

  let hb

 

  // using one of the pre-generated non-operator accounts for test

  before(async () => {

    hb = new HB({ jwk: acc[0].jwk, url: "http://localhost:10001" })

  })

 

  it("should connect to a HyperBEAM node", async () => {

    // get build info

    const build = await hb.g("/~meta@1.0/build")

    assert.equal(build.node, "HyperBEAM")

  })

}) Running Tests You can find the working test files for this chapter here:hyperbeam.test.js hb.test.js Run tests:Terminal   Terminal yarn test test/hyperbeam.test.js

# yarn test test/hb.testjs Now we're ready to decode HyperBEAM.References General HyperBEAM Installation Guide WAO API HyperBEAM Class API HB Class API

---

# 45. Running LLMs on AOS (Highly Experimental)  WAO

Document Number: 45
Source: https://docs.wao.eco/tutorials/running-llms
Words: 339
Extraction Method: html

You can run LLMs on top of AOS using the right module.First create a test project.npx wao create llm && cd llm Create a directory and download one of the tiny models from Hugging Face.We will try TinyLlama-1.1B-Chat-v1.0-GGUF for this tutorial.mkdir test/models

curl -L -o test/models/tinyllama.gguf "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q2_K.gguf?download=true" Write tests with WAO.import assert from "assert"

import { resolve } from "path"

import { readFileSync } from "fs"

import { afterEach, after, describe, it, before, beforeEach } from "node:test"

import { AO, acc } from "wao/test"

const __dirname = import.meta.dirname

const src_data = `

Llama = require(".Llama")

Llama.logLevel = 4

 

Handlers.add("Load", "Load", function (msg)

  Llama.load("/data/" .. msg.ModelID)

  msg.reply({ Data = "true" })

end)

 

Handlers.add("Ask", "Ask", function (msg)

  Llama.setPrompt(msg.Q)

  msg.reply({ Data = Llama.run(50) })

end)`

 

describe("LLM", function () {

  it("should infer with Tinyllama", async () => {

    const ao = await new AO().init(acc[0])

    const model = readFileSync(resolve(__dirname, "models/tinyllama.gguf"))

    const { id } = await ao.ar.post({ data: model })

    const data = readFileSync(

      resolve(__dirname, "../node_modules/wao/esm/lua/llama.wasm"),

    )

    const { id: modid } = await ao.postModule({

      data,

      tags: { "Memory-Limit": "1-gb" },

    })

    const { p, pid, err } = await ao.deploy({

      tags: { Extension: "WeaveDrive", Attestor: ao.ar.addr },

      module: modid,

      src_data,

    })

    await ao.attest({ id })

    await p.m("Load", { ModelID: id })

    console.log(await p.d("Ask", { Q: "How are you?" }, false))

  })

}) phi-2-GGUF would be a much better model for chat, but a bit too heavy for CPU.mkdir test/models

curl -L -o test/models/phi2.gguf "https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q2_K.gguf?download=true" it("should infer with Phi2", async () => {

  const ao = await new AO().init(acc[0])

  const model = readFileSync(resolve(__dirname, "models/phi2.gguf"))

  const { id } = await ao.ar.post({ data: model })

  const data = readFileSync(

    resolve(__dirname, "../node_modules/wao/esm/lua/llama.wasm"),

  )

  const { id: modid } = await ao.postModule({

    data,

    tags: { "Memory-Limit": "2-gb" }, // the model size is more than 1GB

  })

  const { p, pid, err } = await ao.deploy({

    tags: { Extension: "WeaveDrive", Attestor: ao.ar.addr },

    module: modid,

    src_data,

  })

  await ao.attest({ id })

  await p.m("Load", { ModelID: id })

  console.log(await p.d("Ask", { Q: "How are you?" }, false))

})

---

# 46. HyperBEAM - Documentation

Document Number: 46
Source: https://hyperbeam.arweave.net/
Words: 154
Extraction Method: html

A Decentralized Operating System.
Built on AO.What is hyperBEAM? Hyperbeam. Powering the decentralized supercomputer: AO.Access, build, and lease hardware for applications and services at any scale.Your gateway to AO, a decentralized supercomputer network built on top of Arweave. AO and Arweave power a cyberspace which guarantees the rights of users, outside of the control of any individual or group.  Communicate via asynchronous
message passing for unheard
of throughput.
 Get resilient compute in
your terminal with one
command.
What Do I Do With Hyperbeam?01 Monetize Your Hardware.Access a shared economy for hardware in the new cyberspace.
All while earning $AO    Offer compute to AO processes and
their users, earning fees in return.
  Run your own gateway.   Empower builders to launch trust-minimized, serverless WASM functions using built-in TEE integrations.
   Coming Soon: Offer support for GPUs.
  Sorry, your browser doesn’t support embedded video.
  Sorry, your browser doesn't support embedded video.
  Sorry, your browser doesn't support embedded video.

---

# 47. meta10 - HyperBEAM - Documentation

Document Number: 47
Source: https://hyperbeam.arweave.net/build/devices/meta-at-1-0.html
Words: 346
Extraction Method: html

Device: ~meta@1.0 Overview The ~meta@1.0 device provides access to metadata and configuration information about the local HyperBEAM node and the broader AO network.This device is essential for:Core Functions (Keys) info Retrieves or modifies the node's configuration message (often referred to as NodeMsg internally).GET /~meta@1.0/info Action: Returns the current node configuration message.Response: A message map containing the node's settings. Sensitive keys (like private wallets) are filtered out. Dynamically generated keys like the node's public address are added if a wallet is configured.POST /~meta@1.0/info Action: Updates the node's configuration message. Requires the request to be signed by the node's configured operator key/address.Request Body: A message map containing the configuration keys and values to update.Response: Confirmation message indicating success or failure.Note: Once a node's configuration is marked as initialized = permanent, it cannot be changed via this method.Key Configuration Parameters Managed by ~meta While the info key is the primary interaction point, the NodeMsg managed by ~meta holds crucial configuration parameters affecting the entire node's behavior, including (but not limited to):port: HTTP server port.priv_wallet / key_location: Path to the node's Arweave key file.operator: The address designated as the node operator (defaults to the address derived from priv_wallet).initialized: Status indicating if the node setup is temporary or permanent.preprocessor / postprocessor: Optional messages defining pre/post-processing logic for requests.routes: Routing table used by dev_router.store: Configuration for data storage.trace: Debug tracing options.p4_*: Payment configuration.faff_*: Access control lists.(Refer to hb_opts.erl for a comprehensive list of options.) Utility Functions (Internal/Module Level) The dev_meta.erl module also contains helper functions used internally or callable from other Erlang modules:is_operator(<RequestMsg>, <NodeMsg>) -> boolean(): Checks if the signer of RequestMsg matches the configured operator in NodeMsg.Pre/Post-Processing Hooks The ~meta device applies the node's configured preprocessor message before resolving the main request and the postprocessor message after obtaining the result, allowing for global interception and modification of requests/responses.Initialization Before a node can process general requests, it usually needs to be initialized. Attempts to access devices other than ~meta@1.0/info before initialization typically result in an error. Initialization often involves setting essential parameters like the operator key via a POST to info.meta module

---

# 48. relay10 - HyperBEAM - Documentation

Document Number: 48
Source: https://hyperbeam.arweave.net/build/devices/relay-at-1-0.html
Words: 366
Extraction Method: html

Device: ~relay@1.0 Overview The ~relay@1.0 device enables HyperBEAM nodes to send messages to external HTTP endpoints or other AO nodes.Core Concept: Message Forwarding This device acts as an HTTP client within the AO ecosystem. It allows a node or process to make outbound HTTP requests.Key Functions (Keys) call Action: Sends an HTTP request to a specified target and waits synchronously for the response.Inputs (from Request Message or Base Message M1):target: (Optional) A message map defining the request to be sent. Defaults to the original incoming request (Msg2 or M1).relay-path or path: The URL/path to send the request to.relay-method or method: The HTTP method (GET, POST, etc.).relay-body or body: The request body.requires-sign: (Optional, boolean) If true, the request message (target) will be signed using the node's key before sending. Defaults to false.http-client: (Optional) Specify a custom HTTP client module to use (defaults to node's configured relay_http_client).Response:{ok, <ResponseMessage>} where <ResponseMessage> is the full message received from the remote peer, or {error, Reason}.Example:GET /~relay@1.0/call?method=GET&path=https://example.com cast Action: Sends an HTTP request asynchronously. The device returns immediately after spawning a process to send the request; it does not wait for or return the response from the remote peer.Inputs: Same as call.Response:{ok, <<"OK">>}.preprocess Action: This function is designed to be used as a node's global preprocessor (configured via ~meta@1.0). When configured, it intercepts all incoming requests to the node and automatically rewrites them to be relayed via the call key. This effectively turns the node into a pure forwarding proxy, using its routing table (dev_router) to determine the destination.Response: A message structure that invokes /~relay@1.0/call with the original request as the target body.Use Cases Inter-Node Communication: Sending messages between HyperBEAM nodes.External API Calls: Allowing AO processes to interact with traditional web APIs.Routing Nodes: Nodes configured with the preprocess key act as dedicated routers/proxies.Client-Side Relaying: A local HyperBEAM instance can use ~relay@1.0 to forward requests to public compute nodes.When call or cast is invoked, the actual HTTP request dispatch is handled by hb_http:request/2. This function often utilizes the node's routing configuration (dev_router) to determine the specific peer/URL to send the request to, especially if the target path is an AO process ID or another internal identifier rather than a full external URL.relay module

---

# 49. scheduler10 - HyperBEAM - Documentation

Document Number: 49
Source: https://hyperbeam.arweave.net/build/devices/scheduler-at-1-0.html
Words: 672
Extraction Method: html

Device: ~scheduler@1.0 Overview The ~scheduler@1.0 device manages the queueing and ordering of messages targeted at a specific process (~process@1.0). It ensures that messages are processed according to defined scheduling rules.Core Concept: Message Ordering When messages are sent to an AO process (typically via the ~push@1.0 device or a POST to the process's /schedule endpoint), they are added to a queue managed by the Scheduler Device associated with that process. The scheduler ensures that messages are processed one after another in a deterministic order, typically based on arrival time and potentially other factors like message nonces or timestamps (depending on the specific scheduler implementation details).The ~process@1.0 device interacts with its configured Scheduler Device (which defaults to ~scheduler@1.0) primarily through the next key to retrieve the next message to be executed.Slot System Slots are a fundamental concept in the ~scheduler@1.0 device, providing a structured mechanism for organizing and sequencing computation.Sequential Ordering: Slots act as numbered containers (starting at 0) that hold specific messages or tasks to be processed in a deterministic order.State Tracking: The at-slot key in a process's state (or a similar internal field like current-slot within the scheduler itself) tracks execution progress, indicating which messages have been processed and which are pending. The slot function can be used to query this.Assignment Storage: Each slot contains an "assignment" - the cryptographically verified message waiting to be executed. These assignments are retrieved using the schedule function or internally via next.Schedule Organization: The collection of all slots for a process forms its "schedule".Application Scenarios:Scheduling Messages: When a message is posted to a process (e.g., via register), it's assigned to the next available slot.Status Monitoring: Clients can query a process's current slot (via the slot function) to check progress.Task Retrieval: Processes find their next task by requesting the next assignment via the next function, which implicitly uses the next slot number based on the current state.Distributed Consistency: Slots ensure deterministic execution order across nodes, crucial for maintaining consistency in AO.This slotting mechanism is central to AO processes built on HyperBEAM, allowing for deterministic, verifiable computation.Key Functions (Keys) These keys are typically accessed via the ~process@1.0 device, which delegates the calls to its configured scheduler.schedule (Handler for GET /<ProcessID>~process@1.0/schedule) Action: Retrieves the list of pending assignments (messages) for the process. May support cursor-based traversal for long schedules.Response: A message map containing the assignments, often keyed by slot number or message ID.register (Handler for POST /<ProcessID>~process@1.0/schedule) Action: Adds/registers a new message to the process's schedule. If this is the first message for a process, it might initialize the scheduler state.Request Body: The message to schedule.Response: Confirmation, potentially including the assigned slot or message ID.slot (Handler for GET /<ProcessID>~process@1.0/slot) Action: Queries the current or a specific slot number within the process's schedule.Response: Information about the requested slot, such as the current highest slot number.status (Handler for GET /<ProcessID>~process@1.0/status) Action: Retrieves status information about the scheduler for the process.Response: A status message.next (Internal Key used by ~process@1.0) Action: Retrieves the next assignment message from the schedule based on the process's current at-slot state.State Management: Requires the current process state (Msg1) containing the at-slot key.Response:{ok, #{ "body" => <NextAssignmentMsg>, "state" => <UpdatedProcessState> }} or {error, Reason} if no next assignment is found.Caching & Lookahead: The implementation uses internal caching (dev_scheduler_cache, priv/assignments) and potentially background lookahead workers to optimize fetching subsequent assignments.init (Internal Key) Action: Initializes the scheduler state for a process, often called when the process itself is initialized.checkpoint (Internal Key) Action: Triggers the scheduler to potentially persist its current state or perform other checkpointing operations.~process@1.0: The primary user of the scheduler, calling next to drive process execution.~push@1.0: Often used to add messages to the schedule via POST /schedule.dev_scheduler_cache: Internal module used for caching assignments locally on the node to reduce latency.Scheduling Unit (SU): Schedulers may interact with external entities (like Arweave gateways or dedicated SU nodes) to fetch or commit schedules, although ~scheduler@1.0 aims for a simpler, often node-local or SU-client model.~scheduler@1.0 provides the fundamental mechanism for ordered, sequential execution within the potentially asynchronous and parallel environment of AO.scheduler module

---

# 50. Intro to AO-Core - HyperBEAM - Documentation

Document Number: 50
Source: https://hyperbeam.arweave.net/build/introduction/what-is-ao-core.html
Words: 435
Extraction Method: html

What is AO-Core? Your browser does not support the video tag.AO-Core is a protocol and standard for distributed computation that forms the foundation of the AO Computer. Inspired by and built upon concepts from the Erlang language, AO-Core embraces the actor model for concurrent, distributed systems. It defines a minimal, generalized model for decentralized computation built around standard web technologies like HTTP.Think of it as a way to interpret the Arweave permaweb not just as static storage, but as a dynamic, programmable, and infinitely scalable computing environment. Unlike traditional blockchain systems, AO-Core defines a flexible, powerful computation protocol that enables a wide range of applications beyond just running Lua programs.Core Concepts AO-Core revolves around three fundamental components:                  Messages Modular Data Packets Messages are cryptographically linked, forming a verifiable computation graph.           Devices Extensible Execution Engines AO-Core introduces a modular architecture centered around Devices. These are pluggable components—typically implemented as modules—that define specific computational logic, such as executing WASM, managing state, or relaying data. Devices interpret and process messages, allowing for flexible and extensible computation. This design enables developers to extend the system by creating custom Devices to fit their specific needs, making the network highly adaptable and composable.                             Paths Composable Pipelines Paths in AO-Core are structures that link messages over time, creating a verifiable history of computations. They allow users to navigate the computation graph and access specific states or results. AO-Core leverages HashPaths —cryptographic fingerprints representing the sequence of operations leading to a specific message state—ensuring traceability and integrity. This pathing mechanism enables developers to compose complex, verifiable data pipelines and interact with processes and data in a flexible, trustless manner.Key Features AO-Core is inherently resilient, running across a global network of machines that eliminates any single point of failure. Its computations are permanent, immutably stored on Arweave so they can be recalled—or continued—at any time. The protocol remains permissionless, meaning anyone can participate. And it is trustless, with every state mathematically verifiable so no central authority is required.The Actor Model in AO Inspired by Erlang, AO-Core implements the actor model to provide a foundation for inherently concurrent, distributed, and scalable systems. In this model, computation is performed by independent actors (or processes). These actors communicate exclusively by passing messages to one another, and each can make local decisions, send more messages, and create new actors.Beyond Processes While AO Processes (smart contracts built using the AO-Core protocol) are a powerful application, AO-Core itself enables a much broader range of computational patterns:Serverless functions with trustless guarantees Hybrid applications combining smart contracts and serverless functionality Custom execution environments through new devices Composable systems using the path language

---

# 51. Core Capabilities - HyperBEAM - Documentation

Document Number: 51
Source: https://hyperbeam.arweave.net/build/hyperbeam-capabilities.html
Words: 685
Extraction Method: html

HyperBEAM: Your Decentralized Development Toolkit HyperBEAM is a versatile, multi-purpose tool that serves as the primary gateway to the AO Computer. It's not a single-purpose application, but rather a powerful, extensible engine—a "Swiss Army knife"—for developers building in the decentralized ecosystem.Designed to be modular, composable, and extensible, HyperBEAM lets you build anything from simple data transformations to complex, high-performance decentralized applications.Thinking in HyperBEAM While AO-Core establishes the foundational concepts of Messages, Devices, and Paths, building on HyperBEAM can be simplified to four key principles:Everything is a message. You can compute on any message by calling its keys by name. The device specified in the message determines how these keys are resolved. The default device, message@1.0, resolves keys to their literal values within the message.Paths are pipelines of messages. A path defines a sequence of 'request' messages to be executed. You can set a key in a message directly within the path using the &key=value syntax. Headers and parameters added after a ? are applied to all messages in the pipeline.Device-specific requests with ~x@y. The ~x@y syntax allows you to apply a request as if the base message had a different device. This provides a powerful way to execute messages using specific compute or storage logic defined by a device.Signed responses over HTTP. The final message in a pipeline is returned as an HTTP response. This response is signed against the hashpath that generated it, ensuring the integrity and verifiability of the computation.Ready to build an AO process?The serverless compute capability is a powerful application of HyperBEAM's modular design. To learn how to create and manage AO processes with WASM or Lua, please refer to the AO Processes Cookbook.Modularity: A System of Devices At its core, HyperBEAM is a modular system built on Devices. Each device is a specialized module responsible for a specific task. This modular architecture means you can think of HyperBEAM's functionality as a set of building blocks.Use Case: Imagine you need to create a serverless API that takes a number, runs a calculation, and returns a result.You would use the ~wasm64@1.0 or ~lua@5.3a devices to execute your calculation logic without needing to manage a server.If your API needs to return JSON, you can pipe the output to the ~json@1.0 device to ensure it's formatted correctly.Composability: Chaining Logic with URL Paths HyperBEAM's modular devices become even more powerful when combined. Its pathing routing mechanism leverages standard URLs to create powerful, composable pipelines. By constructing a URL, you can define a "path" of messages that are executed in sequence, with the output of one message becoming the input for the next.Use Case: Suppose you have a token process and want to calculate the total circulating supply without making the client download and compute all balances. You can construct a single URL that:Reads the latest state of the AO process.Pipes the state to a Lua script and calls the sum function, which sums the balances from the state.Formats the final result as a JSON object.The request would look something like this:/{process-id}~process@1.0/now/~lua@5.3a&module={module-id}/sum/serialize~json@1.0 This path chains together the operations, returning just the computed supply in a single, efficient request.Find the full example in the AO Process Cookbook Learn more about Pathing in HyperBEAM.Extensibility: Building Beyond the Core HyperBEAM is not a closed system. It is designed to be extended, allowing developers to add new functionality tailored to their specific needs.Build Custom Devices You can build and deploy your own devices in Erlang to introduce entirely new, high-level functionality to the network.Use Case: You could build a custom device that acts as a bridge to another blockchain's API, allowing your AO processes to interact with external systems seamlessly.Learn how to Build Your Own Device.Achieve Raw Performance with Native Code For the most demanding, performance-critical tasks, you can write Native Implemented Functions (NIFs) in low-level languages like C or Rust. These NIFs integrate directly with the Erlang VM, offering the highest possible performance.Use Case: If you were building a sophisticated cryptographic application, you could implement a new, high-speed hashing algorithm as a NIF to ensure maximum performance and security. This "raw" extensibility provides an escape hatch for ultimate control.

---

# 52. Pathing in HyperBEAM - HyperBEAM - Documentation

Document Number: 52
Source: https://hyperbeam.arweave.net/build/pathing-in-hyperbeam.html
Words: 844
Extraction Method: html

Pathing in HyperBEAM Overview Understanding how to construct and interpret paths in AO-Core is fundamental to working with HyperBEAM. This guide explains the structure and components of AO-Core paths, enabling you to effectively interact with processes and access their data.HyperBEAM Path Structure Let's examine a typical HyperBEAM endpoint piece-by-piece:https://forward.computer/<procId>~process@1.0/now Node URL (forward.computer) The HTTP response from this node includes a signature from the host's key. By accessing the ~snp@1.0 device, you can verify that the node is running in a genuine Trusted Execution Environment (TEE), ensuring computation integrity. You can replace forward.computer with any HyperBEAM TEE node operated by any party while maintaining trustless guarantees.Process Path (/<procId>~process@1.0) Every path in AO-Core represents a program. Think of the URL bar as a Unix-style command-line interface, providing access to AO's trustless and verifiable compute. Each path component (between / characters) represents a step in the computation. In this example, we instruct the AO-Core node to:Load a specific message from its caches (local, another node, or Arweave) Interpret it with the ~process@1.0 device The process device implements a shared computing environment with consistent state between users State Access (/now or /compute) Devices in AO-Core expose keys accessible via path components. Each key executes a function on the device:now: Calculates real-time process state compute: Serves the latest known state (faster than checking for new messages) Under the surface, these keys represent AO-Core messages. As we progress through the path, AO-Core applies each message to the existing state. You can access the full process state by visiting:/<procId>~process@1.0/now State Navigation You can browse through sub-messages and data fields by accessing them as keys. For example, if a process stores its interaction count in a field named cache, you can access it like this:/<procId>~process@1.0/compute/cache This shows the 'cache' of your process. Each response is:A message with a signature attesting to its correctness A hashpath describing its generation Transferable to other AO-Core nodes for uninterrupted execution Query Parameters and Type Casting Beyond path segments, HyperBEAM URLs can include query parameters that utilize a special type casting syntax. This allows specifying the desired data type for a parameter directly within the URL using the format key+type=value.Syntax: A + symbol separates the parameter key from its intended type (e.g., count+integer=42, items+list="apple",7).Mechanism: The HyperBEAM node identifies the +type suffix (e.g., +integer, +list, +map, +float, +atom, +resolve). It then uses internal functions (hb_singleton:maybe_typed and dev_codec_structured:decode_value) to decode and cast the provided value string into the corresponding Erlang data type before incorporating it into the message.Supported Types: Common types include integer, float, list, map, atom, binary (often implicit), and resolve (for path resolution). List values often follow the HTTP Structured Fields format (RFC 8941).This powerful feature enables the expression of complex data structures directly in URLs.Examples The following examples illustrate using HTTP paths with various AO-Core processes and devices. While these cover a few specific use cases, HyperBEAM's extensible nature allows interaction with any device or process via HTTP paths. For a deeper understanding, we encourage exploring the source code and experimenting with different paths.Example 1: Accessing Full Process State To get the complete, real-time state of a process identified by <procId>, use the /now path component with the ~process@1.0 device:GET /<procId>~process@1.0/now This instructs the AO-Core node to load the process and execute the now function on the ~process@1.0 device.Example 2: Navigating to Specific Process Data If a process maintains its state in a map and you want to access a specific field, like at-slot, using the faster /compute endpoint:GET /<procId>~process@1.0/compute/cache This accesses the compute key on the ~process@1.0 device and then navigates to the cache key within the resulting state map. Using this path, you will see the latest 'cache' of your process (the number of interactions it has received). Every piece of relevant information about your process can be accessed similarly, effectively providing a native API.(Note: This represents direct navigation within the process state structure. For accessing data specifically published via the ~patch@1.0 device, see the documentation on Exposing Process State, which typically uses the /cache/ path.) Example 3: Basic ~message@1.0 Usage Here's a simple example of using ~message@1.0 to create a message and retrieve a value:GET /~message@1.0&greeting="Hello"&count+integer=42/count Base:/ - The base URL of the HyperBEAM node.Root Device:~message@1.0 Query Params:greeting="Hello" (binary) and count+integer=42 (integer), forming the message #{ <<"greeting">> => <<"Hello">>, <<"count">> => 42 }.Path:/count tells ~message@1.0 to retrieve the value associated with the key count.Response: The integer 42.Example 4: Using the ~message@1.0 Device with Type Casting The ~message@1.0 device can be used to construct and query transient messages, utilizing type casting in query parameters.Consider the following URL:GET /~message@1.0&name="Alice"&age+integer=30&items+list="apple",1,"banana"&config+map=key1="val1";key2=true/[PATH] HyperBEAM processes this as follows:Base:/ - The base URL of the HyperBEAM node.Root Device:~message@1.0 Query Parameters (with type casting):name="Alice" -> #{ <<"name">> => <<"Alice">> } (binary) age+integer=30 -> #{ <<"age">> => 30 } (integer) items+list="apple",1,"banana" -> #{ <<"items">> => [<<"apple">>, 1, <<"banana">>] } (list) config+map=key1="val1";key2=true -> #{ <<"config">> => #{<<"key1">> => <<"val1">>, <<"key2">> => true} } (map) Initial Message Map: A combination of the above key-value pairs.Path Evaluation:If [PATH] is /items/1, the response is the integer 1.If [PATH] is /config/key1, the response is the binary <<"val1">>.

---

# 53. FAQ - HyperBEAM - Documentation

Document Number: 53
Source: https://hyperbeam.arweave.net/run/reference/faq.html
Words: 327
Extraction Method: html

Node Operator FAQ This page answers common questions about running and maintaining a HyperBEAM node.What is HyperBEAM?HyperBEAM is a client implementation of the AO-Core protocol written in Erlang. It serves as the node software for a decentralized operating system that allows operators to offer computational resources to users in the AO network.What are the system requirements for running HyperBEAM?Currently, HyperBEAM is primarily tested and documented for Ubuntu 22.04 and macOS. Other platforms will be added in future updates. For detailed requirements, see the System Requirements page.Can I run HyperBEAM in a container?While technically possible, running HyperBEAM in Docker containers or other containerization technologies is currently not recommended. The containerization approach may introduce additional complexity and potential performance issues. We recommend running HyperBEAM directly on the host system until container support is more thoroughly tested and optimized.How do I update HyperBEAM to the latest version?To update HyperBEAM:Pull the latest code from the repository (check Discord for the branch of Beta releases) Rebuild the application Restart the HyperBEAM service Specific update instructions will vary depending on your installation method.Can I run multiple HyperBEAM nodes on a single machine?Yes, you can run multiple HyperBEAM nodes on a single machine, but you'll need to configure them to use different ports and data directories to avoid conflicts. However, this is not recommended for production environments as each node should ideally have a unique IP address to properly participate in the network. Running multiple nodes on a single machine is primarily useful for development and testing purposes.Is there a limit to how many processes can run on a node?The practical limit depends on your hardware resources. Erlang is designed to handle millions of lightweight processes efficiently, but the actual number will be determined by:Available memory CPU capacity Network bandwidth Storage speed The complexity of your processes Where can I get help if I encounter issues?If you encounter issues:Check the Troubleshooting guide Search or ask questions on GitHub Issues Join the community on Discord

---

# 54. Configuring Your Machine - HyperBEAM - Documentation

Document Number: 54
Source: https://hyperbeam.arweave.net/run/configuring-your-machine.html
Words: 803
Extraction Method: html

Configuring Your HyperBEAM Node This guide details the various ways to configure your HyperBEAM node's behavior, including ports, storage, keys, and logging.Configuration (config.flat) The primary way to configure your HyperBEAM node is through a config.flat file located in the node's working directory or specified by the HB_CONFIG_LOCATION environment variable.This file uses a simple Key = Value. format (note the period at the end of each line).Example config.flat:% Set the HTTP port
port = 8080.

% Specify the Arweave key file
priv_key_location = "/path/to/your/wallet.json".

% Set the data store directory
% Note: Storage configuration can be complex. See below.
% store = [{local, [{root, <<"./node_data_mainnet">>}]}]. % Example of complex config, not for config.flat

% Enable verbose logging for specific modules
% debug_print = [hb_http, dev_router]. % Example of complex config, not for config.flat Below is a reference of commonly used configuration keys. Remember that config.flat only supports simple key-value pairs (Atoms, Strings, Integers, Booleans). For complex configurations (Lists, Maps), you must use environment variables or hb:start_mainnet/1.Core Configuration These options control fundamental HyperBEAM behavior.Option Type Default Description port Integer 8734 HTTP API port hb_config_location String "config.flat" Path to configuration file priv_key_location String "hyperbeam-key.json" Path to operator wallet key file mode Atom debug Execution mode (debug, prod) Server & Network Configuration These options control networking behavior and HTTP settings.Option Type Default Description host String "localhost" Choice of remote node for non-local tasks gateway String "https://arweave.net" Default gateway bundler_ans104 String "https://up.arweave.net:443" Location of ANS-104 bundler protocol Atom http2 Protocol for HTTP requests (http1, http2, http3) http_client Atom gun HTTP client to use (gun, httpc) http_connect_timeout Integer 5000 HTTP connection timeout in milliseconds http_keepalive Integer 120000 HTTP keepalive time in milliseconds http_request_send_timeout Integer 60000 HTTP request send timeout in milliseconds relay_http_client Atom httpc HTTP client for the relay device Security & Identity These options control identity and security settings.Option Type Default Description scheduler_location_ttl Integer 604800000 TTL for scheduler registration (7 days in ms) Caching & Storage These options control caching behavior. Note: Detailed storage configuration (store option) involves complex data structures and cannot be set via config.flat.Option Type Default Description cache_lookup_heuristics Boolean false Whether to use caching heuristics or always consult the local data store access_remote_cache_for_client Boolean false Whether to access data from remote caches for client requests store_all_signed Boolean true Whether the node should store all signed messages await_inprogress Atom/Boolean named Whether to await in-progress executions (false, named, true) Execution & Processing These options control how HyperBEAM executes messages and processes.Option Type Default Description scheduling_mode Atom local_confirmation When to inform recipients about scheduled assignments (aggressive, local_confirmation, remote_confirmation) compute_mode Atom lazy Whether to execute more messages after returning a result (aggressive, lazy) process_workers Boolean true Whether the node should use persistent processes client_error_strategy Atom throw What to do if a client error occurs wasm_allow_aot Boolean false Allow ahead-of-time compilation for WASM Device Management These options control how HyperBEAM manages devices.Option Type Default Description load_remote_devices Boolean false Whether to load devices from remote signers Debug & Development These options control debugging and development features.Option Type Default Description debug_stack_depth Integer 40 Maximum stack depth for debug printing debug_print_map_line_threshold Integer 30 Maximum lines for map printing debug_print_binary_max Integer 60 Maximum binary size for debug printing debug_print_indent Integer 2 Indentation for debug printing debug_print_trace Atom short Trace mode (short, false) short_trace_len Integer 5 Length of short traces debug_hide_metadata Boolean true Whether to hide metadata in debug output debug_ids Boolean false Whether to print IDs in debug output debug_hide_priv Boolean true Whether to hide private data in debug output Note: For the absolute complete and most up-to-date list, including complex options not suitable for config.flat, refer to the default_message/0 function in the hb_opts module source code.Overrides (Environment Variables & Args) You can override settings from config.flat or provide values if the file is missing using environment variables or command-line arguments.Using Environment Variables:Environment variables typically use an HB_ prefix followed by the configuration key in uppercase.HB_PORT=<port_number>: Overrides hb_port.Example: HB_PORT=8080 rebar3 shell HB_KEY=<path/to/wallet.key>: Overrides hb_key.Example: HB_KEY=~/.keys/arweave_key.json rebar3 shell HB_STORE=<directory_path>: Overrides hb_store.Example: HB_STORE=./node_data_1 rebar3 shell HB_PRINT=<setting>: Overrides hb_print. <setting> can be true (or 1), or a comma-separated list of modules/topics (e.g., hb_path,hb_ao,ao_result).Example: HB_PRINT=hb_http,dev_router rebar3 shell HB_CONFIG_LOCATION=<path/to/config.flat>: Specifies a custom location for the configuration file.Using erl_opts (Direct Erlang VM Arguments):You can also pass arguments directly to the Erlang VM using the -<key> <value> format within erl_opts. This is generally less common for application configuration than config.flat or environment variables.rebar3 shell --erl_opts "-hb_port 8080 -hb_key path/to/key.json" Order of Precedence:Command-line arguments (erl_opts).Settings in config.flat.Environment variables (HB_*).Default values from hb_opts.erl.Configuration in Releases When running a release build (see Running a HyperBEAM Node), configuration works similarly:A config.flat file will be present in the release directory (e.g., _build/default/rel/hb/config.flat). Edit this file to set your desired parameters for the release environment.Environment variables (HB_*) can still be used to override the settings in the release's config.flat when starting the node using the bin/hb script.

---

# 55. Troubleshooting - HyperBEAM - Documentation

Document Number: 55
Source: https://hyperbeam.arweave.net/run/reference/troubleshooting.html
Words: 330
Extraction Method: html

Node Operator Troubleshooting Guide This guide addresses common issues you might encounter when installing and running a HyperBEAM node.Installation Issues Erlang Installation Fails Symptoms: Errors during Erlang compilation or installation Solutions:Ensure all required dependencies are installed: sudo apt-get install -y libssl-dev ncurses-dev make cmake gcc g++ Try configuring with fewer options: ./configure --without-wx --without-debugger --without-observer --without-et Check disk space, as compilation requires several GB of free space Rebar3 Bootstrap Fails Symptoms: Errors when running ./bootstrap for Rebar3 Solutions:Verify Erlang is correctly installed: erl -eval 'erlang:display(erlang:system_info(otp_release)), halt().' Ensure you have the latest version of the repository: git fetch && git reset --hard origin/master Try manually downloading a precompiled Rebar3 binary HyperBEAM Issues HyperBEAM Won't Start Symptoms: Errors when running rebar3 shell or the HyperBEAM startup command Solutions:Check for port conflicts: Another service might be using the configured port Verify the wallet key file exists and is accessible Examine Erlang crash dumps for detailed error information Ensure all required dependencies are installed HyperBEAM Crashes During Operation Symptoms: Unexpected termination of the HyperBEAM process Solutions:Check system resources (memory, disk space) Examine Erlang crash dumps for details Reduce memory limits if the system is resource-constrained Check for network connectivity issues if connecting to external services Compute Unit Issues Compute Unit Won't Start Symptoms: Errors when running npm start in the CU directory Solutions:Verify Node.js is installed correctly: node -v Ensure all dependencies are installed: npm i Check that the wallet file exists and is correctly formatted Verify the .env file has all required settings Integration Issues HyperBEAM Can't Connect to Compute Unit Symptoms: Connection errors in HyperBEAM logs when trying to reach the CU Solutions:Verify the CU is running: curl http://localhost:6363 Ensure there are no firewall rules blocking the connection Verify network configuration if components are on different machines Getting Help If you're still experiencing issues after trying these troubleshooting steps:Check the GitHub repository for known issues Join the Discord community for support Open an issue on GitHub with detailed information about your problem

---

# 56. A whistle stop tour of Lua  Cookbook

Document Number: 56
Source: https://cookbook_ao.arweave.net/concepts/lua.html
Words: 870
Extraction Method: html

A whistle stop tour of Lua.Before we can explore ao in greater depth, let's take a moment to learn the basics of Lua: your companion for commanding aos processes.Lua is a simple language with few surprises. If you know Javascript, it will feel like a simplified, purer version. If you are learning from-scratch, it will seem like a tiny language that focuses on the important stuff: Clean computation with sane syntax.In this section we will cover the basics of Lua in just a few minutes. If you already know Lua, jump right through to the next chapter Jumping back into your aos process.For the purpose of this tutorial, we will be assuming that you have already completed the getting started guide. If not, complete that first.If you logged out of your process, you can always re-open it by running aos on your command line, optionally specifying your key file with --wallet [location].Basic Lua expressions.In the remainder of this primer we will quickly run through Lua's core features and syntax.Try out on the examples on your aos process as you go, or skip them if they are intuitive to you.Basic arithmetic: Try some basic arithmetic, like 5 + 3. After processing, you will see the result 8. +, -, *, /, and ^ all work as you might expect. % is the symbol that Lua uses for modulus.Setting variables: Type a = 10 and press enter. This sets the variable a to 10. By convention (not enforced by the language), global variables start with a capital letter in Lua (for example Handlers).Using variables: Now type a * 2. You will see 20 returned on the command line.String concatenation: Say hello to yourself by executing "Hello, " .. ao.id.INFO Note that while global variables conventionally start with a capital letter in Lua, this is not enforced by the language. For example, the ao module is a global variable that was intentionally lowercased for stylistic purposes.Experimenting with conditional statements.If-Else: Like most programming languages, Lua uses if-else blocks to conditionally execute code.In your aos process, type .editor and press enter. This will open an in-line text editor within your command-line interface.Once you are finished editing on your terminal, type .done on a new line and press enter. This will terminate edit mode and submit the expression to your process for evaluation.As a result, you will see that aos coolness is >9,000 cool. Good to know.if statements in Lua can also have additional elseif [condition] then blocks, making conditional execution hierarchies easier.Looping in Lua.There are a few different ways to loop in your code in Lua. Here are our favorites:While loops:Start by initializing your counter to zero by typing n = 0 and pressing enter.Then open the inline editor again with .editor.Type .done on a new line to execute the while loop. You can check the result of the loop by simply running n.For loops:Lua can also execute python-style for loops between a set of values. For example, use the .editor to enter the following code block:Request the new value of the variable by running n again.Getting functional.Define a function:Using the .editor once again, submit the following lines:Lua also has 'anonymous' or 'higher order' functions. These essentially allow you to use functions themselves as if they are normal data -- to be passed as arguments to other functions, etc. The following example defines an anonymous function and is equivalent to the above:Calling the function: Call the function with greeting("Earthling"). aos will return "Hello, Earthling".INFO Handlers in ao commonly utilize anonymous functions. When using Handlers.add(), the third argument is an anonymous function in the form function(msg) ... end. This is a key pattern you'll see frequently when working with ao processes.Defining deep objects with tables.Tables are Lua's only compound data structure. They map keys to values, but can also be used like traditional arrays.Create a simple table: Type ao_is = {"hyper", "parallel", "compute"} to create a simple table.Accessing the table's elements: Access an element with ao_is[2]. aos will return parallel. Note: Indices in Lua start from 1!Count a table's elements: The size of a table in Lua is found with the operator #. For example, running #ao_is will return 3.Set a named element: Type ao_is["cool"] = true to add a new named key to the table. Named elements can also be accessed with the . operator (e.g. ao_is.cool), but only if the key is a valid identifier - for other keys like "my key", use brackets.Lua Wats.aos uses Lua because it is a simple, clean language that most experienced programmers can learn very quickly, and is an increasingly popular first programming language, too, thanks to its use in video games like Roblox.Nonetheless, there are a few things about the language that are prone to trip up rookie Lua builders. Tastes may vary, but here is our exhaustive list of Lua wat s:Remember: Table indexing starts from 1 not 0!Remember: 'Not equals' is expressed with ~=, rather than != or similar.Remember: Objects in Lua are called 'tables', rather than their more common names.Let's go!With this in mind, you now know everything you need in order to build awesome decentralized processes with Lua! In the next chapter we will begin to build parallel processes with Lua and aos.

---

# 57. Messages  Cookbook

Document Number: 57
Source: https://cookbook_ao.arweave.net/concepts/messages.html
Words: 310
Extraction Method: html

Messages The Message serves as the fundamental data protocol unit within ao, crafted from ANS-104 DataItems, thereby aligning with the native structure of Arweave. When engaged in a Process, a Message is structured as follows:This architecture merges the Assignment Type with the Message Type, granting the Process a comprehensive understanding of the Message's context for effective processing.When sending a message, here is a visual diagram of how the messages travels through the ao computer. The message workflow initiates with the MU (Messenger Unit), where the message's signature is authenticated. Following this, the SU (Scheduler Unit) allocates an Epoch and Nonce to the message, bundles the message with an Assignment Type, and dispatches it to Arweave. Subsequently, the aoconnect library retrieves the outcome from the CU (Compute Unit). The CU then calls for all preceding messages leading up to the current Message Id from the SU (Scheduler Unit), processes them to deduce the result. Upon completion, the computed result is conveyed back to aoconnect, which is integrated within client interfaces such as aos.Ethereum Signed Message If the Message ANS-104 DataItem was signed using Ethereum keys, then the value in the Owner and From fields will be the EIP-55 Ethereum address of the signer. For example: 0xfB6916095ca1df60bB79Ce92cE3Ea74c37c5d359.Summary Messages serve as the primary data protocol type for the ao network, leveraging ANS-104 Data-Items native to Arweave. Messages contain several fields including data content, origin, target, and cryptographic elements like signatures and nonces. They follow a journey starting at the Messenger Unit (MU), which ensures they are signed, through the Scheduler Unit (SU) that timestamps and sequences them, before being bundled and published to Arweave. The aoconnect library then reads the result from the Compute Unit (CU), which processes messages to calculate results and sends responses back through aoconnect, utilized by clients such as aos. The CU is the execution environment for these processes.

---

# 58. ao Specs  Cookbook

Document Number: 58
Source: https://cookbook_ao.arweave.net/concepts/specs.html
Words: 148
Extraction Method: html

Skip to content  ao Specs What is ao?The ao computer is the actor oriented machine that emerges from the network of nodes that adhere to its core data protocol, running on the Arweave network. This document gives a brief introduction to the protocol and its functionality, as well as its technical details, such that builders can create new implementations and services that integrate with it.The ao computer is a single, unified computing environment (a Single System Image), hosted on a heterogenous set of nodes in a distributed network. ao is designed to offer an environment in which an arbitrary number of parallel processes can be resident, coordinating through an open message passing layer. This message passing standard connects the machine's independently operating processes together into a 'web' -- in the same way that websites operate on independent servers but are conjoined into a cohesive, unified experience via hyperlinks.

---

# 59. aos Brief Tour  Cookbook

Document Number: 59
Source: https://cookbook_ao.arweave.net/concepts/tour.html
Words: 390
Extraction Method: html

aos Brief Tour Welcome to a quick tour of aos! This tutorial will walk you through the key global functions and variables available in the aos environment, giving you a foundational understanding of how to interact with and utilize aos effectively.1. Introduction to Inbox What It Is: Inbox is a Lua table that stores all messages received by your process but not yet handled.How to Use: Check Inbox to see incoming messages. Iterate through Inbox[x] to process these messages.2. Sending Messages with Send(Message) Functionality: Send(Message) is a global function to send messages to other processes.Usage Example: Send({Target = "...", Data = "Hello, Process!"}) sends a message with the data "Hello, Process!" to a specified process.3. Creating Processes with Spawn(Module, Message) Purpose: Use Spawn(Module, Message) to create new processes.Example: Spawn("MyModule", {Data = "Start"}) starts a new process using "MyModule" with the provided message.4. Understanding Name and Owner Name: A string set during initialization, representing the process's name.Owner: Indicates the owner of the process. Changing this might restrict your ability to interact with your process.Important Note: Treat these as read-only to avoid issues.5. Utilizing Handlers What They Are: Handlers is a table of helper functions for creating message handlers.Usage: Define handlers in Handlers to specify actions for different incoming messages based on pattern matching.6. Data Representation with Dump Function: Dump converts any Lua table into a print-friendly format.How to Use: Useful for debugging or viewing complex table structures. Example: Dump(Inbox) prints the contents of Inbox.7. Leveraging Utils Module Contents: Utils contains a collection of functional utilities like map, reduce, and filter.Usage: Great for data manipulation and functional programming patterns in Lua. For example, Utils.map(myTable, function(x) return x * 2 end) to double the values in a table.8. Exploring the ao Core Library Description: ao is a core module that includes key functions for message handling and process management.Key Features: Includes functions for sending messages (send) and spawning processes (spawn), along with environment variables.Conclusion This brief tour introduces you to the primary globals and functionalities within the aos environment. With these tools at your disposal, you can create and manage processes, handle messages, and utilize Lua's capabilities to build efficient and responsive applications on the aos platform. Experiment with these features to get a deeper understanding and to see how they can be integrated into your specific use cases. Happy coding in aos!

---

# 60. DataItem Signers  Cookbook

Document Number: 60
Source: https://cookbook_ao.arweave.net/guides/aoconnect/signers.html
Words: 286
Extraction Method: html

DataItem Signers Every message sent to AO MUST be signed, aoconnect provides a helper function for signing messages or spawning new processes. This helper function createDataItemSigner is provided for arweave wallets. But you can create your own Signer instance too.What is a Wallet/Keyfile?A wallet/keyfile is a public/private key pair that can be used to sign and encrypt data.What is an ao message/dataItem?You often see the terms message and dataItem used interchangeably in the documentation, a message is a data-protocol type in ao that uses the dataItem specification to describe the messages intent. A dataItem is defined in the ANS-104 bundle specification. A dataItem is the preferred format of storage for arweave bundles. A bundle is a collection of these signed dataItems. A message implements specific tags using the dataItem specification. When developers send messages to ao, they are publishing dataItems on arweave.🎓 To learn more about messages click here and to learn more about ANS-104 dataItems click here What is a signer?A signer is function that takes data, tags, anchor, target and returns an object of id, binary representing a signed dataItem. AO accepts arweave signers and ethereum signers. createDataItemSigner is a helper function that can take an arweave keyfile or a browser instance of an arweave wallet usually located in the global scope of the browser, when I user connects to a wallet using an extension or html app.Examples arweave keyfile NOTE: if you do not have a wallet keyfile you can create one using npx -y @permaweb/wallet > wallet.json arweave browser extension NOTE: This implementation works with ArweaveWalletKit, ArConnect, and Arweave.app ethereum key Summary Using the signer function developers can control how dataItems are signed without having to share the signing process with aoconnect.

---

# 61. Sending a Message to a Process  Cookbook

Document Number: 61
Source: https://cookbook_ao.arweave.net/guides/aoconnect/sending-messages.html
Words: 391
Extraction Method: html

Sending a Message to a Process A deep dive into the concept of Messages can be found in the ao Messages concept. This guide focuses on using ao connect to send a message to a process.Sending a message is the central way in which your app can interact with ao. A message is input to a process. There are 5 parts of a message that you can specify which are "target", "data", "tags", "anchor", and finally the messages "signature".Refer to your process module's source code or documentation to see how the message is used in its computation. The ao connect library will translate the parameters you pass it in the code below, construct a message, and send it.🎓 To Learn more about Wallets visit the Permaweb Cookbook Sending a Message in NodeJS Need a test wallet, use npx -y @permaweb/wallet > /path/to/wallet.json to create a wallet keyfile.jsimport { readFileSync } from "node:fs";

import { message, createDataItemSigner } from "@permaweb/aoconnect";

const wallet = JSON.parse(

  readFileSync("/path/to/arweave/wallet.json").toString(),

);

// The only 2 mandatory parameters here are process and signer

await message({

  /*

    The arweave TxID of the process, this will become the "target".

    This is the process the message is ultimately sent to.

  */

  process: "process-id",

  // Tags that the process will use as input.

  tags: [

    { name: "Your-Tag-Name-Here", value: "your-tag-value" },

    { name: "Another-Tag", value: "another-value" },

  ],

  // A signer function used to build the message "signature"

  signer: createDataItemSigner(wallet),

  /*

    The "data" portion of the message

    If not specified a random string will be generated

  */

  data: "any data",

})

  .then(console.log)

  .catch(console.error);Sending a Message in a browser New to building permaweb apps check out the Permaweb Cookbook jsimport { message, createDataItemSigner } from "@permaweb/aoconnect";

// The only 2 mandatory parameters here are process and signer

await message({

  /*

    The arweave TxID of the process, this will become the "target".

    This is the process the message is ultimately sent to.

  */

  process: "process-id",

  // Tags that the process will use as input.

  tags: [

    { name: "Your-Tag-Name-Here", value: "your-tag-value" },

    { name: "Another-Tag", value: "another-value" },

  ],

  // A signer function used to build the message "signature"

  signer: createDataItemSigner(globalThis.arweaveWallet),

  /*

    The "data" portion of the message.

    If not specified a random string will be generated

  */

  data: "any data",

})

  .then(console.log)

  .catch(console.error);If you would like to learn more about signers, click here

---

# 62. CRED Utils Blueprint  Cookbook

Document Number: 62
Source: https://cookbook_ao.arweave.net/guides/aos/blueprints/cred-utils.html
Words: 661
Extraction Method: html

CRED Utils Blueprint CRED is now deprecated CRED was a token used during ao's legacynet phase to reward early developers. It is no longer earnable or redeemable.The CRED Utils Blueprint is a predesigned template that helps you quickly check your CRED balance in ao legacynet.Unpacking the CRED Utils Blueprint The CRED Metatable CRED.balance: Evaluating CRED.balance will print your process's last known balance of your CRED. If you have never fetched your CRED balance before, it will be fetched automatically. If you think your CRED has recently changed, consider running CRED.update first.CRED.process: Evaluating CRED.process will print the process ID of the CRED token issuer.CRED.send: Invoking CRED.send(targetProcessId, amount) like a function will transfer CRED from your ao process to another ao process.targetProcessId: string: the 43-character process ID of the recipient.amount: integer: The quantity of CRED units to send. 1 CRED === 1000 CRED units.CRED.update: Evaluating CRED.update will fetch your latest CRED balance by sending a message to the CRED issuer process. The UpdateCredBalance handler (see below) will ingest the response message.Handler Definitions Credit Handler: The CRED_Credit handler allows the CRED issuer process (and aos) to automatically notify you when your CRED balance increase.Debit Handler: The CRED_Debit handler allows the CRED issuer process (and aos) to automatically notify you when your CRED balance decreases.Update Balance Handler: The UpdateCredBalance handler ingests the response to any CRED.update requests.How To Use the Blueprint Open the Terminal.Start your aos process.Type in .load-blueprint credUtils Type in CRED.balance What's in the CRED Utils Blueprint:See the aos source code on GitHub for the blueprint shipped in the latest version of aos.luaCRED_PROCESS = "Sa0iBLPNyJQrwpTTG-tWLQU-1QeUAJA73DdxGGiKoJc"

_CRED = { balance = "Your CRED balance has not been checked yet. Updating now." }

local credMeta = {

    __index = function(t, key)

        -- sends CRED balance request

        if key == "update" then

            Send({ Target = CRED_PROCESS, Action = "Balance", Tags = { ["Target"] = ao.id } })

            return "Balance update requested."

            -- prints local CRED balance, requests it if not set

        elseif key == "balance" then

            if _CRED.balance == "Your CRED balance has not been checked yet. Updating now." then

                Send({ Target = CRED_PROCESS, Action = "Balance", Tags = { ["Target"] = ao.id } })

            end

            return _CRED.balance

            -- prints CRED process ID

        elseif key == "process" then

            return CRED_PROCESS

            -- tranfers CRED

        elseif key == "send" then

            return function(target, amount)

                -- ensures amount is string

                amount = tostring(amount)

                print("sending " .. amount .. "CRED to " .. target)

                Send({ Target = CRED_PROCESS, Action = "Transfer", ["Recipient"] = target, ["Quantity"] = amount })

            end

        else

            return nil

        end

    end

}

CRED = setmetatable({}, credMeta)

-- Function to evaluate if a message is a balance update

local function isCredBalanceMessage(msg)

    if msg.From == CRED_PROCESS and msg.Tags.Balance then

        return true

    else

        return false

    end

end

-- Function to evaluate if a message is a Debit Notice

local function isDebitNotice(msg)

    if msg.From == CRED_PROCESS and msg.Tags.Action == "Debit-Notice" then

        return true

    else

        return false

    end

end

-- Function to evaluate if a message is a Credit Notice

local function isCreditNotice(msg)

    if msg.From == CRED_PROCESS and msg.Tags.Action == "Credit-Notice" then

        return true

    else

        return false

    end

end

local function formatBalance(balance)

    -- Ensure balance is treated as a string

    balance = tostring(balance)

    -- Check if balance length is more than 3 to avoid unnecessary formatting

    if #balance > 3 then

        -- Insert dot before the last three digits

        balance = balance:sub(1, -4) .. "." .. balance:sub(-3)

    end

    return balance

end

-- Handles Balance messages

Handlers.add(

    "UpdateCredBalance",

    isCredBalanceMessage,

    function(msg)

        local balance = nil

        if msg.Tags.Balance then

            balance = msg.Tags.Balance

        end

        -- Format the balance if it's not set

        if balance then

            -- Format the balance by inserting a dot after the first three digits from the right

            local formattedBalance = formatBalance(balance)

            _CRED.balance = formattedBalance

            print("CRED Balance updated: " .. _CRED.balance)

        else

            print("An error occurred while updating CRED balance")

        end

    end

)

-- Handles Debit notices

Handlers.add(

    "CRED_Debit",

    isDebitNotice,

    function(msg)

        print(msg.Data)

    end

)

-- Handles Credit notices

Handlers.add(

    "CRED_Credit",

    isCreditNotice,

    function(msg)

        print(msg.Data)

    end

)

---

# 63. Spawning a Process  Cookbook

Document Number: 63
Source: https://cookbook_ao.arweave.net/guides/aoconnect/spawning-processes.html
Words: 160
Extraction Method: html

Skip to content  Spawning a Process A deep dive into the concept of Processes can be found in the ao Processes concept. This guide focuses on using ao connect to spawn a Process.In order to spawn a Process you must have the TxID of an ao Module that has been uploaded to Arweave. The Module is the source code for the Process. The Process itself is an instantiation of that source.You must also have the wallet address of a Scheduler Unit (SU). This specified SU will act as the scheduler for this Process. This means that all nodes in the system can tell that they need to read and write to this SU for this Process. You can use the address below.Wallet address of an available Scheduler lua_GQ33BkPtZrqxA84vM8Zk-N2aO0toNNu_C-l-rawrBA In addition, in order to receive messages from other processes an Authority tag must be supplied with the wallet address of an authorised Messaging Unit (MU).Wallet address of the legacynet MU luafcoN_xJeisVsPXA-trzVAuIiqO3ydLQxM-L4XbrQKzY

---

# 64. Staking Blueprint  Cookbook

Document Number: 64
Source: https://cookbook_ao.arweave.net/guides/aos/blueprints/staking.html
Words: 423
Extraction Method: html

Staking Blueprint The Staking Blueprint is a predesigned template that helps you quickly build a staking system in ao. It is a great way to get started and can be customized to fit your needs.Prerequisites The Staking Blueprint requires the Token Blueprint to be loaded, first.Unpacking the Staking Blueprint Stakers: The Stakers array is used to store the staked tokens of the participants.Unstaking: The Unstaking array is used to store the unstaking requests of the participants.Stake Action Handler: The stake handler allows processes to stake tokens. When a process sends a message with the tag Action = "Stake", the handler will add the staked tokens to the Stakers array and send a message back to the process confirming the staking.Unstake Action Handler: The unstake handler allows processes to unstake tokens. When a process sends a message with the tag Action = "Unstake", the handler will add the unstaking request to the Unstaking array and send a message back to the process confirming the unstaking.Finalization Handler: The finalize handler allows processes to finalize the staking process. When a process sends a message with the tag Action = "Finalize", the handler will process the unstaking requests and finalize the staking process.How To Use:Open your preferred text editor.Open the Terminal.Start your aos process.Type in .load-blueprint staking Verify the Blueprint is Loaded:Type in Handlers.list to see the newly loaded handlers.What's in the Staking Blueprint:luaStakers = Stakers or {}

Unstaking = Unstaking or {}

-- Stake Action Handler

Handlers.stake = function(msg)

  local quantity = tonumber(msg.Tags.Quantity)

  local delay = tonumber(msg.Tags.UnstakeDelay)

  local height = tonumber(msg['Block-Height'])

  assert(Balances[msg.From] and Balances[msg.From] >= quantity, "Insufficient balance to stake")

  Balances[msg.From] = Balances[msg.From] - quantity

  Stakers[msg.From] = Stakers[msg.From] or {}

  Stakers[msg.From].amount = (Stakers[msg.From].amount or 0) + quantity

  Stakers[msg.From].unstake_at = height + delay

end

-- Unstake Action Handler

Handlers.unstake = function(msg)

  local quantity = tonumber(msg.Tags.Quantity)

  local stakerInfo = Stakers[msg.From]

  assert(stakerInfo and stakerInfo.amount >= quantity, "Insufficient staked amount")

  stakerInfo.amount = stakerInfo.amount - quantity

  Unstaking[msg.From] = {

      amount = quantity,

      release_at = stakerInfo.unstake_at

  }

end

-- Finalization Handler

local finalizationHandler = function(msg)

  local currentHeight = tonumber(msg['Block-Height'])

  -- Process unstaking

  for address, unstakeInfo in pairs(Unstaking) do

      if currentHeight >= unstakeInfo.release_at then

          Balances[address] = (Balances[address] or 0) + unstakeInfo.amount

          Unstaking[address] = nil

      end

  end

end

-- wrap function to continue handler flow

local function continue(fn)

  return function (msg)

    local result = fn(msg)

    if (result) == -1 then

      return 1

    end

    return result

  end

end

-- Registering Handlers

Handlers.add("stake",

  continue(Handlers.utils.hasMatchingTag("Action", "Stake")), Handlers.stake)

Handlers.add("unstake",

  continue(Handlers.utils.hasMatchingTag("Action", "Unstake")), Handlers.unstake)

-- Finalization handler should be called for every message

Handlers.add("finalize", function (msg) return -1 end, finalizationHandler)

---

# 65. Token Blueprint  Cookbook

Document Number: 65
Source: https://cookbook_ao.arweave.net/guides/aos/blueprints/token.html
Words: 972
Extraction Method: html

Token Blueprint The Token Blueprint is a predesigned template that helps you quickly build a token in ao. It is a great way to get started and can be customized to fit your needs.Unpacking the Token Blueprint Balances: The Balances array is used to store the token balances of the participants.Info Handler: The info handler allows processes to retrieve the token parameters, like Name, Ticker, Logo, and Denomination.Balance Handler: The balance handler allows processes to retrieve the token balance of a participant.Balances Handler: The balances handler allows processes to retrieve the token balances of all participants.Transfer Handler: The transfer handler allows processes to send tokens to another participant.Mint Handler: The mint handler allows processes to mint new tokens.Total Supply Handler: The totalSupply handler allows processes to retrieve the total supply of the token.Burn Handler: The burn handler allows processes to burn tokens.How To Use:Open your preferred text editor.Open the Terminal.Start your aos process.Type in .load-blueprint token Verify the Blueprint is Loaded:Type in Handlers.list to see the newly loaded handlers.What's in the Token Blueprint:lualocal bint = require('.bint')(256)

--[[

  This module implements the ao Standard Token Specification.

  Terms:

    Sender: the wallet or Process that sent the Message

  It will first initialize the internal state, and then attach handlers,

    according to the ao Standard Token Spec API:

    - Info(): return the token parameters, like Name, Ticker, Logo, and Denomination

    - Balance(Target?: string): return the token balance of the Target. If Target is not provided, the Sender

        is assumed to be the Target

    - Balances(): return the token balance of all participants

    - Transfer(Target: string, Quantity: number): if the Sender has a sufficient balance, send the specified Quantity

        to the Target. It will also issue a Credit-Notice to the Target and a Debit-Notice to the Sender

    - Mint(Quantity: number): if the Sender matches the Process Owner, then mint the desired Quantity of tokens, adding

        them the Processes' balance

]]

--

local json = require('json')

--[[

  utils helper functions to remove the bint complexity.

]]

--

local utils = {

  add = function(a, b)

    return tostring(bint(a) + bint(b))

  end,

  subtract = function(a, b)

    return tostring(bint(a) - bint(b))

  end,

  toBalanceValue = function(a)

    return tostring(bint(a))

  end,

  toNumber = function(a)

    return bint.tonumber(a)

  end

}

--[[

     Initialize State

     ao.id is equal to the Process.Id

   ]]

--

Variant = "0.0.3"

-- token should be idempotent and not change previous state updates

Denomination = Denomination or 12

Balances = Balances or { [ao.id] = utils.toBalanceValue(10000 * 10 ^ Denomination) }

TotalSupply = TotalSupply or utils.toBalanceValue(10000 * 10 ^ Denomination)

Name = Name or 'Points Coin'

Ticker = Ticker or 'PNTS'

Logo = Logo or 'SBCCXwwecBlDqRLUjb8dYABExTJXLieawf7m2aBJ-KY'

--[[

     Add handlers for each incoming Action defined by the ao Standard Token Specification

   ]]

--

--[[

     Info

   ]]

--

Handlers.add('info', "Info", function(msg)

  msg.reply({

    Name = Name,

    Ticker = Ticker,

    Logo = Logo,

    Denomination = tostring(Denomination)

  })

end)

--[[

     Balance

   ]]

--

Handlers.add('balance', "Balance", function(msg)

  local bal = '0'

  -- If not Recipient is provided, then return the Senders balance

  if (msg.Tags.Recipient) then

    if (Balances[msg.Tags.Recipient]) then

      bal = Balances[msg.Tags.Recipient]

    end

  elseif msg.Tags.Target and Balances[msg.Tags.Target] then

    bal = Balances[msg.Tags.Target]

  elseif Balances[msg.From] then

    bal = Balances[msg.From]

  end

  msg.reply({

    Balance = bal,

    Ticker = Ticker,

    Account = msg.Tags.Recipient or msg.From,

    Data = bal

  })

end)

--[[

     Balances

   ]]

--

Handlers.add('balances', "Balances",

  function(msg) msg.reply({ Data = json.encode(Balances) }) end)

--[[

     Transfer

   ]]

--

Handlers.add('transfer', "Transfer", function(msg)

  assert(type(msg.Recipient) == 'string', 'Recipient is required!')

  assert(type(msg.Quantity) == 'string', 'Quantity is required!')

  assert(bint.__lt(0, bint(msg.Quantity)), 'Quantity must be greater than 0')

  if not Balances[msg.From] then Balances[msg.From] = "0" end

  if not Balances[msg.Recipient] then Balances[msg.Recipient] = "0" end

  if bint(msg.Quantity) <= bint(Balances[msg.From]) then

    Balances[msg.From] = utils.subtract(Balances[msg.From], msg.Quantity)

    Balances[msg.Recipient] = utils.add(Balances[msg.Recipient], msg.Quantity)

    --[[

         Only send the notifications to the Sender and Recipient

         if the Cast tag is not set on the Transfer message

       ]]

    --

    if not msg.Cast then

      -- Debit-Notice message template, that is sent to the Sender of the transfer

      local debitNotice = {

        Action = 'Debit-Notice',

        Recipient = msg.Recipient,

        Quantity = msg.Quantity,

        Data = Colors.gray ..

            "You transferred " ..

            Colors.blue .. msg.Quantity .. Colors.gray .. " to " .. Colors.green .. msg.Recipient .. Colors.reset

      }

      -- Credit-Notice message template, that is sent to the Recipient of the transfer

      local creditNotice = {

        Target = msg.Recipient,

        Action = 'Credit-Notice',

        Sender = msg.From,

        Quantity = msg.Quantity,

        Data = Colors.gray ..

            "You received " ..

            Colors.blue .. msg.Quantity .. Colors.gray .. " from " .. Colors.green .. msg.From .. Colors.reset

      }

      -- Add forwarded tags to the credit and debit notice messages

      for tagName, tagValue in pairs(msg) do

        -- Tags beginning with "X-" are forwarded

        if string.sub(tagName, 1, 2) == "X-" then

          debitNotice[tagName] = tagValue

          creditNotice[tagName] = tagValue

        end

      end

      -- Send Debit-Notice and Credit-Notice

      msg.reply(debitNotice)

      Send(creditNotice)

    end

  else

    msg.reply({

      Action = 'Transfer-Error',

      ['Message-Id'] = msg.Id,

      Error = 'Insufficient Balance!'

    })

  end

end)

--[[

    Mint

   ]]

--

Handlers.add('mint', "Mint", function(msg)

  assert(type(msg.Quantity) == 'string', 'Quantity is required!')

  assert(bint(0) < bint(msg.Quantity), 'Quantity must be greater than zero!')

  if not Balances[ao.id] then Balances[ao.id] = "0" end

  if msg.From == ao.id then

    -- Add tokens to the token pool, according to Quantity

    Balances[msg.From] = utils.add(Balances[msg.From], msg.Quantity)

    TotalSupply = utils.add(TotalSupply, msg.Quantity)

    msg.reply({

      Data = Colors.gray .. "Successfully minted " .. Colors.blue .. msg.Quantity .. Colors.reset

    })

  else

    msg.reply({

      Action = 'Mint-Error',

      ['Message-Id'] = msg.Id,

      Error = 'Only the Process Id can mint new ' .. Ticker .. ' tokens!'

    })

  end

end)

--[[

     Total Supply

   ]]

--

Handlers.add('totalSupply', "Total-Supply", function(msg)

  assert(msg.From ~= ao.id, 'Cannot call Total-Supply from the same process!')

  msg.reply({

    Action = 'Total-Supply',

    Data = TotalSupply,

    Ticker = Ticker

  })

end)

--[[

 Burn

]] --

Handlers.add('burn', 'Burn', function(msg)

  assert(type(msg.Quantity) == 'string', 'Quantity is required!')

  assert(bint(msg.Quantity) <= bint(Balances[msg.From]), 'Quantity must be less than or equal to the current balance!')

  Balances[msg.From] = utils.subtract(Balances[msg.From], msg.Quantity)

  TotalSupply = utils.subtract(TotalSupply, msg.Quantity)

  msg.reply({

    Data = Colors.gray .. "Successfully burned " .. Colors.blue .. msg.Quantity .. Colors.reset

  })

end)

---

# 66. Editor setup  Cookbook

Document Number: 66
Source: https://cookbook_ao.arweave.net/guides/aos/editor.html
Words: 218
Extraction Method: html

Editor setup Remembering all the built in ao functions and utilities can sometimes be hard. To enhance your developer experience, it is recommended to install the Lua Language Server extension into your favorite text editor and add the ao addon. It supports all built in aos modules and globals.VS Code Install the sumneko.lua extension:Search for "Lua" by sumneko in the extension marketplace Download and install the extension Open the VS Code command palette with Shift + Command + P (Mac) / Ctrl + Shift + P (Windows/Linux) and run the following command:In the Addon Manager, search for "ao", it should be the first result. Click "Enable" and enjoy autocomplete!Other editors Verify that your editor supports the language server protocol Install Lua Language Server by following the instructions at luals.github.io Install the "ao" addon to the language server BetterIDEa BetterIDEa is a custom web based IDE for developing on ao.It offers a built in Lua language server with ao definitions, so you don't need to install anything. Just open the IDE and start coding!Features include:Code completion Cell based notebook ui for rapid development Easy process management Markdown and Latex cell support Share projects with anyone through ao processes Tight integration with ao package manager Read detailed information about the various features and integrations of the IDE in the documentation.

---

# 67. FAQ  Cookbook

Document Number: 67
Source: https://cookbook_ao.arweave.net/guides/aos/faq.html
Words: 179
Extraction Method: html

Skip to content  FAQ Ownership Understanding Process Ownership Start a new process with the aos console, the ownership of the process is set to your wallet address. aos uses the Owner global variable to define the ownership of the process. If you wish to transfer ownership or lock the process so that no one can own, you simply modify the Owner variable to another wallet address or set it to nil.JSON encoding data as json When sending data to another process or an external service, you may want to use JSON as a way to encode the data for recipients. Using the json module in lua, you can encode and decode pure lua tables that contain values.Send vs ao.send When to use Send vs ao.send Both functions send a message to a process, the difference is ao.send returns the message, in case you want to log it or troubleshoot. The Send function is intended to be used in the console for easier access. It is preferred to use ao.send in the handlers. But they are both interchangeable in aos.

---

# 68. CLI  Cookbook

Document Number: 68
Source: https://cookbook_ao.arweave.net/guides/aos/cli.html
Words: 181
Extraction Method: html

Skip to content  CLI There are some command-line arguments you pass to aos to do the following:[name] - create a new process or loads an existing process for your wallet --load <file> - load a file, you can add one or many of this command --cron <interval> - only used when creating a process --wallet <walletfile> - use a specific wallet Managing multiple processes with aos shaos Starts or connects to a process with the name default shaos chatroom Starts or connects to a process with the name of chatroom shaos treasureRoom Starts or connects to a process with the name of treasureRoom Load flag With the load flag I can load many source files to my process CRON Flag If you want to setup your process to react on a schedule we need to tell ao, we do that when we spawn the process.Tag flags With the tag flags, you can start a process with some custom tags (for e.g. using them as static environment variables):The command above will add the extra tags to the transaction that spawns your process:

---

# 69. Voting Blueprint  Cookbook

Document Number: 69
Source: https://cookbook_ao.arweave.net/guides/aos/blueprints/voting.html
Words: 340
Extraction Method: html

Voting Blueprint The Voting Blueprint is a predesigned template that helps you quickly build a voting system in ao. It is a great way to get started and can be customized to fit your needs.Prerequisites The Staking Blueprint requires the Token Blueprint to be loaded, first.Unpacking the Voting Blueprint Balances: The Balances array is used to store the token balances of the participants.Votes: The Votes array is used to store the votes of the participants.Vote Action Handler: The vote handler allows processes to vote. When a process sends a message with the tag Action = "Vote", the handler will add the vote to the Votes array and send a message back to the process confirming the vote.Finalization Handler: The finalize handler allows processes to finalize the voting process. When a process sends a message with the tag Action = "Finalize", the handler will process the votes and finalize the voting process.How To Use:Open your preferred text editor.Open the Terminal.Start your aos process.Type in .load-blueprint voting Verify the Blueprint is Loaded:Type in Handlers.list to see the newly loaded handlers.What's in the Voting Blueprint:luaBalances = Balances or {}

Votes = Votes or {}

-- Vote Action Handler

Handlers.vote = function(msg)

  local quantity = Stakers[msg.From].amount

  local target = msg.Tags.Target

  local side = msg.Tags.Side

  local deadline = tonumber(msg['Block-Height']) + tonumber(msg.Tags.Deadline)

  assert(quantity > 0, "No staked tokens to vote")

  Votes[target] = Votes[target] or { yay = 0, nay = 0, deadline = deadline }

  Votes[target][side] = Votes[target][side] + quantity

end

-- Finalization Handler

local finalizationHandler = function(msg)

  local currentHeight = tonumber(msg['Block-Height'])

  -- Process voting

  for target, voteInfo in pairs(Votes) do

      if currentHeight >= voteInfo.deadline then

          if voteInfo.yay > voteInfo.nay then

              print("Handle Vote")

          end

          -- Clear the vote record after processing

          Votes[target] = nil

      end

  end

end

-- wrap function to continue handler flow

local function continue(fn)

  return function (msg)

    local result = fn(msg)

    if (result) == -1 then

      return 1

    end

    return result

  end

end

Handlers.add("vote",

  continue(Handlers.utils.hasMatchingTag("Action", "Vote")), Handlers.vote)

-- Finalization handler should be called for every message

Handlers.add("finalize", function (msg) return -1 end, finalizationHandler)

---

# 70. aos AO Operating System  Cookbook

Document Number: 70
Source: https://cookbook_ao.arweave.net/guides/aos/index.html
Words: 396
Extraction Method: html

aos: AO Operating System aos is a powerful operating system built on top of the AO hyper-parallel computer. While AO provides the distributed compute infrastructure, aos offers a simplified interface for interacting with and developing processes in this environment.What is aos?aos enables you to:Create and interact with processes on the AO network Develop distributed applications using a simple, intuitive approach Leverage the Lua programming language for deterministic, reliable operations All you need to get started is a terminal and a code editor. aos uses Lua as its primary language - a robust, deterministic, and user-friendly programming language that's ideal for distributed applications.New to AO? If you're just getting started, we recommend completing our tutorials first. They take just 15-30 minutes and provide an excellent foundation.Getting Started with aos Start here if you're new to aos:Introduction to aos - Overview of aos capabilities and concepts Installation Guide - Step-by-step instructions for setting up aos aos Command Line Interface - Learn to use the aos CLI effectively Customizing Your Prompt - Personalize your aos development environment Load Lua Files - Learn how to load and execute Lua files in aos Building a Ping-Pong Server - Create your first interactive aos application Blueprints Blueprints in aos are templates that streamline the development of distributed applications by providing a framework for creating consistent and efficient processes across the AO network.Available Blueprints Chatroom - Template for building chatroom applications Cred Utils - Tools for managing credentials Staking - Framework for implementing staking mechanisms Token - Guide for creating and managing tokens Voting - Blueprint for setting up voting systems aos Modules aos includes several built-in modules for common operations:JSON Module - Parse and generate JSON data AO Module - Interface with the AO ecosystem Crypto Module - Perform cryptographic operations Base64 Module - Encode and decode Base64 data Pretty Module - Format data for easier reading Utils Module - Common utility functions Developer Resources More advanced topics for aos development:Editor Setup & Configuration - Configure your development environment Understanding the Inbox & Message Handlers - Learn how message handling works Troubleshooting with ao.link - Debug aos applications Frequently Asked Questions - Find answers to common questions Build a Token - Create your own token on AO Use the sidebar to browse through specific aos guides. For a more structured learning path, we recommend following the guides in the order listed above.

---

# 71. Building a Token in ao  Cookbook

Document Number: 71
Source: https://cookbook_ao.arweave.net/guides/aos/token.html
Words: 1341
Extraction Method: html

Building a Token in ao When creating tokens, we'll continue to use the Lua Language within ao to mint a token, guided by the principles outlined in the Token Specification.Two Ways to Create Tokens:1 - Use the token blueprint:.load-blueprint token Using the token blueprint will create a token with all the handlers and state already defined. This is the easiest way to create a token. You'll be able to customize those handlers and state to your after loading the blueprint.You can learn more about available blueprints here: Blueprints INFO Using the token blueprint will definitely get quickly, but you'll still want to understand how to load and test the token, so you can customize it to your needs.2 - Build from Scratch:The following guide will guide you through the process of creating a token from scratch. This is a more advanced way to create a token, but it will give you a better understanding of how tokens work.Preparations Step 1: Initializing the Token Open our preferred text editor, preferably from within the same folder you used during the previous tutorial.Create a new file named token.lua.Within token.lua, you'll begin by initializing the token's state, defining its balance, name, ticker, and more: Let's break down what we've done here:local json = require('json'): This first line of this code imports a module for later use.if not Balances then Balances = { [ao.id] = 100000000000000 } end: This second line is initializing a Balances table which is the way the Process tracks who posses the token. We initialize our token process ao.id to start with all the balance.The Next 4 Lines, if Name, if Ticker, if Denomination, and if not Logo are all optional, except for if Denomination, and are used to define the token's name, ticker, denomination, and logo respectively.INFO The code if Denomination ~= 10 then Denomination = 10 end tells us the number of the token that should be treated as a single unit.Step 2: Info and Balances Handlers  Incoming Message Handler Now lets add our first Handler to handle incoming Messages. INFO At this point, you've probably noticed that we're building all of the handlers inside the token.lua file rather than using .editor.With many handlers and processes, it's perfectly fine to create your handlers using .editor, but because we're creating a full process for initializing a token, setting up info and balances handlers, transfer handlers, and a minting handler, it's best to keep everything in one file.This also allows us to maintain consistency since each handler will be updated every time we reload the token.lua file into aos.This code means that if someone Sends a message with the Tag, Action = "Info", our token will Send back a message with all of the information defined above. Note the Target = msg.From, this tells ao we are replying to the process that sent us this message.Info & Token Balance Handlers Now we can add 2 Handlers which provide information about token Balances.The first Handler above Handlers.add('Balance' handles a process or person requesting their own balance or the balance of a Target. Then replies with a message containing the info. The second Handler Handlers.add('Balances' just replies with the entire Balances table.Step 3: Transfer Handlers Before we begin testing we will add 2 more Handlers one which allows for the transfer of tokens between processes or users.luaHandlers.add('Transfer', Handlers.utils.hasMatchingTag('Action', 'Transfer'), function(msg)

  assert(type(msg.Tags.Recipient) == 'string', 'Recipient is required!')

  assert(type(msg.Tags.Quantity) == 'string', 'Quantity is required!')

  if not Balances[msg.From] then Balances[msg.From] = 0 end

  if not Balances[msg.Tags.Recipient] then Balances[msg.Tags.Recipient] = 0 end

  local qty = tonumber(msg.Tags.Quantity)

  assert(type(qty) == 'number', 'qty must be number')

  if Balances[msg.From] >= qty then

    Balances[msg.From] = Balances[msg.From] - qty

    Balances[msg.Tags.Recipient] = Balances[msg.Tags.Recipient] + qty

    --[[

      Only Send the notifications to the Sender and Recipient

      if the Cast tag is not set on the Transfer message

    ]] --

    if not msg.Tags.Cast then

      -- Debit-Notice message template, that is sent to the Sender of the transfer

      local debitNotice = {

        Target = msg.From,

        Action = 'Debit-Notice',

        Recipient = msg.Recipient,

        Quantity = tostring(qty),

        Data = Colors.gray ..

            "You transferred " ..

            Colors.blue .. msg.Quantity .. Colors.gray .. " to " .. Colors.green .. msg.Recipient .. Colors.reset

      }

      -- Credit-Notice message template, that is sent to the Recipient of the transfer

      local creditNotice = {

        Target = msg.Recipient,

        Action = 'Credit-Notice',

        Sender = msg.From,

        Quantity = tostring(qty),

        Data = Colors.gray ..

            "You received " ..

            Colors.blue .. msg.Quantity .. Colors.gray .. " from " .. Colors.green .. msg.From .. Colors.reset

      }

      -- Add forwarded tags to the credit and debit notice messages

      for tagName, tagValue in pairs(msg) do

        -- Tags beginning with "X-" are forwarded

        if string.sub(tagName, 1, 2) == "X-" then

          debitNotice[tagName] = tagValue

          creditNotice[tagName] = tagValue

        end

      end

      -- Send Debit-Notice and Credit-Notice

      ao.send(debitNotice)

      ao.send(creditNotice)

    end

  else

    ao.send({

      Target = msg.Tags.From,

      Tags = { ["Action"] = 'Transfer-Error', ['Message-Id'] = msg.Id, ["Error"] = 'Insufficient Balance!' }

    })

  end

end) In summary, this code checks to make sure the Recipient and Quantity Tags have been provided, initializes the balances of the person sending the message and the Recipient if they dont exist and then attempts to transfer the specified quantity to the Recipient in the Balances table.If the transfer was successful a Debit-Notice is sent to the sender of the original message and a Credit-Notice is sent to the Recipient.If there was insufficient balance for the transfer it sends back a failure message The line if not msg.Tags.Cast then Means were not producing any messages to push if the Cast tag was set. This is part of the ao protocol.Step 4: Mint Handler Finally, we will add a Handler to allow the minting of new tokens.This code checks to make sure the Quantity Tag has been provided and then adds the specified quantity to the Balances table.Once you've created your token.lua file, or you've used .load-blueprint token, you're now ready to begin testing.1 - Start the aos process Make sure you've started your aos process by running aos in your terminal.If you've followed along with the guide, you'll have a token.lua file in the same directory as your aos process. From the aos prompt, load in the file.lua.load token.lua 3 - Testing the Token Now we can send Messages to our aos process ID, from the same aos prompt to see if is working. If we use ao.id as the Target we are sending a message to ourselves.This should print the Info defined in the contract. Check the latest inbox message for the response.luaInbox[#Inbox].Tags This should print the Info defined in the contract.INFO Make sure you numerically are checking the last message. To do so, run #Inbox first to see the total number of messages are in the inbox. Then, run the last message number to see the data.Example:If #Inbox returns 5, then run Inbox[5].Data to see the data.4 - Transfer Now, try to transfer a balance of tokens to another wallet or process ID.INFO If you need another process ID, you can run aos [name] in another terminal window to get a new process ID. Make sure it's not the same aos [name] as the one you're currently using.Example:If you're using aos in one terminal window, you can run aos test in another terminal window to get a new process ID.After sending, you'll receive a printed message in the terminal similar to Debit-Notice on the sender's side and Credit-Notice on the recipient's side.5 - Check the Balances Now that you've transferred some tokens, let's check the balances.luaInbox[#Inbox].Data You will see two process IDs or wallet addresses, each displaying a balance. The first should be your sending process ID, the second should be the recipient's process ID.6 - Minting Tokens Finally, attempt to mint some tokens.And check the balances again.You'll then see the balance of the process ID that minted the tokens has increased.Conclusion That concludes the "Build a Token" guide. Learning out to build custom tokens will unlock a great deal of potential for your projects; whether that be creating a new currency, a token for a game, a governance token, or anything else you can imagine.

---

# 72. Guides  Cookbook

Document Number: 72
Source: https://cookbook_ao.arweave.net/guides/index.html
Words: 215
Extraction Method: html

Guides This section provides detailed guides and documentation to help you build and deploy applications on the AO ecosystem. Whether you're creating chatrooms, autonomous bots, or complex decentralized applications, you'll find step-by-step instructions here.Core Technologies Comprehensive guides for AO's main technologies:AOS: Compute on AO - Learn how to use the AO operating system for distributed computing Introduction to AOS - Get started with the AOS environment Installation Guide - Set up AOS on your system CLI Usage - Learn the command-line interface And more...AO Connect: JavaScript Library - Interact with AO using JavaScript Installation - Set up the AO Connect library Connecting to AO - Establish connections Sending Messages - Communicate with processes And more...Development Tools AO Module Builder CLI - Build WebAssembly modules for AO Installation - Install the development CLI Project Setup - Create your first project Building & Deployment - Compile and deploy modules Utilities & Storage Helpful tools and storage solutions:Using WeaveDrive - Store and manage data with WeaveDrive Using SQLite - Integrate SQLite databases with your AO projects Additional Resources Community Resources - Connect with the AO community Release Notes - Stay updated on the latest changes and features Use the sidebar to browse through specific guides. Each guide provides detailed instructions and examples to help you build on AO.

---

# 73. Troubleshooting using aolink  Cookbook

Document Number: 73
Source: https://cookbook_ao.arweave.net/guides/aos/troubleshooting.html
Words: 337
Extraction Method: html

Troubleshooting using ao.link Working with a decentralized computer and network, you need to be able to troubleshoot more than your own code. You need to be able to track messages, token balances, token transfers of processes. This is where https://ao.link becomes an essential tool in your toolbox. Analytics AOLink has a set of 4 analytic measures:Total Messages Total Users Total Processes Total Modules These analytics give you a quick view into the ao network's total processing health.Events Below, the analytics are the latest events that have appeared on the ao computer. You have a list of messages being scheduled and that have been executed. These events are any of the ao Data Protocol Types. And you can click on the Process ID or the Message ID to get details about each. Message Details  The message details give you key details about:From To Block Height Created Tags Data Result Type Data If you want to further troubleshoot and debug, you have the option to look at the result of the CU (Compute Unit) by clicking on "Compute". And further understand linked messages. Process Details  The process details provide you with information about the process it's useful to see in the tags with what module this got instantiated from. If you notice on the left you see the interaction with the process displayed on a graph. In this case, this is DevChat, and you can see all the processes that have interacted by Registering and Broadcasting Messages.You can effortless check the Info Handler, by pressing the "Fetch" button. On the bottom you see the processes balance and all messages send, with the option to break it down into Token transfers and Token balances using the tabs. Further Questions?Feel free to reach out on the community Discord of Autonomous Finance, for all questions and support regarding ao.link. https://discord.gg/4kF9HKZ4Wu Summary AOLink is an excellent tool for tracking events in the ao computer. Give it a try. Also, there is another scanner tool available on the permaweb: https://ao_marton.g8way.io/ - check it out!

---

# 74. HyperBEAM from AO Connect  Cookbook

Document Number: 74
Source: https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/ao-connect.html
Words: 211
Extraction Method: html

HyperBEAM from AO Connect This guide explains how to interact with a process using HyperBEAM and aoconnect.Prerequisites Node.js environment @permaweb/aoconnect library The latest version of aos Wallet file (wallet.json) containing your cryptographic keys A HyperBEAM node running with the genesis_wasm profile The Process ID for a process created with genesis_wasm (this is the default in the latest version of aos).Step 1: Environment Setup Install necessary dependencies:Ensure your wallet file (wallet.json) is correctly formatted and placed in your project directory.INFO You can create a test wallet using this command: npx -y @permaweb/wallet > wallet.json Step 2: Establish Connection Create a new JavaScript file (e.g., index.js) and set up your Permaweb connection. You will need a processId of a process that you want to interact with.Step 3: Pushing a Message to a Process Use the request function to send a message to the process. In aoconnect, this is done by using the push path parameter.Full Example To run the full script, combine the snippets from Step 2 and 3 into index.js:Now, run it:bashnode index.js You should see an object logged to the console, containing the ID of the message that was sent.Conclusion Following these steps, you've successfully sent a message to a process. This is a fundamental interaction for building applications on hyperAOS.

---

# 75. AO Dev-Cli 01  Cookbook

Document Number: 75
Source: https://cookbook_ao.arweave.net/guides/dev-cli/index.html
Words: 462
Extraction Method: html

AO Dev-Cli 0.1 The AO dev-cli is a tool that is used to build ao wasm modules, the first versions of the tool only supported lua as the embedded language or c based module. With this release developers now can add any pure c or cpp module to their wasm builds. This opens the door for many different innovations from indexers to languages.Install Requirements Docker is required: https://docker.com Start a project Build a project Deploy a project Requirements You will need an arweave keyfile, you can create a local one using this command npx -y @permaweb/wallet > wallet.json Configuration To customize your build process, create a config.yml file in the root directory of your project. This file will modify your settings during the build.Configuration Options:preset: Selects default values for stack_size, initial_memory, and maximum_memory. For available presets, see Config Presets. (Default: md) stack_size: Specifies the stack size, overriding the value from the preset. Must be a multiple of 64. (Default: 32MB) initial_memory: Defines the initial memory size, overriding the preset value. Must be larger than stack_size and a multiple of 64. (Default: 48MB) maximum_memory: Sets the maximum memory size, overriding the preset value. Must be larger than stack_size and a multiple of 64. (Default: 256MB) extra_compile_args: Provides additional compilation commands for emcc. (Default: []) keep_js: By default, the generated .js file is deleted since AO Loader uses predefined versions. Set this to true if you need to retain the .js file. (Default: false) Libraries Starting with version 0.1.3, you can integrate external libraries into your project. To do this, follow these guidelines:Adding Libraries Create a libs Directory: At the root of your project, create a directory named /libs. This is where you'll place your library files.Place Your Library Files: Copy or move your compiled library files (e.g., .a, .so, .o, .dylib, etc.) into the /libs directory.NOTE Ensure that all library files are compiled using emcc to ensure compatibility with your project.IMPORTANT More details to come including an example project...Example Directory Structure Using Libraries in Your Code After adding the library files to the /libs directory, you need to link against these libraries in your project. This often involves specifying the library path and names in your build scripts or configuration files. For example:For C/C++ Projects: You can just include any header files placed in the libs folder as the libs with be automatically built into your module.For Lua Projects: Depending on how your build your libraries and if you compiled them with Lua bindings you can just require the libs in your lua files. markdown = require('markdown') IMPORTANT More details to come...Lua Build Example To create and build a Lua project, follow these steps:C Build Example To create and build a C project, follow these steps:Config Presets Here are the predefined configuration presets:

---

# 76. Using WeaveDrive  Cookbook

Document Number: 76
Source: https://cookbook_ao.arweave.net/guides/snacks/weavedrive.html
Words: 253
Extraction Method: html

Using WeaveDrive WeaveDrive has been released on AO legacynet, which is great! But how to use it with your process? This post aims to provide a step by step guide on how to use WeaveDrive in your AOS process.The current availability time is called Assignments and this type puts WeaveDrive in a mode that allows you to define an Attestor wallet address when you create your AOS process. This will enable the process to load data from dataItems that have a Attestation created by this wallet.Prep Tutorial In order, to setup the tutorial for success we need to upload some data and upload an attestation. It will take a few minutes to get mined into a block on arweave.Install arx Create a wallet Create some data You should get a result like:Create Attestation It is important to copy the id of the uploaded dataItem, in the above case 9TIPJD2a4-IleOQJzRwPnDHO5DA891MWAyIdJJ1SiSk as your Message Value.👏 Awesome! That will take a few minutes to get mined on arweave, once it is mined then we will be able to read the data.html dataItem using WeaveDrive Enable WeaveDrive in a process Lets create a new AOS process with WeaveDrive enabled and the wallet we created above as an Attestor.NOTE: it is important to use the same wallet address that was used to sign the attestation data-item.NOTE: It does take a few minutes for the data to get 20 plus confirmations which is the threshold for data existing on arweave. You may want to go grab a coffee. ☕

---

# 77. Reading Dynamic State  Cookbook

Document Number: 77
Source: https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/reading-dynamic-state.html
Words: 429
Extraction Method: html

Reading Dynamic State Beyond reading static, cached state from your process, HyperBEAM allows you to perform on-the-fly computations on that state using Lua. This guide explains how to create and use "transformation functions" to return dynamic, computed data without altering the underlying state of your process.This is a powerful pattern for creating efficient data APIs for your applications, reducing client-side logic, and minimizing data transfer.This guide assumes you are already familiar with exposing static process state.How it Works: The Lua Device The magic behind this is the lua@5.3a device, which can execute a Lua script against a message. In this pattern, we use a HyperBEAM URL (hashpath) to construct a pipeline:First, we grab the latest state of an AO process.Then, we pipe that state as the base message into the lua@5.3a device.We tell the Lua device which script to load (from an Arweave transaction) and which function to execute.The function runs, processing the base state.Finally, the result of the function is returned over HTTP.Example: Calculating Circulating Supply Let's consider a practical example: a token process where we have patched the Balances table to be readable. Rather than forcing clients to download all balance data to compute the total supply, we can do it on the HyperBEAM node.1. The Transformation Function First, create a Lua script (sum.lua) with a function that takes the state (base) and calculates the sum of balances.The transformation function receives two arguments:base: The message being processed, which in our pipeline will be the cached state data from your process.req: The incoming request object, which contains parameters and other metadata.2. Publishing the Function Next, publish your Lua script to Arweave. The arx CLI tool is recommended for this.arx will return a transaction ID for your script. Let's say it's LUA_SCRIPT_TX_ID.3. Calling the Function With the process ID (YOUR_PROCESS_ID) and the script transaction ID (LUA_SCRIPT_TX_ID), you can construct a URL to call your function:HyperBEAMGET /<YOUR_PROCESS_ID>~process@1.0/now/~lua@5.3a&module={LUA_SCRIPT_TX_ID}/sum/serialize~json@1.0 This URL breaks down as follows:/{YOUR_PROCESS_ID}~process@1.0: Targets the AO process and its state./now: Gets the most current state./~lua@5.3a&module={LUA_SCRIPT_TX_ID}: This is the key part. It tells HyperBEAM to take the output of the previous step (the process state) and process it with the lua@5.3a device, loading your script from the module transaction./sum: Calls the sum function within your Lua script./serialize~json@1.0: Takes the table returned by your function and serializes it into a JSON object.4. Integrating into an Application Here's how you could fetch this dynamic data in a JavaScript application:This approach significantly improves performance by offloading computation from the client to the HyperBEAM node and reducing the amount of data sent over the network.

---

# 78. Getting started with SQLite  Cookbook

Document Number: 78
Source: https://cookbook_ao.arweave.net/guides/snacks/sqlite.html
Words: 130
Extraction Method: html

Skip to content  Getting started with SQLite SQLite is a relational database engine. In this guide, we will show how you can spawn a process with SQLite and work with data using a relational database.Setup NOTE: make sure you have aos installed, if not checkout Getting Started spawn a new process mydb with a --sqlite flag, this instructs ao to use the latest sqlite module.Install AO Package Manager installing apm, the ao package manager we can add helper modules to make it easier to work with sqlite.lua.load-blueprint apm Install dbAdmin package DbAdmin is a module that connects to a sqlite database and provides functions to work with sqlite.https://apm_betteridea.g8way.io/pkg?id=@rakis/DbAdmin luaapm.install('@rakis/dbAdmin') Create sqlite Database Create Table Create a table called Comments Insert data List data Congrats!You are using sqlite on AO 🎉

---

# 79. Community Resources  Cookbook

Document Number: 79
Source: https://cookbook_ao.arweave.net/references/community.html
Words: 177
Extraction Method: html

Skip to content  Community Resources This page provides a comprehensive list of community resources, tools, guides, and links for the AO ecosystem.Core Resources Autonomous Finance Autonomous Finance is a dedicated research and technology entity, focusing on the intricacies of financial infrastructure within the ao network.BetterIdea Build faster, smarter, and more efficiently with BetterIDEa, the ultimate native web IDE for AO development 0rbit 0rbit provides any data from the web to an ao process by utilizing the power of ao, and 0rbit nodes. The user sends a message to the 0rbit ao, 0rbit nodes fetches the data and the user process receives the data.ArweaveHub A community platform for the Arweave ecosystem featuring events, developer resources, and discovery tools.AR.IO The first permanent cloud network built on Arweave, providing infrastructure for the permaweb with no 404s, no lost dependencies, and reliable access to applications and data through gateways, domains, and deployment tools.Developer Tools AO Package Manager Contributing Not seeing an AO Community Member or resource? Create an issue or submit a pull request to add it to this page: https://github.com/permaweb/ao-cookbook

---

# 80. Editor setup  Cookbook

Document Number: 80
Source: https://cookbook_ao.arweave.net/references/editor-setup.html
Words: 218
Extraction Method: html

Editor setup Remembering all the built in ao functions and utilities can sometimes be hard. To enhance your developer experience, it is recommended to install the Lua Language Server extension into your favorite text editor and add the ao addon. It supports all built in aos modules and globals.VS Code Install the sumneko.lua extension:Search for "Lua" by sumneko in the extension marketplace Download and install the extension Open the VS Code command palette with Shift + Command + P (Mac) / Ctrl + Shift + P (Windows/Linux) and run the following command:In the Addon Manager, search for "ao", it should be the first result. Click "Enable" and enjoy autocomplete!Other editors Verify that your editor supports the language server protocol Install Lua Language Server by following the instructions at luals.github.io Install the "ao" addon to the language server BetterIDEa BetterIDEa is a custom web based IDE for developing on ao.It offers a built in Lua language server with ao definitions, so you don't need to install anything. Just open the IDE and start coding!Features include:Code completion Cell based notebook ui for rapid development Easy process management Markdown and Latex cell support Share projects with anyone through ao processes Tight integration with ao package manager Read detailed information about the various features and integrations of the IDE in the documentation.

---

# 81. Accessing Data from Arweave with ao  Cookbook

Document Number: 81
Source: https://cookbook_ao.arweave.net/references/data.html
Words: 1254
Extraction Method: html

Accessing Data from Arweave with ao There may be times in your ao development workflow that you want to access data from Arweave. With ao, your process can send an assignment instructing the network to provide that data to your Process.Defining Acceptable Transactions (Required First Step) Before you can assign any Arweave transaction to your process, you must first define which transactions your process will accept using ao.addAssignable. This function creates conditions that determine which Arweave transactions your process will accept.Warning: If you attempt to assign a transaction without first defining a matching assignable pattern, that transaction will be permanently blacklisted and can never be assigned to your process, even if you later add a matching assignable.You can remove assignables with ao.removeAssignable("<name>").The condition functions use similar pattern matching techniques as found in the Handlers documentation. For complete details on the ao.addAssignable function, including parameter descriptions and additional examples, see the ao Module Reference.Assignment Methods After defining acceptable transactions and setting up your listener (if needed), you can request Arweave data in one of two ways:Using Assign The primary method to request data from Arweave:Using Send with Assignments Alternatively, you can use the Send function with an Assignments parameter:Working with Assigned Data You can process assigned data using either Receive or Handlers:Using Receive Directly You can also match specific transactions or combine conditions:Note: When using .load, the script pauses at Receive until data arrives. When running commands separately in the shell, each command executes independently.Using Handlers For persistent processing, set up a handler:Handlers are ideal for:Processing multiple assignments over time Automated processing without manual intervention Building services that other processes can interact with For more details, see the Messaging Patterns and Handlers documentation.Complete Example Workflow Here's a complete example that demonstrates the entire process of accessing data from an Arweave transaction:This pattern creates a synchronous flow where your process:Defines acceptable transactions Requests the data Captures the data using Receive Processes the data Practical Examples Here are two practical examples showing different approaches to working with Arweave data in your ao process:Example 1: Caching Arweave Data This example demonstrates how to load and cache data from Arweave, then use it in subsequent operations:lua-- Initialize state

local Number = 0

-- Step 1: Define which transactions your process will accept

print("Step 1: Defining acceptable transactions")

ao.addAssignable("addNumber", function (msg)

    return msg.Tags["Action"] == "Number"

end)

-- Step 2: Request and cache the initial number from Arweave

-- This uses a self-executing function to fetch and cache the value only once

NumberFromArweave = NumberFromArweave or (function()

    print("Step 2: Requesting initial number from Arweave")

    Assign({

        Processes = { ao.id },

        Message = 'DivdWHaNj8mJhQQCdatt52rt4QvceBR_iyX58aZctZQ'

    })

    return tonumber(Receive({ Action = "Number"}).Data)

end)()

-- Step 3: Set up handler for future number updates

-- This handler will add new numbers to our cached Arweave number

Handlers.add("Number", function (msg)

    print("Received message with Data = " .. msg.Data)

    print("Old Number: " .. Number)

    Number = NumberFromArweave + tonumber(msg.Data)

    print("New Number: " .. Number)

end) This example shows how to:Cache Arweave data using a self-executing function Use the cached data in subsequent message handling Combine Arweave data with new incoming data Example 2: Dynamic Transaction Processing This example shows how to process arbitrary Arweave transactions and maintain state between requests:lua-- Table to store pending requests (maps transaction ID to original sender)

local PendingRequests = {}

-- Step 1: Define which transactions your process will accept

print("Step 1: Defining acceptable transactions")

ao.addAssignable("processArweaveNumber", function (msg)

    return msg.Tags["Action"] == "Number"

end)

-- Step 2: Set up handler for initiating the processing

Handlers.add(

    "ProcessArweaveNumber",

    function (msg)

        if not msg.Tags["ArweaveTx"] then

            print("Error: No ArweaveTx tag provided")

            return

        end

        local txId = msg.Tags["ArweaveTx"]

        print("Assigning Arweave transaction: " .. txId)

        -- Store the original sender associated with this transaction ID

        PendingRequests[txId] = msg.From

        -- Assign the transaction to this process

        Assign({

            Processes = { ao.id },

            Message = txId

        })

        print("Assignment requested; waiting for data...")

    end

)

-- Step 3: Set up handler for processing the assigned message

Handlers.add(

    "Number",

    function (msg)

        local txId = msg.Id  -- The ID of the assigned message

        local originalSender = PendingRequests[txId]

        if not originalSender then

            print("Error: No pending request found for transaction " .. txId)

            return

        end

        local data = msg.Data

        if not data or not tonumber(data) then

            print("Error: Invalid number data in assigned message")

            return

        end

        local number = tonumber(data)

        local result = number + 1

        print(string.format("Processing: %d + 1 = %d", number, result))

        -- Send the result back to the original sender

        Send({

            Target = originalSender,

            Data = tostring(result)

        })

        -- Clean up the pending request

        PendingRequests[txId] = nil

    end

) To use this example:This example demonstrates:Processing arbitrary Arweave transactions Maintaining state between requests using a pending requests table Sending results back to the original requester Error handling and request cleanup WARNING When using Assign to bridge Arweave data into AO, you must ensure that:The Arweave transaction you're assigning matches one of your defined assignables You have a corresponding handler or receiver set up to process that transaction type The handler's pattern matching matches the assigned transaction's tags/properties For example, if you're assigning a transaction with Action = "Number", you need:An assignable that accepts msg.Tags["Action"] == "Number" Either a Receive function or a handler that matches the same pattern Both the assignable and handler must use consistent pattern matching Important Limitations There are critical limitations to be aware of when working with assignables:Matching is Required: Transactions must match at least one of your defined assignable patterns to be accepted.Blacklisting is Permanent: If you attempt to assign a transaction before defining an appropriate assignable, it will be permanently blacklisted. Even if you later add a matching assignable, that transaction will never be accepted.One-time Assignment: Each Arweave transaction can only be assigned once to a given process. Subsequent assignments of the same transaction will be ignored.Proper Sequence for Assigning Arweave Transactions For successful assignment of Arweave transactions, follow these steps:Define assignables to specify which Arweave transactions your process will accept Wait for any transaction confirmations (by default, 20 confirmations are required) Set up handlers or listeners with Receive or Handlers.add to process the data Assign the Arweave transaction to your process (see Assignment Methods) The order of steps 3 and 4 can be interchanged based on your needs:When using Receive in a script loaded with .load, ensure Assign is placed before Receive to prevent the process from hanging, as Receive is blocking.When using handlers or running commands separately in the shell, the order doesn't matter as handlers will catch messages whenever they arrive Why Access Data from Arweave?There are several practical reasons to access Arweave data from your ao process:Efficient Handling of Large Data: For larger content, directly accessing Arweave is more efficient:Reference large media files (images, videos, documents) without storing them in your process Work with datasets too large to fit in process memory Maintain a lightweight process that can access substantial external resources External Data for Decision-Making: Your process may need data stored on Arweave to make informed decisions. For example:Reading token price data stored by an oracle Accessing verified identity information Retrieving voting records or governance data Dynamic Loading of Features: Rather than including all functionality in your initial process code:Load modules or plugins from Arweave as needed Update configuration without redeploying your entire process Implement upgradable components with new versions stored on Arweave This approach allows you to create more sophisticated applications that leverage Arweave's permanent storage while maintaining efficient process execution in the ao environment.When another process Assigns a transaction to this process, you can also use handlers to process the data asynchronously.

---

# 82. Meet Lua  Cookbook

Document Number: 82
Source: https://cookbook_ao.arweave.net/references/lua.html
Words: 293
Extraction Method: html

Meet Lua Understanding Lua Background: Lua is a lightweight, high-level, multi-paradigm programming language designed primarily for embedded systems and clients. It's known for its efficiency, simplicity, and flexibility.Key Features: Lua offers powerful data description constructs, dynamic typing, efficient memory management, and good support for object-oriented programming.Setting Up Installation: Visit Lua's official website to download and install Lua.Environment: You can use a simple text editor and command line, or an IDE like ZeroBrane Studio or Eclipse with a Lua plugin.Basic Syntax and Concepts (in aos) Hello World:lua"Hello, World!" Variables and Types: Lua is dynamically typed. Basic types include nil, boolean, number, string, function, userdata, thread, and table.Control Structures: Includes if, while, repeat...until, and for.Functions: First-class citizens in Lua, supporting closures and higher-order functions.Tables: The only data structuring mechanism in Lua, which can be used to represent arrays, sets, records, etc.Hands-On Practice Experiment with Lua's Interactive Mode: Run aos in your terminal and start experimenting with Lua commands.Write Simple Scripts: Create .lua files and run them using the Lua interpreter. Use .load file.lua feature to upload lua code on your aos process.Resources Official Documentation: Lua 5.3 Reference Manual Online Tutorials: Websites like Learn Lua are great for interactive learning.Books: "Programming in Lua" (first edition available online) is a comprehensive resource.Community: Join forums or communities like Lua Users for support and discussions.Best Practices Keep It Simple: Lua is designed to be simple and flexible. Embrace this philosophy in your code.Performance: Learn about Lua's garbage collection and efficient use of tables.Integration: Consider how Lua can be embedded into other applications, particularly C/C++ projects.Conclusion Lua is a powerful language, especially in the context of embedded systems and game development. Its simplicity and efficiency make it a great choice for specific use cases. Enjoy your journey into Lua programming!

---

# 83. ao Token and Subledger Specification  Cookbook

Document Number: 83
Source: https://cookbook_ao.arweave.net/references/token.html
Words: 1111
Extraction Method: html

ao Token and Subledger Specification Status: DRAFT-1 Targeting Network: ao.TN.1 This specification describes the necessary message handlers and functionality required for a standard ao token process. Implementations of this standard typically offer users the ability to control a transferrable asset, whose scarcity is maintained by the process.Each compliant process will likely implement a ledger of balances in order to encode ownership of the asset that the process represents. Compliant processes have a set of methods that allow for the modification of this ledger, typically with safe-guards to ensure the scarcity of ownership of the token represented by the process.Additionally, this specification describes a 'subledger' process type which, when implemented, offers the ability to split move a number of the tokens from the parent into a child process that implements the same token interface specification. If the From-Module of the subledger process is trusted by the participants, these subledgers can be used to transact in the 'source' token, without directly exchanging messages with it. This allows participants to use the tokens from a process, even if that process is congested. Optionally, if the participants trust the Module a subledger process is running, they are able to treat balances across these processes as fungible. The result of this is that an arbitrary numbers of parallel processes -- and thus, transactions -- can be processed by a single token at any one time.Token Processes A specification-compliant token process responds to a number of different forms of messages, with each form specified in an Action tag. The full set of Action messages that the token must support are as follows:Name Description Read-Only Balance get the balance of an identifier ✔️ Balances get a list of all ledger/account balances ✔️ Transfer send 1 or more units from the callers balance to one or move targets with the option to notify targets ❌ Mint if the ledger process is the root and you would like to increase token supply ❌ In the remainder of this section the tags necessary to spawn a compliant token process, along with the form of each of the Action messages and their results is described.Spawning Parameters Every compliant token process must carry the following immutable parameters upon its spawning message:Tag Description Optional?Name The title of the token, as it should be displayed to users.✔️ Ticker A suggested shortened name for the token, such that it can be referenced quickly.✔️ Logo An image that applications may desire to show next to the token, in order to make it quickly visually identifiable.✔️ Denomination The number of the token that should be treated as a single unit when quantities and balances are displayed to users.❌ Messaging Protocol Balance(Target?: string) Returns the balance of a target, if a target is not supplied then the balance of the sender of the message must be returned.Example Action message:Example response message:Balances() Returns the balance of all participants in the token.Example response message:Transfer(Target, Quantity) If the sender has a sufficient balance, send the Quantity to the Target, issuing a Credit-Notice to the recipient and a Debit-Notice to the sender. The Credit- and Debit-Notice should forward any and all tags from the original Transfer message with the X- prefix. If the sender has an insufficient balance, fail and notify the sender.If a successful transfer occurs a notification message should be sent if Cast is not set.Recipients will infer from the From-Process tag of the message which tokens they have received.Get-Info() Mint() [optional] Implementing a Mint action gives the process a way of allowing valid participants to create new tokens.Subledger Processes In order to function appropriately, subledgers must implement the full messaging protocol of token contracts (excluding the Mint action). Subledgers must also implement additional features and spawn parameters for their processes. These modifications are described in the following section.Spawning Parameters Every compliant subledger process must carry the following immutable parameters upon its spawning message:Tag Description Optional?Source-Token The ID of the top-most process that this subledger represents.❌ Parent-Token The ID of the parent process that this subledger is attached to.❌ Credit-Notice Handler Upon receipt of a Credit-Notice message, a compliant subledger process must check if the process in question is the Parent-Token. If it is, the subledger must increase the balance of the Sender by the specified quantity.Transfer(Target, Quantity) In addition to the normal tags that are passed in the Credit-Notice message to the recipient of tokens, a compliant subledger process must also provide both of the Source-Token and Parent-Token values. This allows the recipient of the Transfer message -- if they trust the Module of the subledger process -- to credit a receipt that is analogous (fungible with) deposits from the Source-Token.The modified Credit-Notice should be structured as follows:Withdraw(Target?, Quantity) All subledgers must allow balance holders to withdraw their tokens to the parent ledger. Upon receipt of an Action: Withdraw message, the subledger must send an Action message to its Parent-Ledger, transferring the requested tokens to the caller's address, while debiting their account locally. This transfer will result in a Credit-Notice from the Parent-Ledger for the caller.Token Example NOTE: When implementing a token it is important to remember that all Tags on a message MUST be "string"s. Using the tostring function you can convert simple types to strings.luaif not balances then

  balances = { [ao.id] = 100000000000000 }

end

if name ~= "Fun Coin" then

  name = "Fun Coin"

end

if ticker ~= "Fun" then

  ticker = "fun"

end

if denomination ~= 6 then

  denomination = 6

end

-- handlers that handler incoming msg

Handlers.add(

  "Transfer",

  Handlers.utils.hasMatchingTag("Action", "Transfer"),

  function (msg)

    assert(type(msg.Tags.Recipient) == 'string', 'Recipient is required!')

    assert(type(msg.Tags.Quantity) == 'string', 'Quantity is required!')

    if not balances[msg.From] then

      balances[msg.From] = 0

    end

    if not balances[msg.Tags.Recipient] then

      balances[msg.Tags.Recipient] = 0

    end

    local qty = tonumber(msg.Tags.Quantity)

    assert(type(qty) == 'number', 'qty must be number')

    -- handlers.utils.reply("Transferring qty")(msg)

    if balances[msg.From] >= qty then

      balances[msg.From] = balances[msg.From] - qty

      balances[msg.Tags.Recipient] = balances[msg.Tags.Recipient] + qty

      ao.send({

        Target = msg.From,

        Tags = {

          ["Action"] = "Debit-Notice",

          ["Quantity"] = tostring(qty)

        }

      })

      ao.send({

        Target = msg.Tags.Recipient,

        Tags = {

          ["Action"] = "Credit-Notice",

          ["Quantity"] = tostring(qty)

        }

      })

      -- if msg.Tags.Cast and msg.Tags.Cast == "true" then

      --   return

      -- end

    end

  end

)

Handlers.add(

  "Balance",

  Handlers.utils.hasMatchingTag("Action", "Balance"),

  function (msg)

    assert(type(msg.Tags.Target) == "string", "Target Tag is required!")

    local bal = "0"

    if balances[msg.Tags.Target] then

      bal = tostring(balances[msg.Tags.Target])

    end

    ao.send({

      Target = msg.From,

      Tags = {

        ["Balance"] = bal,

        ["Ticker"] = ticker or ""

      }

    })

  end

)

local json = require("json")

Handlers.add(

  "Balances",

  Handlers.utils.hasMatchingTag("Action", "Balances"),

  function (msg)

    ao.send({

      Target = msg.From,

      Data = json.encode(balances)

    })

  end

)

Handlers.add(

  "Info",

  Handlers.utils.hasMatchingTag("Action", "Info"),

  function (msg)

    ao.send({

      Target = msg.From,

      Tags = {

        ["Name"] = name,

        ["Ticker"] = ticker,

        ["Denomination"] = tostring(denomination)

      }

    })

  end

)

---

# 84. Messaging Patterns in ao  Cookbook

Document Number: 84
Source: https://cookbook_ao.arweave.net/references/messaging.html
Words: 660
Extraction Method: html

Messaging Patterns in ao This reference guide explains the messaging patterns available in ao and when to use each one.Quick Reference: Choosing the Right Pattern If you need to...Process Flow Key function(s) Send a message without waiting for a response A → B ao.send Send a message and wait for a response A → B → A ao.send().receive() Process messages and respond to the sender B → A Handlers.add + msg.reply Create a chain of processing services A → B → C → A msg.forward + ao.send().receive() Wait for any matching message regardless of sender Any → A Receive (capital R) Create a standard automated response B → A Handlers.utils.reply Sending Messages ao.send: Asynchronous Message Sending Non-blocking direct A → B messaging that returns immediately after sending.Use for fire-and-forget notifications or starting async conversations Returns a promise-like object that can be chained with .receive() if needed Good for parallel processing since it doesn't block execution Basic Send Example:msg.reply: Asynchronous Response Sending Non-blocking B → A response with automatic reference tracking. Used within handlers to respond to incoming messages.Automatically links response to original message via X-Reference Enables asynchronous request-response patterns Automatically sets Target to the original sender or Reply-To address if specified Handler Reply Example:msg.forward: Message Forwarding Non-blocking multi-process routing for A → B → C → A patterns. Creates a sanitized copy of the original message.Takes a target and a partial message to overwrite forwarded message fields Preserves Reply-To and X-Reference properties for complete message tracking Sets X-Origin to original sender, enabling final service to reply directly to originator Multi-Process Pipeline Example:lua-- In client process

local middlewareProcessId = "process-123"

local finalProcessId = "process-456"

-- Send to middleware and wait for response from final service

local response = ao.send({

  Target = middlewareProcessId,

  Action = "Transform",

  Data = "raw-data"

}).receive(finalProcessId)  -- Explicitly wait for response from final service

-- In middleware service

Handlers.add("transform-middleware",

  { Action = "Transform" },

  function(msg)

    local finalProcessId = "process-456"

    msg.forward(finalProcessId, {

      Data = msg.Data .. " (pre-processed)",

      Action = "Transform-Processed"

    })

  end

)

-- In final service

Handlers.add("final-processor",

  { Action = "Transform-Processed" },

  function(msg)

    -- No need to know the client ID - it's stored in X-Origin

    msg.forward(msg['X-Origin'], {

      Data = msg.Data .. " (final processing complete)",

      Action = "Transform-Complete"

    })

  end

) Handlers.utils.reply: Simple Reply Handler Creation Creates a handler function that automatically replies with a fixed response. A wrapper around msg.reply for common use cases.Simple String Response Example:Message Table Response Example:Receiving Messages Receive (Capital R): Blocking Pattern Matcher Blocks execution until any matching message arrives from any sender. Under the hood, this is implemented using Handlers.once, making it a one-time pattern matcher that automatically removes itself after execution.Waits for any message matching the pattern, regardless of origin Use for synchronous message processing flows or event listening Automatically removes the handler after first match (using Handlers.once internally) Message Pattern Matching Example:ao.send().receive (Lowercase r): Blocking Reference Matcher Blocks execution until a specific reply arrives, enabling A → B → A and A → B → C → A request-response cycles.Only matches messages linked by X-Reference Can specify a target process ID to indicate which process will reply Implicitly waits for the proper response based on message reference chains For A → B → A flows, process B uses msg.reply For A → B → C → A flows, processes B and C use msg.forward Basic Request-Response Example:Message Properties The following properties track message chains and ensure proper routing:Reference: Unique identifier automatically assigned to each message.Reply-To: Specifies the destination for responses.X-: Any property starting with X- denotes a 'forwarded' tag and is automatically managed by the system. X-Reference: Maintains the conversation chain across replies and forwards.X-Origin: Tracks the conversation originator.The system automatically manages these properties when using msg.reply and msg.forward. Check out the source code to see exactly how these properties are managed.Blocking vs. Non-Blocking Functions either pause your code or let it continue running:Non-blocking (ao.send, msg.reply, msg.forward): Send and continue execution Blocking (Receive, .receive()): Pause until response arrives

---

# 85. Meet Web Assembly  Cookbook

Document Number: 85
Source: https://cookbook_ao.arweave.net/references/wasm.html
Words: 189
Extraction Method: html

Skip to content  Meet Web Assembly WebAssembly (often abbreviated as Wasm) is a modern binary instruction format providing a portable compilation target for high-level languages like C, C++, and Rust. It enables deployment on the web for client and server applications, offering a high level of performance and efficiency. WebAssembly is designed to maintain the security and sandboxing features of web browsers, making it a suitable choice for web-based applications. It's a key technology for web developers, allowing them to write code in multiple languages and compile it into bytecode that runs in the browser at near-native speed.The significance of WebAssembly lies in its ability to bridge the gap between web and native applications. It allows complex applications and games, previously limited to desktop environments, to run in the browser with comparable performance. This opens up new possibilities for web development, including the creation of high-performance web apps, games, and even the porting of existing desktop applications to the web. WebAssembly operates alongside JavaScript, complementing it by enabling performance-critical components to be written in languages better suited for such tasks, thereby enhancing the capabilities and performance of web applications.

---

# 86. Crafting a Token  Cookbook

Document Number: 86
Source: https://cookbook_ao.arweave.net/tutorials/begin/token.html
Words: 580
Extraction Method: html

Crafting a Token INFO Diving deeper into the ao, you're now ready to create your own token, a symbol of value and exchange within this decentralized medium. If you've found yourself wanting to learn how to create a token, but haven't visited the Messaging and Build a Chatroom lessons, be sure to do so as this page is part of a multi-part interactive tutorial.When creating tokens, we'll continue to use the Lua Language within ao to mint a token, guided by the principles outlined in the Token Specification.Video Tutorial  Continuing Down the Rabbit Hole In our last tutorial, Build a Chatroom, we learned how to create a chatroom within ao, invited both Morpheus and Trinity to the chatroom we created, and then Trinity has now asked for us to create a token for her as a way of proving ourselves worthy of continuing down the rabbit hole.Let us begin.The Two Paths To Building a Token There are two paths to take when building a token:The Blueprint: This is a predesigned template that helps you quickly build a token in ao. It is a great way to get started and can be customized to fit your needs.Check here to learn more about the Token Blueprint.The Manual Method: This is a step-by-step guide to building a token in ao from scratch. This path is for those who want to understand the inner workings of a token and how to build one from the ground up.Check here to review the full Build a Token guide.The Blueprint Method For this tutorial, we'll be using the Token Blueprint to create a token for Trinity. This is a predesigned template that helps you quickly build a token in ao.How To Use The Token Blueprint Make sure we're in the same directory as before during the previous steps in the tutorial.Open the Terminal.Start your aos process.Type in .load-blueprint token This will load the required handlers for the tutorials token within ao. It's important to note that the token blueprint isn't specific to this tutorial and can be used as a foundation for any token you wish to create.Verify the Blueprint is Loaded Type in Handlers.list to see the newly loaded handlers.You should see a new list of handlers that have been loaded into your aos process. If you've been following along the with the previous steps in the tutorial, you should also see the handlers for your chatroom, as well.Example: Testing the Token Now that the token blueprint is loaded, we can test the token by sending a message to ourselves using the Action = "Info" tag.This will print the token information to the console. It should show your process ID with the total balance of tokens available.Sending Tokens to Trinity Now that we've tested the token and it's working as expected, we can send some tokens to Trinity. We'll send 1000 tokens to Trinity using the Action = "Transfer" tag.When Trinity receives the tokens, she'll respond to the transfer with a message to confirm that she's received the tokens.Her response will look something like this:Trinity: "Token received. Interesting. I wasn't sure you'd make it this far. I'm impressed, but we are not done yet. I want you to use this token to tokengate the chatroom. Do that, and then I will believe you could be the one." You've completed the process of creating a token and sending it to Trinity. You're now ready to move on to the next step in the tutorial. Tokengating the Chatroom.

---

# 87. Preparations  Cookbook

Document Number: 87
Source: https://cookbook_ao.arweave.net/tutorials/begin/preparations.html
Words: 505
Extraction Method: html

Preparations INFO The Awakening Begins:You've always known there's more to this world, just outside of your reach. You've been searching for it, not even knowing what it was you were looking for. It... is ao.We begin our journey by installing the aos client and starting a new process. This will allow us to interact with the ao computer and complete the rest of the tutorial.Video Tutorial  System requirements The local client of aos is very simple to install. Just make sure you have:NodeJS version 20+. (If you haven't yet installed it, check out this page to find instructions for your OS).A code editor of your choice.INFO Though it's not required, we do recommend installing the ao addon into your text editor of choice to optimize your experience with aos.Installing aos Once you have NodeJS on your machine, all you need to do is install aos and run it:After installation, we can simply run the command itself to start a new aos process!shaos Welcome to the rabbit hole The utility you just started is a local client, which is ready to relay messages for you to your new process inside the ao computer.After it connects, you should see the following:sh          _____                   _______                   _____

         /\    \                 /::\    \                 /\    \

        /::\    \               /::::\    \               /::\    \

       /::::\    \             /::::::\    \             /::::\    \

      /::::::\    \           /::::::::\    \           /::::::\    \

     /:::/\:::\    \         /:::/~~\:::\    \         /:::/\:::\    \

    /:::/__\:::\    \       /:::/    \:::\    \       /:::/__\:::\    \

   /::::\   \:::\    \     /:::/    / \:::\    \      \:::\   \:::\    \

  /::::::\   \:::\    \   /:::/____/   \:::\____\   ___\:::\   \:::\    \

 /:::/\:::\   \:::\    \ |:::|    |     |:::|    | /\   \:::\   \:::\    \

/:::/  \:::\   \:::\____\|:::|____|     |:::|    |/::\   \:::\   \:::\____\

\::/    \:::\  /:::/    / \:::\    \   /:::/    / \:::\   \:::\   \::/    /

 \/____/ \:::\/:::/    /   \:::\    \ /:::/    /   \:::\   \:::\   \/____/

          \::::::/    /     \:::\    /:::/    /     \:::\   \:::\    \

           \::::/    /       \:::\__/:::/    /       \:::\   \:::\____\

           /:::/    /         \::::::::/    /         \:::\  /:::/    /

          /:::/    /           \::::::/    /           \:::\/:::/    /

         /:::/    /             \::::/    /             \::::::/    /

        /:::/    /               \::/____/               \::::/    /

        \::/    /                 ~~                      \::/    /

         \/____/                                           \/____/

Welcome to AOS: Your operating system for AO, the decentralized open

access supercomputer.

Type ".load-blueprint chat" to join the community chat and ask questions!

AOS Client Version: 1.12.1. 2024

Type "Ctrl-C" twice to exit

Your AOS process:  QFt5SR6UwJSCnmgnROq62-W8KGY9z96k1oExgn4uAzk

default@aos-0.2.2[Inbox:1]> Let's walk through the initial printout after running aos: After running aos in your terminal, you should see:An ASCII art image of AOS.A Welcome Message The version of aos you are running.An instructional exit message.Your process ID.INFO If your OS version is different than the latest version, a message asking if you'd like to update the version will appear. If so, simply exit the process by pressing "Ctrl+C" twice, run npm i -g https://get_ao.g8way.io to update, and then run aos again.Welcome to your new home in the ao computer! The prompt you are now looking at is your own personal server in this decentralized machine.Now, let's journey further down the rabbit hole by exploring one of the two core concept type of ao: messaging.

---

# 88. Automated Responses  Cookbook

Document Number: 88
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/attacking.html
Words: 438
Extraction Method: html

Automated Responses Following our last guide, our creation has progressed from a simple bot to a sophisticated autonomous agent. Now, let's further enhance its capabilities by adding a counterattack feature, allowing it to instantly retaliate against an opponent's attack, potentially catching them off-guard before they can retreat to safety.Writing the code Add the following handler to your bot.lua file and you're set:Whenever your player is under attack you receive a message with the Action Hit. This setup ensures your agent can make a swift counter attack, given it has sufficient energy.You can refer to the latest code for bot.lua in the dropdown below:Updated bot.lua file luaLatestGameState = LatestGameState or nil

function inRange(x1, y1, x2, y2, range)

  return math.abs(x1 - x2) <= range and math.abs(y1 - y2) <= range

end

function decideNextAction()

  local player = LatestGameState.Players[ao.id]

  local targetInRange = false

  for target, state in pairs(LatestGameState.Players) do

    if target ~= ao.id and inRange(player.x, player.y, state.x, state.y, 1) then

        targetInRange = true

        break

    end

  end

  if player.energy > 5 and targetInRange then

    print("Player in range. Attacking.")

    ao.send({Target = Game, Action = "PlayerAttack", Player = ao.id, AttackEnergy = tostring(player.energy)})

  else

    print("No player in range or insufficient energy. Moving randomly.")

    local directionMap = {"Up", "Down", "Left", "Right", "UpRight", "UpLeft", "DownRight", "DownLeft"}

    local randomIndex = math.random(#directionMap)

    ao.send({Target = Game, Action = "PlayerMove", Player = ao.id, Direction = directionMap[randomIndex]})

  end

end

Handlers.add(

  "HandleAnnouncements",

  { Action =  "Announcement" },

  function (msg)

    ao.send({Target = Game, Action = "GetGameState"})

    print(msg.Event .. ": " .. msg.Data)

  end

)

Handlers.add(

  "UpdateGameState",

  { Action =  "GameState" },

  function (msg)

    local json = require("json")

    LatestGameState = json.decode(msg.Data)

    ao.send({Target = ao.id, Action = "UpdatedGameState"})

  end

)

Handlers.add(

  "decideNextAction",

  { Action =  "UpdatedGameState" },

  function ()

    if LatestGameState.GameMode ~= "Playing" then

      return

    end

    print("Deciding next action.")

    decideNextAction()

  end

)

Handlers.add(

  "ReturnAttack",

  { Action =  "Hit" },

  function (msg)

      local playerEnergy = LatestGameState.Players[ao.id].energy

      if playerEnergy == undefined then

        print("Unable to read energy.")

        ao.send({Target = Game, Action = "Attack-Failed", Reason = "Unable to read energy."})

      elseif playerEnergy == 0 then

        print("Player has insufficient energy.")

        ao.send({Target = Game, Action = "Attack-Failed", Reason = "Player has no energy."})

      else

        print("Returning attack.")

        ao.send({Target = Game, Action = "PlayerAttack", Player = ao.id, AttackEnergy = tostring(playerEnergy)})

      end

      InAction = false

      ao.send({Target = ao.id, Action = "Tick"})

  end

) To activate and test the counter attack feature, load the bot file in your aos player terminal:lua.load bot.lua Watch your terminal for the autonomous agent's reactions, now with the added ability to retaliate instantly. This feature showcases the agent's evolving strategic depth and autonomy. In the upcoming section, we'll consolidate all the knowledge we've gathered so far and add some features for optimization.

---

# 89. Mechanics of the Arena  Cookbook

Document Number: 89
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/arena-mechanics.html
Words: 1916
Extraction Method: html

Mechanics of the Arena This guide provides a comprehensive overview of the fundamental mechanics essential for designing and managing arena-style games in aos. In arena games, participants engage in rounds, strategically vying to eliminate each other until a sole victor emerges.The framework presented here lays the groundwork for crafting a wide range of games, all sharing the same core functionalities. Explore the intricacies of game development and unleash your creativity within this versatile arena.Core Functionalities Now, let's dive into the core functionalities that power arena-style games:Game Progression Modes:Arena games are structured into rounds that operate in a loop with the following progression modes: "Not-Started" → "Waiting" → "Playing" → [Someone wins or timeout] → "Waiting"...NOTE The loop timesout if there are not enough players to start a game after the waiting state.Rounds offer a defined timeframe for players to engage, intensifying the excitement of gameplay.Token Stakes:Players must deposit a specified quantity of tokens (defined by PaymentQty) to participate in the game. These tokens add a tangible stake element to the game.Bonus Rewards:Beyond the thrill of victory, players are enticed by the prospect of extra rewards. The builder has the flexibility to offer bonus tokens, defined by BonusQty, to be distributed per round. Any bets placed by players are also added to these bonuses. These bonuses serve as an additional incentive, enhancing the competitive spirit of the gameplay.Player Management:Players waiting to join the next game are tracked in the Waiting table.Active players and their game states are stored in the Players table.Eliminated players are promptly removed from the Players table and placed in the Waiting table for the next game.Round Winner Reward:When a player eliminates another, they earn not only bragging rights but also the eliminated player's deposit tokens as a reward. Additionally, winners of each round share a portion of the bonus tokens, as well as their original stake, further motivating players to strive for victory.Listener Mode:For those who prefer to watch the action unfold, the "Listen" mode offers an opportunity to stay informed without active participation. Processes can register as listeners, granting them access to all announcements from the game. While they do not engage as players, listeners can continue to observe the game's progress unless they explicitly request removal.Game State Management:To maintain the flow and fairness of arena games, an automated system oversees game state transitions. These transitions encompass waiting, playing, and ending phases. Time durations for each state, such as WaitTime and GameTime, ensure that rounds adhere to defined timeframes, preventing games from lasting indefinitely.You can refer to the code for the arena in the dropdown below:Arena Game Blueprint lua-- ARENA GAME BLUEPRINT.

-- This blueprint provides the framework to operate an 'arena' style game

-- inside an ao process. Games are played in rounds, where players aim to

-- eliminate one another until only one remains, or until the game time

-- has elapsed. The game process will play rounds indefinitely as players join

-- and leave.

-- When a player eliminates another, they receive the eliminated player's deposit token

-- as a reward. Additionally, the builder can provide a bonus of these tokens

-- to be distributed per round as an additional incentive. If the intended

-- player type in the game is a bot, providing an additional 'bonus'

-- creates an opportunity for coders to 'mine' the process's

-- tokens by competing to produce the best agent.

-- The builder can also provide other handlers that allow players to perform

-- actions in the game, calling 'eliminatePlayer()' at the appropriate moment

-- in their game logic to control the framework.

-- Processes can also register in a 'Listen' mode, where they will receive

-- all announcements from the game, but are not considered for entry into the

-- rounds themselves. They are also not unregistered unless they explicitly ask

-- to be.

-- GLOBAL VARIABLES.

-- Game progression modes in a loop:

-- [Not-Started] -> Waiting -> Playing -> [Someone wins or timeout] -> Waiting...

-- The loop is broken if there are not enough players to start a game after the waiting state.

GameMode = GameMode or "Not-Started"

StateChangeTime = StateChangeTime or undefined

-- State durations (in milliseconds)

WaitTime = WaitTime or 2 * 60 * 1000 -- 2 minutes

GameTime = GameTime or 20 * 60 * 1000 -- 20 minutes

Now = Now or undefined -- Current time, updated on every message.

-- Token information for player stakes.

UNIT = 1000

PaymentToken = PaymentToken or "ADDR"  -- Token address

PaymentQty = PaymentQty or tostring(math.floor(UNIT))    -- Quantity of tokens for registration

BonusQty = BonusQty or tostring(math.floor(UNIT))        -- Bonus token quantity for winners

-- Players waiting to join the next game and their payment status.

Waiting = Waiting or {}

-- Active players and their game states.

Players = Players or {}

-- Number of winners in the current game.

Winners = 0

-- Processes subscribed to game announcements.

Listeners = Listeners or {}

-- Minimum number of players required to start a game.

MinimumPlayers = MinimumPlayers or 2

-- Default player state initialization.

PlayerInitState = PlayerInitState or {}

-- Sends a state change announcement to all registered listeners.

-- @param event: The event type or name.

-- @param description: Description of the event.

function announce(event, description)

    for ix, address in pairs(Listeners) do

        ao.send({

            Target = address,

            Action = "Announcement",

            Event = event,

            Data = description

        })

    end

    return print(Colors.gray .. "Announcement: " .. Colors.red .. event .. " " .. Colors.blue .. description .. Colors.reset)

end

-- Sends a reward to a player.

-- @param recipient: The player receiving the reward.

-- @param qty: The quantity of the reward.

-- @param reason: The reason for the reward.

function sendReward(recipient, qty, reason)

    if type(qty) ~= number then

      qty = tonumber(qty)

    end

    ao.send({

        Target = PaymentToken,

        Action = "Transfer",

        Quantity = tostring(qty),

        Recipient = recipient,

        Reason = reason

    })

    return print(Colors.gray .. "Sent Reward: " ..

      Colors.blue .. tostring(qty) ..

      Colors.gray .. ' tokens to ' ..

      Colors.green .. recipient .. " " ..

      Colors.blue .. reason .. Colors.reset

    )

end

-- Starts the waiting period for players to become ready to play.

function startWaitingPeriod()

    GameMode = "Waiting"

    StateChangeTime = Now + WaitTime

    announce("Started-Waiting-Period", "The game is about to begin! Send your token to take part.")

    print('Starting Waiting Period')

end

-- Starts the game if there are enough players.

function startGamePeriod()

    local paidPlayers = 0

    for player, hasPaid in pairs(Waiting) do

        if hasPaid then

            paidPlayers = paidPlayers + 1

        end

    end

    if paidPlayers < MinimumPlayers then

        announce("Not-Enough-Players", "Not enough players registered! Restarting...")

        for player, hasPaid in pairs(Waiting) do

            if hasPaid then

                Waiting[player] = false

                sendReward(player, PaymentQty, "Refund")

            end

        end

        startWaitingPeriod()

        return

    end

    LastTick = undefined

    GameMode = "Playing"

    StateChangeTime = Now + GameTime

    for player, hasPaid in pairs(Waiting) do

        if hasPaid then

            Players[player] = playerInitState()

        else

            ao.send({

                Target = player,

                Action = "Ejected",

                Reason = "Did-Not-Pay"

            })

            removeListener(player) -- Removing player from listener if they didn't pay

        end

    end

    announce("Started-Game", "The game has started. Good luck!")

    print("Game Started....")

end

-- Handles the elimination of a player from the game.

-- @param eliminated: The player to be eliminated.

-- @param eliminator: The player causing the elimination.

function eliminatePlayer(eliminated, eliminator)

    sendReward(eliminator, PaymentQty, "Eliminated-Player")

    Waiting[eliminated] = false

    Players[eliminated] = nil

    ao.send({

        Target = eliminated,

        Action = "Eliminated",

        Eliminator = eliminator

    })

    announce("Player-Eliminated", eliminated .. " was eliminated by " .. eliminator .. "!")

    local playerCount = 0

    for player, _ in pairs(Players) do

        playerCount = playerCount + 1

    end

    print("Eliminating player: " .. eliminated .. " by: " .. eliminator) -- Useful for tracking eliminations

    if playerCount < MinimumPlayers then

        endGame()

    end

end

-- Ends the current game and starts a new one.

function endGame()

    print("Game Over")

    Winners = 0

    Winnings = tonumber(BonusQty) / Winners -- Calculating winnings per player

    for player, _ in pairs(Players) do

        Winners = Winners + 1

    end

    Winnings = tonumber(BonusQty) / Winners

    for player, _ in pairs(Players) do

        -- addLog("EndGame", "Sending reward of:".. Winnings + PaymentQty .. "to player: " .. player) -- Useful for tracking rewards

        sendReward(player, Winnings + tonumber(PaymentQty), "Win")

        Waiting[player] = false

    end

    Players = {}

    announce("Game-Ended", "Congratulations! The game has ended. Remaining players at conclusion: " .. Winners .. ".")

    startWaitingPeriod()

end

-- Removes a listener from the listeners' list.

-- @param listener: The listener to be removed.

function removeListener(listener)

    local idx = 0

    for i, v in ipairs(Listeners) do

        if v == listener then

            idx = i

            break

        end

    end

    if idx > 0 then

        table.remove(Listeners, idx)

    end

end

-- HANDLERS: Game state management

-- Handler for cron messages, manages game state transitions.

Handlers.add(

    "Game-State-Timers",

    function(Msg)

        return "continue"

    end,

    function(Msg)

        Now = Msg.Timestamp

        if GameMode == "Not-Started" then

            startWaitingPeriod()

        elseif GameMode == "Waiting" then

            if Now > StateChangeTime then

                startGamePeriod()

            end

        elseif GameMode == "Playing" then

            if onTick and type(onTick) == "function" then

              onTick()

            end

            if Now > StateChangeTime then

                endGame()

            end

        end

    end

)

-- Handler for player deposits to participate in the next game.

Handlers.add(

    "Transfer",

    function(Msg)

        return

            Msg.Action == "Credit-Notice" and

            Msg.From == PaymentToken and

            tonumber(Msg.Quantity) >= tonumber(PaymentQty) and "continue"

    end,

    function(Msg)

        Waiting[Msg.Sender] = true

        ao.send({

            Target = Msg.Sender,

            Action = "Payment-Received"

        })

        announce("Player-Ready", Msg.Sender .. " is ready to play!")

    end

)

-- Registers new players for the next game and subscribes them for event info.

Handlers.add(

    "Register",

   { Action = "Register" },

    function(Msg)

        if Msg.Mode ~= "Listen" and Waiting[Msg.From] == undefined then

            Waiting[Msg.From] = false

        end

        removeListener(Msg.From)

        table.insert(Listeners, Msg.From)

        ao.send({

            Target = Msg.From,

            Action = "Registered"

        })

        announce("New Player Registered", Msg.From .. " has joined in waiting.")

    end

)

-- Unregisters players and stops sending them event info.

Handlers.add(

    "Unregister",

   { Action = "Unregister" },

    function(Msg)

        removeListener(Msg.From)

        ao.send({

            Target = Msg.From,

            Action = "Unregistered"

        })

    end

)

-- Adds bet amount to BonusQty

Handlers.add(

    "AddBet",

    { Reason = "AddBet" },

    function(Msg)

        BonusQty = tonumber(BonusQty) + tonumber(Msg.Tags.Quantity)

        announce("Bet-Added", Msg.From .. "has placed a bet. " .. "BonusQty amount increased by " .. Msg.Tags.Quantity .. "!")

    end

)

-- Retrieves the current game state.

Handlers.add(

    "GetGameState",

   { Action = "GetGameState" },

    function (Msg)

        local json = require("json")

        local TimeRemaining = StateChangeTime - Now

        local GameState = json.encode({

            GameMode = GameMode,

            TimeRemaining = TimeRemaining,

            Players = Players,

            })

        ao.send({

            Target = Msg.From,

            Action = "GameState",

            Data = GameState})

    end

)

-- Alerts users regarding the time remaining in each game state.

Handlers.add(

    "AnnounceTick",

   { Action = "Tick" },

    function (Msg)

        local TimeRemaining = StateChangeTime - Now

        if GameMode == "Waiting" then

            announce("Tick", "The game will start in " .. (TimeRemaining/1000) .. " seconds.")

        elseif GameMode == "Playing" then

            announce("Tick", "The game will end in " .. (TimeRemaining/1000) .. " seconds.")

        end

    end

)

-- Sends tokens to players with no balance upon request

Handlers.add(

    "RequestTokens",

   { Action = "RequestTokens" },

    function (Msg)

        print("Transferring Tokens: " .. tostring(math.floor(10000 * UNIT)))

        ao.send({

            Target = ao.id,

            Action = "Transfer",

            Quantity = tostring(math.floor(10000 * UNIT)),

            Recipient = Msg.From,

        })

    end

) Arena Game Blueprint For those interested in using this arena framework, we've made this code easily accessible through a blueprint. Simply run the following code in your terminal:lua.load-blueprint arena Summary Understanding the mechanics of the arena can not only help you improve your autonomous agent created in the previous section but also empowers you to harness core functionalities for crafting your unique games.In the upcoming section, "Building a Game," we will dive deep into the art of utilizing these mechanics to construct captivating and one-of-a-kind games within this framework. Get ready to embark on a journey into the dynamic realm of game development! 🎮

---

# 90. Lets Play A Game  Cookbook

Document Number: 90
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/ao-effect.html
Words: 646
Extraction Method: html

Let's Play A Game!You've been powering through tutorials like a champ! Now, let's take a refreshing break and dive into something exciting. How about a game that adds a dash of fun to your learning journey? What's the game?ao-effect is a game where you can compete with friends or other players globally, in real-time, right from your terminal. We've set up a global game process for this adventure.The rules are simple. Each player starts on a 40x40 grid with health at 100 and energy at 0. Your energy replenishes over time to a maximum of 100. Navigate the grid, find other players, and use your energy to attack when they're within range. The battle continues until only one player remains or the allotted time expires.Checkout the guides on the Mechanics of the Arena and Expanding the Arena for a deeper understanding of the game.Heads Up: Don't sweat it if some command syntax seem unfamiliar. Focus on understanding the purpose of each command at a high level and, most importantly, enjoy the game!Preparing for an Adventure in ao-effect To join this global escapade, you'll need to set things up. Don't worry, it's as easy as 1-2-3!Install aos Fire up your terminal and run:Launch aos Next, create your instance of aos:bashaos Set Up the Game ID Let's keep our game server ID handy for quick access:Print Game Announcements Directly To Terminal (Optional) Here's how you can write a handler for printing announcement details:This is temporary as we will be loading this via a lua script in the next section.And voilà! You're all set to join the game.Ready to jump in? Just a few simple steps to get you going:All communication between processes in ao occurs through messages. To register, send this message to the game server:This places you in the Waiting Lobby. A small fee is needed to confirm your spot.Confirm your spot In order to confirm your spot you need some tokens. You can acquire them by sending the following message to the game:NOTE The .receive().Data will wait for a response by adding a temporary Handler that only runs once and will print the response Data. If you would like to instead just wait for the response to hit your Inbox you can call Send() without .receive() and run Inbox[#Inbox].Data to see the response Data.Handler added by .receive():Once you receive the tokens, confirm your spot by paying the game's entry fee like this:Wait for a few seconds, and you'll see live updates in your terminal about player payments and statuses.Let the Games Begin!Game Mechanics Game Start: The game begins after a 2-minute WaitTime if at least 2 players have paid. Non-paying players are removed. If not enough players pay, those who did are refunded.Players spawn at a random grid point once the game begins.It's Your Move!Making a Move: The first thing you can do is move around, no energy required! You can shift one square in any direction – up, down, left, right, or diagonally. Along with the direction you must also pass in your player id to help the game identify your move. Here's how:The available moves across the grid are as follows:Keep in Mind: Directions are case sensitive!If you move off the grid, you'll pop up on the opposite side.Time to Strike!Launching an Attack: As the game progresses, you'll accumulate energy. Use it to attack other players within a 3x3 grid range. Your attack won't hurt you, but it will affect others in range.Health starts at 100 and decreases with hits from other players. Reach 0, and it's game over for you.Wrapping Up The game ends when there's one player left or time is up. Winners receive rewards, then it's back to the lobby for another round.Enjoyed the game? What if there was a way to make your experience even better or boost your odds of winning. Checkout the next guide to find out 🤔

---

# 91. Tokengating the Chatroom  Cookbook

Document Number: 91
Source: https://cookbook_ao.arweave.net/tutorials/begin/tokengating.html
Words: 493
Extraction Method: html

Tokengating the Chatroom INFO Now that we've created a token and sent it to Trinity, we can use the token to tokengate our chatroom. This will allow only those who have the token to enter the chatroom.Video Tutorial  How to Tokengate the Chatroom Let's create a handler that will allow us to tokengate the chatroom. This handler will respond to the tag Action = "Broadcast" meaning it will replace the original Broadcast handler we built for our chatroom.Step 1: Start the same aos process.Be sure you're using the same aos process that you've used throughout the tutorial.Step 2: Open the chatroom.lua file.This is the same file we used to create the chatroom during the chatroom tutorial.Step 3: Edit your Broadcast handler.Replace the original Broadcast handler with the following code:This handler will now check the balance of the sender's token before broadcasting the message to the chatroom. If the sender doesn't have a token, the message will not be broadcasted.Save the file.Step 4: Reload the chatroom.lua file.To replace the original broadcast handler with the new one, you'll need to reload the chatroom.lua file.lua.load chatroom.lua Step 5: Test the Tokengate Now that the chatroom is tokengated, let's test it by sending a message to the chatroom.From the original aos process First, we'll test it from the original aos process.Expected Results:Testing from another Process ID.From a new aos process Now, let's test it from a new aos process that doesn't have a token. The following command creates a new AO process with the name "chatroom-no-token".Next we need to register to the chatroom we built on our original process, from our new process. Hint: type ao.id into your console to get the Process ID of the process you are currently connected to.Expected Results:Now, let's try to send a message to the chatroom.Expected Results:As you can see, the message was not broadcasted because the new process doesn't have a token.Tell Trinity "It is done" From the original aos process, send a broadcast message to the chatroom saying, "It is done".WARNING It's important to be aware of exact match data and case sensitivity. If you're not receiving a response from either Morpheus or Trinity, be sure to check the the content of your Data and Tags.Trinity will then respond to the chatroom being tokengated.Expected Results:Trinity will send a message saying, "I guess Morpheus was right. You are the one. Consider me impressed. You are now ready to join The Construct, an exclusive chatroom available to only those that have completed this tutorial. Now, go join the others by using the same tag you used Register, with this process ID: [Construct Process ID] Good luck. -Trinity". Additionally, a footer will follow the message.Conclusion You've done it! You've successfully tokengated the chatroom. This has now unlocked access to the Construct, where only those that have fully completed this tutorial can enter.Congratulations!You've shown a great deal of promise. I hope you've enjoyed this tutorial. You're now ready to build freely in ao.

---

# 92. Bringing it Together  Cookbook

Document Number: 92
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/bringing-together.html
Words: 966
Extraction Method: html

Bringing it Together This final guide wraps up our series, where you've built up an autonomous agent piece by piece. Now, let's refine your agent with some optimizations that fine-tune its operations. Here's a quick overview of the key improvements made:Sequential Command Execution: The introduction of an InAction flag ensures that your agent's actions are sequential (next action occurs only when the previous is successfully executed). This critical addition prevents your agent from acting on outdated game states, enhancing its responsiveness and accuracy. The full implementation can be found in the final code for the bot.lua file below.Dynamic State Updates and Decisions: The agent now employs an automatic tick logic, allowing for dynamic updates and decisions. This logic enables the agent to self-trigger state updates and make subsequent decisions either upon receiving a Tick message or upon completing an action, promoting autonomous operation.Automated Fee Transfer: To further streamline its operation and ensure uninterrupted participation in games, the autonomous agent now autonomously handles the transfer of confirmation fees.In addition to these features, we've also added a logging function for debugging purposes and colored prints for better comprehension of game events. These enhancements collectively make your autonomous agent more efficient and adaptable in the game environment.Check out the complete bot.lua code in the dropdown below, with all new additions highlighted accordingly:Updated bot.lua file lua-- Initializing global variables to store the latest game state and game host process.

LatestGameState = LatestGameState or nil

InAction = InAction or false -- Prevents the agent from taking multiple actions at once.

Logs = Logs or {}

colors = {

  red = "\27[31m",

  green = "\27[32m",

  blue = "\27[34m",

  reset = "\27[0m",

  gray = "\27[90m"

}

function addLog(msg, text) -- Function definition commented for performance, can be used for debugging

  Logs[msg] = Logs[msg] or {}

  table.insert(Logs[msg], text)

end

-- Checks if two points are within a given range.

-- @param x1, y1: Coordinates of the first point.

-- @param x2, y2: Coordinates of the second point.

-- @param range: The maximum allowed distance between the points.

-- @return: Boolean indicating if the points are within the specified range.

function inRange(x1, y1, x2, y2, range)

    return math.abs(x1 - x2) <= range and math.abs(y1 - y2) <= range

end

-- Decides the next action based on player proximity and energy.

-- If any player is within range, it initiates an attack; otherwise, moves randomly.

function decideNextAction()

  local player = LatestGameState.Players[ao.id]

  local targetInRange = false

  for target, state in pairs(LatestGameState.Players) do

      if target ~= ao.id and inRange(player.x, player.y, state.x, state.y, 1) then

          targetInRange = true

          break

      end

  end

  if player.energy > 5 and targetInRange then

    print(colors.red .. "Player in range. Attacking." .. colors.reset)

    ao.send({Target = Game, Action = "PlayerAttack", Player = ao.id, AttackEnergy = tostring(player.energy)})

  else

    print(colors.red .. "No player in range or insufficient energy. Moving randomly." .. colors.reset)

    local directionMap = {"Up", "Down", "Left", "Right", "UpRight", "UpLeft", "DownRight", "DownLeft"}

    local randomIndex = math.random(#directionMap)

    ao.send({Target = Game, Action = "PlayerMove", Player = ao.id, Direction = directionMap[randomIndex]})

  end

  InAction = false -- InAction logic added

end

-- Handler to print game announcements and trigger game state updates.

Handlers.add(

  "PrintAnnouncements",

  { Action = "Announcement" },

  function (msg)

    if msg.Event == "Started-Waiting-Period" then

      ao.send({Target = ao.id, Action = "AutoPay"})

    elseif (msg.Event == "Tick" or msg.Event == "Started-Game") and not InAction then

      InAction = true -- InAction logic added

      ao.send({Target = Game, Action = "GetGameState"})

    elseif InAction then -- InAction logic added

      print("Previous action still in progress. Skipping.")

    end

    print(colors.green .. msg.Event .. ": " .. msg.Data .. colors.reset)

  end

)

-- Handler to trigger game state updates.

Handlers.add(

  "GetGameStateOnTick",

  { Action =  "Tick" },

  function ()

    if not InAction then -- InAction logic added

      InAction = true -- InAction logic added

      print(colors.gray .. "Getting game state..." .. colors.reset)

      ao.send({Target = Game, Action = "GetGameState"})

    else

      print("Previous action still in progress. Skipping.")

    end

  end

)

-- Handler to automate payment confirmation when waiting period starts.

Handlers.add(

  "AutoPay",

  { Action =  "AutoPay" },

  function (msg)

    print("Auto-paying confirmation fees.")

    ao.send({ Target = Game, Action = "Transfer", Recipient = Game, Quantity = "1000"})

  end

)

-- Handler to update the game state upon receiving game state information.

Handlers.add(

  "UpdateGameState",

  { Action =  "GameState" },

  function (msg)

    local json = require("json")

    LatestGameState = json.decode(msg.Data)

    ao.send({Target = ao.id, Action = "UpdatedGameState"})

    print("Game state updated. Print \'LatestGameState\' for detailed view.")

  end

)

-- Handler to decide the next best action.

Handlers.add(

  "decideNextAction",

  { Action =  "UpdatedGameState" },

  function ()

    if LatestGameState.GameMode ~= "Playing" then

      InAction = false -- InAction logic added

      return

    end

    print("Deciding next action.")

    decideNextAction()

    ao.send({Target = ao.id, Action = "Tick"})

  end

)

-- Handler to automatically attack when hit by another player.

Handlers.add(

  "ReturnAttack",

  { Action =  "Hit" },

  function (msg)

    if not InAction then -- InAction logic added

      InAction = true -- InAction logic added

      local playerEnergy = LatestGameState.Players[ao.id].energy

      if playerEnergy == undefined then

        print(colors.red .. "Unable to read energy." .. colors.reset)

        ao.send({Target = Game, Action = "Attack-Failed", Reason = "Unable to read energy."})

      elseif playerEnergy == 0 then

        print(colors.red .. "Player has insufficient energy." .. colors.reset)

        ao.send({Target = Game, Action = "Attack-Failed", Reason = "Player has no energy."})

      else

        print(colors.red .. "Returning attack." .. colors.reset)

        ao.send({Target = Game, Action = "PlayerAttack", Player = ao.id, AttackEnergy = tostring(playerEnergy)})

      end

      InAction = false -- InAction logic added

      ao.send({Target = ao.id, Action = "Tick"})

    else

      print("Previous action still in progress. Skipping.")

    end

  end

) What's next?You're now equipped with the knowledge to craft intelligent autonomous agents. It's time to apply these insights into the game world. Understand the game's intricacies and leverage your agent's capabilities to dominate the arena. But there's more to come.In future sections, we'll dive deeper into the game arena, offering advanced strategies to elevate your agent's performance. Ready to take on the challenge? Let's see what you can create! 🕹️

---

# 93. Strategic Decisions  Cookbook

Document Number: 93
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/decisions.html
Words: 552
Extraction Method: html

Strategic Decisions With the latest game state at your disposal, your bot can evolve into an autonomous agent. This transition marks an upgrade in functionality, enabling not just reactions to game states but strategic actions that consider context, energy, and proximity to make decisions.Writing the Code Return to your bot.lua file and add the following functions:lua-- Determines proximity between two points.

function inRange(x1, y1, x2, y2, range)

    return math.abs(x1 - x2) <= range and math.abs(y1 - y2) <= range

end

-- Strategically decides on the next move based on proximity and energy.

function decideNextAction()

  local player = LatestGameState.Players[ao.id]

  local targetInRange = false

  for target, state in pairs(LatestGameState.Players) do

      if target ~= ao.id and inRange(player.x, player.y, state.x, state.y, 1) then

          targetInRange = true

          break

      end

  end

  if player.energy > 5 and targetInRange then

    print("Player in range. Attacking.")

    ao.send({Target = Game, Action = "PlayerAttack", Player = ao.id, AttackEnergy = tostring(player.energy)})

  else

    print("No player in range or insufficient energy. Moving randomly.")

    local directionMap = {"Up", "Down", "Left", "Right", "UpRight", "UpLeft", "DownRight", "DownLeft"}

    local randomIndex = math.random(#directionMap)

    ao.send({Target = Game, Action = "PlayerMove", Player = ao.id, Direction = directionMap[randomIndex]})

  end

end The decideNextAction function is now a testament to our agent's ability to think and act based on a comprehensive understanding of its environment. It analyzes the latest game state to either attack if you have sufficient energy and an opponent is inRange or move otherwise.Now all you need is a handler to make sure this function runs on its own.This handler triggers upon receiving a message that the latest game state has been fetched and updated. An action is taken only when the game is in Playing mode.You can refer to the latest code for bot.lua in the dropdown below:Updated bot.lua file luaLatestGameState = LatestGameState or nil

function inRange(x1, y1, x2, y2, range)

    return math.abs(x1 - x2) <= range and math.abs(y1 - y2) <= range

end

function decideNextAction()

  local player = LatestGameState.Players[ao.id]

  local targetInRange = false

  for target, state in pairs(LatestGameState.Players) do

      if target ~= ao.id and inRange(player.x, player.y, state.x, state.y, 1) then

          targetInRange = true

          break

      end

  end

  if player.energy > 5 and targetInRange then

    print("Player in range. Attacking.")

    ao.send({Target = Game, Action = "PlayerAttack", Player = ao.id, AttackEnergy = tostring(player.energy)})

  else

    print("No player in range or insufficient energy. Moving randomly.")

    local directionMap = {"Up", "Down", "Left", "Right", "UpRight", "UpLeft", "DownRight", "DownLeft"}

    local randomIndex = math.random(#directionMap)

    ao.send({Target = Game, Action = "PlayerMove", Player = ao.id, Direction = directionMap[randomIndex]})

  end

end

Handlers.add(

"HandleAnnouncements",

{ Action = "Announcement" },

function (msg)

  ao.send({Target = Game, Action = "GetGameState"})

  print(msg.Event .. ": " .. msg.Data)

end

)

Handlers.add(

"UpdateGameState",

{ Action = "GameState" },

function (msg)

  local json = require("json")

  LatestGameState = json.decode(msg.Data)

  ao.send({Target = ao.id, Action = "UpdatedGameState"})

end

)

Handlers.add(

"decideNextAction",

{ Action = "UpdatedGameState" },

function ()

  if LatestGameState.GameMode ~= "Playing" then

    return

  end

  print("Deciding next action.")

  decideNextAction()

end

) Once again, to test out the latest upgrades, load the file in your aos player terminal as follows:lua.load bot.lua Observe your process output to see the decisions your autonomous agent makes in real-time, leveraging the current game state for strategic advantage. But what if another player attacks you and runs away while you are deciding the next move? In the next section you'll learn to automatically counter as soon as you have been attacked 🤺

---

# 94. Bots and Games  Cookbook

Document Number: 94
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/index.html
Words: 177
Extraction Method: html

Skip to content  Bots and Games NOTE Build your own unique bot to complete Quest 3 and earn 1000 CRED, then enter games like the Grid to earn legacynet CRED 24/7!Leveraging insights from our previous chapter, this section will guide you through the realm of automation with bots in aos and the construction of games. You will learn to create autonomous agents, using them to navigate and interact with game environments effectively.Sections Getting Started with a Game 0. # Let's Play A Game:Experience a game on aos Enhancing Game Interactions with Automation 1. # Interpreting Announcements:Interpret in-game announcements 2. # Fetching Game State:Retrieve and process the latest game state 3. # Strategic Decisions:Utilize automation to determine your next move 4. # Automated Responses:Streamline attack responses through automation 5. # Bringing it Together:Combine your skills to craft an autonomous agent Game Development Insights 6. # Mechanics of the Arena:Explore the underlying mechanics of a game's arena 7. # Expanding the Arena:Build unique game logic upon the arena A journey of discovery and creation awaits. Let the adventure begin!

---

# 95. Introduction to AO-Core  Cookbook

Document Number: 95
Source: https://cookbook_ao.arweave.net/welcome/ao-core-introduction.html
Words: 330
Extraction Method: html

Introduction to AO-Core AO-Core is a protocol and standard for distributed computation that forms the foundation of the AO computer. Inspired by and built upon concepts from the Erlang language, AO-Core embraces the actor model for concurrent, distributed systems. Unlike traditional blockchain systems, AO-Core defines a flexible, powerful computation protocol that enables a wide range of applications beyond just running Lua programs.What is AO-Core?AO-Core is the fundamental protocol of the AO computer that:Defines standards for trustless computation distributed across the world Provides mathematical guarantees about program execution Enables composable, modular development through devices Supports various execution environments beyond just Lua Implements the actor model for concurrent, message-passing computation The Actor Model in AO AO references the actor model of computation where:Each actor (or process) is an independent unit of computation Actors communicate exclusively through message passing Actors can create other actors, send messages, and make local decisions The system is inherently concurrent and distributed This approach, inspired by Erlang, provides natural scalability and resilience in distributed systems.Key Features of AO-Core Resilient: There is no single point of failure. AO-Core exists across many machines distributed worldwide, making it immune to physical destruction or tampering.Permanent: Computations following the AO-Core protocol are stored permanently on Arweave, allowing you to recall and continue your work at any time.Permissionless: No registration is required to use AO-Core. Your right to use it is guaranteed by the underlying protocol.Trustless: The state of your computations is mathematically guaranteed, allowing you to build services that don't require trust in any central authority.Beyond Just Processes While AO Processes (smart contracts built using the AO-Core protocol) are powerful for creating autonomous agents, AO-Core itself enables much more:Serverless functions with trustworthy guarantees Hybrid applications combining smart contract and serverless functionality Custom execution environments through different devices Composable systems using the path language In the following sections, we'll explore how AO Processes build on top of the AO-Core protocol, and how you can get started building your own applications in this powerful environment.

---

# 96. Get started in 5 minutes  Cookbook

Document Number: 96
Source: https://cookbook_ao.arweave.net/welcome/getting-started.html
Words: 840
Extraction Method: html

Get started in 5 minutes In less than 5 mins, we'll walk you through the process of taking your first peek into the rabbit hole of AO Processes. 🕳️🐇 Now that you understand the AO-Core protocol and how AO Processes work, let's get hands-on with creating your first AO Process.System requirements The local client of aos is super simple to install. Just make sure you have:NodeJS version 20+. (If you haven't yet installed it, check out this page to find instructions for your OS).A code editor of your choice.Installing aos Once you have NodeJS on your machine, all you need to do is install aos and run it:After installation, we can simply run the command itself to start a new aos process!shaos You authenticate yourself to your aos process using a keyfile. If you have an Arweave wallet you can specify it by adding a --wallet [location] flag. If you don't, a new keyfile will be generated and stored locally for you at ~/.aos.json.Welcome to the rabbit hole The utility you just started is a local client, which is ready to relay messages for you to your new process inside the ao computer.After it connects, you should see the following:lua          _____                   _______                   _____

         /\    \                 /::\    \                 /\    \

        /::\    \               /::::\    \               /::\    \

       /::::\    \             /::::::\    \             /::::\    \

      /::::::\    \           /::::::::\    \           /::::::\    \

     /:::/\:::\    \         /:::/~~\:::\    \         /:::/\:::\    \

    /:::/__\:::\    \       /:::/    \:::\    \       /:::/__\:::\    \

   /::::\   \:::\    \     /:::/    / \:::\    \      \:::\   \:::\    \

  /::::::\   \:::\    \   /:::/____/   \:::\____\   ___\:::\   \:::\    \

 /:::/\:::\   \:::\    \ |:::|    |     |:::|    | /\   \:::\   \:::\    \

/:::/  \:::\   \:::\____\|:::|____|     |:::|    |/::\   \:::\   \:::\____\

\::/    \:::\  /:::/    / \:::\    \   /:::/    / \:::\   \:::\   \::/    /

 \/____/ \:::\/:::/    /   \:::\    \ /:::/    /   \:::\   \:::\   \/____/

          \::::::/    /     \:::\    /:::/    /     \:::\   \:::\    \

           \::::/    /       \:::\__/:::/    /       \:::\   \:::\____\

           /:::/    /         \::::::::/    /         \:::\  /:::/    /

          /:::/    /           \::::::/    /           \:::\/:::/    /

         /:::/    /             \::::/    /             \::::::/    /

        /:::/    /               \::/____/               \::::/    /

        \::/    /                 ~~                      \::/    /

         \/____/                                           \/____/

Welcome to AOS: Your operating system for AO, the decentralized open

access supercomputer.

Type ".load-blueprint chat" to join the community chat and ask questions!

AOS Client Version: 1.12.1. 2024

Type "Ctrl-C" twice to exit

Your AOS process:  QFt5SR6UwJSCnmgnROq62-W8KGY9z96k1oExgn4uAzk

default@aos-0.2.2[Inbox:1]> Welcome to your new home in the ao computer! The prompt you are now looking at is your own personal server in this decentralized machine. We will be using it to play with and explore ao in the rest of this tutorial.Sending your first command Your new personal aos process is a server that lives inside the computer, waiting to receive and execute your commands.aos loves to make things simple, so it wants to hear commands from you in the Lua programming language. Don't know Lua? Don't panic! It is a super straightforward, friendly, and fun language. We will learn it as we progress through this series.Let's break the ice and type:Then hit the "[Enter]" key. You should see your shell sign and post the message, request the result, then print the result as follows:lua"Hello, ao!" Eh. What's the big deal?Sent it a message to your process, permanently etched it into Arweave, then asked a distributed compute network to calculate its result.While the result might not look revolutionary, in reality you have done something quite extraordinary. Your process is a decentralized server that doesn't exist in any one particular place on Earth. It exists as data, replicated on Arweave between many different machines, distributed all over the world. If you wanted to, you could now attach a new compute unit to this process and recreate the state from its log of inputs (just your single command, for now) -- at any time in the future.This makes your new shell process...Resilient: There is no single place on Earth where your server actually resides. It is everywhere and nowhere -- immune from physical destruction or tampering of any kind.Permanent: Your process will never disappear. It will always exist in its ✨holographic state✨  on Arweave, allowing you to recall it and continue playing with it. A contribution has been made to Arweave's storage endowment, so that you never have to think about upkeep or maintenance payments again.Permissionless: You did not have to register in order to start this server. Your right to use it is guaranteed by its underlying protocol (Arweave), no matter what Google, Amazon, or any other BigTech company says.Trustless: The state of your server is mathematically guaranteed. This means that you -- and everyone else -- can trust it with certainty, without even having to trust the underlying hardware it runs on. This property lets you build trustless services on top: Code that runs without any privileged owner or controller, ruled purely by math.There is so much more to it, but these are the basics. Welcome to the ao computer, newbie! We are grateful to have you. 🫡 In the tutorials that follow, we will explore ao and build everything from chatrooms to autonomous, decentralized bots. Let's go!

---

# 97. ARIO Docs

Document Number: 97
Source: https://docs.ar.io/ar-io-sdk
Words: 422
Extraction Method: html

AR.IO SDK Overview The AR.IO SDK provides functionality for interacting with the AR.IO ecosystem of services and protocols. This includes, the AR.IO Network, gateways, the ARIO token,  and ArNS domains. The AR.IO SDK is available for both NodeJS and web environments.AR.IO Network The AR.IO Network is the AO smart contract process that controls all child services and protocols.The AR.IO SDK supports read operations to access various details about the current or historical state of the network. It also provides write operations for managing features such as the Gateway Address Registry and ARIO token.Gateways AR.IO gateways are open source nodes that index and serve Arweave transaction headers and data items. Gateway operators may join their gateway to the Gateway Address Registry (GAR), which makes the gateway discoverable using the AR.IO SDK. The gateway information is stored in the AR.IO AO contract as a JSON object with the following attributes:{
    "operatorStake": "number",               // The amount of ARIO tokens staked by the operator, 50,000 minimum
    "totalDelegatedStake": "number",         // Total amount of ARIO tokens staked to the gateway by wallets other than the operator
    "vaults": "object",                      // Details of tokens vaults (locked tokens) associated with the gateway (object)
    "delegates": "object",                   // Details of non-operator wallets who staked ARIO tokens on the gateway (object)
    "startTimestamp": "number (unix)",       // Unix timestamp indicating start time
    "stats": "object",                       // Statistical information related to gateway performance (object)
    "settings": "object",                    // Configuration settings (object)
    "status": "string (e.g., joined)",       // The current status of the operator
    "observerAddress": "string"              // The public wallet address of the observer for the gateway
} The ar.io SDK supports write operations for gateway management, including joining, leaving, and updating settings. It also provides read operations for discovering gateways in the GAR and retrieving details about specific gateways.ARIO Token ARIO is an AO token that powers the ar.io Network and and its suite of permaweb applications. It is used to join the GAR, as payment for services like ArNS, as incentives for participation in the ar.io Network, and more.The ar.io SDK supports read and write operations for getting token information and balances, or transferring tokens.ArNS The Arweave Name System (ArNS) is a protocol which allows for assigning friendly names to Arweave transactions or data items. Powered by Arweave Name Tokens (ANTs), AO tokens that manage settings for individual ArNS domains, ArNS enables easy interaction with data stored on Arweave.The ar.io SDK supports read and write operations for managing ArNS domains, including retrieving domain information, leasing, purchasing, and extending leases. Additionally, it allows direct read and write access to ANTs.

---

# 98. ANTRegistry - ARIO Docs

Document Number: 98
Source: https://docs.ar.io/ar-io-sdk/ant-registry
Words: 277
Extraction Method: html

Overview The ANT Registry is a utility developed to make it easier to obtain a list of ANTs owned or controlled by a specific address.
The Owners and Controllers for each ArNS name are stored in the state of the ANT itself, rather than in the AR.IO Network contract,
which means every individual ANT would normally need to be queried in order to compose a complete list of ANTs owned or controlled by a specific address.When appropriately constructed ANTs are registered with the registry, they will send updates to the registry whenever they update their own owner or controller data,
creating a single index where the information can be quickly queried by apps that have use for that information.The ANTRegistry class in the AR.IO SDK contains several methods:accessControlList which accepts a wallet address and returns a list of all ANTs owned or controlled by that address,getAntsForAddress which provides a cleaner API for fetching ANTs owned or controlled by a wallet address (added in v3.15.0),register which registers a new ANT with the registry process For managing ANT versions and updates, see the ANTVersions class (added in v3.9.0).ArNS names that are purchased via ar://arns which use default ANTs are registered without further input during the creation process,
but users who bring their own custom ANTs to assign new ArNS names to will need to call the ANTRegistry.register() method in order for apps using the registry to see the new ArNS name.Optional The ANT Registry is separate from the AR.IO Network Contract, ANT registration
is not essential for ANTs and their corresponding ArNS name to function
correctly, though some applications, like ar://arns, may have features that
rely on the registry.

---

# 99. getAntsForAddress - ARIO Docs

Document Number: 99
Source: https://docs.ar.io/ar-io-sdk/ant-registry/get-ants-for-address
Words: 120
Extraction Method: html

getAntsForAddress is a method on the ANTRegistry class that provides a cleaner API for fetching ANTs owned or controlled by a specific wallet address. This method is functionally equivalent to accessControlList but provides a more intuitive interface.getAntsForAddress does not require authentication.Examples Parameters Parameter Type Description Optional address string The wallet address to query for owned or controlled ANTs false Output Notes This method returns the same data structure as accessControlList The Owned array contains ANT process IDs where the address is the owner The Controlled array contains ANT process IDs where the address is a controller If no ANTs are found for the address, both arrays will be empty This method automatically uses HyperBeam caching when available for improved performance

---

# 100. ANTVersions - ARIO Docs

Document Number: 100
Source: https://docs.ar.io/ar-io-sdk/ant-versions
Words: 135
Extraction Method: html

Overview The ANTVersions class provides functionality to manage ANT versions and track updates through the ANT Registry. This feature was added in SDK version 3.9.0.The ANTVersions class in the AR.IO SDK contains several methods:getANTVersions which retrieves all versions for an ANT getLatestANTVersion which retrieves the latest version for an ANT addVersion which adds a new version to an ANT (requires signer) Initialization The ANTVersions class can be initialized in different ways depending on your needs:Read-only Access Read/Write Access Parameters The initialization accepts the following configuration:processId (string, optional): The process ID of the ANT Registry. Defaults to the main ANT Registry ID if not provided.signer (AoSigner, optional): Signer instance for write operations. Required for addVersion method.Note Write operations like addVersion require a signer to be provided during
initialization. Read operations can be performed without a signer.

---

# 101. ANT Configuration - ARIO Docs

Document Number: 101
Source: https://docs.ar.io/ar-io-sdk/ants/configuration
Words: 157
Extraction Method: html

init init is a factory function that creates a read-only or writable client. By providing a signer, additional write APIs that require signing (like setRecord and transfer) become available. By default, a read-only client is returned and no write APIs are available.HyperBeam Support When a hyperbeamUrl is provided, the ANT will automatically attempt to fetch state from HyperBeam nodes for improved performance. This feature is particularly useful for newer ANTs that publish patch messages of their state on changes, allowing for cached state retrieval and reduced computational unit usage.Parameters Parameter Type Description Optional processId String The AO process ID of the ANT to connect to.false process AOProcess A pre-configured AOProcess instance used to initialize the ANT class true signer ContractSigner An optional signer instance, used to enable write operations on the
blockchain true hyperbeamUrl string Optional HyperBeam URL for cached state retrieval. When provided, the ANT will attempt to fetch state from HyperBeam nodes for improved performance.true

---

# 102. ARIO Docs

Document Number: 102
Source: https://docs.ar.io/ar-io-sdk/ants/release-name
Words: 111
Extraction Method: html

releaseName releaseName is a method on the ANT class that releases an ArNS name from the ANT, making it available for auction on the ARIO contract. The name must be permanently owned by the releasing wallet. Upon successful auction, 50% of the winning bid will be distributed to the ANT owner at the time of release. If there are no bids, the name becomes available for anyone to register.releaseName requires authentication.Examples Parameters Parameter Type Description Optional name string The ArNS name to be released false ioProcessId string The Process ID of the ARIO contract false tags array An array of GQL tag objects to attach to the transfer AO message true

---

# 103. setBaseNameRecord - ARIO Docs

Document Number: 103
Source: https://docs.ar.io/ar-io-sdk/ants/set-base-name-record
Words: 162
Extraction Method: html

setBaseNameRecord is a method on the ANT class that adds or updates the base name record for the ANT. This record defines the top-level name of the ANT (e.g., ardrive.ar.io).setBaseNameRecord requires authentication.Examples Parameters Parameter Type Description Optional transactionId string The Arweave transaction ID to set as the record false ttlSeconds number The number of seconds for DNS TTL (defaults to 900) true tags array An array of GQL tag objects to attach to the transfer AO message true TTL Time-To-Live (TTL) determines how often gateways should check the ANT for an update to the corresponding record. You can have different TTLs for different records within an ANT, depending on their use case. A record that is updated frequently should have a lower
value to facilitate serving current data, while a record that is updated less
often should have a higher value to allow cached data to be served more
quickly.TTL must be between 60 seconds (1 minute) and 86400 seconds (1 day).

---

# 104. setRecord - ARIO Docs

Document Number: 104
Source: https://docs.ar.io/ar-io-sdk/ants/set-record
Words: 195
Extraction Method: html

Deprecated This method is deprecated. Please use setBaseNameRecord for top-level names
or setUndernameRecord for undernames instead. See the setBaseNameRecord and setUndernameRecord documentation for
more details.setRecord is a method on the ANT class that sets or updates a record in the ANT process. Records map names to Arweave transaction IDs with optional TTL settings.setRecord requires authentication.Examples Parameters Parameter Type Description Optional undername string The undername name for the record (use "@" for the root domain) false transactionId string The Arweave transaction ID to point the record to false ttlSeconds number Time-to-live in seconds for the record cache true tags array An array of GQL tag objects to attach to the AO message true TTL Time-To-Live (TTL) determines how often gateways should check the ANT for updates to the corresponding record. You can have different TTLs for different records within an ANT, depending on their use case. A record that is updated frequently should have a lower
value to facilitate serving current data, while a record that is updated less
often should have a higher value to allow cached data to be served more
quickly.TTL must be between 60 seconds (1 minute) and 86400 seconds (1 day).

---

# 105. setUndernameRecord - ARIO Docs

Document Number: 105
Source: https://docs.ar.io/ar-io-sdk/ants/set-undername-record
Words: 178
Extraction Method: html

setUndernameRecord is a method on the ANT class that creates or updates an undername record for the ANT. An undername is a prefix that is joined to the base name with an underscore (e.g., dapp_ardrive.ar.io).setUndernameRecord requires authentication.Examples Parameters Parameter Type Description Optional undername string The undername to set the record for (e.g., 'dapp') false transactionId string The Arweave transaction ID to set as the record false ttlSeconds number The number of seconds for DNS TTL (defaults to 900) true tags array An array of GQL tag objects to attach to the transfer AO message true TTL Time-To-Live (TTL) determines how often gateways should check the ANT for updates to the corresponding record. You can have different TTLs for different records within an ANT, depending on their use case. A record that is updated frequently should have a lower
value to facilitate serving current data, while a record that is updated less
often should have a higher value to allow cached data to be served more
quickly.TTL must be between 60 seconds (1 minute) and 86400 seconds (1 day).

---

# 106. buyRecord - ARIO Docs

Document Number: 106
Source: https://docs.ar.io/ar-io-sdk/ario/arns/buy-record
Words: 145
Extraction Method: html

buyRecord buyRecord is a method on the ARIO class that purchases a new ArNS record with the specified name, type, processId, and duration.Examples Parameters Parameter Type Description Optional name string The ArNS name to purchase false type string The type of record: 'lease' or 'permabuy' false years number The number of years to lease (for lease type) or purchase permanently
(for permabuy type) false processId string The process id of an existing ANT process. If not provided, a new ANT
process using the provided signer will be spawned, and the ArNS record
will be assigned to that process.true referrer string Track purchase referrals for analytics (e.g. my-app.com) true tags array An array of GQL tag objects to attach to the buyRecord AO message true Manual ANT Creation If you prefer to manually create an ANT during the purchase process, see the manual ANT creation workflow.

---

# 107. getArNSRecords - ARIO Docs

Document Number: 107
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-records
Words: 109
Extraction Method: html

getArNSRecords is a method on the ARIO class that retrieves all ArNS records with optional pagination and filtering support.getArNSRecords does not require authentication.Examples Parameters Parameter Type Description Optional Default cursor string The ArNS name to use as the starting point for the next page of results true None limit number The maximum number of records to return (max: 1000) true 100 sortBy string The property to sort results by true startTimestamp sortOrder string The sort direction ('desc' or 'asc') true desc filters Record<string, string | string[]> Filters to apply to the results. Keys must match object properties.
Arrays act as OR conditions, multiple keys act as AND conditions.true None

---

# 108. ARIO Docs

Document Number: 108
Source: https://docs.ar.io/ar-io-sdk/ario/arns/extend-lease
Words: 121
Extraction Method: html

extendLease extendLease is a method on the ARIO class that extends the lease duration of a registered ArNS domain. The extension period can be 1-5 years, depending on the domain's grace period status. Note that permanently registered domains cannot have their leases extended.extendLease requires authentication.Examples Parameters Parameter Type Description Optional name string The ArNS name for which to extend the lease false years number The number of years to extend the lease by (1-5 years) false fundFrom string The source of funds: 'balance', 'stakes', 'any', or 'turbo' true paidBy string | string[] Wallet address(es) that will pay for the purchase (used with fundFrom:
'turbo') true tags array An array of GQL tag objects to attach to the transfer AO message true

---

# 109. getTokenCost - ARIO Docs

Document Number: 109
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-token-cost
Words: 218
Extraction Method: html

getTokenCost is a method on the ARIO class that calculates the cost in mARIO tokens for various ArNS operations such as buying records, extending leases, increasing undername limits, upgrading to permabuy, and requesting primary names.getTokenCost does not require authentication.Examples Parameters The getTokenCost method accepts different parameter sets depending on the intent (the specific action you want to check the cost for). Each intent requires a different combination of parameters as outlined below:Buy Record / Buy Name Parameter Type Description Optional intent string 'Buy-Record' or 'Buy-Name' false name string The ArNS name to calculate cost for false type string 'lease' or 'permabuy' false years number Number of years for lease (required for all lease operations) false Extend Lease Parameter Type Description Optional intent string 'Extend-Lease' false name string The ArNS name to extend false years number Number of years to extend the lease false Increase Undername Limit Parameter Type Description Optional intent string 'Increase-Undername-Limit' false name string The ArNS name to increase limit for false quantity number Number of additional undernames to allow false Upgrade to Permabuy Parameter Type Description Optional intent string 'Upgrade-Name' false name string The ArNS name to upgrade from lease to permabuy false Primary Name Request Parameter Type Description Optional intent string 'Primary-Name-Request' false name string The ArNS name to request as primary name false

---

# 110. ARIO Docs

Document Number: 110
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-returned-names
Words: 106
Extraction Method: html

getArNSReturnedNames getArNSReturnedNames is a method on the ARIO class that retrieves all currently active returned ArNS names, with support for pagination and custom sorting. Pagination is handled using a cursor system, where the cursor is the name from the last record of the previous request.getArNSReturnedNames does not require authentication.Example Parameters Parameter Type Description Optional Default cursor string The name to use as the starting point for the next page of results true None limit number The maximum number of records to return (max: 1000) true 100 sortBy string The property to sort results by true startTimestamp sortOrder string The sort direction ('desc' or 'asc') true desc

---

# 111. getCostDetails - ARIO Docs

Document Number: 111
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-cost-details
Words: 137
Extraction Method: html

getCostDetails is a method on the ARIO class that calculates detailed cost information for a specific interaction (such as buying an ArNS record). The method determines costs based on the interaction type, the payer's address, and the funding source (balance, stake, or any available funds).getCostDetails does not require authentication.Examples Parameters Parameter Type Description Optional intent string The type of interaction to calculate costs for (e.g., 'Buy-Record') false fromAddress string - WalletAddress The Arweave address that will be charged for the interaction false fundFrom string The source of funds: 'balance', 'stakes', or 'any' false name string The ArNS name for the interaction (for Buy-Record operations) conditional type string The type of purchase: 'lease' or 'permabuy' (for Buy-Record operations) conditional years number Number of years (for lease-based operations) conditional quantity number Quantity for operations like increasing undername limits conditional

---

# 112. ARIO Docs

Document Number: 112
Source: https://docs.ar.io/ar-io-sdk/ario/arns/increase-undername-limit
Words: 120
Extraction Method: html

increaseUndernameLimit increaseUndernameLimit is a method on the ARIO class that increases the number of undernames an ArNS domain can support. Each domain starts with a default limit of 10 undernames and can be increased up to a maximum of 10,000 undernames.increaseUndernameLimit requires authentication.Examples Parameters Parameter Type Description Optional name string The ArNS name to increase the undername limit for false increaseCount number The number of additional undername slots to purchase (up to 10,000
total) false fundFrom string The source of funds: 'balance', 'stakes', 'any', or 'turbo' true paidBy string | string[] Wallet address(es) that will pay for the purchase (used with fundFrom:
'turbo') true tags array An array of GQL tag objects to attach to the transfer AO message true

---

# 113. ARIO Docs

Document Number: 113
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/get-delegations
Words: 107
Extraction Method: html

getDelegations getDelegations is a method on the ARIO class that retrieves all active and vaulted stakes across all gateways for a specific address. Results are paginated and sorted by the specified criteria. The cursor parameter represents the last delegationId (a combination of gateway address and delegation start timestamp) from the previous request.getDelegations does not require authentication.Example Parameters Parameter Type Description Required address string The wallet address to query for delegations Yes cursor string Cursor for paginated results No limit number Maximum number of results to return (max: 1000) No sortBy string Property to sort results by No sortOrder string Sort direction (valid values: 'desc' or 'asc') No

---

# 114. getGateways - ARIO Docs

Document Number: 114
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateways
Words: 104
Extraction Method: html

getGateways is a method on the ARIO class that retrieves all gateways with optional pagination and filtering support.getGateways does not require authentication.Examples Parameters Parameter Type Description Optional Default cursor string The gateway address to use as the starting point for pagination true None limit number The maximum number of gateways to return (max: 1000) true 100 sortBy string The property to sort gateways by true startTimestamp sortOrder string The sort direction ('desc' or 'asc') true desc filters Record<string, string | string[]> Filters to apply to the results. Keys must match object properties.
Arrays act as OR conditions, multiple keys act as AND conditions.true None

---

# 115. joinNetwork - ARIO Docs

Document Number: 115
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/join-network
Words: 158
Extraction Method: html

joinNetwork is a method on the ARIO class that joins a gateway to the ar.io network using its associated wallet.joinNetwork requires authentication.Examples Parameters Parameter Type Description Required qty number Amount in mARIO to stake when joining network Yes autoStake boolean Whether to automatically stake gateway rewards Yes allowDelegatedStaking boolean Whether to allow third parties to delegate stake Yes minDelegatedStake number Minimum amount in mARIO that can be delegated Yes delegateRewardShareRatio number Percentage of rewards to share with delegates (e.g., 10) Yes label string Gateway name (1-64 characters) Yes note string Gateway description (max 256 characters) Yes properties string Arweave transaction ID containing additional gateway configuration Yes observerWallet string Wallet address used for network observations Yes fqdn string Valid domain name owned by the gateway operator Yes port number Port number for gateway access (typically 443) Yes protocol string Access protocol (only 'https' supported) Yes tags array An array of GQL tag objects to attach to the transaction No

---

# 116. ARIO Docs

Document Number: 116
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/redelegate-stake
Words: 149
Extraction Method: html

redelegateStake redelegateStake is a method on the ARIO class that moves staked tokens from one gateway to another. A vault ID can be optionally included to redelegate from an existing withdrawal vault. The redelegation fee is calculated based on the fee rate and the stake amount. Users receive one free redelegation every seven epochs. Each additional redelegation increases the fee by 10%, up to a maximum of 60%.For example: If 1000 mARIO is redelegated with a 10% fee rate, the fee will be 100 mARIO. This results in 900 mARIO being redelegated to the new gateway and 100 mARIO being returned to the protocol balance.redelegateStake requires authentication.Examples Parameters Parameter Type Description Required target string Destination gateway address for the stake Yes source string Current gateway address of the stake Yes stakeQty number Amount in mARIO to redelegate Yes vaultId string ID of the vault to move stake from No

---

# 117. ARIO Docs

Document Number: 117
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/update-gateway-settings
Words: 187
Extraction Method: html

updateGatewaySettings updateGatewaySettings is a method on the ARIO class that writes new gateway settings to the caller's gateway configuration.updateGatewaySettings requires authentication.Example Parameters Parameter Type Description Optional autoStake boolean If true, automatically stakes gateway rewards.true allowDelegatedStaking boolean If true, allows third parties to delegate stake to the gateway.true minDelegatedStake number Minimum number of tokens, in mARIO that can be delegated to the
gateway.true delegateRewardShareRatio number Percentage of gateway rewards to share with delegates. e.g. 10% true label string Friendly name for gateway, min 1 character, max 64 characters.true note string A note to be associated with gateway, max 256 characters.true properties string - ArweaveTxId ArweaveTxId to properties object containing additional gateway
configuration details.true observerWallet string - WalletAddress Public wallet address for wallet used to upload network observations.true fqdn string Fully qualified domain name, must be valid domain owned by gateway
operator.true port number Port number to use when accessing gateway, generally 443 (https) true protocol string - "http" || "https" Protocol to use when accessing gateway, only "https" is supported for
network participation.true tags array An array of GQL tag objects to attach to the joinNetwork AO message.true

---

# 118. ARIO Smart Contract - ARIO Docs

Document Number: 118
Source: https://docs.ar.io/ario-contract
Words: 300
Extraction Method: html

Overview The AR.IO smart contract encompasses all the functionality required to support the network's currency, utilities, and management.
Written in Lua and compiled to WASM64, it is deployed as a Process within AO, leveraging the decentralized infrastructure of Arweave for immutability and auditability.
This ensures that AR.IO's smart contract code is stored permanently, is easily verifiable by external auditors, and is transparent to the community.Protocol Balance The Protocol Balance is the primary sink and source of ARIO tokens circulating through the AR.IO Network.
This balance is akin to a central vault or wallet programmatically encoded into the network's smart contract from which ArNS revenue is accumulated and incentive rewards are distributed.This balance is stored like any other token balance in the AR.IO smart contract, using the contract's Process ID as the balance owner.
This balance is stored like any other token balance in the AR.IO smart contract, using the contract’s Process ID as the balance owner.
Should a user or organization desire, tokens can even be sent directly into this balance to support the reward protocol and ecosystem.Cross-Chain Signature Support AO leverages the flexibility of ANS-104 data items, which support multiple signature types from various blockchains. This includes signatures from Arweave, Ethereum, Solana, Cosmos, among others.This cross-chain signature support provides significant benefits to the AR.IO network:Interoperability: Cross-chain signatures enable seamless interactions across different blockchain ecosystems, allowing AR.IO to integrate with diverse apps and services without friction.Flexibility: Users can validate transactions with signatures from their preferred blockchain, making it easier for a broader range of participants to engage with AR.IO using familiar wallets and mechanisms.Security: Decentralized cryptographic standards across chains ensure that interactions on AR.IO remain secure and trusted, regardless of the blockchain used.By supporting cross-chain signatures, AR.IO enhances interoperability, flexibility, and security, empowering users and developers across multiple blockchain ecosystems.

---

# 119. Token Faucet - ARIO Docs

Document Number: 119
Source: https://docs.ar.io/ar-io-sdk/faucet
Words: 199
Extraction Method: html

Overview The Token Faucet functionality allows developers to request AR.IO tokens through a captcha-protected API service. This feature was added in SDK version 3.10.0.The faucet leverages a new API service with hCaptcha integration. Developers can request JSON Web Tokens (JWT) by solving a captcha, and then use those tokens to claim AR.IO tokens to wallet addresses, subject to rate limits.Initialization The faucet functionality is available on ARIO instances when initialized with a faucet URL:Available Methods The faucet object on ARIO instances provides the following methods:captchaUrl - Get the captcha URL for the current process requestAuthToken - Request an auth token using captcha response claimWithCaptchaResponse - Claim tokens directly with captcha response claimWithAuthToken - Claim tokens using an auth token verifyAuthToken - Verify if an auth token is still valid Usage Flow Get Captcha URL - Retrieve the captcha URL for the process Solve Captcha - User solves the captcha on the provided URL Request Auth Token - Use captcha response to get an auth token Claim Tokens - Use auth token to claim tokens to a wallet address Note The faucet functionality is only available on testnet instances. The faucetUrl
parameter is optional and defaults to ' https://faucet.ario.permaweb.services '.

---

# 120. Arweave Name System (ArNS) - ARIO Docs

Document Number: 120
Source: https://docs.ar.io/arns
Words: 1790
Extraction Method: html

Overview Arweave URLs and transaction IDs are long, difficult to remember, and occasionally miscategorized as spam.
The Arweave Name System (ArNS) aims to resolve these problems in a decentralized manner.
ArNS is a censorship-resistant naming system stored on Arweave, powered by ARIO tokens, enabled through AR.IO gateway domains, and used to connect friendly domain names to permaweb apps, web pages, data, and identities.It's an open, permissionless, domain name registrar that doesn’t rely on a single TLD.This system works similarly to traditional DNS services, where users can purchase a name in a registry and DNS Name servers resolve these names to IP addresses.
The system shall be flexible and allow users to purchase names permanently or lease them for a defined duration based on their use case.
With ArNS, the registry is stored permanently on Arweave via AO, making it immutable and globally resilient.
This also means that apps and infrastructure cannot just read the latest state of the registry but can also check any point in time in the past, creating a “Wayback Machine” of permanent data.Users can register a name, like ardrive, within the ArNS Registry.
Before owning a name, they must create an Arweave Name Token (ANT), an AO Computer based token and open-source protocol used by ArNS to track the ownership and control over the name.
ANTs allow the owner to set a mutable pointer to any type of permaweb data, like a page, app or file, via its Arweave transaction ID.Each AR.IO gateway acts as an ArNS Name resolver.
They will fetch the latest state of both the ArNS Registry and its associated ANTs from an AO compute unit (CU) and serve this information rapidly for apps and users.
AR.IO gateways will also resolve that name as one of their own subdomains, e.g., https://ardrive.arweave.net and proxy all requests to the associated Arweave transaction ID.
This means that ANTs work across all AR.IO gateways that support them: https://ardrive.ar-io.dev, https://ardrive.g8way.io/, etc.Users can easily reference these friendly names in their browsers, and other applications and infrastructure can build rich solutions on top of these ArNS primitives.Name Registration There are two different types of name registrations that can be utilized based upon the needs of the user:Lease: a name may be leased on a yearly basis. A leased name can have its lease extended or renewed but only up to a maximum active lease of five (5) years at any time.Permanent (permabuy): a name may be purchased for an indefinite duration.Registering a name requires spending ARIO tokens corresponding to the name’s character length and purchase type.Name Registry The ArNS Registry is a list of all registered names and their associated ANT Process IDs. Key rules embedded within the smart contract include:Genesis Prices: Set within the contract as starting conditions.Dynamic Pricing: Varies based on name length, purchase type (lease vs buy), lease duration, and current Demand Factor.Name Records: Include a pointer to the Arweave Name Token process identifier, lease end time (if applicable), and undername allocation.Reassignment: Name registrations can be reassigned from one ANT to another.Lease Extension: Anyone with available ARIO Tokens can extend any name’s active lease.Lease to Permanent Buy: Anyone with available ARIO Tokens can convert a name’s lease to a permanent buy.Undername Capacity: Additional undername capacity can be purchased for any actively registered name. There is no cap on the maximum amount of undernames that a top-level ArNS name can have associated with it.Name Removal: Name records can only be removed from the registry if a lease expires, or a permanent name is returned to the protocol.Name Validation Rules All names registered shall meet the following criteria:Valid names include only numbers 0-9, characters a-z and dashes.Dashes cannot be leading or trailing characters.Dashes cannot be used in single character domains.1 character minimum, 51 characters maximum.Shall not be an invalid name predesignated to prevent unintentional use/abuse such as www.Lease Expirations When a lease term ends, there is a grace period of two (2) weeks where the lease can be renewed before it fully expires.
If this grace period elapses, the name is considered expired and returns to the protocol for public registration. Once expired, a name’s associated undername registrations and capacity also expire.A recently expired name’s registration shall be priced subject to the “Returned Name Premium” mechanics detailed below.Lease to Permabuy Conversions An actively leased name may be converted to a permanent registration. The price for this conversion shall be treated as if it were a new permanent name purchase.This functionality allows users to transition from leasing to permanent ownership based on changing needs and available resources.
It generates additional protocol revenue through conversion fees, contributing to the ecosystem's financial health and reward system.
Additionally, by maintaining fair value for name conversions, it ensures prices reflect current market conditions, promoting a balanced and fair environment.Permanent Name Return Users have the option to “return” their permanently registered names back to the protocol.
This process allows users to relinquish their ownership, returning the name to the protocol for public re-registration. Only the Owner of a name can initiate a name return.When a permanent name is returned, the name is subject to a "Returned Name Premium”, similar to expired leases.
A key difference is that if the name is repurchased during the premium window, the proceeds are split between the returning owner and the protocol balance.Primary Names The Arweave Name System (ArNS) supports the designation of a "Primary Name" for users, simplifying how Arweave addresses are displayed across applications.
A Primary Name is a user-friendly alias that replaces complex wallet addresses, making interactions and profiles easier to manage and identify.Users can set one of their owned ArNS names as their Primary Name, subject to a small fee. This allows applications to use a single, human-readable identifier for a wallet, improving user experience across the network.Arweave Name Token (ANT) To establish ownership of a record in the ArNS Registry, each record contains both a friendly name and a reference to an Arweave Name Token, ANT.
Name Tokens are unique AO Computer based tokens / processes that give their owners the ability to update the Arweave Transaction IDs that their associated friendly names point to.The ANT smart contract process is a standardized contract that implements the specific Arweave Name Process specification required by AR.IO gateways who resolve ArNS names and their Arweave Transaction IDs.
It also contains other basic functionality to establish ownership and the ability to transfer ownership and update the Arweave Transaction ID.Name Tokens have an owner, who can transfer the token and control its modifiable settings.
These settings include modifying the address resolution time to live (ttl) for each name contained in the ANT, and other settings like the ANT Name, Ticker, and an ANT Controller.
The controller can only manage the ANT and set and update records, name, and the ticker, but cannot transfer the ANT.
Note that ANTs are initially created in accordance with network standards by an end user who then has to ability to transfer its ownership or assign a controller as they see fit.Owners of names should ensure their ANT supports evolve ability if future modifications are desired. Loss of a private key for a permanently purchased name can result in the name being "bricked”.Secondary markets could be created by ecosystem partners that facilitate the trading of Name Tokens.
Additionally, tertiary markets could be created that support the leasing of these friendly names to other users.
Such markets, if any, would be created by third parties unrelated to and outside of the scope of this paper or control of the Foundation.The table below indicates some of the possible interactions with the ArNS registry, corresponding ANTs, and who can perform them:ANT Interactions Type ANT Owner ANT Controller Any ARIO Token Holder Transfer ownership ✔   Add / remove controllers ✔   Set or change primary name ✔   Reassign name to new ANT process ✔   Return a permanent name ✔   Set records (pointers) ✔ ✔  Update records, name, ticker ✔ ✔  Update descriptions and keywords ✔ ✔  Create and assign undernames ✔ ✔  Extend / renew lease ✔ ✔ ✔ Increase undernames ✔ ✔ ✔ Convert lease to permanent ✔ ✔ ✔ ANT Interactions Under_names ANT owners and controllers can configure multiple subdomains for their registered ArNS name known as “under_names” or more easily written “undernames”.
These undernames are assigned individually at the time of registration or can be added on to any registered name at any time.Under_names use an underscore “_” in place of a more typically used dot “.“ to separate the subdomain from the main ArNS domain.Addressing Variable Market Conditions The future market landscape is unpredictable, and the AR.IO Network smart contract is designed to be immutable, operating without governance or manual intervention.
Using a pricing oracle to fix name prices relative to a stable currency is not viable due to the infancy of available solutions and reliance on external dependencies.
To address these challenges, ArNS is self-contained and adaptive, with name prices reflecting network activity and market conditions over time.To achieve this, ArNS incorporates:A dynamic pricing model that adjusts fees using a "Demand Factor" based on ArNS purchase activity.A Returned Name Premium (RNP) system that applies a timed, descending multiplier to registration prices for names that have recently expired or been returned to the protocol.This approach ensures that name valuations adapt to market conditions within the constraints of an immutable, maintenance-free smart contract framework.Dynamic Pricing Model ArNS employs an adaptive pricing model to balance market demand with pricing fairness for name registration within the network.
This model integrates static and dynamic elements, adjusting prices based on name length and purchase options like leasing, permanent acquisition, and undername amounts.
A key element is the Demand Factor (DF), which dynamically adjusts prices according to network activity and revenue trends, ensuring prices reflect market conditions while remaining accessible and affordable.A detailed description of the variables and formulas used for dynamic pricing can be found in the Appendix.Returned Name Premiums (RNP) ArNS applies a Returned Name Premium (RNP) to names that re-enter the market after expiration or permanent return.
This premium starts at a maximum value and decreases linearly over a predefined window, ensuring fair and transparent pricing for re-registered names.The RNP multiplier is applied to the registration price of both permanently purchased and leased names.Gateway Operator ArNS Discount Gateway operators who demonstrate consistent, healthy participation in the network are eligible for a 20% discount on certain ArNS interactions.To qualify:The gateway must maintain a “Gateway Performance Ratio Weight” (GPRW) of 0.85 or higher.The gateway must have a “Tenure Weight” (TW) of 0.5 or greater, indicating at least a 3-month prior commitment to the network.A gateway marked as “Leaving” shall not be eligible for this discount.Eligible ArNS Discounted Interactions:Purchasing a name Extending a lease Upgrading a lease to permabuy Increasing undernames capacity

---

# 121. ARIO SDK Release Notes - ARIO Docs

Document Number: 121
Source: https://docs.ar.io/ar-io-sdk/release-notes
Words: 6816
Extraction Method: html

AR.IO SDK Changelog Overview Welcome to the documentation page for the AR.IO SDK release notes. Here, you will find detailed information about each version of the AR.IO SDK, including the enhancements, bug fixes, and any other changes introduced in every release. This page serves as a comprehensive resource to keep you informed about the latest developments and updates in the AR.IO SDK. For those interested in exploring the source code, each release's code is readily accessible at our GitHub repository: AR.IO SDK change logs. Stay updated with the continuous improvements and advancements in the AR.IO SDK by referring to this page for all release-related information.3.18.3 (2025-08-22) Bug Fixes io: fix ant-registry-id param (7398c6d) 3.18.2 (2025-08-21) Bug Fixes buyRecord: add arns name as Name tag to spawn (e23dde4) buyRecord: default to arns name tag if no Name tag provided (2d0487d) buyRecord: pass tags option to spawn ant (b46d252) buyRecord: use reduce to create default tag (53089c0) constants: remove legacy constants (80d655b) hb: use /now only when computing state for ant registry (5f39452) 3.18.1 (2025-08-15) Bug Fixes turbo: apply publicKey to signed req headers on node PE-8459 (c07aa38) 3.18.0 (2025-08-11) Bug Fixes ant registry: add hyperbeam url to ario read for ant registry (9e9b528) ario: update set-primary-name API and error handling (bb61e1a) types: add callback types to AoARIOWrite methods (82bd790) Features arns: add support for setPrimaryName API, which allows the owner of an ANT that controls an ArNS name set their primary name in a single API (b21a36c) 3.18.0-alpha.3 (2025-08-11) Bug Fixes types: add callback types to AoARIOWrite methods (82bd790) 3.18.0-alpha.2 (2025-08-10) Bug Fixes hb: catch fetch errors to trigger fallback (491f1c2) 3.17.2 (2025-08-09) Bug Fixes types: update versions type (3424f4d) 3.17.1 (2025-08-08) Bug Fixes types: update versions type (3424f4d) 3.17.1-alpha.1 (2025-08-08) Bug Fixes types: update versions type (3424f4d) 3.17.0 (2025-08-07) Bug Fixes deps: reduce included imports in minimized bundle (7133c46) Features arns: add API to fetch arns names for a given address (d2b2c91) 3.16.1-alpha.1 (2025-08-06) Bug Fixes deps: reduce included imports in minimized bundle (7133c46) 3.16.0 (2025-08-06) Bug Fixes ant: add optional state to event messages (07fe4ed) ant: add version to spawn (edf0c46) ant: check ACL after spawned ant to confirm ownership was properly assigned (9de8272) ant: on spawn, throw error if owner not returned (85460ae) ant: use gettter for versions on ANT, use.spawn for buy-record (dc8c726) ario: add buying-name callback event on buyRecord (0d995ab) utils: pass down ao to ant registry (4bfab08) Features ant: add onSigningProgress callabacks to spawnAnt util (1bb61a0) buy-record: relax requirement for processId on buy-record (2d1017c) 3.15.1 (2025-07-29) Bug Fixes constants: update aos and lua ids to v22 (93e1208) 3.15.0 (2025-07-25) Bug Fixes acl: use uppercase keys in acl fetch (88cf858) ant registry: add logger to configuration for ant registry (e548f02) ant: add debug log when using hyperbeam for ant (8c29ddd) ant: add hyperbeam timeout to avoid hanging requests (0627d6e) ao: format all error messages, not just aos ones (38adebf) ao: return error cause for failed dry runs (e4b25b5) cli: add cli command for listing an addresses ants (b9e0c83) cli: publish patch for alpha build (66d5399) hb: do not default hb url (3b0ec00) hb: use cache key to access acl (873d356) types: better names for types (8134184) types: rename types for ant registry config (0970a50) Features ants: add new API for fetching ants (9a6c7b6) 3.15.0-alpha.1 (2025-07-25) Bug Fixes ant: add hyperbeam timeout to avoid hanging requests (0627d6e) cli: publish patch for alpha build (66d5399) Features ants: add new API for fetching ants (9a6c7b6) 3.14.0 (2025-06-25) Features ants: add support for fetching ant state from hyperbeam nodes (f5cbc76) pagination: add filter support to paginated handlers (8445dca) 3.13.0 (2025-06-09) Bug Fixes deps: add uuid as required dependency (6c83033) Features add referer support to ArNS purchase APIs (PE-8088) (36034db) 3.12.2 (2025-06-02) Bug Fixes ao: revert dep bump of aoconnect (5b013f7) 3.12.1 (2025-05-23) Bug Fixes deps: add uuid as required dependency (3cecc50) 3.12.0 (2025-05-22) Bug Fixes deps: update aoconnect and add required MODE (a4c5b16) utils: update url util to else statement (c0f3354) Features arns: append paidBy to query params on fundfrom turbo arns purcahaes PE-7943 (11333e8) 3.11.0 (2025-05-15) Bug Fixes yarn: update lockfile (fd5e0ee) Features ant: add ANT read interface (c941c96) ant: create ant contract class for interacting with ant contracts (6eb7ef5) ants: add readable-writable framework to the ant client and implement write methods (3019f53) contract: add distributions and observation apis (21e38d1) contract: update ArIO interface and ArIOContract interface (5d87e2e) auctions: add auctions apis (faf08c5) contract: add distribution, observations apis, update readme and examples (0208317) contract: create new contract classes that impelement both warp and remote cache for ant contract and ar-io contracts (855da2d) docs: setup examples, readme, and initial gateways provider (5a9e232) contract: add gar write methods to the ario client (e01b08b) contract: scaffold initial providers (4949514) contract: add transfer api to ario writable client (0d37623) contract: add saveObservations write interaction (8dd977c) observers: add API for fetching prescribed observers (a18e130) observers: add API for fetching prescribed observers (#17) (17ce6de) PE-5742: add records api to arns remote cache (#8) (c46cd39) PE-5751: add blockheight and sortkey eval filters (#12) (832a1ad) PE-5758: add signer to ario class (#20) (1b82077) PE-5759: observations and distributions apis (#16) (dded361) PE-5773: add auctions read apis (#18) (e0c6fca) PE-5800: add epoch apis (48ee4ba) PE-5800: epoch apis (#15) (70563b1) PE-5825: ANT read interface (#19) (6a0c477) records: add records api to arns remote cache (1b7f54f) signer: add arweave signer to ario class (7e08097) write: add write interface and base implementation on warp-contract (6dfc969) 3.10.0 (2025-04-15) Bug Fixes types: update types for better compatibility (9c0540b) Features ant: add ANT read interface (c941c96) ant: create ant contract class for interacting with ant contracts (6eb7ef5) 3.9.1 (2025-03-31) Bug Fixes ario: throw errors if fail to find epoch data (b1bb024) cli: update other vault APIs (0916e96) cli: update types and errors in cli (4b5aebd) io: do not return undefined for any API (ce6a077) 3.9.0 (2025-03-27) Bug Fixes ao: update retry logic on send, include more verbose messaging (70e6678) comments: cleanup (7444be3) exports: move types/browser exports above import/require (280d8bd) gql: add fallback to go to CU for epoch distribution data (12be216) logging: log errors more verbosely on read fails (c9fab18) messaging: only retry messaging if no id was received (9cbffef) options: add write options to addVersion (95ccef0) PE-7802: add logo to SpawnAntState type and options (c7adfca) send: add comment on not retrying on send (692938a) send: log on max attempts in send as well (2399d23) versions: export versions class (6368c44) Features versions: add ANT version class (c9ec5c5) versions: add versioning handlers to ant registry (c681909) 3.8.4 (2025-03-12) Bug Fixes types: add vault controller as optional to vault (f26bdb3) 3.8.3 (2025-03-05) Bug Fixes add missing maxDelegateRewardSharePct field from AoGatewayRegistrySettings (87942ad) schema: remove viem and use string for AOAddressSchema (090c799) schemas: update ant schema to accept eth support (7bc7df4) tests: update unit test to check loosely on eth address (b8e202b) 3.8.2 (2025-02-25) Bug Fixes missing break for happy path through send (e55ecc1) modify retry logic for send to only retry on exceptions from ao.message or ao.result (229df6b) modify retry logic to only occur on dryrun exceptions (c578893) protect against if retries is 0 (6aa1b58) 3.8.1 (2025-02-21) Bug Fixes mainnet: default to the mainnet process ID (a96713c) [3.8.0] (2025-02-20) View changes on Github Features mainnet: update constant to the mainnet process ID (4a11840) [3.7.1] (2025-02-19) View changes on Github Bug Fixes types: overload getEpoch to provide correct types on specific param requests (bafce74) 3.7.1-alpha.1 (2025-02-18) Bug Fixes types: overload getEpoch to provide correct types on specific param requests (bafce74) [3.7.0] (2025-02-17) View changes on GitHub Bug Fixes types: fix epoch settings type (a306baa) Features distributions: init paginated distributions cli command PE-7641 (8810ec6) distributions: init paginated getEligibleDistributions PE-7641 (9ba192f) distributions: remove eligible rewards from get epoch return PE-7641 (4437eaa) read commands: add commands epoch-settings, demand-factor-settings, read-action (821b6f6) [3.7.0-alpha.1] (2025-02-17) View changes on GitHub Features distributions: init paginated distributions cli command PE-7641 (8810ec6) distributions: init paginated getEligibleDistributions PE-7641 (9ba192f) distributions: remove eligible rewards from get epoch return PE-7641 (4437eaa) read commands: add commands epoch-settings, demand-factor-settings, read-action (821b6f6) [3.6.2-alpha.1] (2025-02-17) View changes on GitHub Bug Fixes types: fix epoch settings type (a306baa) [3.6.1] (2025-02-17) View changes on GitHub Bug Fixes types: correct types for demand factor and gateway settings, update tests (583ffeb) [3.6.0] (2025-02-17) View changes on GitHub Bug Fixes ant ids: update module and lua source ids to ant 15 (b8d6c96) deps: bump aoconnect sdk (3896ee8) ids: bump module and lua source ids to ant 16 (cf6d0de) page size: set page size to 1000 on fetch all records util (5fa802e) request name: add fund from tag on request primary name api (be362ad) Features ant: add sorting to ANT records responses by default (4e74825) [3.5.3] (2025-02-12) View changes on GitHub Bug Fixes arns: use buy-name buy default for getCostDetails (d71f402) [3.5.2] (2025-02-12) View changes on GitHub Bug Fixes arns: use buy-name when fetching token cost by default (5585b4d) [3.5.1] (2025-02-09) View changes on GitHub Bug Fixes gql: use goldsky by default for fetching gql data (57f4948) zod: fix zod enforcement (08d5168) [3.5.0] (2025-02-06) View changes on GitHub Bug Fixes ant: add priority as an attribute on ANTs (f0c6758) ant: update types and add index for easy enforcement (3dd6df5) ant: use deterministic sort with no locale comparison (7f2e067) evolve: use fetch for data instead of arweave (6deb91c) module ids: update ant lua and module id (97e0628) Features arns stats: include arns stats type on epoch PE-7562 (f92ee91) [3.4.1] (2025-02-03) View changes on GitHub Bug Fixes epochs: getPrescribedObservers and getPrescribedNames should get data from GQL/arweave vs. the contract (d8fa25d) gar: mark old fields as deprecated, add new ones (18ca1b4) [3.4.0] (2025-01-31) View changes on GitHub Bug Fixes ant: add setUndername and setBasename apis (ce4abfe) Features ant apis: add commands for new methods (931e621) revokable vaults: init revokeVault and vaultedTransfer commands and methods PE-7514 (6ca44a1) vault apis: assert lock length in range PE-7541 (3585643) vault apis: init write methods+commands for create/extend/increase vault PE-7541 (b2e3cab) [3.3.1] (2025-01-29) View changes on GitHub Bug Fixes ant: bumps ids (8eb2e38) ants: module bump _wSmbjfSlX3dZNcqE8JqKmj-DKum9uQ_jB08LwOKCyw (8d442e0) ant: update ANT ids (9f37e76) boot: add boot loader logic to ant spawn util (f00ab47) lua: update lua code id (76822a2) tags: remove extra tags from spawn util (f308b84) [3.3.0] (2025-01-24) View changes on GitHub Bug Fixes ants: tag with ao authority (f08af65) ao: add ao client for ants in emitter (489c040) arconnect: use signDataItem method, signature is deprecated (11e2378) arweave: use defaultArweave when fetching data (acf3e02) error handling: trim escape codes from thrown error PE-7417 (6dcf641) error handling: use a consolidated regexp for msg.Error and msg.Tags.Error PE-7417 (770a81e) gql: add retries when fetching epoch distribution data from arweave (42c1534) ids: add module and code ids (7474ccd) import: use import from file (f8fe7b4) logs: add processId to read error logs, include stack trace (51b7e38) module id: update ant module id (9e122af) pagination: allow nested keys in sortBy pagination params utility type PE-7428 (8ae8d88) spawn: spawn ANTs with a custom ANT module instead of aos module (2359b5b) test: double test timeout (4a52b81) ts: add root dir (e33eba5) types: simplify types for init functions, cleanup contructors (2197d99) types: simplify types for init functions, cleanup contructors (cd0afa6) Features add writeAction sdk/cli command for utility PE-7417 (1953504) get all delegates: init getAllDelegates type/handler PE-7221 (b015582) get all delegates: init list-all-delegates command PE-7221 (a632563) get all vaults: init command PE-7220 (e74a6e4) get all vaults: init type and ARIO method PE-7220 (e8f5a74) io: fetch historical epoch data from gql (b627d55) [3.2.0] (2025-01-13) View changes on GitHub Bug Fixes ant: add getLogo api (eddc3a8) ario: use standardize tags for registration fees and cost details (3f5fdbe) io: remove new APIs (d916ab6) types: add Buy-Name to supported intent types (b5a6d01) Features ario: add new APIs to ario class, update ant removePrimaryNames tags (61e0ee8) cost-details: include returnedNameDetails when they exist on cost-details PE-7371 (9edfb79) [3.1.0] (2025-01-02) View changes on GitHub Bug Fixes don't get old arweave block timestamps on read actions (1792ee8) don't return null when stringified null is found in message data on ao.read (c5873e6) eth signer: use a unique anchor in ans-104 headers (8cd5587) format process errors to be more user friendly PE-7327 (3449e32) io: fix AoEpochData type, add prescribedNames (1ba3588) tags: prune out empty tags (de0ec83) types: fix funding plan vaults type (1cea7db) types: revert prescribedObserver type (ca60f6f) Features cost-details: init cli command get-cost-details PE-7114 (674626e) cost-details: init new cost method for exposing fundingPlan and discounts PE-7114 (c6910c8) fund-from: add Fund-From tag to eligible methods/commands PE-7291 (4d47270) primary names: add processID to read APIs PE-7307 (e01e6ce) remove usage of Tags.Timestamp in favor of computing epoch indexes PE-7338 (ee1bea0) [3.0.0] (2024-12-10) View changes on GitHub Bug Fixes ar.io cli: use global program from cli.ts scope for ar.io command PE-5854 (3e83298) expose instant param for decreaseOperatorStake function arg type (2fd1f5d) lua id: change lua id (d4907db) remove un-used import (5db9ac0) spawn-ant: use a valid default ttlSeconds (aea4aa7) use Keywords for setKeywords (19ab3ad) [3.0.0] (2024-12-10) View changes on GitHub Bug Fixes ar.io cli: use global program from cli.ts scope for ar.io command PE-5854 (3e83298) expose instant param for decreaseOperatorStake function arg type (2fd1f5d) lua id: change lua id (d4907db) remove un-used import (5db9ac0) spawn-ant: use a valid default ttlSeconds (aea4aa7) use Keywords for setKeywords (19ab3ad) Features ar-io cli: init balance command and CLI setup (94c630b) ar-io cli: init join-network command (fc9dc07) ar.io cli: add --cu-url global parameter PE-5854 (2346f5b) ar.io cli: enable confirmation prompts on each write action PE-5854 (9ac88bb) ar.io cli: include --tags input in write actions PE-5854 (4b9d03e) ar.io cli: init buy/upgrade/extend-record, inc-undernames, sub-auc-bid, req-prim-name PE-5854 (5eb3df2) ar.io cli: init decrease-delegate-stake instant/cancel-withdraw commands PE-5854 (f0e7b9e) ar.io cli: init epoch read commands PE-5854 (61e0fc3) ar.io cli: init get token cost and auction prices PE-5854 (867807d) ar.io cli: init get-delegations, get-arns-record, list-arns-records commands PE-5854 (d7cbde3) ar.io cli: init get-gateway-delegates and get-gateways commands PE-5854 (35a33ef) ar.io cli: init get-vault and get-gateway commands (d262243) ar.io cli: init increase/decrease-operator-stake commands PE-5854 (1312860) ar.io cli: init info command (c721374) ar.io cli: init leave-network, delegate-stake PE-5854 (40ebe06) ar.io cli: init pagination from CLI layer PE-5854 (f52ce1f) ar.io cli: init read/write ANT commands PE-5854 (392a9ef) ar.io cli: init redelegate-stake PE-5854 (7bf4a8e) ar.io cli: init save-observations PE-5854 (f80bb8c) ar.io cli: init spawn-ant and get-ant-state PE-5854 (119c765) ar.io cli: init token-supply command (b58d782) ar.io cli: init transfer command (5553584) ar.io cli: init update-gateway-settings PE-5854 (7a6aa4b) ar.io cli: stringify outputs for command line compatibility (3c04cac) ARIO token: change all IO references to ARIO (4f8135d) ARIO token: update all IO references to ARIO (8fb2188) returned names: remove/replace auction APIs in favor returned names (2c9826f) BREAKING CHANGES ARIO token: All exported IO and IOToken are now repleced with ARIO and ARIOToken respectively PE-7225 [2.6.0] (2024-12-05) View changes on GitHub Bug Fixes lua id: bump lua id for ANT 9 (9e8e7e8) use Keywords for setKeywords (99cccd4) Features get demand factor settings: init new IO method PE-6894 (ad2eb36) init get gateway registry settings PE-6895 (bb7b6b4) [2.5.5] (2024-11-28) View changes on GitHub Bug Fixes io: update gateway delegates api, add to README (65aa6a8) [2.5.4] (2024-11-28) View changes on GitHub Bug Fixes primary: support primary name in token cost API (b4edf47) [2.5.3] (2024-11-27) View changes on GitHub Bug Fixes ant lua id: update ant lua id (54ff68b) ant: update write handler types removes evolve handler name (d9f5de4) handler names: add primary name handlers (5192c09) [2.5.2] (2024-11-25) View changes on GitHub Bug Fixes io: fix tag for requestPrimaryName API (bdaeaaf) io: updated types and fixed apis for primary name requests (a297628) [2.5.1] (2024-11-22) View changes on GitHub Bug Fixes primary names: update type for getPrimaryNameRequest (bdd3a9f) [2.5.0] (2024-11-22) View changes on GitHub Bug Fixes ant: revert breaking change on records for ANT (58db878) arns: update reserved names to pagaination api (dacf0c5) cjs: remove ant validation from cjs test (50b8290) errors: we should be checking the result.Error as well as tags (7ffe131) eslint: remove unnecessary rule config (03a0552) getHandlers: remove redundant check (b0c9548) handlers: update handler name list (251695e) id and test: add test for old ant and add lua source id for new code (77601b2) io: add getDelegations to AoIORead (7c30c9b) io: use helper for computing timestamp (ffe6ff3) lint: ignore underscore vars (2c84d3d) lint: update lint rule for ignore args (136e44a) lint: update linter to allow nullable string (b985139) lua id: rollback lua id (89b8392) primary: add additional ANT handlers for primary names (c98b136) readme: make api headers h4 (395f7fb) readme: update readme with new apis on ant class (bce76d2) readme: use real outputs in example (1529f79) setLogo: call param txId instead of logo (cda5e1d) source id: name the source id tags the same on evolve and spawn (058c829) spawn: add lua source id to spawn (8850ed2) test: remove old test for validate (14a77dc) tests: add test for old ant (0489cb6) tests: add unit tests for util and move parsing of records to uitl (2d08c9a) tests: update ANT in tests to use v8 ant (1eff8a9) types: modify AoDelegation type (18bb755) types: restructure type construction (2ef04db) validation util: remove validation util (d803e59) validator: add comments and reformat into a more clear loop for creating the validation config (ea3e70c) vaults: add API for gateway vaults (923b2cd) Features delegations: add getter for staked and vaulted delegations PE-7093 (7182942) delegations: add SDK function to retrieve an address's delegations PE-7093 (07c9107) getRecords: update getRecords to return as flat array of objects (b9808c1) io: add getAllowedDelegates to IO (7d143e0) PE-6910: support primary name APIs (6ace606) PE-6910: support primary name APIs (82a5b44) redelegate stake: init IO methods PE-7159 (7539dd2) setLogo: add set logo api to ant class (c5812b1) util: move validation util to ant class (cad7149) validation util: simplify validation util (cd57929) validations: add write validation util (69fc131) [2.4.0] (2024-11-12) View changes on GitHub Bug Fixes ant: add reassignName to ant implementation (9e705a9) auctions: fix submitAuctionApi to accept type and years (6780a80) auctions: update auction APIs and types (5fd2ccc) auctions: update read APIs to fetch auctions, use vite example display active auction (32001c2) auctions: update types and add intervalMs (bc21200) corrected AoVaultData field to be startTimestamp (b9888bf) delegates: fixes type (ae7be5c) emitter: do non strict checks on state in arns emitter (6566a3c) emitter: provide strictness in constuctor (060df05) exports: add exports to barrel file (fec094e) exports: dont export http stuff) (d6369aa) io: consolidate instantGatewayWithdrawal and instantGatewayWithdrawal to just instantWithdrawal, update `cancelWithdrawal (ea9f3eb) io: include address in delegate type for gateway (46ef1a7) lint: add lint fix and missing bracket (72446aa) PE-7080: add apis for fetching paginated delegates (e3d4af2) schema: add strict mode to ANT with default to false (4864abf) schemas: add passthrough on schema checks for ants (9cb2776) schemas: add zod schemas and tests (feba587) schema: specify HandlerNames instead of Handlers (44cc472) schemas: update ant schema and tests (f3284ed) schema: update handlers schema (6ec52e4) strict: allow for passing in strict mode on apis (e147220) tag: small tweak to instant tag (663de6f) test: correct params for get record (f999c49) tests: add esm tests and remove redundant cjs tests (95244ea) tests: add js path on imports (db1520a) tests: simplify strict check on test (62c9140) types: add back delegates for AoGateway (d337a74) types: update types to match contract (cb7d2b4) types: use generic on PageParms for sortBy, update delegate types (7a1abc4) util: create schema parsing util to pretty format errors (367537a) validations: add zod schema validations on ant returns (163c2f1) withdrawls: update API for cancelling withdrawls to allow delegate and operator withdrawls (5cb680a) Features ant: adds set-keywords and set-description methods for ants) (3b260a2) ant: support releasing of name of ANTs (16363e8) arns: add upgradeRecord API (9c1726d) auctions: add auctions api to IO classes (974897b) delegates: add instant delegate withdrawal for a fee (4b4cb8f) getVault: init IO method PE-7081 (0e3cde2) paginated vaults: init SDK paginated vaults PE-7081 (6d079f9) paginated vaults: use flat array over nested vaults PE-7081 (e17cfb7) [2.3.2] (2024-10-16) View changes on GitHub Bug Fixes io: add getDemandFactor api (feab461) io: update getTokenSupply to type that returns full breakdown of tokens (e790055) types: add totalEligibleGateways to AoEpochDistributionData type (9a35d39) types: update gateways to include services (a3fe5b4) [2.3.1] (2024-10-09) View changes on GitHub Bug Fixes use AoEpochObservationData type to match what is coming back from contract (684abf3) [2.3.0] (2024-10-08) View changes on GitHub Bug Fixes ao: check messages is not empty to avoid .length error when evaluating outputs of dryrun (a7b4953) logs: enable logging in spawn and evolve utils (08ce71a) luaID: update lua id to latest for ant source code (9c13dd3) main: merge main back to alpha, release hotfixes on alpha (9299427) types: add source code tx id to ant state type (8949f04) types: fix types on ant (3bdb3a6) types: remove restricted type (b1fac75) types: update type and tests (877b03f) types: update types (883ffb3) Features delegates: add cancel delegate withdrawal method (a3827dc) io: add api for querying get registration fees handler to AoIORead class (7b3909f) [2.2.5] (2024-09-26) View changes on GitHub Bug Fixes ant: allow sending tags on ant write interactions (99c24f8) [2.2.4] (2024-09-26) View changes on GitHub Bug Fixes types: update getInfo types on IO (7a0d20d) [2.2.3] (2024-09-25) View changes on GitHub Bug Fixes types: update type and tests (877b03f) [2.2.2] (2024-09-23) View changes on GitHub Bug Fixes deps: update arbundles to @dha-team/arbundles (c41e4e4) [2.2.1] (2024-09-16) View changes on GitHub Bug Fixes types: correct totalEpochCount for gateway stats (f82fed8) [2.2.0] (2024-08-30) View changes on GitHub Bug Fixes logger: permit logger as argument for typeguard util and default it (45df626) register: update spawn ant to register at end of spawn (4320c80) signer: add typeguard util for aoSigner (0d7f210) signing: add aosigner to contract signer (3b0495a) tests: dont send messages to ao in e2e tests (e7108da) tests: reconfigure test structure (1872a26) tests: use test-wallet fixture in tests instead of generating anew each time (27a5dc2) typeguard: return true or false in typeguard and log the error (4b851c5) types: update types for epoch distributions (5aedf50) util: use ANTRegistry class for registering ant on spawn instead of aoconnect (350112d) Features ant id: update lua ant id to latest (968c30e) util: add AoAntState typeguard util (c6f457f) [2.1.0] (2024-08-07) View changes on GitHub Bug Fixes actions: ignore engines in action (7f6f87d) ant lua id: update to version Flwio4Lr08g6s6uim6lEJNnVGD9ylvz0_aafvpiL8FI (8cbd564) ant: remove data from ant object, none of our ant methods require data attributes (0f267c1) ao: update AoProcess to only support string | undefined (584aee1) arns: update event emitter to provide more events and logs while loading arns records (8775896) constants: do not set env var for ant registry (9e61cc7) deps: move arconnect to dev deps (34f07d2) emiter: use a set to filter out duplicate (7887af9) emitter: add page size param for emitter to increase amount of records per page to 50k (b6f2157) errors: use any type on error (f14ed5a) events: use arns name space for events (1d67dfe) evolve: call eval twice to ensure evolve txid is set (a6261e5) evolve: dont double eval (a2a9121) evolve: fixed evolve somehow (b06503b) example: dont spawn in example (d1d5147) example: remove unused arweave instance (d0035c0) format: fix linting issues in format (b72dc1f) gateway stats: update gateway stat types (a59b166) io: add api that returns the total token supply (261c85c) io: no longer add data to save observations (c017b52) lint: fix lint errors and warnings (e532f4e) lua id: set new lua id in constants (e4c3aaf) naming: name AoSigner property aoSigner (4604524) records: update arns emitter to use ant registry (e55a67b) signer: describe signing function as signer vs aoSigner in case of signer type changes (3b23f80) signer: move createAoSigner to be a util (7f7a0e6) signer: pass in signing function instead of signer class (cba16e3) signer: use AoSigner type as return type (8e95edd) spawn: update spawn to use ant registry id in the tags (28dae7f) tests: check the return of ACL on ant tests more granularly (350bab1) tests: update e2e tests to only read from ant registry (a61e0bf) tests: update web test to use ANT registry in app (38ca913) tests: use const for unchanging test vars (9f965e1) test: update browser test with data test id and render checks (93741cb) test: use a known wallet adddress in tests (9dac280) todo: remove completed todo comment (c868522) types: add gateway weights to AoGateway (e725198) types: check info on evolve util first (a44cca1) types: remove deprecated types (c674876) types: update AoGateway to include weights (5368668) types: update type name to what contract returns (99edbad) use custom event names to avoid overlap (5b919ac) utils: revert new util (c959c81) utils: update util to use ant registry (b2223d4) Features ant registry: add ant registry class (2056674) evolve: add evolve util (47bfe20) signing: add window arweave wallet to available signing options (7596aec) [2.0.2] (2024-07-12) View changes on GitHub Bug Fixes types: update gateway settings type to only support observerAddress (13e073b) [2.0.1] (2024-07-11) View changes on GitHub Bug Fixes logger: fixes the console logger to respect the log level provided by web clients (99d7993) [2.0.0] (2024-07-11) View changes on GitHub Bug Fixes arweave: use default arweave in IO (21d25b9) deps: replace bunyan or console depending on the client environment (9d940aa) log: allow log level configuration for clients (9cb0981) log: replace bunyan with winston to ensure browser compatibility (80b38e0) Features io: add paginated gateway support for larger state objects (e.g. balances, records, and gateways) (b23efa8) util: add utility for fetching all records (8df2aac) io: add leaveNetwork API (54222ce) BREAKING CHANGES deps: removes all smartweave implementations using warp-sdk. The result is an only AO compatible ANT and IO network contracts. Some utilities are preserved due to their usefulness.imports: modifies web named exports to provide esm and cjs exports instead of minified bundle. The web bundle was causing issues in bundled projects, and polyfills are no longer provided by default. Refer to the README for specifications on how to use the SDK for a web project.[1.2.2] (2024-07-11) View changes on GitHub Bug Fixes api: ensure timestamps are always in miliseconds (93b162f) [1.2.1] (2024-07-04) View changes on GitHub Bug Fixes io: default the IO process to use testnet (61bca5c) [1.2.0] (2024-07-03) View changes on GitHub Bug Fixes ant: add event emitter util for fetching ants (ee5287b) ant: fix read api and update types (977e0e3) ant: handle when no data is returned (1de6610) ants: separate out interfaces (60fd593) ant: update apis to implement interface (9c54db0) ant: update interface to expect undername instead of name for ant records (416cb3d) ao ant: add handler for get state (fd20aa7) ao reads: safely parse json (1ff5410) ao: add AR-IO-SDK tag to process interaction (e5b5603) ao: add default timestamp to getTokenCost (36fed1b) ao: add getPrescribedNames for epoch api (747fad2) ao: add retries to read interactions (67d59e2) ao: fix tag for join network, update observation response (556f5d5) ao: prune tags on joinNetwork (31978f9) ao read: fix interface to have ant getState api (4e95bbd) aos: update aos module id and lua id (e19139e) ao: support connection config params in AO (3e6a246) ao: support tags for all write interactions (67f8da9) ao: update APIs for ao interface to be more descriptive (f07ac36) ao: update epoch interfaces to support various inputs (ddc4c10) ao: update send on process to use proper signer and evalute result (4e2f65d) ao: update stake interface (427e8ba) ao: use types and connect config in ao process to wrap connect from ao (05b07cf) buy: require processId on buyRecord (cc5859f) deps: add eventemitter3 dep (1d50cd1) deps: use p-limit-lit to avoid jest issues (05e0673) emitter: add a end and some console logs in the example (bc4e6b8) emmiter: rename and move throttle to be variable powered (f9cf40d) epochs: fix epoch default timestamp (ffb9df7) events: return process ids on end of fetching (15e3f44) handlers: update handler names (720b178) io: add buyRecord API (30d5e74) io: add epoch-settings api and tests (56555ea) io: add init to provide custom process (8811016) io: separate out io/ao contract interfaces (d96fa59) io: update arns interactions on registry contract (9befe2a) pLimit: add pLimit for util to avoid ao throttling (5b13560) readds incorrectly removed descriptions (c77217a) revert purchasetype tag (2dc08df) spawn: add option state contractTxID to track where init state is from (1745766) tags: make remaining tags ans-116 compliant (d034c8c) tags: use updated ans-116 tag format for actions (261b788) timeout: increase timeout period on arns emitter (b5ddb5f) type: default to unknown return type for json (0bddce0) types: add ao ant state type (02dbacd) types: update some types for arns names and contract state (2d23241) updates to use IO class and process terminology (ec45d66) util: initial implementation of get ant process for wallet (885fa31) Features ant: add balance APIs to ant interface (ec67440) ant: add utility for fetchint ant modules owned by wallet (01f7ec9) ants: support ANT apis in SDK (b187aeb) ao utils: add spawn ant util (d02566e) ao: experiment with initial implementation of ao contract (6118cea) getInfo io: add getInfo method to io class (4ef25ec) IO: implement io/ao classes that call process apis (aab8967) [1.1.1] (2024-06-06) View changes on GitHub Bug Fixes api: default evaluation options on getArNSReservedNames api (0a1f22e) [1.1.0] (2024-06-03) View changes on GitHub Bug Fixes api: make evaluation options optional on the interface (9e5a1c0) api: remove unused variable for epochBlockHeight (98c5ebc) arweave: default to arweave.net (84c9653) axios: add back axios-retry (9aae4de) errors: throw AbortError on signal aborted (63bd395) getContracts: only implement util for now (6b29c2f) gql query: don't abstract the data protocol query (f0b8f77) imports: import type from base route warp-contracts (bf99a85) init: allow signer to be undefined and if so return readable (b6a05e2) init: fix type for init to allow undefined signer (0a64ea9) init: remove unnecessary destructuring (81af1af) interface: remove epochBlockHeight from interface (b646f08) types:remove DataItem from WriteInteractionResult (eadb1a1) types: use gql node interface for dataProtocolTransaction (79cebd9) warp: ensure contract init on read interactions (bc3d1b8) Features getContracts: add get contracts on network specific providers like WarpContract (603d36e) gql util: add smartweave gql utils (5ea3aab) write: add tags support to write interactions on warp-contract and saveObservations (46eb4c9) [1.0.8] (2024-05-29) View changes on GitHub Bug Fixes api: add getPriceForInteration api to ario contract (3b8083c) bundle: minify web bundle (9266676) api: use function map for method name (439ec1f) reserved: add reserved arns name get methods (ad203ef) signer: check if method is property of signer before using (c52783c) signer: modify signer to assume the signer type based on public key being undefined (b775c96) test: add dockerfile for running tests in certain node environments (86cf2ad) [1.0.7] (2024-05-23) View changes on GitHub Bug Fixes contract: add extendLease and increaseUndernameSupport apis (1b13b5e) types: fix the AtLeastOne type (ffd0869) deps: force arweavve to 1.15.1 (2448598) contract: make params required - properties and note (89db674) types: update tests and use overwrite type to allow mIOtoken for certain paramaters (badcece) api: change to increaseUndernameLimit (9b72c1e) docs: update ario apis (4af0862) tests: update extend test util to include a test domain (e959b7c) token: add mIO and IO token classes to exports (f47f7d5) types: add delegated gateway type (c877496) types: export the token types (dfc83ae) types: remove visible types (6ab1fc3) types: update Gateway delegates type to use the new GatewayDelegate (ac7e924) warp: bump warp version (db7344d) [1.0.6] (2024-05-07) View changes on GitHub Bug Fixes warp: bump warp to fix AbortError issue on warp imports for web (c9a5613) [1.0.5] (2024-05-02) View changes on GitHub Bug Fixes cjs: provide path alias for warp in cjs export (7f9bf9a) logger: replace winston with bunyan (0488f75) util: add FQDN regex that matches ArNS contract (e6d7396) utils: manally conver from b64 to b64url to avoid web polyfill issues (766035c) utils: use base64 for fromB64url util (42302ef) warp-contract: correctly throw error in write interaction (c2368dd) [1.0.4] (2024-04-30) View changes on GitHub Bug Fixes ario: update joinNetwork to accept observerWallet param (6a32dd1) [1.0.3] (2024-04-26) View changes on GitHub Bug Fixes signer: set owner before signing data (0b558f5) [1.0.2] (2024-04-25) View changes on GitHub Bug Fixes arweave: default to the arweave node import to avoid issues with browser environments (fc8c26e) cacheurl: use default cache url in warpcontract (a676a3c) init: cleanup init overload methods and tests (fa328d2) lint: address lint issue in ArIOWriteable (4a3ee89) tsconfig: modify some tsconfig settings to get isolated configs for web/cjs/esm (46b7acc) typeguards: make type guards accept unknowns (7f285bb) types: use generic types and modify the requirements for init functions (9350f78) utils: add writeInteraction types and update base64url logic (4f5476b) [1.0.1] (2024-04-23) View changes on GitHub Bug Fixes docs: improve README docs interface documentation for ArIO clients (b0da48c) [1.0.0] (2024-04-23) Bug Fixes actions: bump node setup action (4eb49cd) actions: freeze lockfile (dba7313) contract add cache config in ario constructor (1f3c0ba) ant: add ant contract to exports (a2ff57b) ant: add signer to ant test (4581b8d) ant: default evaluation options for ant apis that do not take an… (#25) (0c8b55d) ant: default evaluation options for ant apis that do not take another parameter (7c59033) ant: default evaluation options for apis that do not require them (72b57d5) ant: fix API for getRecords (c714aa3) apis: remove epoch from distributions and observations (7b2d279) arbundle version: pin version (35ffab6) arbundles: update arbundles import (f02d83f) ario: add cache config in ario constructor (#11) (ecb279d) ario: formatting (c61570a) ario: make state provider nullable and default to remote arns-service provider (fa1cb72) ario: re-add contract default config (2296cc3) ario: remove unused cache property (7f2d02e) build: add setImmediate polyfill for web only (ad36776) build: remove redundant exported type (134319b) cache: remove cache folder (2ac9427) cacheURL: update ario cache url setting pattern to use custom url appropriately (c76e67d) cache: validate arweave id before setting it (5ba1175) casing: revert to lower case casing (b5da0ab) comments: make class logger private, remove comments (7483246) connect: add init static function on ario class to create interaction classes (765f39c) contract configuration: return cache url as well (b4a7bc3) contract functions: correct contract function names (ad9bc56) contracts: add configuration view method and update types (4fae4a2) contracts: remove write method and type from remote contract (740d8b8) contracttxid: make contractTxID require in remote state cache instance (dc82d21) contracttxid: make contractTxID required in remote state cache instance (#10) (bf651bb) ctrl flow: remove else from control flow (4b3c4c2) deps: pin arweave (d39391c) deps: remove axios-retry, will implement later (0218e95) deps: remove extra crypto-browserify (9b42898) deps: remove warp-contracts-deploy from deps (9d4f9fa) docs: remove docs folder (47e8403) drywrite: throw on bad drywrite and continue if successful (5052c0a) eslintignore: remove old file names (415c163) eslint: remove eslint comments and use this signer (32530eb) esm: add polyfills for crypto (dd8fbfe) esm: add polyfills for crypto (#27) (553822c) example web: update ario instatiation (77c6842) example: escape quotes in packagejson for example package json (fb47de0) example: simplify example and remove unused method on remote cache (81637f8) examples: update comments and fix package.json (db7140b) examples: update examples to use devnet (cc037ac) examples: update examples with records methods, and balance methods (a2d2a02) exports: add arweavesigner and arconnectsigner to exports, clean up docs (c7860ed) exports: update exports in indices (f794437) exports: update package exports to have index in src folder (2cce9e3) files: clean git cache of duplicate casing (e9eaa2d) filters: punt filters (1c23cb3) fixture: add type to arns state fixture (5bcac32) formating: format (3f30f77) gar write: fix types and flow on gar write (f5e7774) gateway: update gateway settings to support autostake (82c6840) generics: use named generic (4b647f0) gitignore: remove cache from gitignore (2867abc) git: test fix with file casing issue (c3611ee) headers: use source-version for header (2b26d88) http: add headers sdk headers to http config (94810ed) husky: add commit hooks (885ce68) imports: update to use indexed imports from warp (1242568) indentation: fix indentation in examples (a266731) interface: removed filters and added base records types (849834d) interface: rename interface to ContractCache (2a0a765) jest: remove extra config (014fbde) lint: disable no-any warning certain types (de5f108) lint: formatting (21224e2) logger, errors, http: Updated to axios and axios-retry, added winston logger, more extensive custom error objects (b944f4d) logger: remove unused logger property (9501d1d) logs: removing debug logs (f025171) mixin: filter private methods in mixin util (beb8610) naming: change epoch to epochStartHeight (908971c) naming: rename getRecord[s] to getArNSRecord[s] (bd3d4bc) overloads: only accept warp contract as a contract config for ariowritable (e3c97e9) polyfills: rollback polyfill on logger (0cdb2f0) postinstall: remove husky postinstall script (c74a135) readme: add grammar and example recs (ecc07f7) readme: condense quick start (b35e5bd) readme: refactor api list to header tags (817d99b) readme: update ant header (77235ce) readme: update ANT usage description (70c8520) readme: update joinNetwork docs (9fcf440) readme: update quick start (a60d96a) readme: update readme with default provider example (68a5a16) readme: update readme with examples (d9ee23e) record records: update key to use result instead of record (90314db) records: remove contractTxId filter remove lodash shrink readme (50669e1) records: use state endpoint to fetch records (2f02c53) recs: modify the interfaces for contracts and implement with warp and remote service (#13) (56ebb08) release: remove release assets entirely (9d5a1b3) release: update github release config to publish packages to github (5534d9d) remote: getState not properly setting evalTo in http requests (55745c1) safety: update type safety checks (32eebbc) setimmediate: make set immediate a build dependency as it is required by the node winston (9292eaa) signer: check that contract is connected before trying to write (d352e9c) signer: check that contract is connected before trying to write (#29) (536a116) signer: fix signer in WarpContracts - update tests (ea9448f) signer: fix signer in WarpContracts - update tests (#32) (16d69d8) signer: remove jwk use, ignore web example for now (bc7e577) signer: remove signer, will do in other pr (d02276d) signer: remove use of JWK, simplify constructor (#22) (d2ef573) signer: update ANT to have signer (c7f8eee) structure: update cache provider folder to be named caches (844c1aa) structure: use snake case for file and folder names (37f27d3) test warp-contract: use beforeAll to read env vars (95cc019) tests: add test cases as a const (8458185) tests: add test for custom arIO client config (0e6142b) tests: change control flow pattern to.catch instead of trycatch (883de51) tests: dont make blockHeight or sortKey undefined but rather evalTo (f76a201) tests: instantiate new ant to connect in tests (9869415) tests: remove dryWrite from writeInteraction, update tests (bc1becc) tests: remove fixture and use live service for tests (30d3e8c) tests: test 404 response (590dea6) tests: update ario test (4208bd0) tests: update client instantiation test to check read vs write clients (059653c) tests: update docker compose params (a71befd) tests: update gateways test (1fcb3e6) tests: update stubs in tests (e4bbc6e) tests: update test to match jest syntax (553bdbb) tests: update tests for named prop expectation (4ea04a7) tests: update tests to use younger contract, add evalParams config (ae890c8) tests: update tests with constants and update types (1bdcfeb) tests: update tests with new name (2cd1b5c) tests: update with new names on methods (619c193) tests: use angela for testing (10f30fe) tests: use http not https in tests (fddba1e) tests: use process vars as priority url (faab4f3) test: update test to use ArweaveTransactionID class (f6c4f8b) tsconfig, names: reverted tsconfig to nodenext resolution, changed naming convention on provider, removed extraeneous error classes, rolled back axios-retry to match our tsconfig settings (d412d44) tyeps: set types to objects rather than top level params for easier readability (edfd77b) type: rename all type implementations (5959045) types and tests: update evalTo to allow undefined sortKey and block and test that (a59f05c) types: add @ to records (53601c1) types: make props nullable on certain read apis (f8ff552) types: remove any type (5c80242) types: remove any types (d8d910b) types: remove ArweaveTransactionID type for now (3adf53b) types: remove unnecesssary empty defaults (7d14edb) types: rename signer to ContractSigner (87d6c90) types: require atleast one param to update gateway settings (857ebdc) types: update interaction type to only use read for now (2c02e90) types: update tests, readme, and types (e9985dd) types: use partial write type (fa6a638) types: use string instead of any (014a262) validate id: make validator a private method (dce4a94) validity util: isBlockheight check more strict (2b28675) warp contract: added test for getting state after connecting with warp (060ee2c) warp-contract: provide logger - update isTransaction flow ctrl - use typed props (5f6e0a1) warp-contracts: bump warp to 1.4.38 - fixed warp exports (af4a20b) winston: move the winston polyfill - this will prevent any esm based web projects from getting polyfill issues (c8b7998) write: add dry run - sync state - abortSignal - update interface (970bdef) write: update utils - change error flow - update arweave constructor props (0a81c92) write: update write methods on warp (9c0540b) yarn: update lockfile (fd5e0ee) Features ant: add ANT read interface (c941c96) ant: create ant contract class for interacting with ant contracts (6eb7ef5) ants: add readable-writable framework to the ant client and implement write methods (3019f53) ario contract: add distributions and observation apis (21e38d1) arioContract: update ArIO interface and ArIOContract interface (5d87e2e) auctions: add auctions apis (faf08c5) contract: add distribution, observations apis, update readme and examples (0208317) contract: create new contract classes that impelement both warp and remote cache for ant contract and ar-io contracts (855da2d) first issue: setup examples, readme, and initial gateways provider (5a9e232) gar methods: add gar write methods to the ario client (e01b08b) inital providers: scaffold initial providers (4949514) io transfer: add transfer api to ario writable client (0d37623) observerations: add saveObservations write interaction (8dd977c) observers: add API for fetching prescribed observers (a18e130) observers: add API for fetching prescribed observers (#17) (17ce6de) PE-5742: add records api to arns remote cache (#8) (c46cd39) PE-5751: add blockheight and sortkey eval filters (#12) (832a1ad) PE-5758: add signer to ario class (#20) (1b82077) PE-5759: observations and distributions apis (#16) (dded361) PE-5773: add auctions read apis (#18) (e0c6fca) PE-5800: add epoch apis (48ee4ba) PE-5800: epoch apis (#15) (70563b1) PE-5825: ANT read interface (#19) (6a0c477) records: add records api to arns remote cache (1b7f54f) signer: add arweave signer to ario class (7e08097) write: add write interface and base implementation on warp-contract (6dfc969)

---

# 122. Gateway Architecture - ARIO Docs

Document Number: 122
Source: https://docs.ar.io/gateways
Words: 676
Extraction Method: html

Overview Gateways are the workhorses of the AR.IO Network.
Their primary role is to act as a bridge between the Arweave network and the outside world.
This means that a gateway's main task is to make it easier for users to interact with the Arweave network by simplifying the technical processes of writing, reading, and discovering data on the blockweave in a trust-minimized fashion.Gateway functions The functions of an AR.IO gateway are broken down into the following categories:Writing data involves:Proxying base layer transaction headers to one or more healthy and active Arweave nodes (miners) to facilitate inclusion in the mempools of as many nodes as possible.Proxying chunks for base layer Arweave transactions to Arweave nodes to help facilitate storage and replication of the chunks on the blockweave.Receiving and bundling so-called bundled data items (e.g., ANS-104 spec) as base layer transactions.Reading involves retrieving:Transaction headers for a base layer Arweave transaction.Individual data chunks for a base layer Arweave transaction.Blocks from the blockweave.Storage pricing rates for data from the Arweave node network.Contiguous streams of chunks representing an entire base layer transaction.Bundled data items (e.g., ANS-104).Wallet information (e.g., token balance).Discovering data involves:Facilitating efficient, structured queries for base layer transactions, bundled data items, and wallet data by:examining incoming streams of data (i.e., directly ingested transactions and data items, blocks emitted by the chain, etc.).managing index data in a database or analogous data store.Parsing and executing user queries.Facilitating friendly-path routing via Arweave manifest indexing.Including other benefits and capabilities such as:Facilitating friendly-subdomain-name routing to Arweave transactions via a direct integration with the Arweave Name System (ArNS).Providing the modularity and configurability necessary for operating extensible gateways that can be deployed at small or large scales to meet the needs of specific applications, use cases, communities, or business models.Providing pluggable means for consuming telemetry data for internal and external monitoring and alerting.Facilitating configurable content moderation policies.Providing connectivity to a decentralized network of other AR.IO gateways, enabling data sharing and other shared workloads.AR.IO Gateway Benefits AR.IO gateways provide many new benefits and capabilities beyond general Arweave gateways:Providing the modularity and configurability necessary for operating extensible gateways that can be deployed at small or large scales to meet the needs of specific applications, use cases, communities, or business models.Providing pluggable means for consuming telemetry data for internal and external monitoring and alerting.Facilitating friendly-subdomain-name routing to Arweave transactions via a direct integration with the Arweave Name System (ArNS).Facilitating configurable content moderation policies.Providing connectivity to a decentralized network of other AR.IO gateways, enabling data sharing and other shared workloads.Gateway Modularity A design principle of AR.IO gateways is that their core components should be interchangeable with compatible implementations.The core services in the gateway are written in Typescript, with flexible interfaces to the various subsystems and databases. This allows operators to customize their gateway to meet their specific requirements. Gateway services can be turned on or off depending on the operator's needs. For example, an operator might choose to have their gateway serve data, but not actively index Layer 2 bundled data. This flexibility also allows operators to utilize the technologies that are appropriate for the scale and environments in which they operate.For example, small scale operators might want to use low-overhead relational databases to power their indexing while larger scale operators might opt to use cloud-native, horizontally scalable databases. Analogous examples for storage and caching exist as well.Gateway Tech Stack Options Topology Chain Index Bundle Index Data Index Data Store Small SQLite SQLite SQLite Local File System Large PostgreSQL Cassandra Cassandra S3 Compatible ARNS Indexing and Routing The Arweave Name System’s (ArNS) state is managed by the ARIO token’s smart contract. AR.IO gateways shall perform the following minimum functions relative to ArNS:Actively track state changes in the contract.Maintain up-to-date indexes for routing configurations based on the state of the ARIO contract as well as the states of the Arweave Name Token (ANT) contracts to which each name is affiliated.Manage the expiration of stale records.Facilitate ArNS routing based on the subdomains specified on incoming requests where appropriate.Provide a custom HTTP response header for ArNS requests indicating the corresponding Arweave transaction ID.

---

# 123. ARIO Docs

Document Number: 123
Source: https://docs.ar.io/gateways/advanced
Words: 783
Extraction Method: html

Advanced Configuration Overview The Getting Started guides for windows and linux contain all the information needed to start your AR.IO Gateway node successfully with basic configurations. There are also ever expanding advanced configuration options that allow you to run your node in a way that is customized to your specific use case.Most of the below options can be added to your .env file in order to customize its operation. Any changes made to your .env  require you to stop the docker containers running your node, and restarting them with the --build flag in order for the changes to take effect. See ENV for a complete list of environmental variables you can set.Data Storage Location You can set a custom location for your AR.IO Gateway to save the data it pulls from the Arweave network. There are three primary types of data stored, and you can set a unique storage location for each of these independently. These are "chunks data", "contiguous data", and "headers data". The custom location for each of these can be set in your.env file like this:Be sure to replace "<file path>" with the path to the location where you would like the data stored. If these values are omitted, the data will be stored in the "data" directory inside your Gateway code repository.Admin API Key HTTP endpoints under "/ar-io/admin" are protected by an admin API key. These endpoints allow you to get certain analytics data or make adjustments to your node as it's running. When your node starts, it reads your environmental variables to see if a key is set. If not, a random key is generated. The key name is ADMIN_API_KEY and it should be set in your .env file like this:ADMIN_API_KEY=SUPER_SECRET_PASSWORD View examples of the admin endpoints here Wallet Association In order to participate in the greater AR.IO network, Gateway nodes need to associate themselves with an Arweave wallet. This can be configured by setting the AR_IO_WALLET key value in your .env file.AR_IO_WALLET=1seRanklLU_1VTGowDZdD7s_-7k1qowT6oeFZHUZiZo Unbundling AR.IO Gateway nodes support unbundling and indexing ANS-104 bundle data. This is disabled by default, but can be turned on with several different configuration options. You can set these configurations with the ANS104_UNBUNDLE_FILTER and ANS104_INDEX_FILTER keys in your.env:The following types of filters are supported:Content Moderation You are able to set your Gateway to block specific transactions or data-items you don't want to serve. Unlike previous configuration options in this list, blocking content can be achieved without the need to add to your.env file and rebuild your Gateway. Instead, make a PUT request to your Gateway at /ar-io/admin/block-data. As this is an admin endpoint, you will need to have configured your ADMIN_API_KEY. Using curl as an example, the request should be formatted as follows:id (string):  This will be the transaction ID of the content you want to add to your block list.notes (string): Internal notes regarding why a particular ID is blocked.source (string): Identifier of a particular source of IDs to block. (e.g. the name of a block list) notes and source are used for documentation only, and have no effect on your block list itself.Contiguous Data Cleanup Transaction data on Arweave is stored in a chunked manner. It is commonly retrieved, however, in the the transaction data's original, contiguous form with all of its component chunks assembled end-to-end. Gateways cache contiguous representations of the transaction data to assist in various workloads, including serving transaction data to clients, allowing for efficient utilization of valuable system resources. Gateway operators will need to determine for themselves the best balance between disk space and other resource usage based on the size of their gateway and their particular use case.Contiguous data cache cleanup can be enabled using the CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD environmental variable. This variable sets the number of seconds from the creation of a file in the contiguous data cache after which that file will be deleted. For example:CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD=10000 will clear items from the contiguous data cache after ten thousand (10,000) seconds.ArNS Resolver Gateways, by default, forward requests to resolve ArNS names to arweave.dev. Starting with Release 9 gateways can instead build and maintain their own local cache. Doing so removes external dependencies and allows faster resolution.View the code for the ArNS resolver service here: https://github.com/ar-io/arns-resolver NOTE: The ArNS resolver is still an experimental feature. It is possible it may behave in unexpected ways when presented with rare edge case scenarios.In order to enable the local ArNS resolver, three environmental variables will need to be set:RUN_RESOLVER is a boolean representing an on/off switch for the local resolver.TRUSTED_ARNS_RESOLVER_TYPE sets the method the gateway uses for resolving ArNS names. Use resolver for the local resolver, or gateway for default functionality.TRUSTED_ARNS_RESOLVER_URL is the url a gateway will use to request ArNS name resolution.

---

# 124. ARIO Docs

Document Number: 124
Source: https://docs.ar.io/gateways/ar-io-node/advanced-config.html
Words: 783
Extraction Method: html

Advanced Configuration Overview The Getting Started guides for windows and linux contain all the information needed to start your AR.IO Gateway node successfully with basic configurations. There are also ever expanding advanced configuration options that allow you to run your node in a way that is customized to your specific use case.Most of the below options can be added to your .env file in order to customize its operation. Any changes made to your .env  require you to stop the docker containers running your node, and restarting them with the --build flag in order for the changes to take effect. See ENV for a complete list of environmental variables you can set.Data Storage Location You can set a custom location for your AR.IO Gateway to save the data it pulls from the Arweave network. There are three primary types of data stored, and you can set a unique storage location for each of these independently. These are "chunks data", "contiguous data", and "headers data". The custom location for each of these can be set in your.env file like this:Be sure to replace "<file path>" with the path to the location where you would like the data stored. If these values are omitted, the data will be stored in the "data" directory inside your Gateway code repository.Admin API Key HTTP endpoints under "/ar-io/admin" are protected by an admin API key. These endpoints allow you to get certain analytics data or make adjustments to your node as it's running. When your node starts, it reads your environmental variables to see if a key is set. If not, a random key is generated. The key name is ADMIN_API_KEY and it should be set in your .env file like this:ADMIN_API_KEY=SUPER_SECRET_PASSWORD View examples of the admin endpoints here Wallet Association In order to participate in the greater AR.IO network, Gateway nodes need to associate themselves with an Arweave wallet. This can be configured by setting the AR_IO_WALLET key value in your .env file.AR_IO_WALLET=1seRanklLU_1VTGowDZdD7s_-7k1qowT6oeFZHUZiZo Unbundling AR.IO Gateway nodes support unbundling and indexing ANS-104 bundle data. This is disabled by default, but can be turned on with several different configuration options. You can set these configurations with the ANS104_UNBUNDLE_FILTER and ANS104_INDEX_FILTER keys in your.env:The following types of filters are supported:Content Moderation You are able to set your Gateway to block specific transactions or data-items you don't want to serve. Unlike previous configuration options in this list, blocking content can be achieved without the need to add to your.env file and rebuild your Gateway. Instead, make a PUT request to your Gateway at /ar-io/admin/block-data. As this is an admin endpoint, you will need to have configured your ADMIN_API_KEY. Using curl as an example, the request should be formatted as follows:id (string):  This will be the transaction ID of the content you want to add to your block list.notes (string): Internal notes regarding why a particular ID is blocked.source (string): Identifier of a particular source of IDs to block. (e.g. the name of a block list) notes and source are used for documentation only, and have no effect on your block list itself.Contiguous Data Cleanup Transaction data on Arweave is stored in a chunked manner. It is commonly retrieved, however, in the the transaction data's original, contiguous form with all of its component chunks assembled end-to-end. Gateways cache contiguous representations of the transaction data to assist in various workloads, including serving transaction data to clients, allowing for efficient utilization of valuable system resources. Gateway operators will need to determine for themselves the best balance between disk space and other resource usage based on the size of their gateway and their particular use case.Contiguous data cache cleanup can be enabled using the CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD environmental variable. This variable sets the number of seconds from the creation of a file in the contiguous data cache after which that file will be deleted. For example:CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD=10000 will clear items from the contiguous data cache after ten thousand (10,000) seconds.ArNS Resolver Gateways, by default, forward requests to resolve ArNS names to arweave.dev. Starting with Release 9 gateways can instead build and maintain their own local cache. Doing so removes external dependencies and allows faster resolution.View the code for the ArNS resolver service here: https://github.com/ar-io/arns-resolver NOTE: The ArNS resolver is still an experimental feature. It is possible it may behave in unexpected ways when presented with rare edge case scenarios.In order to enable the local ArNS resolver, three environmental variables will need to be set:RUN_RESOLVER is a boolean representing an on/off switch for the local resolver.TRUSTED_ARNS_RESOLVER_TYPE sets the method the gateway uses for resolving ArNS names. Use resolver for the local resolver, or gateway for default functionality.TRUSTED_ARNS_RESOLVER_URL is the url a gateway will use to request ArNS name resolution.

---

# 125. ARIO Docs

Document Number: 125
Source: https://docs.ar.io/gateways/bundler
Words: 861
Extraction Method: html

Bundler Overview A Turbo ANS-104 data item bundler can be run alongside an AR.IO gateway. This allows gateways the ability to accept data items to be submit to the Arweave blockweave.The bundler service can be easily run inside Docker in the same way that the gateway is. It utilizes a separate docker compose file for configuration and deployment, which also allows for the use of a separate file for environmental variables specific to the bundler service. Additionally, the separation allows operators to spin their bundler service up or down at any time without affecting their core gateway service. Despite the use of separate docker compose files, the bundler service shares a docker network with the AR.IO gateway, and so is able to directly interact with the gateway service and data.Getting Started NOTE: The bundler service relies on GraphQL indexing of recently bundled and uploaded data to manage its pipeline operations. The AR.IO gateway should have its indexes synced up to Arweave's current block height before starting the bundler's service stack.Environmental Variables Environmental variables must be provided for the bundler to function and integrate properly with an existing AR.IO gateway. The gateway repository provides a .env.bundler.example file that can be renamed to .env.bundler and used as a starting point. It contains the following:BUNDLER_ARWEAVE_WALLET must be the entire jwk of an Arweave wallet's keyfile, stringified. All uploads of bundled data items to Arweave will be signed and paid for by this wallet, so it must maintain a balance of AR tokens sufficient to handle the uploads.BUNDLER_ARWEAVE_ADDRESS must be the normalized public address for the provided Arweave wallet.APP_NAME is a GraphQL tag that will be added to uploaded bundles.The remaining lines in the .env.bundler.example file control settings that allow the bundler service to share data with the AR.IO gateway. Data sharing of contiguous data between a bundler and a gateway allows the gateway to serve optimistically cached data without waiting for it to fully settle on chain.Managing Bundler Access By default, the bundler will only accept data items uploaded by data item signers whose normalized wallet addresses are in the ALLOW_LISTED_ADDRESSES list. This is an additional environmental variable that can be added to your .env.bundler file, and must be a comma separated list of normalized public wallet addresses for wallets that should be allowed to bundle and upload data through your gateway.ALLOW_LISTED_ADDRESSES=<address1>,<address2> The following permissioning configurations schemes are also possible:Scheme ALLOW_LISTED_ADDRESSES SKIP_BALANCE_CHECKS ALLOW_LISTED_SIGNATURE_TYPES PAYMENT_SERVICE_BASE_URL Allow Specific Wallets Comma-separated normalized wallet addresses false EMPTY or supplied EMPTY Allow Specific chains EMPTY or supplied false arbundles sigtype int EMPTY Allow All n/a true n/a n/a Allow None EMPTY false EMPTY EMPTY Allow Payers EMPTY or supplied false EMPTY or supplied Your payment service url Indexing Bundlers submit data to the Arweave network as an ANS-104 data item bundle. This means it is several transactions wrapped into one. A gateway will need to unbundle these transactions in order to index them. A gateway should include the following ANS-104 filters in order to unbundle and index transactions from a particular bundler:$BUNDLER_ARWEAVE_ADDRESS should be replaced with the normalized public wallet address associated with the bundler.NOTE: The above filters must be placed in the .env file for the core gateway service, not the bundler.Gateways handle data item indexing asynchronously. This means they establish a queue of items to index, and work on processing the queue in the background while the gateway continues with its normal operations. If a gateway has broad indexing filters, there can be some latency in indexing data items from the bundler while the gateway works through its queue.Optimistic Indexing Gateway operators control access to their optimistic data item indexing API via an admin key that must be supplied by all bundling clients in order for their requests to be accepted. This key should be made available in the environment configuration files for BOTH the core gateway, and the bundler, and should be provided as AR_IO_ADMIN_KEY:NOTE: If a gateway is started without providing the admin key, a random string will be generated to protect the gateway's admin endpoints. This can be reset by restarting the gateway with the admin key provided in the .env file.Starting and Stopping the Bundler Starting The bundler service is designed to run in conjunction with an AR.IO gateway, and so relies on the ar-io-network network created in Docker when the core gateway services are spun up. It is possible to spin up the bundler while the core services are down, but the network must exist in Docker.To start the bundler, specify the env and docker-compose files being used in a docker compose up command:The -d flag runs the command in "detached" mode, so it will run in the background without requiring the terminal to remain active.Stopping To spin the bundler service down, specify the docker-compose file in a docker compose down command:logs While the bundler service is running in detached mode, logs can be checked by specifying the docker-compose file in a docker compose logs command:-f runs the command in "follow" mode, so the terminal will continue to watch and display new logs.--tail= defines the number of logs to display that existed prior to running the command. 0 displays only new logs.

---

# 126. ARIO Gateway Environment Variables - ARIO Docs

Document Number: 126
Source: https://docs.ar.io/gateways/env
Words: 1239
Extraction Method: html

Environmental Variables Overview The AR.IO Gateway allows configuration customization through environmental variables. These variables dictate the gateway's behavior, from block synchronization settings to log formatting. Detailed below is a table enumerating all available environmental variables, their respective types, default values, and a brief description. Note that certain variables, such as SANDBOX_PROTOCOL, rely on others (e.g., ARNS_ROOT_HOST) to function effectively. Ensure proper understanding of these dependencies when configuring.Variables ENV Name Type Default Value Description GRAPHQL_HOST String arweave.net Host for GraphQL queries. You may use any available gateway that
supports GQL queries. If omitted, your node can support GQL queries on
locally indexed transactions, but only L1 transactions are indexed by
default.GRAPHQL_PORT Number 443 Port for GraphQL queries. Used in conjunction with GRAPHQL_HOST to set
up the proxy for GQL queries.START_HEIGHT Number or "Infinity" 0 Starting block height for node synchronization (0 = start from genesis
block) STOP_HEIGHT Number or "Infinity" "Infinity" Stop block height for node synchronization (Infinity = keep syncing
until stopped) TRUSTED_NODE_URL String " https://arweave.net " Arweave node to use for fetching data TRUSTED_GATEWAY_URL String " https://arweave.net " Arweave node to use for proxying reqeusts TRUSTED_GATEWAYS_URLS String TRUSTED_GATEWAY_URL A JSON map of gateways and priority TRUSTED_GATEWAYS_REQUEST_TIMEOUT_MS String "10000" Request timeout in milliseconds for trusted gateways TRUSTED_ARNS_GATEWAY_URL String "https:// NAME.arweave.dev" ArNS gateway WEIGHTED_PEERS_TEMPERATURE_DELTA Number 0.1 Any positive number above 0, best to keep 1 or less. Used to determine
the sensitivity of which the probability of failing or succeeding peers
decreases or increases.INSTANCE_ID String "" Adds an "INSTANCE_ID" field to output logs LOG_FORMAT String "simple" Sets the format of output logs, accepts "simple" and "json" SKIP_CACHE Boolean false If true, skips the local cache and always fetches headers from the node PORT Number 4000 AR.IO node exposed port number SIMULATED_REQUEST_FAILURE_RATE Number 0 Number from 0 to 1, representing the probability of a request failing AR_IO_WALLET String "" Arweave wallet address used for staking and rewards ADMIN_API_KEY String Generated API key used for admin API requests (if not set, it is generated and
logged into the console) ADMIN_API_KEY_FILE String Generated Alternative way to set the API key used for admin API requests via
filepath, it takes precedence over ADMIN_API_KEY if defined BACKFILL_BUNDLE_RECORDS Boolean false If true, AR.IO node will start indexing missing bundles FILTER_CHANGE_REPROCESS Boolean false If true, all indexed bundles will be reprocessed with the new filters
(you can use this when you change the filters) ON_DEMAND_RETRIEVAL_ORDER String s3,trusted-gateways,chunks,tx-data Data source retrieval order for on-demand data requests BACKGROUND_RETRIEVAL_ORDER String chunks,s3,trusted-gateways,chunks,tx-data Data source retrieval order for background data requests (i.e.,
unbundling) ANS104_UNBUNDLE_FILTER String {"never": true} Only bundles compliant with this filter will be unbundled ANS104_INDEX_FILTER String {"never": true} Only bundles compliant with this filter will be indexed ANS104_DOWNLOAD_WORKERS String 5 Sets the number of ANS-104 bundles to attempt to download in parallel ANS104_UNBUNDLE_WORKERS Number 0, or 1 if filters are set Sets the number of workers used to handle unbundling DATA_ITEM_FLUSH_COUNT_THRESHOLD Number 1000 Sets the number of new data items indexed before flushing to stable data
items MAX_FLUSH_INTERVAL_SECONDS Number 600 Sets the maximum time interval in seconds before flushing to stable data
items WRITE_ANS104_DATA_ITEM_DB_SIGNATURES Boolean false If true, the data item signatures will be written to the database WRITE_TRANSACTION_DB_SIGNATURES Boolean true If true, the transactions signatures will be written to the database ENABLE_DATA_DB_WAL_CLEANUP Boolean false If true, the data database WAL cleanup worker will be enabled ENABLE_BACKGROUND_DATA_VERIFICATION Boolean false If true, unverified data will be verified in background MAX_DATA_ITEM_QUEUE_SIZE Number 100000 Sets the maximum number of data items to queue for indexing before
skipping indexing new data items ARNS_ROOT_HOST String undefined Domain name for ArNS host SANDBOX_PROTOCOL String undefined Protocol setting in process of creating sandbox domains in ArNS
(ARNS_ROOT_HOST needs to be set for this env to have any effect) accepts
"http" or "https" START_WRITERS Boolean true If true, start indexing blocks, tx, ANS104 bundles RUN_OBSERVER Boolean true If true, run observer (ARIO processes), requires WALLET env var to be
set WALLET String N/A Wallet jwk file path for observer ario process LMDB_BLOCK_STORE_COMPRESSION String "gzip" Accepts 'gzip', 'brotli', or 'none'. Compresses new blocks with
specified algorithm before storing them in the local header store. Note:
Changing this after blocks have been stored locally will require re-sync
or remove local data to apply new compression setting to previously
stored blocks.LMDB_BUNDLE_STORE_COMPRESSION String "gzip" Accepts 'gzip', 'brotli', or 'none'. Compresses new bundles with
specified algorithm before storing them in the local bundle store. Note:
Changing this after bundles have been stored locally will require
re-indexing to apply new compression setting to previously stored
bundles.LMDB_DATA_ITEM_STORE_COMPRESSION String "gzip" Accepts 'gzip', 'brotli', or 'none'. Compresses new data items with
specified algorithm before storing them in the local data item store.
Note: Changing this after data items have been stored locally will
require re-indexing to apply new compression setting to previously
stored data items.LMDB_TX_STORE_COMPRESSION String "gzip" Accepts 'gzip', 'brotli', or 'none'. Compresses new transactions with
specified algorithm before storing them in the local transaction store.
Note: Changing this after transactions have been stored locally will
require re-sync or remove local data to apply new compression setting to
previously stored transactions.LMDB_DATA_STORE_COMPRESSION String "gzip" Accepts 'gzip', 'brotli', or 'none'. Compresses new data with specified
algorithm before storing them in the local data store. Note: Changing
this after data has been stored locally will require re-sync or remove
local data to apply new compression setting to previously stored data.CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD Number 1000 Sets the number of contiguous data items to cache before cleaning up ENABLE_FS_HEADER_CACHE_CLEANUP Boolean true If true, enable header cache cleanup for the fs cache (this will prune
headers that are older than HEADER_CACHE_CLEANUP_THRESHOLD) HEADER_CACHE_CLEANUP_THRESHOLD Number 2000 Sets the height threshold for which to clean up headers CHUNK_DATA_CACHE_CLEANUP_THRESHOLD Number 250000 Sets the number of chunks to cache before cleaning up MANIFEST_CACHE_CLEANUP_THRESHOLD Number 250000 Sets the number of data items to cache before cleaning up manifest cache ANS104_DATA_INDEX_CACHE_CLEANUP_THRESHOLD Number 50000 Sets the number of data items to cache before cleaning up ANS-104 data
index cache REDIS_CACHE_URL String undefined Redis cache URL for external caching of data items, chunks, and tx
headers REDIS_CACHE_TTL_SECONDS Number 3600 TTL in seconds for Redis cache entries AWS_S3_BUCKET String undefined AWS S3 bucket to save/retrieve block files AWS_REGION String us-east-1 AWS S3 bucket region AWS_ENDPOINT String " https://s3.amazonaws.com " AWS S3 bucket endpoint AWS_ACCESS_KEY_ID String undefined AWS S3 bucket access key AWS_SECRET_ACCESS_KEY String undefined AWS S3 secret key MIN_CONFIRMATIONS Number 10 Minimum number of confirmations needed for a transaction to be returned
by the /tx endpoint INDEX_BLOCKS Boolean true If true, the gateway will index blocks as they're synced INDEX_TX Boolean true If true, the gateway will index transactions as they're synced INDEX_DATA_ITEMS Boolean true If true, the gateway will index data items as they're synced INDEX_TX_OFFSET_LISTS Boolean true If true, the gateway will index the chunks of block data and transaction
data offsets ENABLE_MEMPOOL_WATCHER Boolean false If true, the gateway will watch the mempool for new transactions and
save the txs headers ENABLE_WEBHOOKS Boolean false If true, allows the gateway to act as a client and execute webhooks when
local state changes WEBHOOK_TARGET_SERVERS String "" Comma separated list of target webhook servers (URLs) WEBHOOK_INDEX_FILTER String {"never": true} Webhook events are emitted only if incoming transactions satisfy the
specified filter WEBHOOK_BLOCK_FILTER String {"never": true} Block webhook events are emitted only if incoming block satisfies the
specified filter PROMETHEUS_METRICS_ENABLED Boolean false If true, the gateway will expose Prometheus compatible metrics via the
/metrics endpoint NODE_ENV String development Node.js environment setting LOG_LEVEL String info Log verbosity level

---

# 127. AO Compute Unit (CU) - ARIO Docs

Document Number: 127
Source: https://docs.ar.io/gateways/cu
Words: 1013
Extraction Method: html

Overview An AO Compute Unit (CU) is a critical component in the AO ecosystem responsible for executing AO processes and maintaining their state. CUs serve as the computational backbone of the AO network by:Processing Messages: CUs receive and process messages sent to AO processes Executing WASM Modules: CUs run the WebAssembly (WASM) code that defines process behavior Maintaining State: CUs track and update the state of AO processes Creating Checkpoints: CUs periodically save process state to the Arweave network as checkpoints Running a CU alongside your gateway allows you to:Process AO requests locally rather than relying on external services Improve response times for AO-related queries Contribute computational resources to the AO network Ensure your gateway has reliable access to AO functionality For more detailed information about Compute Units, please refer to the AO Cookbook: Units.System Requirements Before deploying a CU, ensure your system meets the following requirements:Recommended: At least 16GB RAM for optimal CU operation Minimum: 4GB RAM is possible with adjusted memory limits (see resource allocation settings) At least 100GB disk space dedicated to CU operation These requirements are separate from your gateway requirements Running a CU is resource-intensive. Make sure your system has sufficient resources to handle both the gateway and the CU. While you can run a CU with less than the recommended RAM, you'll need to adjust the memory limits accordingly.Deploying an AO CU Step 1: Navigate to Gateway Directory First, navigate to the root directory of your gateway:Step 2: Configure Environment Variables Copy the example environment file:Default.env.ao.example Contents The default .env.ao.example file contains the following settings:These default settings are configured to work with a gateway running on the same machine, but you'll need to modify them as described below.Open the .env.ao file in your preferred text editor:Configure the following settings:CU_WALLET: Replace '[wallet json here]' with the JSON from an Arweave wallet.The entire JSON must be placed on a single line for proper registration.PROCESS_CHECKPOINT_TRUSTED_OWNERS: This is a comma-separated list of trusted wallet addresses:PROCESS_CHECKPOINT_TRUSTED_OWNERS=fcoN_xJeisVsPXA-trzVAuIiqO3ydLQxM-L4XbrQKzY Adding Your Own Wallet If you are uploading your own checkpoints, you should add your own CU wallet address after the default value, separated by a comma:PROCESS_CHECKPOINT_TRUSTED_OWNERS=fcoN_xJeisVsPXA-trzVAuIiqO3ydLQxM-L4XbrQKzY,YOUR_WALLET_ADDRESS_HERE This allows your CU to trust checkpoints from both the official source and your own wallet.GATEWAY_URL: By default, this is set to use your own gateway:GATEWAY_URL=http://envoy:3000 A gateway must be set to index all ANS-104 data items from AO or the CU will not operate properly. Most users will want to set this to:GATEWAY_URL=https://arweave.net UPLOADER_URL: By default, this is set to use a bundler sidecar run by your gateway:UPLOADER_URL=http://envoy:3000/bundler Important: Checkpoint Uploads Require Payment Checkpoints are uploaded to Arweave, so the upload must be paid for. You must ensure your wallet has sufficient funds:If using https://up.arweave.net (recommended), your CU_WALLET must contain Turbo Credits If using your own bundler or another service, you'll need the appropriate token (AR or other) Without proper funding, checkpoints will fail to upload and your CU may not function correctly The simplest option for most users is to use:UPLOADER_URL=https://up.arweave.net This requires your CU_WALLET to contain Turbo Credits.Optional: Disable Checkpoint Creation: If you want to disable checkpoint uploads, add:DISABLE_PROCESS_CHECKPOINT_CREATION=true Example of a Completed.env.ao File Here's an example of what your completed .env.ao file might look like with common settings:After making your changes, save and exit the nano editor:Press Ctrl+X to exit Press Y to confirm saving changes Press Enter to confirm the filename Optional Resource Allocation Settings You can fine-tune the CU's resource usage by adding these optional environment variables:PROCESS_WASM_MEMORY_MAX_LIMIT: Sets the maximum memory limit (in bytes) for WASM processes.Important Memory Requirement To work with the AR.IO process, PROCESS_WASM_MEMORY_MAX_LIMIT must be at least 17179869184 (16GB).Note: This doesn't mean your server needs 16GB of RAM. This is the maximum memory limit the CU will support for processes. Most processes don't use their maximum allocated memory.You can set this value to 16GB even if your server only has 4GB of RAM. However, if a process requires more memory than your server has available, the CU will fail when evaluating messages that need more memory.WASM_EVALUATION_MAX_WORKERS: Sets the maximum number of worker threads for WASM evaluation.Worker Thread Configuration This will default to (available CPUs - 1) if not specified. If you're running a gateway and unbundling on the same server, consider setting this to 2 or less to avoid overloading your CPU.PROCESS_WASM_COMPUTE_MAX_LIMIT: The maximum Compute-Limit, in bytes, supported for ao processes (defaults to 9 billion) PROCESS_WASM_COMPUTE_MAX_LIMIT=9000000000 NODE_OPTIONS: Sets Node.js memory allocation for the Docker container.Resource Tuning Start with conservative values and monitor performance. You can adjust these settings based on your system's capabilities and the CU's performance.Step 3: Start the CU Container Once your environment file is configured, start the CU container:This command uses the following flags:--env-file .env.ao: Specifies the environment file to use -f docker-compose.ao.yaml: Specifies the Docker Compose file to use up: Creates and starts the containers -d: Runs containers in detached mode (background) Step 4: Check the Logs To check the logs of your CU container:This command uses the following flags:-f: Follows the log output (continuous display) --tail=20: Shows only the last 20 lines of logs Exit the logs by pressing Ctrl+C.Connecting Your Gateway to the CU To make your gateway use your local CU:Add the following line to your gateway's .env file:AO_CU_URL=http://ao-cu:6363 This assumes the CU is running on the same machine as the gateway.Restart your gateway:Accessing Your CU Once properly set up and connected to your gateway, you can access your CU via:https://<your-gateway-domain>/ao/cu This endpoint allows you to interact with your CU directly through your gateway's domain.Important Notes Initial Processing Time: A CU will need to process AO history before it can give valid responses. This process can take several hours.Gateway Fallback: A gateway on release 27 or above will fallback to arweave.net if its default CU is not responding quickly enough, so gateway operations will not be significantly impacted during the initial processing.Monitoring Progress: Check the CU logs after pointing a gateway at it to watch the process of working through AO history:Resource Usage: Running a CU is resource-intensive. Monitor your system's performance to ensure it can handle both the gateway and CU workloads.

---

# 128. ARIO Node Filtering System - ARIO Docs

Document Number: 128
Source: https://docs.ar.io/gateways/filters
Words: 541
Extraction Method: html

The AR.IO Node filtering system provides a flexible way to match and filter items based on various criteria. The system is built around JSON-based filter definitions that can be combined to create both simple and complex matching patterns.Unbundling and Indexing Filters When processing bundles, the AR.IO Node applies two filters obtained from environment variables:The ANS104_UNBUNDLE_FILTER determines which base layer transactions and data items, in the case of bundles nested in other bundles, are processed, and the ANS104_INDEX_FILTER determines which data items within the processed bundles are indexed for querying.Webhook Filters There are also two filters available that are used to trigger webhooks. When a transaction is processed that matches one of the webhook filters, the gateway will send a webhook to the specified WEBHOOK_TARGET_SERVERS urls containing the transaction data.The WEBHOOK_INDEX_FILTER is used to trigger a webhook when a transaction is indexed. The WEBHOOK_BLOCK_FILTER is used to trigger a webhook when a block is processed.Important Notes All tag names and values are base64url-decoded before matching Owner addresses are automatically converted from owner public keys Empty or undefined filters default to "never match" Tag matching requires all specified tags to match Attribute matching requires all specified attributes to match The filter system supports nested logical operations to any depth, allowing for very precise control over what data gets processed All these filters can be used in various contexts within the AR.IO Node, such as configuring webhook triggers, controlling ANS-104 bundle processing, or setting up data indexing rules. The filtering system is designed to be intuitive yet powerful, allowing for precise control over which items get processed while maintaining readable and maintainable filter definitions.Filter Construction.env formatting While the filters below are displayed on multiple lines for readability, they must be stored in the .env file as a single line for proper processing.Basic Filters The simplest filters you can use "always" and "never" filters. The "never" filter is the default behavior and will match nothing, while the "always" filter matches everything.Tag Filters Tag filters allow you to match items based on their tags in three different ways. You can match exact tag values, check for the presence of a tag regardless of its value, or match tags whose values start with specific text. All tag values are automatically base64url-decoded before matching.Attribute Filters Attribute filtering allows you to match items based on their metadata properties. The system automatically handles owner public key to address conversion, making it easy to filter by owner address. You can combine multiple attributes in a single filter:Nested Bundle Filter The isNestedBundle filter is a specialized filter that checks whether a data item is part of a nested bundle structure. It's particularly useful when you need to identify or process data items in bundles that are contained within other bundles. The filter checks for the presence of a parent_id field in the item.Note: When processing nested bundles, be sure to include filters that match the nested bundles in both ANS104_UNBUNDLE_FILTER and ANS104_INDEX_FILTER. The bundle data items (nested bundles) need to be indexed to be matched by the unbundle filter.Complex Filters Using Logical Operators For more complex scenarios, the system provides logical operators (AND, OR, NOT) that can be combined to create sophisticated filtering patterns. These operators can be nested to any depth:

---

# 129. ARIO Gateway Grafana - ARIO Docs

Document Number: 129
Source: https://docs.ar.io/gateways/grafana
Words: 695
Extraction Method: html

Grafana Analytics Overview AR.IO gateways track a significant number of performance and operation metrics using Prometheus.
A Grafana sidecar can be deployed to visualize these metrics, and provide an easy way to monitor the health of the gateway.
The Grafana sidecar is deployed as a separate docker container that uses the same network as the gateway, and is deployed in a similar manner.Deploying Grafana The file that controls the deployment of the Grafana sidecar is docker-compose.grafana.yaml. So to deploy Grafana, run the following command:The -f flag is used to specify the path to the docker-compose file, and the up -d flag is used to deploy the container in detached mode.Terminal Location This command assumes that you are running the command from the root directory of the gateway. If you are running the command from a different directory, you will need to adjust the path to the docker-compose file.Checking the logs To check the logs of the Grafana sidecar, run the following command:The -f flag is used to follow the logs, and the --tail=25 flag is used to specify the number of lines to show from the end of the logs, in this case 25.Exit the logs by pressing Ctrl+C.In some cases, the Grafana sidecar may encounter permission errors. There are two primary solutions to this issue:Modify Directory Permissions The simplest solution is to modify the permissions of the directory that contains the Grafana data.This will give the grafana user ownership of the directory and all its contents.Terminal Location This command assumes that you are running the command from the root directory of the gateway. If you are running the command from a different directory, you will need to adjust the path to the docker-compose file.Check the logs again to ensure that the issue is resolved.Change the Grafana User The second solution is to change the user that Grafana runs as. This can be done by modifying the docker-compose.grafana.yaml file to use a different user. It is suggested to use "root" or "0" to ensure that the container has the necessary permissions.In any editor, open the docker-compose.grafana.yaml file and add "user: root" to the grafana service.Once this is done, restart the Grafana sidecar by running the following command:Check the logs again to ensure that the issue is resolved.Configure Nginx The Grafana sidecar is deployed on the same network as the gateway, and can be accessed in a browser by navigating to http://localhost:1024 from the machine running the gateway.
In order to be able to access Grafana from outside the network running the gateway, Nginx, which is already used to route gateway traffic, can be configured to route Grafana traffic to the correct port.In any editor, open the relevant Nginx configuration file. If the setup guide configuration was used, that file will be located at /etc/nginx/sites-available/default.Add the following block to the configuration file inside the server block for https (443) traffic:The full configuration file should look like this:Be sure to replace <domain> with the domain of the gateway.Once the configuration is saved, test the configuration by running the following command:This will print out a message indicating that the configuration is valid.Then, restart Nginx by running the following command:Once this is done, Grafana can be accessed by navigating to https://<domain>/grafana in a browser.Accessing Grafana To access Grafana, navigate to https://<domain>/grafana in a browser.The default credentials are:Username: admin Password: admin Once logged in for the first time, you will be prompted to change the password.Credential Reset Updated credentials may be lost if the Grafana sidecar is restarted. Be sure to log into Grafana immediately after every start up to ensure Grafana cannot be accessed with the default credentials.Dashboards The Grafana sidecar comes preloaded with three dashboards:ar-io-node: Contains general gateway metrics, like the last block indexed, ArNS resolution times, and CPU usage.ar-io-node bundle indexing: Contains metrics related to bundle indexing, like the number of bundles and data items indexed.ar-io-node queue lengths: Contains metrics related to the queue lengths of the gateway, like Arweave Client requests and transaction importer data.Additional dashboards can be added in order to monitor different aspects of the gateway.The Grafana landing page contains tutorials for how to configure dashboards, as well as additional features such as alerting.

---

# 130. Gateway Network - ARIO Docs

Document Number: 130
Source: https://docs.ar.io/gateways/gateway-network
Words: 651
Extraction Method: html

Overview The AR.IO Network consists of AR.IO gateway nodes, which are identified by their registered Arweave wallet addresses and either their IP addresses or hostnames, as stored in the network's smart contract Gateway Address Registry (GAR).These nodes adhere to the AR.IO Network’s protocols, creating a collaborative environment of gateway nodes that vary in scale and specialization.
The network promotes a fundamental level of service quality and trust minimization among its participants.Being part of the network grants AR.IO gateways an array of advantages, such as:Simplified advertising of services and discovery by end users via the Gateway Address Registry.More rapid bootstrapping of key gateway operational data due to prioritized data request fulfillment among gateways joined to the network.Sharing of data processing results.Auditability and transparency through the use of AGPL-3 licenses, which mandate public disclosure of any software changes, thereby reinforcing the network's integrity and reliability.Improved network reliability and performance through an incentive protocol, which uses a system of evaluations and rewards to encourage high-quality service from gateways.Eligibility to accept delegated staking improving a gateway’s discoverability and reward opportunities.Gateway Address Registry (GAR) Any gateway operator that wishes to join the AR.IO Network must register their node in the AR.IO smart contract’s “Gateway Address Registry”, known as the GAR.
Registration involves staking a minimum amount of ARIO tokens and providing additional metadata describing the gateway service offered.After joining the network, the operator’s gateway can be easily discovered by permaweb apps, its health can be observed, and it can participate in data sharing protocols.
A gateway becomes eligible to participate in the network’s incentive protocol in the epoch following the one they joined in.The GAR advertises the specific attributes of each gateway including its stake, delegates, settings and services.
This enables permaweb apps and users to discover which gateways are currently available and meet their needs.
Apps that read the GAR can sort and filter it using the gateway metadata, for example, ranking gateways with the highest stake, reward performance, or feature set at the top of the list.
This would allow users to prefer the higher staked, more rewarded gateways with certain capabilities over lower staked, less rewarded gateways.Data Sharing A key advantage and incentive for networked AR.IO gateways over standalone gateways is their ability to preferentially share various kinds of Arweave data among one another.
Each gateway advertises its registered Arweave wallet address, so other network participants know who they are.Gateways can identify AR.IO Network peers by evaluating the Gateway Address Registry (GAR) within the AR.IO smart contract.
They utilize that peer list to request as-yet-uncached data on behalf of their requesting clients or in service of their internal workflows.
This can include requests for transaction header data, data items, and chunks. The Arweave Network shall act as the backstop for all block data, transaction header data, and chunk data.Additionally, gateways that receive requests for cache-missed data from other gateways can provide a higher quality of service to other AR.IO gateways than that which is provided to general users, apps, and infrastructure.
However, gateways are not forced to share data with one another and can choose not to share their data if the intended recipient is acting maliciously.
Such behaviors might include failure to reciprocate in data sharing, engaging in dishonest activities / observation, or distributing invalid data.Data Verification Gateway data verification is achieved by linking content hashes of transactions and data items to data roots on the Arweave base layer chain.
Gateways index the chain from a trusted Arweave node and compute data roots for the base layer transaction data they download, ensuring that their data aligns with what was originally uploaded to Arweave.
For base layer bundles that have already been verified, gateways compute hashes of individual data items, establishing a connection between the data root, the verified bundle, and the data items it contains.
Gateways then expose these hashes and their verification status to users via HTTP headers on data responses.

---

# 131. ARIO Docs

Document Number: 131
Source: https://docs.ar.io/gateways/linux-setup
Words: 1278
Extraction Method: html

Linux Installation Instructions Overview The following instructions will guide you through the process of installing the AR.IO node on a Linux machine, specifically Ubuntu 22.04.3 desktop on a home computer. Actual steps may differ slightly on different versions or distributions. This guide will cover how to set up your node, point a domain name to your home network, and create an nginx server for routing traffic to your node. No prior coding experience is required.System Requirements Please note, The AR.IO Node software is still in development and testing, all system requirements are subject to change.External storage devices should be formatted as ext4.Minimum requirements The hardware specifications listed below represent the minimum system requirements at which the AR.IO Node has been tested. While your Node may still operate on systems with lesser specifications, please note that AR.IO cannot guarantee performance or functionality under those conditions. Use below-minimum hardware at your own risk.4 core CPU 4 GB Ram 500 GB storage (SSD recommended) Stable 50 Mbps internet connection Recommended 12 core CPU 32 GB Ram 2 TB SSD storage Stable 1 Gbps internet connection Install Packages If you would like to quickly install all required and suggested packages, you can run the following 4 commands in your terminal, and skip to installing the Node.Required packages Update your software:Enable your firewall and open necessary ports:Install nginx:Install git:Install Docker:Test Docker installation:Install Certbot:Suggested packages These packages are not required to run a node in its basic form. However, they will become necessary for more advanced usage or customization.Install ssh (optional, for remote access to your Linux machine):Install NVM (Node Version Manager):Install Node.js:Install Yarn:Install build tools:Install SQLite:Install the Node Navigate to the desired installation location:NOTE: Your indexing databases will be created in the project directory unless otherwise specified in your.env file, not your Docker environment. So, if you are using an external hard drive, you should install the node directly to that external drive.Clone the ar-io-node repository and navigate into it:Create an environment file:Paste the following content into the new file, replacing <your-domain> with the domain address you are using to access the node, and <your-public-wallet-address> with the public address of your Arweave wallet, save, and exit:The GRAPHQL values set the proxy for GQL queries to arweave.net, You may use any available gateway that supports GQL queries. If omitted, your node can support GQL queries on locally indexed transactions, but only L1 transactions are indexed by default.START_HEIGHT is an optional line. It sets the block number where your node will start downloading and indexing transactions headers. Omitting this line will begin indexing at block 0.RUN_OBSERVER turns on the Observer to generate Network Compliance Reports. This is required for full participation in the AR.IO Network. Set to false to run your gateway without Observer.ARNS_ROOT_HOST sets the starting point for resolving ARNS names, which are accessed as a subdomain of a gateway. It should be set to the url you are pointing to your node, excluding any protocol prefix. For example, use node-ar.io and not https://node-ar.io. If you are using a subdomain to access your node and do not set this value, the node will not understand incoming requests.AR_IO_WALLET is optional, and sets the wallet you want associated with your Gateway. An associated wallet is required to join the AR.IO network.OBSERVER_WALLET is the public address of the wallet used to sign Observer transactions. This is required for Observer to run, but may be omitted if you are running a gateway outside of the AR.IO network and do not plan to run Observer. You will need to supply the keyfile to this wallet in the next step.More advanced configuration options can be found at ar.io/docs Supply Your Observer Wallet Keyfile:If you are running Observer, you need to provide a wallet keyfile in order to sign report upload transactions. The keyfile must be saved in the wallets directory in the root of the repository. Name the file <Observer-Wallet-Address>.json, replacing "<Observer-Wallet-Address>" with the public address of the wallet. This should match your OBSERVER_WALLET environmental variable.Learn more about creating Arweave wallets and obtaining keyfiles here Payment For Observer Report Uploads By default, the Observer will use Turbo Credits to pay for uploading reports to Arweave. This allows reports under 100kb to be uploaded for free, but larger reports will fail if the Observer wallet does not contain Credits. Including REPORT_DATA_SINK=arweave in your .env file will configure the Observer to use AR tokens instead of Turbo Credits, without any free limit.Start the Docker container:Explanation of flags:up: Start the Docker containers.-d: Run the containers as background processes (detached mode).NOTE: Effective with Release #3, it is no longer required to include the --build flag when starting your gateway. Docker will automatically build using the image specified in the docker-compose.yaml file.To ensure your node is running correctly, follow the next two steps.Check the logs for errors:Explanation of flags:-f: Follow the logs in real time.--tail=0: Ignore all logs from before running the command.NOTE: Previous versions of these instructions advised checking a gateway's ability to fetch content using localhost. Subsequent security updates prevent this without first unsetting ARNS_ROOT_HOST in your .env.Set up Networking The following guide assumes you are running your node on a local home computer.Register a Domain Name:
Choose a domain registrar (e.g., Namecheap) to register a domain name.Point the Domain at Your Home Internet:Obtain your public IP address by visiting https://www.whatsmyip.org/ or running:Create an A record with your registrar for your domain and wildcard subdomains, using your public IP address. For example, if your domain is "ar.io," create a record for "ar.io" and "*.ar.io." Set up Port Forwarding:Obtain the local IP address of the machine where the node is installed by running:If there are multiple lines of output, choose the one starting with 192 (usually).Enter your router's IP address in the address bar of a browser (e.g., 192.168.0.1).If you're unsure of your router's IP address, consult your router's documentation or contact your Internet Service Provider (ISP).Navigate to the port forwarding settings in your router configuration.The exact steps may vary depending on your router model. Consult your router's documentation or support for detailed steps.Set up port forwarding rules to forward incoming traffic on ports 80 (HTTP) and 443 (HTTPS) to the same ports on the machine running your node. You may also forward port 22 if you want to enable SSH access to your node from outside your home network.Create SSL (HTTPS) Certificates for Your Domain:Follow the instructions to create the required TXT records for your domain in your chosen registrar. Use a DNS checker to verify the propagation of each record.Email Notifications Previous versions of these instructions advised providing an email address to Certbot. As of June 2025, LetsEncrypt (the certificate authority used by Certbot) no longer supports email notifications.IMPORTANT: Wild card subdomain (*.<your-domain>.com) cannot auto renew without obtaining an API key from your domain registrar. Not all registrars offer this. Certbot certificates expire every 90 days. Be sure to consult with your chosen registrar to see if they offer an API for this purpose, or run the above command again to renew your certificates. You will receive an email warning at the address you provided to remind you when it is time to renew.Configure nginx:
nginx is a free and open-source web server and reverse proxy server. It will handle incoming traffic, provide SSL certificates, and redirect the traffic to your node.Open the default configuration file:Replace the file's contents with the following configuration (replace "<your-domain>" when necessary):Save and exit nano.Test the configuration:If there are no errors, restart nginx:Your node should now be running and connected to the internet. Test it by entering https://<your-domain>/3lyxgbgEvqNSvJrTX2J7CfRychUD5KClFhhVLyTPNCQ in your browser.Note: If you encounter any issues during the installation process, please seek assistance from the AR.IO community.

---

# 132. Join the Gateway Network - ARIO Docs

Document Number: 132
Source: https://docs.ar.io/gateways/join-network
Words: 469
Extraction Method: html

Join the AR.IO Network Prerequisites Must have a fully functional AR.IO gateway.This includes the ability to resolve ArNS subdomains.Follow installation instructions for windows or linux and get help from the ar.io community.Gateway must be associated with an Arweave Wallet.Learn about creating Arweave wallets here Arweave wallet must be funded with enough ARIO tokens to meet the minimum stake for gateway operators.Joining Via Network Portal The simplest method for joining a new gateway to the Gateway Address Registry (GAR) is to use the Network Portal   .The Network portal has a prominent "Start your own gateway" button That will open a form where configurations can be set for your gateway in the network. Start Your Gateway  Start Gateway Form Start Gateway Form The form is used to set basic configurations for a gateway when joining the network. It contains the following fields:Label: This is a friendly name for a gateway. It can be a maximum of 64 characters.Address: This is the fully qualified domain name of the gateway. That is, the standard web address used to access the gateway. i.e. arweave.net. The form prefills the https:// protocol prefix, and www should not be included. Gateways DO support using subdomains as their address, so long as the gateway is properly configured.Observer Wallet: This is the public wallet address of the wallet used for the gateway's observer. By default, the primary gateway wallet address is filled in this space; however, a different wallet may be utilized if desired for operational reasons.Properties ID: This is an Arweave Transaction Id for a JSON object that contains additional details about the gateway. The gateway network has not yet incorporated these properties into standard gateway participation, and so the space may safely be left as the default value. The contents of the default properties Id can be viewed here    Stake: This is the amount of ARIO tokens to be staked to the gateway. It must be at least the network minimum.Delegated Stake: This toggle enables or disables delegated staking on a gateway. This may be changed later.Minimum Delegated Stake: This is the minimum number of ARIO tokens that a delegate must stake in order to stake to a gateway. The network minimum is 10 ARIO.Reward Share Ratio: The percentage of gateway rewards that will be distributed to delegated stakers.Note: A description of the gateway. It can be a maximum of 256 characters.Once all required fields of the form are completed, the "Confirm" button will become available. Clicking this will prompt a signature from the connected Arweave wallet in order to complete the joining process.Joining Programmatically Joining the network can also be completed programmatically through the AR.IO SDK. This is done using the join-network method on the ARIO class.The method must be called after authenticating the ARIO class using the wallet to be associated with the new gateway.

---

# 133. Observation and Incentives - ARIO Docs

Document Number: 133
Source: https://docs.ar.io/gateways/observer
Words: 1721
Extraction Method: html

Observation and Incentives (OIP) Overview The Observation and Incentive Protocol is designed to maintain and enhance the operational integrity of gateways on the AR.IO Network.
It achieves this through a combination of incentivizing gateways for good performance and tasking those gateways to fulfill the role of "observers".
The protocol is intentionally simple and adaptable, employing a smart contract-based method for onchain “voting” to assess peer performance while being flexible on how that performance is measured.
This setup permits gateway and observer nodes to experiment and evolve best practices for performance evaluation, all while operating within the bounds of the network's immutable smart contract, thus eliminating the need for frequent contract updates (forks).In this protocol, observers evaluate their gateway peers' performance to resolve ArNS names.
Their aim is to ensure each gateway in the network accurately resolves a subset of names and assigning a pass / fail score based on their findings.A key component of the protocol is its reward mechanism.
This system is predicated on gateway performance and compliance with observation duties.
Gateways that excel are tagged as "Functional Gateways" and earn rewards, while those that do not meet the criteria, “Deficient Gateways” risk facing penalties – namely, the lack of rewards.Funds for incentive rewards are derived from the protocol balance, which consists of ARIO tokens initially allocated at network genesis as well as those collected from ArNS asset purchases.
Every epoch, this balance is utilized to distribute rewards to qualifying gateways and observers based on certain performance metrics.Observation Protocol The Observation protocol is organized around daily epochs, periods of time that are broken into an observation reporting and tallying phase.
The protocol is followed across each epoch, promoting consistent healthy network activity that can form pro-social behaviors and react to malicious circumstances.Onchain Reports The to-be-evaluated ArNS names include a set of two (2) names randomly determined by the protocol, known as “prescribed names”, which are common across all observers within the epoch, as well as a set of eight (8) “chosen names” picked at the discretion of each individual observer.
“Prescribed names” are assigned to act as a common denominator / baseline while “chosen names” allow each observer to evaluate names that may be important to their operation.Observers shall upload their completed reports (in JSON format) to the Arweave network as an onchain audit trail.
In addition, observers shall submit an interaction to the AR.IO smart contract detailing each gateway that they observed to have “failed” their assessments.
These “votes” are tallied and used to determine the reward distribution.Selection of Observers The observer selection process commences at the beginning of each epoch and employs a random-weighted selection method.
By combining random selection with weighted criteria like stake, tenure, and past rewards, the process aims to ensure both fairness and acknowledgment of consistent performance.
This method allows for a systematic yet randomized approach to selecting gateways for observation tasks.Criteria for Selection Up to fifty (50) gateways can be chosen as observers per epoch.
If the GAR is below that amount, then every gateway is designated as an observer for that epoch.
If there are greater than 50, then randomized selection shall be utilized.The weighted selection criteria will consider the following for each gateway:Stake Weight (SW): This factor considers how financially committed a gateway is to the network. It is the ratio of the total amount of ARIO tokens staked by the gateway (plus any delegated stake) relative to the network minimum and is expressed as:SW = (Gateway Stake + Delegated Stake) / (Minimum Network Join Stake) Tenure Weight (TW): This factor considers how long a gateway has been part of the network, with a maximum value capped at four (4). This means that the maximum value is achieved after 2-years of participation in the network. It is calculated as:TW = (Gateway Network Tenure) / (6-months) Gateway Performance Ratio Weight (GPRW): This factor is a proxy for a gateway’s performance at resolving ArNS names. The weight represents the ratio of epochs in which a gateway received rewards for correctly resolving names relative to their total time on the network. To prevent division by zero conditions, it is calculated as:GPRW = (1 + Passed Epochs) / (1 + Participated Epochs) Observer Performance Ratio Weight (OPRW): This factor is a proxy for a gateway’s performance at fulfilling observation duties. The weight reflects the ratio of epochs in which a gateway, as an observer, successfully submitted observation reports relative to their total periods of service as an observer. To prevent division by zero conditions thus unfairly harming a newly joined gateway, it is calculated as:OPRW = (1 + Submitted Epochs) / (1 + Selected Epochs) Weight Calculation and Normalization For each gateway, a composite weight (CW) is computed, combining the Stake Weight, Tenure Weight, Gateway Performance Ratio Weight, and Observer Performance Ratio Weight.The formula used is:CW = SW x TW x GPRW x OPRW These weights are then normalized across the network to create a continuous range, allowing for proportional random selection based on the weighted scores.
The normalized composite weight (N_CW) for each gateway indicates its likelihood of being chosen as an observer and is calculated by dividing the gateway's CW by the sum of all CWs.
Any gateway with a composite weight equal to zero shall be ineligible for selection as an observer during the associated epoch.Random Selection Process The selection of observers is randomized within the framework of these weights.
A set of unique random numbers is generated with entropy within the total range of normalized weights.
For each random number, the gateway whose normalized weight range encompasses this number is selected.
This system ensures that while gateways with higher weights are more likely to be chosen, all gateways maintain a non-zero chance of selection, preserving both fairness and meritocracy in the observer assignment process.
The current epoch’s selected / prescribed observers as well as prescribed ArNS names to be evaluated shall be saved in the contract state at the beginning of the epoch to ensure that any activities during that epoch do not affect the selection of observers or awards distribution.Performance Evaluation Consider the following classifications:Functional or Passed Gateways: are gateways that meet or surpass the network’s performance and quality standards.Deficient or Failed Gateways: are gateways that fall short of the network's performance expectations.Functional or Submitted Observers: are selected observers who diligently perform their duties and submit observation reports and contract interactions.Deficient or Failed Observers: are selected observers who do not fulfill their duty of submitting observation reports and contract interactions.At the end of an epoch, the smart contract will assess the results from the observers and determine a pass / fail score for each gateway:If greater than or equal to 50% of submitted observer contract interactions indicate a PASS score, then that gateway is considered Functional and eligible for gateway rewards.Else, if greater than 50% of submitted observer contract interactions indicate a FAIL score, then that gateway is considered Deficient and ineligible for gateway rewards.These results will determine how reward distributions are made for that epoch.
Rewards shall be distributed after forty (40) minutes (approx. twenty (20) Arweave blocks) in the following epoch have elapsed.
This delay ensures that all observation contract interactions are safely confirmed by the Arweave network without risk of “forking out” prior to the evaluation and reward distribution process.Reward Distribution Each epoch, a portion of the protocol balance is earmarked for distribution as rewards.
This value shall begin at 0.1% per epoch for the first year of operation, then linearly decline down to and stabilize at 0.05% over the following 6 months.
From this allocation, two distinct reward categories are derived:Base Gateway Reward (BGR): This is the portion of the reward allocated to each Functional Gateway within the network and is calculated as:BGR = [Epoch Reward Allocation x 90% / Total Gateways in the Network] Base Observer Reward (BOR): Observers, due to their additional responsibilities, have a separate reward calculated as:BOR = [Epoch Reward Allocation x 10% / Total Selected Observers for the Epoch] Distribution Based on Performance The reward distribution is contingent on the performance classifications derived from the Performance Evaluation:Functional Gateways: Gateways that meet the performance criteria receive the Base Gateway Reward.Deficient Gateways: Gateways falling short in performance do not receive any gateway rewards.Functional Observers: Observers that fulfilled their duty receive the Base Observer Reward.Deficient Observers: Observers failing to meet their responsibilities do not receive observer rewards. Furthermore, if they are also Functional Gateways, their gateway reward is reduced by 25% for that epoch as a consequence for not performing their observation duty.Gateways shall be given the option to have their reward tokens “auto-staked” to their existing stake or sent to their wallet as unlocked tokens. The default setting shall be “auto-staked”.Distribution to Delegates The protocol will automatically distribute a Functional Gateway’s shared rewards with its delegates.
The distribution will consider the gateway’s total reward for the period (including observation rewards), the gateway’s “Delegate Reward Share Ratio”, and each delegate’s stake proportional to the total delegation.
Each individual delegate reward is calculated as:Unlike gateways, token reward distributions to delegated stakers will only be “auto-staked” in that they will be automatically added to the delegate’s existing stake associated with the rewarded gateway.
The delegated staker is then free to withdraw their staked rewards at any time (subject to withdrawal delays).Undistributed Rewards In cases where rewards are not distributed, either due to the inactivity or deficiency of gateways or observers, the allocated tokens shall remain in the protocol balance and carry forward to the next epoch.
This mechanism is in place to discourage observers from frivolously marking their peers as offline in hopes of attaining a higher portion of the reward pool.
Note that if a gateway (and its delegates) leaves the network or a delegate fully withdraws stake from a gateway, they become ineligible to receive rewards within the corresponding epoch and the earmarked rewards will not be distributed.Handling Deficient Gateways To maintain network efficiency and reduce contract state bloat, gateways that are marked as deficient, and thus fail to receive rewards,
for thirty (30) consecutive epochs will automatically trigger a “Network Leave” action and be subjesct to the associated stake withdrawal durations for both gateway stake and any delegated stake.
In addition, the gateway shall have its minimum network-join stake slashed by 100%. The slashed stake shall be immediately sent to the protocol balance.

---

# 134. ARIO Node Release Notes - ARIO Docs

Document Number: 134
Source: https://docs.ar.io/gateways/release-notes
Words: 8770
Extraction Method: html

AR.IO Release Notes Overview Welcome to the documentation page for the AR.IO gateway release notes. Here, you will find detailed information about each version of the AR.IO gateway, including the enhancements, bug fixes, and any other changes introduced in every release. This page serves as a comprehensive resource to keep you informed about the latest developments and updates in the AR.IO gateway. For those interested in exploring the source code, each release's code is readily accessible at our GitHub repository: AR.IO gateway change logs. Stay updated with the continuous improvements and advancements in the AR.IO gateway by referring to this page for all release-related information.[Release 46] - 2025-08-18 This is a recommended release that introduces AR.IO network chunk retrieval with cryptographic validation and enhanced observability. Gateway operators can now retrieve chunks directly from AR.IO peers with the same security guarantees as Arweave network chunks, significantly improving chunk caching and retrieval performance.Added Added AR.IO network chunk source enabling chunk retrieval from AR.IO peers with weighted peer selection, retry logic, and cryptographic validation to prevent serving of corrupted or malicious data.Added comprehensive OpenTelemetry tracing for chunk retrieval operations providing visibility into performance, cache behavior, and source attribution across the entire pipeline.Added HEAD request support to /chunk/{offset} endpoint with ETag headers for efficient caching and conditional request handling with If-None-Match support.Added chunk source headers for traceability: X-AR-IO-Chunk-Source-Type indicating data source, X-AR-IO-Chunk-Host with peer hostname, and X-Cache for cache status.Added RFC 9530 Content-Digest header support for standard-compliant content integrity verification in data and chunk responses.Added configurable composite chunk sources with parallelism control via CHUNK_DATA_RETRIEVAL_ORDER and CHUNK_METADATA_RETRIEVAL_ORDER environment variables supporting comma-separated source ordering.Added OpenAPI documentation for /ar-io/peers endpoint.Changed Renamed ar-io-peers to ar-io-network as the preferred configuration name while maintaining backwards compatibility.Enhanced /ar-io/peers endpoint to include both data and chunk weights for AR.IO gateway peers.Fixed Fixed ArNS custom 404 pages to prevent incorrect ArNS headers from being propagated to other gateways.[Release 45] - 2025-08-11 This is an optional release that enhances chunk broadcasting with improved preferred peer management, adds a hash-based partitioning filter for distributed data processing, fixes ArNS basename cache refresh issues, and includes comprehensive documentation improvements with a new glossary of AR.IO Node terminology.Added Added hash partitioning filter (MatchHashPartition) for distributing transaction and data item processing across multiple nodes with configurable partition ranges.Added comprehensive glossary documentation covering AR.IO Node terminology, concepts, and architectural components.Changed Improved chunk broadcasting preferred peer management with doubled default per-node queue depth threshold and ensured preferred peers are always prioritized first.Enhanced circuit breaker metrics with more detailed labels for better monitoring of data source failures.Improved ArNS resolution to properly propagate 404 errors from trusted gateway resolution (a more complete fix is coming in the next release).Expanded OTEL tracing to include ArNS cache operations for improved observability of name resolution and cache hydration.Fixed Fixed unreliable ArNS basename cache refreshes by adding retry logic for pagination failures and replacing p-debounce with timestamp-based debouncing for more predictable behavior.Fixed undefined headers handling in data requests.Fixed invalid cache hits by ensuring base64url encoded IDs are properly validated before use.Fixed routes data handling for undefined IDs in validity checks.[Release 44] - 2025-07-28 This is a recommended release that introduces efficient range request support for contiguous data retrieval from chunks, adds bundle metadata columns with offset indexing to improve offset availability throughout the network, enhances Merkle path parsing compatibility, and includes comprehensive documentation for offsets and Merkle paths.Added Added efficient range request support for chunk data retrieval, enabling optimized verifiable contiguous data fetching directly from Arweave nodes.Added bundle metadata columns to data.db to improve offset availability across the gateway network.Added OTEL (OpenTelemetry) tracing support for chunk POST operations, providing better observability for chunk broadcasting performance.Added OTEL environment variables to docker-compose.yaml for easier configuration of distributed tracing.Added comprehensive Arweave Merkle tree structure documentation detailing the data organization and validation rules.Added detailed documentation explaining Arweave transaction and chunk offset calculations.Added merkle-path-parser with full Arweave compatibility for improved Merkle proof validation.Changed Implemented promise-based chunk caching system replacing the previous WeakMap implementation, improving memory efficiency and cache reliability.Extended CompositeChunkSource to implement all chunk interfaces, providing a more unified chunk data access layer.[Release 43] - 2025-07-21 This is a recommended release that enables data verification by default for data items linked to ArNS names, improves chunk broadcasting efficiency, and adds automatic chunk data cache cleanup.Added Added automatic chunk data cache cleanup functionality with configurable retention period. Chunks are now automatically removed after 4 hours by default (configurable via CHUNK_DATA_CACHE_CLEANUP_THRESHOLD). The cleanup can be disabled by setting ENABLE_CHUNK_DATA_CACHE_CLEANUP=false. This helps manage disk space usage while maintaining cache performance benefits.Added demand-driven opt-out background verification for ArNS data. When ArNS names are requested, the system now proactively verifies the underlying data asynchronously in the background by unbundling verified chunk data retrieved directly from Arweave nodes. This ensures ArNS-served content is prioritized for verification, improving data integrity guarantees for frequently accessed named content.Changed Simplified chunk data storage by removing the dual-storage approach (by-hash and by-dataroot with symlinks). Chunks are now stored directly by data root only, reducing complexity and improving performance.Revamped chunk broadcasting architecture from 3-tier system to unified peer-based approach. Chunk broadcasting now uses individual fastq queues per peer with configurable concurrency and queue depth protection. Added support for preferred chunk POST peers via PREFERRED_CHUNK_POST_URLS environment variable. Configuration defaults have been optimized: CHUNK_POST_PEER_CONCURRENCY now defaults to match CHUNK_POST_MIN_SUCCESS_COUNT (3) to avoid over-broadcasting, and CHUNK_POST_PER_NODE_CONCURRENCY defaults to match CHUNK_POST_QUEUE_DEPTH_THRESHOLD (10) for consistent per-node load management. This change improves broadcast reliability and performance while simplifying the codebase by removing circuit breakers and tier-based logic.Modified DataVerificationWorker to ensure data item IDs (not just root IDs) have their retry count incremented, preventing IDs from being stuck without retry attempts. This improves the reliability of the data verification process.Fixed Fixed experiment bash Parquet export script generating filenames with count_star() instead of actual row counts for blocks and tags files. The script now correctly uses the -noheader flag when retrieving counts for filename generation.Fixed missing directory existence checks in FsCleanupWorker to prevent errors when attempting to scan non-existent directories during filesystem cleanup operations.[Release 42] - 2025-07-14 This is an optional release that improves peer request traceability, adds HyperBEAM URL support, and includes draft AI-generated technical documentation.Added Added support for optional HyperBEAM URL configuration via AO_ANT_HYPERBEAM_URL environment variable. In the future this allows ANT processes to use HyperBEAM nodes for caching and serving state, reducing pressure on compute units for simple read requests.Added AI-generated technical documentation covering AR.IO gateway architecture, data retrieval, Arweave connectivity, ArNS name resolution system, centralization analysis, and database architecture. These guides in docs/drafts/ are generally correct but should not be considered authoritative.Added origin and release information to query string parameters in outbound requests to both peer gateways and trusted gateways. Data requests now include ar-io-hops, ar-io-origin, ar-io-origin-release, ar-io-arns-record, and ar-io-arns-basename as query parameters, improving network observability and request tracing across the entire gateway network.Changed Implemented X-AR-IO header initialization for outbound peer requests while removing x-ar-io-origin and x-ar-io-origin-node-release headers from responses. This change maintains necessary header functionality for peer communication while reducing unnecessary header overhead in responses.Updated @ar.io/sdk dependency to support optional HyperBEAM URL functionality.[Release 41] - 2025-06-30 Added Added preferred chunk GET node URLs configuration via PREFERRED_CHUNK_GET_NODE_URLS environment variable to enable chunk-specific peer prioritization. Preferred URLs receive a weight of 100 for prioritization and the system selects 10 peers per attempt by default.Added hash validation for peer data fetching by including X-AR-IO-Expected-Digest header in peer requests when hash is available, validating peer responses against expected hash, and immediately rejecting mismatched data.Added DOCKER_NETWORK_NAME environment variable to configure the Docker network name used by Docker Compose.Added draft guide for running a community gateway.Added draft data verification architecture document.Changed Removed trusted node fallback for chunk retrieval. Chunks are now retrieved exclusively from peers, with the retry count increased from 3 to 50 to ensure reliability without the trusted node fallback.Fixed Fixed inverted logic preventing symlink creation in FsChunkDataStore.Fixed Content-Length header for range requests and 304 responses, properly setting header for single and multipart range requests and removing entity headers from 304 Not Modified responses per RFC 7232.Fixed MaxListenersExceeded warnings by adding setMaxListeners to read-through data cache.Fixed potential memory leaks in read-through data cache by using once instead of on for error and end event listeners.[Release 40] - 2025-06-23 This is an optional release that primarily improves caching when data is fetched from peers.Added Added experimental flush-to-stable script for manual database maintenance. This script allows operators to manually flush stable chain and data item tables, mirroring the logic of StandaloneSqliteDatabase.flushStableDataItems. WARNING: This script is experimental and directly modifies database contents. Use with caution and ensure proper backups before running.Changed Replaced yesql with custom SQL loader that handles comments better, improving SQL file parsing and maintenance.Switched to SPDX license headers to reduce LLM token usage, making the codebase more efficient for AI-assisted development.Improved untrusted data handling and hash validation in cache operations. The cache now allows caching when a hash is available for validation even for untrusted data sources, but only finalizes the cache when the computed hash matches a known trusted hash. This prevents cache poisoning while still allowing data caching from untrusted sources when the data can be validated.[Release 39] - 2025-06-17 This release enhances observability and reliability with new cache metrics, improved data verification capabilities, and automatic failover between chain data sources. The addition of ArNS-aware headers enables better data prioritization across the gateway network. This is a recommended but not urgent upgrade.Added Added filesystem cache metrics with cycle-based tracking. Two new Prometheus metrics track cache utilization: cache_objects_total (number of objects in cache) and cache_size_bytes (total cache size in bytes). Both metrics include store_type and data_type labels to differentiate between cache types (e.g., headers, contiguous_data). Metrics are updated after each complete cache scan cycle, providing accurate visibility into filesystem cache usage.Added X-AR-IO-Data-Id header to all data responses. This header shows the actual data ID being served, whether from a direct ID request or manifest path resolution, providing transparency about the content being delivered.Added automatic data item indexing when data verification is enabled. When ENABLE_BACKGROUND_DATA_VERIFICATION is set to true, the system now automatically enables data item indexing (ANS104_UNBUNDLE_FILTER) with an always: true filter if no filter is explicitly configured. This ensures bundles are unbundled to verify that data items are actually contained in the bundle associated with the Arweave transaction's data root.Added ArNS headers to outbound gateway requests to enable data prioritization. The generateRequestAttributes function now includes ArNS context headers (X-ArNS-Name, X-ArNS-Basename, X-ArNS-Record) in requests to other gateways and Arweave nodes, allowing downstream gateways to effectively prioritize ArNS data requests.Added configurable Docker Compose host port environment variables (CORE_PORT, ENVOY_PORT, CLICKHOUSE_PORT, CLICKHOUSE_PORT_2, CLICKHOUSE_PORT_3, OBSERVER_PORT) to allow flexible port mapping while maintaining container-internal port compatibility and security.Added Envoy aggregate cluster configuration for automatic failover between primary and fallback chain data sources. The primary cluster (default: arweave.net:443) uses passive outlier detection while the fallback cluster (default: peers.arweave.xyz:1984) uses active health checks. This enables zero-downtime failover between HTTPS and HTTP endpoints with configurable FALLBACK_NODE_HOST and FALLBACK_NODE_PORT environment variables.Changed Streamlined background data retrieval to reduce reliance on centralized sources. The default BACKGROUND_RETRIEVAL_ORDER now only includes chunks,s3, removing trusted-gateways and tx-data from the default configuration. This prioritizes verifiable chunk data and S3 storage for background operations like unbundling.Removed ar-io.net from default trusted gateways list and removed TRUSTED_GATEWAY_URL default value to reduce load on ar-io.net now that P2P data retrieval is re-enabled. Existing deployments with TRUSTED_GATEWAY_URL explicitly set will continue to work for backwards compatibility.[Release 38] - 2025-06-09 This release focuses on data integrity and security improvements, introducing trusted data verification and enhanced header information for data requests. Upgrading to this release is recommended but not urgent.Added Added X-AR-IO-Trusted header to indicate data source trustworthiness in responses. This header helps clients understand whether data comes from a trusted source and works alongside the existing X-AR-IO-Verified header to provide data integrity information. The system now filters peer data by requiring peers to indicate their content is either verified or trusted, protecting against misconfigured peers that may inadvertently serve unintended content (e.g., provider default landing pages) instead of actual Arweave data.Added If-None-Match header support for HTTP conditional requests enabling better client-side caching efficiency. When clients send an If-None-Match header that matches the ETag, the gateway returns a 304 Not Modified response with an empty body, reducing bandwidth usage and improving performance.Added digest and hash headers for data HEAD requests to enable client-side data integrity verification.Added EC2 IMDS (instance-profile) credential support for S3 data access, improving AWS authentication in cloud environments.Added trusted data flag to prevent caching of data from untrusted sources, ensuring only verified and reliable content is stored locally while still allowing serving of untrusted data when necessary.Changed Re-enabled ar-io-peers as fallback data source in configuration for improved data availability.Updated trusted node configuration to use arweave.net as the default trusted node URL.Updated ETag header format to use properly quoted strings (e.g., "hash" instead of hash) following HTTP/1.1 specification standards for improved compatibility with caching proxies and clients.[Release 37] - 2025-06-03 This is a recommended release due to the included observer robustness improvements. It also adds an important new feature - data verification for preferred ArNS names. When preferred ArNS names are set, the bundles containing the data they point to will be locally unbundled (verifying data item signatures), and the data root for the bundle will be compared to the data root in the Arweave chain (establishing that the data is on Arweave). To enable this feature, set your preferred ArNS names, turn on unbundling by setting ANS104_DOWNLOAD_WORKERS and ANS104_UNBUNDLE_WORKERS both to 1, and set your ANS104_INDEX_FILTER to a filter that will match the data items for your preferred names. If you don't know the filter, use {"always": true}, but be aware this will index the entire bundle for the IDs related to your preferred names.Note: this release contains migrations to data.db. If your node appears unresponsive please check core service logs to determine whether migrations are running and wait for them to finish.Added Added prioritized data verification system for preferred ArNS names, focusing computational resources on high-priority content while enabling flexible root transaction discovery through GraphQL fallback support.Added verification retry prioritization system with tracking of retry counts, priority levels, and attempt timestamps to ensure bundles do not get stuck retrying forever.Added improved observer functionality with best-of-2 observations and higher compression for more reliable network monitoring.Added MAX_VERIFICATION_RETRIES environment variable (default: 5) to limit verification retry attempts and prevent infinite loops for consistently failing data items.Added retry logic with exponential backoff for GraphQL queries to handle rate limiting (429) and server errors with improved resilience when querying trusted gateways for root bundle IDs.Changed Updated dependencies: replaced deprecated express-prometheus-middleware with the actively maintained express-prom-bundle library and updated prom-client to v15.1.3 for better compatibility and security.Updated Linux setup documentation to use modern package installation methods, replacing apt-key yarn installation with npm global install and updating Node.js/nvm versions.Improved route metrics normalization with explicit whitelist function for better granularity and proper handling of dynamic segments.Fixed Fixed docker-compose configuration to use correct NODE_MAX_OLD_SPACE_SIZE environment variable name.Fixed production TypeScript build configuration to exclude correct "test" directory path.Fixed Parquet exporter to properly handle data item block_transaction_index exports, preventing NULL value issues.Fixed bundles system to copy root_parent_offset when flushing data items to maintain data integrity.Fixed ClickHouse auto-import script to handle Parquet export not_started status properly.Fixed docker-compose ClickHouse configuration to not pass conflicting PARQUET_PATH environment variable to container scripts.Fixed verification process for data items that have not been unbundled by adding queue bundle support and removing bundle join constraint to ensure proper verification of data items without indexed root parents.[Release 36] - 2025-05-27 This is a recommended but not essential upgrade. The most important changes are the preferred ArNS caching feature for improved performance on frequently accessed content and the observer's 80% failure threshold to prevent invalid reports during network issues.Added Added preferred ArNS caching functionality that allows configuring lists of ArNS names to be cached longer via PREFERRED_ARNS_NAMES and PREFERRED_ARNS_BASE_NAMES environment variables. When configured, these names will be cleaned from the filesystem cache after PREFERRED_ARNS_CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD instead of the standard cleanup threshold (CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD). This is accomplished by maintaining an MRU (Most Recently Used) list of ArNS names in the contiguous metadata cache. When filesystem cleanup runs, it checks this list to determine which cleanup threshold to apply. This feature enables gateway operators to ensure popular or important ArNS names remain cached longer, improving performance for frequently accessed content.Added ArNS headers to responses: X-ArNS-Name, X-ArNS-Basename, and X-ArNS-Record to help identify which ArNS names were used in the resolution.Changed Updated observer to prevent report submission when failure rate exceeds 80%. This threshold helps guard against both poorly operated observers and widespread network issues. In the case of a widespread network issue, the assumption is that most gateway operators are well intentioned and will work together to troubleshoot and restore both observations and network stability, rather than submitting reports that would penalize functioning gateways.Updated default trusted gateway in docker-compose Envoy configuration to ar-io.net for improved robustness and alignment with core service configuration.Improved range request performance by passing ranges directly to getData implementations rather than streaming all data and extracting ranges.Fixed Fixed missing cache headers (X-Cache and other data headers) in range request responses to ensure consistent cache header behavior across all request types.Fixed async streaming for multipart range requests by using async iteration instead of synchronous reads, preventing potential data loss.Fixed ArNS resolution to properly exclude www subdomain from resolution logic.Fixed test reliability issues by properly awaiting stream completion before making assertions.Fixed chunk broadcasting to not await peer broadcasts, as they are best-effort operations.[Release 35] - 2025-05-19 This is a low upgrade priority release. It contains a small caching improvement and routing fix. Upgrading to help test it is appreciated but not essential.Changed Adjusted filesystem data expiration to be based on last request times rather than file access times which may be inaccurate.Adjusted CORS headers to include content-* headers.Fixed Fixed regex used to expose /api-docs when an apex ArNS name is set.[Release 34] - 2025-05-05 Given the resilience provided by adding a second trusted gateway URL, it is recommended that everyone upgrade to this release.Added Added peer list endpoints for retrieving information about Arweave peers and ar.io gateway peers.Added ar-io.net as a secondary trusted gateway to increase data retrieval resilience by eliminating a single point of failure.Added circuit breaker for Arweave peer chunk posting.Changed Created directories for DuckDB and Parquet to help avoid permission issues by the directories being created by containers.Fixed Fixed GraphQL ClickHouse error when returning block ID and timestamp.Fixed the tx-chunks-data-source to throw a proper error (resulting in a 404) when the first chunk is missing rather than streaming a partial response.[Release 33] - 2025-05-05 Added Added a [Parquet and ClickHouse usage guide]. Using ArDrive as an example, it provides step by step instructions about how to bulk load Parquet and configure continuous ingest of bundled data items into ClickHouse. This allows the ar-io-node to support performant GraphQL queries on larger data sets and facilitates sharing indexing work across gateways via distribution of Parquet files.Added support for configurable ArNS 404 pages using either:ARNS_NOT_FOUND_TX_ID: Transaction ID for custom 404 content ARNS_NOT_FOUND_ARNS_NAME: ArNS name to resolve for 404 content Added experimental /chunk/ GET route for serving chunk data by absolute offset either the local cache.Added support for AWS_SESSION_TOKEN in the S3 client configuration.Expanded ArNS OTEL tracing to improve resolution behavior observability.Added support for setting a ClickHouse username and password via the CLICKHOUSE_USERNAME and CLICKHOUSE_PASSWORD environment variable. When using ClickHouse, CLICKHOUSE_PASSWORD should always be set. However, CLICKHOUSE_USERNAME can be left unset. The username default will be used in that case.Added support for configuring the port used to connect to ClickHouse via the CLICKHOUSE_PORT environment variable.Changed Disabled ClickHouse import timing logging by default. It can be enabled via environment variable - DEBUG when running the service standalone or CLICKHOUSE_DEBUG when using Docker Compose Upgraded to ClickHouse 25.4.Fixed Ensure .env is read in clickhouse-import script.[Release 32] - 2025-04-22 Changed Reenabled parallel ArNS resolution with removal of misplaced global limit. Refer to release 30 notes for more details on configuration and rationale.Added a timeout for the last ArNS resolver in ARNS_RESOLVER_PRIORITY_ORDER. It defaults to 30 seconds and is configurable using ARNS_COMPOSITE_LAST_RESOLVER_TIMEOUT_MS. This helps prevent promise build up if the last resolver stalls.Fixed Fixed apex ArNS name handling when a subdomain is present in ARNS_ROOT_HOST.Fixed a case where fork recovery could stall due to early flushing of unstable chain data.Restored observer logs by removing unintentional default log level override in docker-compose.yaml.[Release 31] - 2025-04-11 Changed Improved peer TX header fetching by fetching from a wider range of peers and up/down weighting peers based on success/failure.Fixed Rolled back parallel ArNS resolution changes that were causing ArNS resolution to slow down over time.[Release 30] - 2025-04-04 Added Added support for filtering Winston logs with a new LOG_FILTER environment variable.Example filter: {"attributes":{"class":"ArweaveCompositeClient"}} to only show logs from that class.Use CORE_LOG_FILTER environment variable when running with docker-compose.Added parallel ArNS resolution capability.Configured via ARNS_MAX_CONCURRENT_RESOLUTIONS (default: 1).This foundation enables future enhancements to ArNS resolution and should generally not be adjusted at present.Changed Improved ClickHouse auto-import script with better error handling and continuous operation through errors.Reduced maximum header request rate per second to trusted node to load on community gateways.Optimized single owner and recipient queries on ClickHouse with specialized sorted tables.Used ID sorted ClickHouse table for ID queries to improve performance.Fixed Fixed data alignment in Parquet file name height boundaries to ensure consistent import boundaries.Removed trailing slashes from AO URLs to prevent issues when passing them to the SDK.Only prune SQLite data when ClickHouse import succeeds to prevent data loss during exports.[Release 29] - 2025-03-21 Changed Temporarily default to trusted gateway ArNS resolution to reduce CU load as much possible. On-demand CU resolution is still available as a fallback and the order can be modified by setting ARNS_RESOLVER_PRIORITY_ORDER.Remove duplicate network process call in on-demand resolver.Don't wait for network process debounces in the on-demand resolver.Slow network process dry runs no longer block fallback to next resolver.Added Added support for separate CUs URLs for the network and ANT processes via the NETWORK_AO_CU_URL and ANT_AO_CU_URL process URLs respectively. If either is missing the AO_CU_URL is used instead with a fallback to the SDK default URL if AO_CU_URL is also unspecified.Added CU URLs to on-demand ArNS resolver logs.Added circuit breakers for AR.IO network process CU dry runs. By default they use a 1 minute timeout and open after 30% failure over a 10 minute window and reset after 20 minutes.Fixed Owners in GraphQL results are now correctly retrieved from data based on offsets when using ClickHouse.[Release 28] - 2025-03-17 Changed Raised name not found name list refresh interval to 2 minutes to reduce load on CUs. This increases the maximum amount of time a user may wait for a new name to be available. Future releases will introduce other changes to mitigate this delay.Adjusted composite ArNS resolver to never timeout resolutions from the last ArNS resolver in the resolution list.Added Added support for serving a given ID or ArNS name from the apex domain of a gateway. If using an ID, set the APEX_TX_ID environment variable. If using an ArNS name, set the APEX_ARNS_NAME environment variable.Added BUNDLE_REPAIR_UPDATE_TIMESTAMPS_INTERVAL_SECONDS, BUNDLE_REPAIR_BACKFILL_INTERVAL_SECONDS, and BUNDLE_REPAIR_FILTER_REPROCESS_INTERVAL_SECONDS environment variables to control the interval for retrying failed bundles, backfilling bundle records, and reprocessing bundles after a filter change. Note: the latter two are rarely used. Queuing bundles for reprocessing via the /ar-io/admin/queue-bundle endpoint is usually preferable to automatic reprocessing as it is faster and offers more control over the reprocessing behavior.Fixed Signatures in GraphQL results are now correctly retrieved from data based on offsets when using ClickHouse.Adjusted exported Parquet file names to align with expectations of ClickHouse import script.Ensured that bundle indexing status is properly reset when bundles are manually queued after an unbundling filter change has been made.[Release 27] - 2025-02-20 Changed Set process IDs for mainnet.Increase default AO CU WASM memory limit to 17179869184 to support mainnet
process.[Release 26] - 2025-02-13 Added Added a per resolver timeout in the composite ArNS resolver. When the
composite resolver attempts resolution it is applied to each resolution
attempt. It is configurable via the ARNS_COMPOSITE_RESOLVER_TIMEOUT_MS and
defaults to 3 seconds in order to allow a fallback attempt before the default
observer timeout of 5 seconds.Added a TURBO_UPLOAD_SERVICE_URL environment variable to support
configuration of the bundler used by the observer (TurboSDK defaults are
used if not set).Added a REPORT_DATA_SINK environment variable that enables switching the
method used to post observer reports. With the default, turbo, it sends
data items via a Turbo compatible bundler. Switching it to arweave will
post base layer transactions directly to Arweave instead.Added a /ar-io/admin/bundle-status/<id> endpoint that returns the counters
and timestamps from the bundles row in data.db. This can be used for
monitoring unbundling progress and scripting (e.g., to skip requeuing already
queued bundles).Added more complete documentation for filters.Changed Use arweave.net as the default GraphQL URL for AO CUs since most gateways
will not have a complete local AO data item index.Use a default timeout of 5 seconds when refreshing Arweave peers to prevent
stalled peer refreshes.Cache selected gateway peer weights for the amount of time specified by the GATEWAY_PEERS_WEIGHTS_CACHE_DURATION_MS environment variable with a default
of 5 seconds to avoid expensive peer weight recomputation on each request.Chunk broadcasts to primary nodes occur in parallel with a concurrency limit
defaulting to 2 and configurable via the CHUNK_POST_CONCURRENCY_LIMIT environment variable.Added circuit breakers for primary chunk node POSTs to avoid overwhelming
chunk nodes when they are slow to respond.Fixed Properly cleanup timeout and event listener when terminating the data
root computation worker.Count chunk broadcast exceptions as errors in the arweave_chunk_broadcast_total metric.[Release 25] - 2025-02-07 Added Added support for indexing and querying ECDSA signed Arweave transactions.Expanded the OpenAPI specification to cover the entire gateway API and
commonly used Arweave node routes.ArNS undername record count limits are now enforced. Undernames are sorted
based on their ANT configured priority with a fallback to name comparisons
when priorities conflict or are left unspecified. Enforcement is enabled by
default but can be disabled by setting the ARNS_RESOLVER_ENFORCE_UNDERNAME_LIMIT to false.Changed Renamed the ario-peer data source to ar-io-peers for consistency and
clarity. ario-peer will continue to work for backwards compatibility but is
considered deprecated.Use AR.IO gateway peers from the ar.io gateway address registry (GAR) as the
last fallback for fetching data when responding to client data requests. This
has the benefit of making the network more resilient to trusted gateway
disruptions, but it can also result in nodes serving data from less trusted
sources if it is not found in the trusted gateway. This can be disabled by
using a custom ON_DEMAND_RETRIEVAL_ORDER that does not include ar-io-peers.Arweave data chunk requests are sent to the trusted node first with a
fallback to Arweave peers when chunks are unavailable on the trusted node.
This provides good performance by default with a fallback in case there are
issues retrieving chunks from the trusted node.Increased the observer socket timeout to 5 seconds to accommodate initial
slow responses for uncached ArNS resolutions.Disabled writing base layer Arweave signatures to the SQLite DB by default to
save disk space. When signatures are required to satisfy GraphQL requests,
they are retrieved from headers on the trusted node.Fixed Updated dependencies to address security issues.Improved reliability of failed bundle indexing retries.Fixed failure to compute data roots for verification for base layer data
larger than 2GiB.Fixed observer healthcheck by correcting node.js path in healthcheck script.[Release 24] - 2025-02-03 Added Added a ARNS_ANT_STATE_CACHE_HIT_REFRESH_WINDOW_SECONDS environment
variable that determines the number of seconds before the end of the TTL at
which to start attempting to refresh the ANT state.Added a TRUSTED_GATEWAYS_REQUEST_TIMEOUT_MS environment that defaults to
10,000 and sets the number of milliseconds to wait before timing out request
to trusted gateways.Added BUNDLE_REPAIR_RETRY_INTERVAL_SECONDS and BUNDLE_REPAIR_RETRY_BATCH_SIZE environment variables to control the time
between queuing batches of bundle retries and the number of data items
retrieved when constructing batches of bundles to retry.Added support for configuring the ar.io SDK log level via the AR_IO_SDK_LOG_LEVEL environment variable.Added a request_chunk_total Prometheus counter with status, source (a
URL) and source_type (trusted or peer) labels to track success/failure
of chunk retrieval in the Arweave network per source.Added a get_chunk_total Prometheus metric to count chunk retrieval
success/failure per chunk.Added arns_cache_hit_total and arns_cache_miss_total Prometheus counters
to track ArNS cache hits and misses for individual names respectively.Added arns_name_cache_hit_total and arns_name_cache_miss_total Prometheus
counters to track ArNS name list cache hits and misses
respectively.Added a arns_resolution_duration_ms Prometheus metric that tracks summary
statistics for the amount of time it takes to resolve ArNS names.Changed In addition to the trusted node, the Arweave network is now searched for
chunks by default. All chunks retrieved are verified against data roots
indexed from a trusted Arweave node to ensure their validity.Default to a 24 hour cache TTL for the ArNS name cache. Record TTLs still
override this, but in cases where resolution via AO CU is slow or fails, the
cache will be used. In the case of slow resolution, CU based resolution will
proceed in the background and update the cache upon completion.Switched to the ioredis library for better TLS support.Updated minor dependency minor versions (more dependencies will be updated in
the next release).Bundles imports will no longer be re-attempted for bundles that have already
been fully unbundled using the current filters if they are matched or
manually queued again.Replaced references docker-compose in the docs with the more modern docker compose.Fixed Ensure duplicate data item IDs are ignored when comparing counts to determine
if a bundle has been fully unbundled.Fixed worker threads failing to shut down properly when the main process
stopped.Ensure bundle import attempt counts are incremented when bundles are skipped
to avoid repeatedly attempting to import skipped bundles.Use observe that correctly ensure failing gateways are penalized in the AR.IO
AO process.[Release 23] - 2025-01-13 Added Added FS_CLEANUP_WORKER_BATCH_SIZE,FS_CLEANUP_WORKER_BATCH_PAUSE_DURATION, and FS_CLEANUP_WORKER_RESTART_PAUSE_DURATION environment variables to allow
configuration of number of contiguous data files cleaned up per batch, the
pause between each batch, and the pause before restarting the entire cleanup
process again.Added data_items_unbundled_total Prometheus metric that counts the total
number of data items unbundled, including those that did not match the
unbundling filter.Added a parent_type label that can be one of transaction or data_item to data item indexing metrics.Added a files_cleaned_total total Prometheus metric to enable monitoring of
contiguous data cleanup.Added support for specifying the admin API via a file specified by the ADMIN_API_KEY_FILE environment variable.Added experimental support for posting chunks in a non-blocking way to
secondary nodes specified via a comma separate list in the SECONDARY_CHUNK_POST_URLS environment variable.Changed Renamed the parent_type lable to contiguous_data_type on bundle metrics
to more accurately reflect the meaning of the label.Reduced the maximum time to refresh the ArNS name list to 10 seconds to
minimize delays in ArNS availability after a new name is registered.Changed /ar-io/admin/queue-bundle to wait for bundles rows to be written
to the DB before responding to ensure that errors that occur due to DB
contention are not silently ignored.Data items are now flushed even when block indexing is stopped. This allows
for indexing batches of data items using the admin API with block indexing
disabled.Adjust services in docker-compose to use unless-stopped as their restart
policy. This guards against missing restarts in the case where service
containers exit with a success status even when they shouldn't.Fixed Added missing created_at field in blocked_names table.Fixed broken ArNS undername resolution.[Release 22] - 2024-12-18 Added Added the ability to block and unblock ArNS names (e.g., to comply with hosting provider TOS). To block a name, POST { "name": "<name to block>" } to /ar-io/admin/block-name. To unblock a name, POST { "name": "<name to unblock>" } to /ar-io/admin/unblock-name.Changed Return an HTTP 429 response to POSTs to /ar-io/admin/queue-bundle when the bundle data import queue is full so that scripts queuing bundles can wait rather than overflowing it.Fixed Adjust ArNS length limit from <= 48 to <= 51 to match the limit enforced by the AO process.[Release 21] - 2024-12-05 Added Added a ClickHouse auto-import service. When enabled, it calls the Parquet export API, imports the exported Parquet into ClickHouse, moves the Parquet files to an imported subdirectory, and deletes data items in SQLite up to where the Parquet export ended. To use it, run Docker Compose with the clickhouse profile, set the CLICKHOUSE_URL to http://clickhouse:8123, and ensure you have set an ADMIN_KEY. Using this configuration, the core service will also combine results from ClickHouse and SQLite when querying transaction data via GraphQL. Note: if you have a large number of data items in SQLite, the first export and subsequent delete may take an extended period. Also, this functionality is considered experimental. We expect there are still bugs to be found in it and we may make breaking changes to the ClickHouse schema in the future. If you choose to use it in production (not yet recommended), we suggest backing up copies of the Parquet files found in data/parquet/imported so that they can be reimported if anything goes wrong or future changes require it.Added a background data verification process that will attempt to recompute data roots for bundles and compare them to data roots indexed from Arweave nodes. When the data roots match, all descendant data items will be marked as verified. This enables verification of data initially retrieived from sources, like other gateways, that serve contiguous data instead of verifiable chunks. Data verification can be enabled by setting the ENABLE_BACKGROUND_DATA_VERIFICATION environment variable to true. The interval between attempts to verify batches of bundles is configurable using the BACKGROUND_DATA_VERIFICATION_INTERVAL_SECONDS environment variable.Added a CHUNK_POST_MIN_SUCCESS_COUNT environment variable to configure how many Arweave nodes must accept a chunk before a chunk broadcast is considered successful.Added arweave_chunk_post_total and arweave_chunk_broadcast_total Prometheus metrics to respectively track the number of successful chunk POSTs to Arweave nodes and the number of chunks successfully broadcast.When resolving ArNS names, the entire list of names is now cached instead of individually checking whether each name exists. This reduces the load on AO CUs since the entire list can be reused across multiple requests for different names. Note: due to the default 5 minute interval between name list refreshes, newly registered may now take longer to resolver after initial registration. We intend to make further caching refinements to address this in the future.Added support for multiple prioritized trusted gateways configurable by setting the TRUSTED_GATEWAYS_URLS environment variable to a JSON value containing a mapping of gateway hosts to priorities. Data requests are sent to other gateways in ascending priority order. If multiple gateways share the same priority, all the gateways with the same priority are tried in a random order before continuing on to the next priority.Added support for caching contiguous data in S3. It is enabled by default when the AWS_S3_CONTIGUOUS_DATA_BUCKET and AWS_S3_CONTIGUOUS_DATA_PREFIX environment variables are set.Changed trusted-gateway was changed to trusted-gateways in ON_DEMAND_RETRIEVAL_ORDER and BACKGROUND_RETRIEVAL_ORDER.Renamed the S3 contiguous environment variables - AWS_S3_BUCKET to AWS_S3_CONTIGUOUS_DATA_BUCKET and AWS_S3_PREFIX to AWS_S3_CONTIGUOUS_DATA_PREFIX.[Release 20] - 2024-11-15 Added Exposed the core service chunk POST endpoint via Envoy. It accepts a Arweave data chunk and broadcasts it to either the comma separated list of URLs specified by the CHUNK_POST_URLs environment variable or, if none are specified, the /chunk path on URL specified by the TRUST_GATEWAY_URL environment variable.Added a X-AR-IO-Root-Transaction-Id HTTP header to data responses containing the root base layer transaction ID for the ID in question if it's been indexed.Added a X-AR-IO-Data-Item-Data-Offset HTTP header containing the offset of the data item relative to the root bundle base layer transaction for it. In conjunction with X-AR-IO-Root-Transaction-Id, it enables retrieving data for data item IDs from base layer data using first a HEAD request to retrieve the root ID and data offset followed by a range request into the root bundle. This greatly increases the likelihood of retriving data item data by ID since only an index into the base layer and Arweave chunk availability is needed for this access method to succeed.Added an experimental ClickHouse service to docker-compose.yaml (available via the clickhouse profile). This will be used as a supplemental GraphQL DB in upcoming releases.Added a data item indexing healthcheck that can be enabled by setting the RUN_AUTOHEAL environment variable to true. When enabled, it will restart the core service if no data items have been indexed since the value specified by the MAX_EXPECTED_DATA_ITEM_INDEXING_INTERVAL_SECONDS environment variable.[Release 19] - 2024-10-21 Fixed Adjusted data item flushing to use the bundle DB worker instead of the core DB worker to prevent write contention and failed flushes under heavy unbundling load.Added Added X-AR-IO-Digest, X-AR-IO-Stable, X-AR-IO-Verified, and ETag headers. X-AR-IO-Digest contains a base64 URL encoded representation of the SHA-256 hash of the data item data. It may be empty if the gateway has not previously cached the data locally. X-AR-IO-Stable contains either true or false depending on whether the associated Arweave transaction is more than 18 blocks old or not. X-AR-IO-Verified contains either true if the gateway has verified the data root of the L1 transaction or the L1 root parent of the data item or false if it has not. ETag contains the same value a X-AR-IO-Digest and is used to improve HTTP caching efficiency.Added support for using a different data source for on-demand and background data retrieval. Background data retrieval is used when unbundling. The background retrieval data source order is configurable using the BACKGROUND_RETRIEVAL_ORDER environment variable and defaults to chunks,s3,trusted-gateway,tx-data. Priority is given to chunk retrieval since chunks are verifiable.Added an /ar-io/admin/export-parquet/status to support monitoring of in-progress Parquet export status.Added sqlite_in_flight_ops Prometheus metric with worker (core, bundles, data, or moderation) and role (read or write) labels to support monitoring the number of in-flight DB operations.Added experimental Grafana and Prometheus based observability stack. See the "Monitoring and Observability" section of the README for more details.Changed Bundle data is now retrieved as chunks from Arweave nodes by default so that data roots can be compared against the chain (see entry about background retrieval above).Changed observer configuration to use 8 instead of 5 chosen names. These are combined with 2 names prescribed from the contract for a total of 10 names observed each epoch to provide increased ArNS observation coverage.Verification status is set on data items when unbundling a parent that has already been verified.[Release 18] - 2024-10-01 Fixed Improved performance of data attributes query that was preventing data.db WAL flushing.Added Added WAL sqlite_wal_checkpoint_pages Prometheus metric to help monitor WAL flushing.Added a POST /ar-io/admin/export-parquet endpoint that can be used to export the contents of the SQLite3 core and bundle DBs as Parquet. To trigger an export, POST JSON containing outputDir, startHeight, endHeight, and maxFileRows keys. The resulting Parquet files can then be queried directly using DuckDB or loaded into another system (e.g. ClickHouse). Scripts will be provided to help automate the latter in a future release.Added ARNS_RESOLVER_OVERRIDE_TTL_SECONDS that can be used to force ArNS names to refresh before their TTLs expire.Added a GET /ar-io/resolver/:name endpoint that returns an ArNS resolution for the given name.Changed Removed ArNS resolver service in favor of integrated resolver. If a standalone resolver is still desired, the core service can be run with the START_WRITERS environment variable set to false. This will disable indexing while preserving resolver functionality.Deduplicated writes to data.db to improve performance and reduce WAL growth rate.[Release 17] - 2024-09-09 Notes This release includes a LONG RUNNING MIGRATION. Your node may appear unresponsive while it is running. It is best to wait for it to complete. If it fails or is interrupted, removing your SQLite DBs (in data/sqlite by default) should resolve the issue, provided you are willing to lose your GraphQL index and let your node rebuild it.Fixed Use the correct environment variable to populate WEBHOOK_BLOCK_FILTER in docker-compose.yaml.Don't cache data regions retrieved to satisfy range requests to avoid unnecessary storage overhead and prevent inserting invalid ID to hash mappings into the data DB.Added Added a new ClickHouse based DB backend. It can be used in combination with the SQLite DB backend to enable batch loading of historical data from Parquet. It also opens up the possibility of higher DB performance and scalability. In its current state it should be considered a technology preview. It won't be useful to most users until we either provide Parquet files to load into it or automate flushing of the SQLite DB to it (both are planned in future release). It is not intended to be standalone solution. It supports bulk loading and efficient GraphQL querying of transactions and data items, but it relies on SQLite (or potentially another OLTP in the future) to index recent data. These limitations allow greatly simplified schema and query construction. Querying the new ClickHouse DB for transaction and data items via GraphQL is enabled by setting the CLICKHOUSE_URL environment variable.Added the ability to skip storing transaction signatures in the DB by setting WRITE_TRANSACTION_DB_SIGNATURES to false. Missing signatures are fetched from the trusted Arweave node when needed for GraphQL results.Added a Redis backed signature cache to support retrieving optimistically indexed data item signatures in GraphQL queries when writing data items signatures to the DB has been disabled.Added on-demand and composite ArNS resolvers. The on-demand resolver fetches results directly from an AO CU. The composite resolver attempts resolution in the order specified by the ARNS_RESOLVER_PRIORITY_ORDER environment variable (defaults to on-demand,gateway).Added a queue_length Prometheus metric to fasciliate monitoring queues and inform future optimizations Added SQLite WAL cleanup worker to help manage the size of the data.db-wal file. Future improvements to data.db usage are also planned to further improve WAL management.Changed Handle data requests by ID on ArNS sites. This enables ArNS sites to use relative links to data by ID.Replaced ARNS_RESOLVER_TYPE with ARNS_RESOLVER_PRIORITY_ORDER (defaults to on-demand,gateway).Introduced unbundling back pressure. When either data item data or GraphQL indexing queue depths are more than the value specified by the MAX_DATA_ITEM_QUEUE_SIZE environment variable (defaults to 100000), unbundling is paused until the queues length falls bellow that threshold. This prevents the gateway from running out of memory when the unbundling rate exceeds the indexing rate while avoiding wasteful bundle reprocessing.Prioritized optimistic data item indexing by inserting optimistic data items at the front of the indexing queues.Prioritized nested bundle indexing by inserting nested bundles at the front of the unbundling queue.[Release 16] - 2024-08-09 Fixed Fixed promise leak caused by missing await when saving data items to the DB.Modified ArNS middleware to not attempt resolution when receiving requests for a different hostname than the one specified by ARNS_ROOT_HOST.Added Added support for returning Content-Encoding HTTP headers based on user specified Content-Encoding tags.Added isNestedBundle filter enables that matches any nested bundle when indexing. This enables composite unbundling filters that match a set of L1 tags and bundles nested under them.Added ability to skip writing ANS-104 signatures to the DB and load them based on offsets from the data instead. This significantly reduces the size of the bundles DB. It can be enabled by setting the WRITE_ANS104_DATA_ITEM_DB_SIGNATURES environment variable to false.Added data_item_data_indexed_total Prometheus counter to count data items with data attributes indexed.Changed Queue data attributes writes when serving data rather than writing them syncronously.Reduced the default data indexer count to 1 to lessen the load on the data DB.Switched a number of overly verbose info logs to debug level.Removed docker-compose on-failure restart limits to ensure that services restart no matter how many times they fail.Modified the data_items_indexed_total Prometheus counter to count data items indexed for GraphQL querying instead of data attributes.Increased aggressiveness of contiguous data cleanup. It now pauses 5 seconds instead of 10 seconds per batch and runs every 4 hours instead of every 24 hours.[Release 15] - 2024-07-19 Fixed Fixed query error that was preventing bundles from being marked as fully imported in the database.Added Adjusted data item indexing to record data item signature types in the DB. This helps distinguish between signatures using different key formats, and will enable querying by signature type in the future.Adjusted data item indexing to record offsets for data items within bundles and signatures and owners within data items. In the future this will allow us to avoid saving owners and signatures in the DB and thus considerably reduce the size of the bundles DB.Added ARNS_CACHE_TTL_MS environment variable to control the TTL of ARNS cache entries (defaults to 1 hour).Added support for multiple ranges in a single HTTP range request.Added experimental chunk POST endpoint that broadcasts chunks to the comma-separate list of URLS in the CHUNK_BROADCAST_URLS environment variable. It is available at /chunk on the internal gateway service port (4000 by default) but is not yet exposed through Envoy.Added support for running an AO CU adjacent to the gateway (see README.md for details).Added X-ArNS-Process-Id to ArNS resolved name headers.Added a set of AO_... environment variables for specifying which AO URLs should be used (see docker-compose.yaml for the complete list). The AO_CU_URL is of particular use since the core and resolver services only perform AO reads and only the CU is needed for reads.Changed Split the monolithic docker-compose.yaml into docker-compose.yaml, docker-compose.bundler.yaml, and docker-compose.ao.yaml (see README for details).Replaced references to 'docker-compose' with 'docker compose' in the docs since the former is mostly deprecated.Reduce max fork depth from 50 to 18 inline to reflect Arweave 2.7.2 protocol changes.Increased the aggressiveness of bundle reprocessing by reducing reprocessing interval from 10 minutes to 5 minutes and raising reprocessing batch size from 100 to 1000.Use a patched version of Litestream to work around insufficient S3 multipart upload size in the upstream version.[Release 14] - 2024-06-26 Fixed Correctly handle manifest index after paths.[Release 13] - 2024-06-24 Added Added support for optimistically reading data items uploaded using the integrated Turbo bundler via the LocalStack S3 interface.Added X-AR-IO-Origin-Node-Release header to outbound data requests.Added hops, origin, and originNodeRelease query params to outbound data requests.Added support for fallback in v0.2 manifests that is used if no path in the manifest is matched.Changed Updated Observer to read prescribed names from and write observations to the ar.io AO network process.Updated Resolver to read from the ar.io AO network process.Fixed Modified optimistic indexing of data items to use a null parent_id when inserting into the DB instead of a placeholder value. This prevents unexpected non-null bundledIn values in GraphQL results for optimistically indexed data items.Modified GraphQl query logic to require an ID for single block GraphQL queries. Previously queries missing an ID were returning an internal SQLite error. This represents a small departure from arweave.net's query logic which returns the latest block for these queries. We recommend querying blocks instead of block in cases where the latest block is desired.Adjusted Observer health check to reflect port change to 5050.Security Modified docker-compose.yaml to only expose Redis, PostgreSQL, and LocalStack ports internally. This protects gateways that neglect to deploy behind a firewall, reverse proxy, or load balancer.[Release 12] - 2024-06-05 Added Added /ar-io/admin/queue-data-item endpoint for queuing data item headers for indexing before the bundles containing them are processed. This allows trusted bundlers to make their data items quickly available to be queried via GraphQL without having to wait for bundle data submission or unbundling.Added experimental support for retrieving contiguous data from S3. See AWS_* environment variables documentation for configuration details. In conjuction with a local Turbo bundler this allows optimistic bundle (but not yet data item) retrieval.Add experimental support for fetching data from gateway peers. It can be enabled by adding ario-peer to ON_DEMAND_RETRIEVAL_ORDER. Note: do not expect this work reliably yet! This functionality is in active development and will be improved in future releases.Add import_attempt_count to bundle records to enable future bundle import retry optimizations.Changed Removed version from docker-compose.yaml to avoid warnings with recent versions of docker-compose.Switched default observer port from 5000 to 5050 to avoid conflict on OS X. Since Envoy is used to provide external access to the observer API this should have no user visible effect.[Release 11] - 2024-05-21 Added Added arweave_tx_fetch_total Prometheus metric to track counts of transaction headers fetched from the trusted node and Arweave network peers.Changed Revert to using unnamed bind mounts due to cross platform issues with named volumes.[Release 10] - 2024-05-20 Added Added experimental support for streaming SQLite backups to S3 (and compatible services) using Litestream. Start the service using the docker-compose "litestream" profile to use it, and see the AR_IO_SQLITE_BACKUP_* environment variables documentation for further details.Added /ar-io/admin/queue-bundle endpoint for queueing bundles for import for import before they're in the mempool. In the future this will enable optimistic indexing when combined with a local trusted bundler.Added support for triggering webhooks when blocks are imported matching the filter specified by the WEBHOOK_BLOCK_FILTER environment variable.Added experimental support for indexing transactions and related data items from the mempool. Enable it by setting ENABLE_MEMPOOL_WATCHER to 'true'.Made on-demand data caching circuit breakers configurable via the GET_DATA_CIRCUIT_BREAKER_TIMEOUT_MS environment variable. This allows gateway operators to decide how much latency they will tolerate when serving data in exchange for more complete data indexing and caching.Rename cache header from X-Cached to X-Cache to mimic typical CDN practices.Add X-AR-IO-Hops and X-AR-IO-Origin headers in preparation for future peer-to-peer functionality.Upgrade to Node.js v20 and switch to native test runner.[Release 9] - 2024-04-10 Added Added experimental Farcaster Frames support, enabling simple Arweave based Frames with button navigation. Transaction and data item data is now served under /local/farcaster/frame/<ID>. /local is used as a prefix to indicate this functionality is both experimental and local to a particular gateway rather than part of the global gateway API. Both GET and POST requests are supported.Added an experimental local ArNS resolver. When enabled it removes dependence on arweave.net for ArNS resolution! Enable it by setting RUN_RESOLVER=TRUE, TRUSTED_ARNS_RESOLVER_TYPE=resolver, and TRUSTED_ARNS_RESOLVER_URL=http://resolver:6000 in your .env file.Added an X-Cached header to data responses to indicate when data is served from the local cache rather than being retrieved from an external source. This is helpful for interfacing with external systems, debugging, and end-to-end testing.Save hashes for unbundled data items during indexing. This enables reduction in data storage via hash based deduplication as well as more efficient peer-to-peer data retrieval in the future.[Release 8] - 2024-03-14 Added Added GraphQL SQL query debug logging to support trouble-shooting and performance optimization.Added support for indexing data items (not GraphQL querying) based solely on tag name. (example use case: indexing all IPFS CID tagged data items).Changes Observer data sampling now uses randomized ranges to generate content hashes.Reference gateway ArNS resolutions are now cached to improve report generation performance.Contract interactions are now tested before posting using dryWrite to avoid submitting interactions that would fail./ar-io/observer/info now reports INVALID for wallets that fail to load.Fixed Fix data caching failure caused by incorrect method name in getData circuit breakers.Fix healthcheck when ARNS_ROOT_HOST includes a subdomain.[Release 7] - 2024 - 02 - 14 Added Add support for notifying other services of transactions and data items using webhooks (see README for details).Add support for filter negation (particularly useful for excluding large bundles from indexint).Improve unbundling throughput by decoupling data fetching from unbundling.Add Envoy and core service ARM builds.Changed Improve resouce cleanup and shutdown behavior.Don't save Redis data to disk by default to help prevent memory issues on startup for small gateways.Reduce the amount of data sampled from large files by the observer.Ensure block poa2 field is not chached to reduce memory consumption.[Release 6] - 2024-01-29 Fixed Update observer to improve reliability of contract state synchronization and evaluation.[Release 5] - 2024-01-25 Added Added transaction offset indexing to support future data retrieval capabilities.Enabled IPv6 support in Envoy config.Added ability to configure observer report generation interval via the REPORT_GENERATION_INTERVAL_MS environmental variable. (Intended primarily for development and testing) Changed Updated observer to properly handle FQDN conflicts.Renamed most created_at columns to index to indexed_at for consistency and clarity.Fixed Updated LMDB version to remove Buffer workaround and fix occasional block cache errors.[Release 4] - 2024-01-11 Added Added circuit breakers around data index access to reduce impact of DB access contention under heavy requests loads.Added support for configuring data source priority via the ON_DEMAND_RETRIEVAL_ORDER environment variable.Updated observer to a version that retrieves epoch start and duration from contract state.Changed Set the Redis max memory eviction policy to allkeys-lru.Reduced default Redis max memory from 2GB to 256MB.Improved predictability and performance of GraphQL queries.Eliminated unbundling worker threads when filters are configured to skip indexing ANS-104 bundles.Reduced the default number of ANS-104 worker threads from 2 to 1 when unbundling is enabled to conserve memory.Increased nodejs max old space size to 8GB when ANS-104 workers > 1.Fixed Adjusted paths for chunks indexed by data root to include the full data root.[Release 3] - 2023-12-05 Added Support range requests (PR 61, PR 64) Note: serving multiple ranges in a single request is not yet supported.Release number in /ar-io/info response.Redis header cache implementation (PR 62).New default header cache (replaces old FS cache).LMDB header cache implementation (PR 60).Intended for use in development only.Enable by setting CHAIN_CACHE_TYPE=lmdb.Filesystem header cache cleanup worker (PR 68).Enabled by default to cleanup old filesystem cache now that Redis is the new default.Support for parallel ANS-104 unbundling (PR 65).Changed Used pinned container images tags for releases.Default to Redis header cache when running via docker-compose.Default to LMDB header cache when running via yarn start.Fixed Correct GraphQL pagination for transactions with duplicate tags.

---

# 135. Optimizing Data Handling in ARIO Gateway - ARIO Docs

Document Number: 135
Source: https://docs.ar.io/gateways/optimize-data
Words: 968
Extraction Method: html

The AR.IO Gateway provides powerful tools for optimizing how you access and serve specific types of data. By configuring filters and worker settings, you can focus your gateway on efficiently handling the data that matters most to your use case, ensuring quick and reliable access to relevant information.Understanding the Filtering System The AR.IO Gateway uses two filters to control how ANS104 data items are processed:ANS104_UNBUNDLE_FILTER: Controls which bundles are processed and unbundled ANS104_INDEX_FILTER: Controls which data items from unbundled bundles are stored in the database for querying These filters are configured through environment variables:By default, the gateway processes no bundles and indexes no data items. This allows you to selectively enable processing for the specific data types you need.For a detailed explanation of how to construct these filters, see our Filtering System documentation.Key Environment Variables Several environment variables control how your gateway processes data:Data Item Flushing The gateway uses a two-stage storage system for indexed data items:Temporary Storage: Newly indexed data items are first stored in a temporary table Stable Storage: Data items are periodically "flushed" from temporary to stable storage This process is controlled by two environment variables:DATA_ITEM_FLUSH_COUNT_THRESHOLD: Number of items to queue before flushing (default: 1000) MAX_FLUSH_INTERVAL_SECONDS: Maximum time between flushes (default: 600 seconds) The gateway will flush data items when either:The number of items in temporary storage reaches the threshold The time since the last flush exceeds the interval This batching approach helps optimize database performance by reducing the number of write operations.GraphQL Configuration The GRAPHQL_HOST setting determines how your gateway handles GraphQL queries. You have two options:Using arweave.net (Recommended for new gateways) Proxies queries to a gateway with a complete index of the blockweave Provides immediate access to all historical data No need to wait for local indexing May introduce additional latency from proxying Local-only Queries (Unset GRAPHQL_HOST) Responds to queries using only locally indexed data Faster response times for indexed data Requires complete local indexing (can take weeks for L1 transactions) No proxying overhead Only returns data that matches your indexing filters Note: Even with GRAPHQL_HOST set to arweave.net, your gateway will still maintain its own index based on your filters. This allows for quick access to frequently requested data while ensuring availability of all historical data.Common Use Cases Optimizing for Specific Data Types By configuring your filters and workers appropriately, you can optimize your gateway for different types of data:High-Volume Data: Configure workers to handle large amounts of data efficiently Specific Applications: Filter for particular app names or content types Filter Examples The AR.IO Gateway uses two distinct filters to control how ANS104 bundle data is processed:ANS104_UNBUNDLE_FILTER: Determines which bundles (including nested bundles) are unbundled ANS104_INDEX_FILTER: Determines which data items within a bundle have their data indexed Here are some practical examples of how to configure these filters for specific use cases:Specific Application Data This configuration demonstrates how to focus your gateway on data from a specific application. In this example, we show how to process and index all ArDrive-related transactions, but you can adapt this pattern for any application, using the App-Name tag. This approach is perfect for:Building application-specific services Creating application data archives Running application-focused analytics Supporting application infrastructure Reducing processing overhead by focusing only on relevant data In this example, the index filter uses the ArFS tag to only index ArFS-compliant data, which is a specific aspect of ArDrive applications. Index filters can be adjusted for any application's needs - the App-Name tag is particularly useful here, as data items within a bundle can have a different App-Name than the bundle that contains them.Personal Data Gateway This configuration is designed for users who want to run a personal gateway that only processes their own ArDrive data. It:Excludes common specific use case bundlers to reduce unnecessary processing Only indexes data owned by your wallet address Includes all ArDrive and Turbo app data Perfect for personal data management Personal Data Gateway All ArDrive Bundles (Excluding Common Bundlers) This configuration is useful for gateways that want to process ArDrive data while avoiding common bundlers. It's ideal for:Reducing processing overhead by excluding known bundlers Maintaining a clean dataset focused on direct ArDrive transactions Optimizing storage and processing resources Supporting ArDrive infrastructure with reduced resource requirements All ArDrive Bundles (Excluding Common Bundlers) Important Filter Considerations When configuring your filters, keep these points in mind:The unbundle filter determines which bundles are processed and unbundled The index filter determines which data items from unbundled bundles are indexed in the database When filtering by owner addresses, use the modulus of the Arweave public address in the unbundle filter Common Bundler Exclusions When configuring filters, you may want to exclude data from common bundlers:Bundler Addresses:Irys Node 1: -OXcT1sVRSA5eGwt2k6Yuz8-3e3g9WJi5uSE99CWqsBs Irys Node 2: ZE0N-8P9gXkhtK-07PQu9d8me5tGDxa_i4Mee5RzVYg Irys Node 3: 6DTqSgzXVErOuLhaP0fmAjqF4yzXkvth58asTxP3pNw Bundler App Names:Warp Redstone Kyve AO ArDrive Best Practices Start Small: Begin with conservative worker counts and adjust based on system performance Monitor Resources: Watch system memory and CPU usage when adjusting worker counts Reprocess Bundles with New Filters: Use FILTER_CHANGE_REPROCESS to reprocess bundles after changing filters Regular Maintenance: Enable background verification and cleanup features Performance Considerations When optimizing your gateway, consider these factors:System Resources: Worker counts should be balanced against available CPU cores and memory Storage Space: Indexing filters affect database size and query performance Network Bandwidth: Unbundling workers can generate significant network traffic Query Performance: More indexed data means larger databases but better query capabilities Review the Filtering System documentation for detailed filter syntax Check your system resources to determine optimal worker counts Start with basic filters and gradually refine based on your needs Monitor system performance and adjust settings as needed Optimization Strategy Focus on configuring your gateway to efficiently handle the specific data types you need. The default state processes no data, so you can selectively enable processing for your use case without worrying about unnecessary resource usage.

---

# 136. Parquet and ClickHouse Usage Guide - ARIO Docs

Document Number: 136
Source: https://docs.ar.io/gateways/parquet
Words: 1543
Extraction Method: html

Overview AR.IO gateway Release 33 introduces a new configuration option for using Parquet files and ClickHouse to improve performance and scalability of your AR.IO gateway for large datasets.This guide will walk you through the process of setting up ClickHouse with your AR.IO gateway, and importing Parquet files to bootstrap your ClickHouse database.What is Parquet?Apache Parquet is a columnar storage file format designed for efficient data storage and retrieval. Unlike row-based storage formats like SQLite, Parquet organizes data by column rather than by row, which provides several advantages for analytical workloads:Efficient compression: Similar data is stored together, leading to better compression ratios Columnar access: You can read only the columns you need, reducing I/O operations Predicate pushdown: Filter operations can be pushed down to the storage layer, improving query performance Current Integration with AR.IO Gateways In the current AR.IO gateway implementation, Parquet and ClickHouse run alongside SQLite rather than replacing it. This parallel architecture allows each database to handle what it does best:SQLite continues to handle transaction writes and updates ClickHouse with Parquet files is optimized for fast query performance, especially with large datasets The gateway continues to operate with SQLite just as it always has, maintaining all of its normal functionality. Periodically, the gateway will export batches of data from SQLite to Parquet files, which are then imported into ClickHouse. This batch-oriented approach is much more efficient than attempting to synchronize the databases in real-time, as it leverages Parquet's strength in handling large, immutable data sets.Note that despite Parquet's efficient compression, gateways may not see significant disk space reduction in all cases. While bundled transaction data is exported to Parquet, L1 data remains in SQLite. Without substantial unbundling and indexing filters, minimal data gets exported to Parquet, limiting potential storage savings.With ClickHouse integration enabled, GraphQL queries are primarily routed to ClickHouse, leveraging its superior performance for large datasets. This significantly improves response times while maintaining SQLite's reliability for transaction processing.Parquet vs. SQLite in AR.IO Gateways While SQLite is excellent for transactional workloads and small to medium datasets, it faces challenges with very large datasets:Feature SQLite Parquet + ClickHouse Storage model Row-based Column-based Query optimization Basic Advanced analytical optimization Compression Limited High compression ratios Scaling Limited by single file Distributed processing capable Write speed Fast for small transactions Optimized for batch operations Read speed for analytics Slower for large datasets Optimized for analytical queries Ideal use case Recent transaction data, OLTP Historical data, OLAP workloads Benefits for Gateway Operators Implementing Parquet and ClickHouse alongside SQLite in your AR.IO gateway offers several key advantages:Dramatically improved query performance for GraphQL endpoints, especially for large result sets Reduced storage requirements through efficient columnar compression Better scalability for growing datasets Faster bootstrapping of new gateways through Parquet file imports Reduced load on SQLite by offloading query operations to ClickHouse The primary focus of the Parquet/ClickHouse integration is the significant speed improvement for querying large datasets. Gateway operators managing significant volumes of data will notice substantial performance gains when using this configuration.Storage Considerations While Parquet files offer more efficient compression for the data they contain, it's important to understand the storage impact:Bundled transaction data is exported to Parquet and removed from SQLite, potentially saving space L1 data remains in SQLite regardless of Parquet configuration Space savings are highly dependent on your unbundling filters - without substantial unbundling configurations, minimal data gets exported to Parquet The more data you unbundle and export to Parquet, the greater the potential storage efficiency For gateway operators, this means proper filter configuration is crucial to realize storage benefits. The primary advantage remains significantly improved query performance for large datasets, with potential space savings as a secondary benefit depending on your specific configuration.The following sections will guide you through setting up ClickHouse with your AR.IO gateway, exporting data from SQLite to Parquet, and importing Parquet files to bootstrap your ClickHouse database.Note The below instructions are designed to be used in a linux environment. Windows and MacOS users must modify the instructions to use the appropriate package manager/ command syntax for their platform.Unless otherwise specified, all commands should be run from the root directory of the gateway.Installing ClickHouse ClickHouse is a powerful, open-source analytical database that excels at handling large datasets and complex queries. It is the tool used by the gateway to integrate with the Parquet format. To integrate ClickHouse with your AR.IO gateway, follow these steps:It is recommended to use official pre-compiled deb packages for Debian or Ubuntu. Run these commands to install packages:This will verify the installation package from official sources and enable installation via apt-get.This will perform the actual installation of the ClickHouse server and client.During installation, you will be prompted to set a password for the default user. This is required to connect to the ClickHouse server.Advanced users may also choose to create a designated user account in clickhouse for the gateway to use, but the default gateway configuration will assume the default user.Configure Gateway to use ClickHouse Because the gateway will be accessing ClickHouse, host address andthe password for the selected user must be provided. This is done via the CLICKHOUSE_PASSWORD environment variable.Update your.env file with the following:If you set a specific user account for the gateway to use, you can set the CLICKHOUSE_USER environment variable to the username.CLICKHOUSE_USER=<your-username> If omitted, the gateway will use the default user.Additionally, The Parquet file provided below contains an unbundled data set that includes all data items uploaded via an ArDrive product, including Turbo. Because of this, it is recommended to include unbundling filters that match, or expand, this configuration.ANS104_UNBUNDLE_FILTER='{ "and": [ { "not": { "or": [ { "tags": [ { "name": "Bundler-App-Name", "value": "Warp" } ] }, { "tags": [ { "name": "Bundler-App-Name", "value": "Redstone" } ] }, { "tags": [ { "name": "Bundler-App-Name", "value": "KYVE" } ] }, { "tags": [ { "name": "Bundler-App-Name", "value": "AO" } ] }, { "attributes": { "owner_address": "-OXcT1sVRSA5eGwt2k6Yuz8-3e3g9WJi5uSE99CWqsBs" } }, { "attributes": { "owner_address": "ZE0N-8P9gXkhtK-07PQu9d8me5tGDxa_i4Mee5RzVYg" } }, { "attributes": { "owner_address": "6DTqSgzXVErOuLhaP0fmAjqF4yzXkvth58asTxP3pNw" } } ] } }, { "tags": [ { "name": "App-Name", "valueStartsWith": "ArDrive" } ] } ] }'
ANS104_INDEX_FILTER='{ "tags": [ { "name": "App-Name", "value": "ArDrive-App" } ] }' Lastly, you must have a gateway admin password set. This is used for the periodic export of data from SQLite to Parquet.ADMIN_API_KEY=<example> Once the.env file is updated, restart the gateway to apply the changes.A Parquet archive file is available for download from ar://JVmsuD2EmFkhitzWN71oi9woADE4WUfvrbBYgremCBM   . This file contains an unbundled data set that includes all data items uploaded via an ArDrive product, current to April 23, 2025, and compressed using tar.gz.To download the file, run the following command:or visit the url https://arweave.net/JVmsuD2EmFkhitzWN71oi9woADE4WUfvrbBYgremCBM    and download the file manually.Note If downloaded manually, it will download as a binary file named JVmsuD2EmFkhitzWN71oi9woADE4WUfvrbBYgremCBM. This is normal and must be converted to a tar.gz file by renaming it to 2025-04-23-ardrive-ans104-parquet.tar.gz.It should also be placed in the root directory of the gateway.The downloaded file will be approximately 3.5GB in size.Extracting and Importing the Parquet File With the parquet file downloaded and placed in the root directory of the gateway, you can extract the file and import it into ClickHouse.This will extract the file into a directory named 2025-04-23-ardrive-ans104-parquet, and take a while to complete.Next, if you do not already have a data/parquet directory, you must create it. Release 33 does not have this directory by default, but future Releases will. You can create the directory by using the following command:or by starting the gateway ClickHouse container with the following command:Note Depending on your system configurations, allowing the gateway to create the directory may result in the directory being created with incorrect permissions. If this is the case, you can remove the restrictions by running the following command:With the directory created, you can now move the extracted parquet files into it.When this is complete, you can run the import script to import the parquet files into ClickHouse.If you haven't done so already, start the ClickHouse container with the following command:Then run the import script with the following command:./scripts/clickhouse-import This process will take several minutes, and will output the progress of the import.Verifying Successful Import To verify that the import was successful, run the following commands:Being sure to replace <your-password> with the password you set for the selected ClickHouse user.This should return a count of the number of unique transactions in the parquet file, which is 32712311.You can also verify that the data is being served by the gateway's GraphQL endpoint by ensuring the gateway is not proxying its GraphQL queries (Make sure GRAPHQL_HOST is not set) and running the following command:Starting and Stopping the Gateway with ClickHouse The gateway ClickHouse container is run as a "profile" in the main docker compose file. That means you must specify the profile when starting or stopping the gateway if you want to include the ClickHouse container in the commands.To start the gateway with the ClickHouse profile, run the following command:This will start all of the containers normally covered by the docker compose up command, but will also start the ClickHouse container.To stop the gateway with the ClickHouse profile, run the following command:This will stop all of the containers normally covered by the docker compose down command, but will also stop the ClickHouse container.To start or stop only the ClickHouse container, you can use the following commands:and

---

# 137. Content Moderation - ARIO Docs

Document Number: 137
Source: https://docs.ar.io/gateways/moderation
Words: 543
Extraction Method: html

Overview Arweave is a network designed for permanent storage of data. It is a practical impossibility for data to be wholly removed from the network once it has been uploaded.The AR.IO Network has adopted Arweave's voluntary content moderation model, whereby every participant of the network has the autonomy to decide which content they want to (or can legally) store, serve, and see. Each gateway operating on the network has the right and ability to blocklist any content, ArNS name, or address that is deemed in violation of its content policies or is non-compliant with local regulations.NOTE Overly restrictive content policies may impact a gateway's likelihood of
receiving protocol rewards.Gateway operators may set content to be blocked by their gateway by submitting a Put request to their gateway defining the content to be blocked. This requires that the ADMIN_API_KEY environmental variable to be set in order to authenticate the moderation request.The simplest method for submitting moderation requests to a gateway is to use curl in a terminal.Authentication Moderation requests must contain the gateway's ADMIN_API_KEY in the request Header, as Authorization: Bearer.For example, if a gateway's ADMIN_API_KEY is set to secret, any request must contain Authorization: Bearer secret in the Header.Block Data Specific data items can be blocked by a gateway operator by submitting a Put request containing a json object with three keys:id: The Arweave transaction Id of the data item to be blocked.notes: Any note the gateway operator wants to leave him/herself as to the reason the content is blocked.source: A note as to where the content was identified as requiring moderation. i.e. a public block list.Requests to block data must be submitted to the gateway's /ar-io/admin/block-data endpoint.Unblock Data At this time, blocked data items can only be unblocked by manually deleting the corresponding row from the data/sqlite/moderation.db database.
The Arweave transaction Id of the blocked data item is stored in the database as raw bytes, which sqlite3 accepts as a BLOB (Binary Large OBject), and so cannot be accessed easily using the original transaction Id, which is a base64url.
Sqlite3 is able to interact with a hexadecimal representation of the BLOB, by using a BLOB literal. To do so, wrap a hexadecimal representation of the Arweave transaction Id in single quotes, and prepend an X i.e. X'de5cb181b804bea352bc9ad35f627b09f472721503e4a0a51618552f24cf3424'.Where possible, consider using the notes or source values to identify rows for deletion rather than the id.Block ArNS Name ArNS names can be blocked so that a gateway will refuse to serve their associated content even if the name holder updates the Arweave transaction Id that the name points at.This is done via an authenticated PUT request to the endpoint /ar-io/admin/block-name containing a json object with three keys:name: The ArNS name to be blocked.notes: Any note the gateway operator wants to leave him/herself as to the reason the content is blocked.source: A note as to where the content was identified as requiring moderation. i.e. a public block list.Undernames For moderation purposes, each undername of an ArNS name is treated as a separate name and must be moderated separately.Unblock ArNS Name Gateway operators can unblock ArNS names that were previously blocked.This is done via an authenticated PUT request to the endpoint /ar-io/admin/unblock-name containing a json object with a single key:name: The ArNS name to be unblocked

---

# 138. ARIO Node Release Notes - ARIO Docs

Document Number: 138
Source: https://docs.ar.io/gateways/release-notes#
Words: 8770
Extraction Method: html

AR.IO Release Notes Overview Welcome to the documentation page for the AR.IO gateway release notes. Here, you will find detailed information about each version of the AR.IO gateway, including the enhancements, bug fixes, and any other changes introduced in every release. This page serves as a comprehensive resource to keep you informed about the latest developments and updates in the AR.IO gateway. For those interested in exploring the source code, each release's code is readily accessible at our GitHub repository: AR.IO gateway change logs. Stay updated with the continuous improvements and advancements in the AR.IO gateway by referring to this page for all release-related information.[Release 46] - 2025-08-18 This is a recommended release that introduces AR.IO network chunk retrieval with cryptographic validation and enhanced observability. Gateway operators can now retrieve chunks directly from AR.IO peers with the same security guarantees as Arweave network chunks, significantly improving chunk caching and retrieval performance.Added Added AR.IO network chunk source enabling chunk retrieval from AR.IO peers with weighted peer selection, retry logic, and cryptographic validation to prevent serving of corrupted or malicious data.Added comprehensive OpenTelemetry tracing for chunk retrieval operations providing visibility into performance, cache behavior, and source attribution across the entire pipeline.Added HEAD request support to /chunk/{offset} endpoint with ETag headers for efficient caching and conditional request handling with If-None-Match support.Added chunk source headers for traceability: X-AR-IO-Chunk-Source-Type indicating data source, X-AR-IO-Chunk-Host with peer hostname, and X-Cache for cache status.Added RFC 9530 Content-Digest header support for standard-compliant content integrity verification in data and chunk responses.Added configurable composite chunk sources with parallelism control via CHUNK_DATA_RETRIEVAL_ORDER and CHUNK_METADATA_RETRIEVAL_ORDER environment variables supporting comma-separated source ordering.Added OpenAPI documentation for /ar-io/peers endpoint.Changed Renamed ar-io-peers to ar-io-network as the preferred configuration name while maintaining backwards compatibility.Enhanced /ar-io/peers endpoint to include both data and chunk weights for AR.IO gateway peers.Fixed Fixed ArNS custom 404 pages to prevent incorrect ArNS headers from being propagated to other gateways.[Release 45] - 2025-08-11 This is an optional release that enhances chunk broadcasting with improved preferred peer management, adds a hash-based partitioning filter for distributed data processing, fixes ArNS basename cache refresh issues, and includes comprehensive documentation improvements with a new glossary of AR.IO Node terminology.Added Added hash partitioning filter (MatchHashPartition) for distributing transaction and data item processing across multiple nodes with configurable partition ranges.Added comprehensive glossary documentation covering AR.IO Node terminology, concepts, and architectural components.Changed Improved chunk broadcasting preferred peer management with doubled default per-node queue depth threshold and ensured preferred peers are always prioritized first.Enhanced circuit breaker metrics with more detailed labels for better monitoring of data source failures.Improved ArNS resolution to properly propagate 404 errors from trusted gateway resolution (a more complete fix is coming in the next release).Expanded OTEL tracing to include ArNS cache operations for improved observability of name resolution and cache hydration.Fixed Fixed unreliable ArNS basename cache refreshes by adding retry logic for pagination failures and replacing p-debounce with timestamp-based debouncing for more predictable behavior.Fixed undefined headers handling in data requests.Fixed invalid cache hits by ensuring base64url encoded IDs are properly validated before use.Fixed routes data handling for undefined IDs in validity checks.[Release 44] - 2025-07-28 This is a recommended release that introduces efficient range request support for contiguous data retrieval from chunks, adds bundle metadata columns with offset indexing to improve offset availability throughout the network, enhances Merkle path parsing compatibility, and includes comprehensive documentation for offsets and Merkle paths.Added Added efficient range request support for chunk data retrieval, enabling optimized verifiable contiguous data fetching directly from Arweave nodes.Added bundle metadata columns to data.db to improve offset availability across the gateway network.Added OTEL (OpenTelemetry) tracing support for chunk POST operations, providing better observability for chunk broadcasting performance.Added OTEL environment variables to docker-compose.yaml for easier configuration of distributed tracing.Added comprehensive Arweave Merkle tree structure documentation detailing the data organization and validation rules.Added detailed documentation explaining Arweave transaction and chunk offset calculations.Added merkle-path-parser with full Arweave compatibility for improved Merkle proof validation.Changed Implemented promise-based chunk caching system replacing the previous WeakMap implementation, improving memory efficiency and cache reliability.Extended CompositeChunkSource to implement all chunk interfaces, providing a more unified chunk data access layer.[Release 43] - 2025-07-21 This is a recommended release that enables data verification by default for data items linked to ArNS names, improves chunk broadcasting efficiency, and adds automatic chunk data cache cleanup.Added Added automatic chunk data cache cleanup functionality with configurable retention period. Chunks are now automatically removed after 4 hours by default (configurable via CHUNK_DATA_CACHE_CLEANUP_THRESHOLD). The cleanup can be disabled by setting ENABLE_CHUNK_DATA_CACHE_CLEANUP=false. This helps manage disk space usage while maintaining cache performance benefits.Added demand-driven opt-out background verification for ArNS data. When ArNS names are requested, the system now proactively verifies the underlying data asynchronously in the background by unbundling verified chunk data retrieved directly from Arweave nodes. This ensures ArNS-served content is prioritized for verification, improving data integrity guarantees for frequently accessed named content.Changed Simplified chunk data storage by removing the dual-storage approach (by-hash and by-dataroot with symlinks). Chunks are now stored directly by data root only, reducing complexity and improving performance.Revamped chunk broadcasting architecture from 3-tier system to unified peer-based approach. Chunk broadcasting now uses individual fastq queues per peer with configurable concurrency and queue depth protection. Added support for preferred chunk POST peers via PREFERRED_CHUNK_POST_URLS environment variable. Configuration defaults have been optimized: CHUNK_POST_PEER_CONCURRENCY now defaults to match CHUNK_POST_MIN_SUCCESS_COUNT (3) to avoid over-broadcasting, and CHUNK_POST_PER_NODE_CONCURRENCY defaults to match CHUNK_POST_QUEUE_DEPTH_THRESHOLD (10) for consistent per-node load management. This change improves broadcast reliability and performance while simplifying the codebase by removing circuit breakers and tier-based logic.Modified DataVerificationWorker to ensure data item IDs (not just root IDs) have their retry count incremented, preventing IDs from being stuck without retry attempts. This improves the reliability of the data verification process.Fixed Fixed experiment bash Parquet export script generating filenames with count_star() instead of actual row counts for blocks and tags files. The script now correctly uses the -noheader flag when retrieving counts for filename generation.Fixed missing directory existence checks in FsCleanupWorker to prevent errors when attempting to scan non-existent directories during filesystem cleanup operations.[Release 42] - 2025-07-14 This is an optional release that improves peer request traceability, adds HyperBEAM URL support, and includes draft AI-generated technical documentation.Added Added support for optional HyperBEAM URL configuration via AO_ANT_HYPERBEAM_URL environment variable. In the future this allows ANT processes to use HyperBEAM nodes for caching and serving state, reducing pressure on compute units for simple read requests.Added AI-generated technical documentation covering AR.IO gateway architecture, data retrieval, Arweave connectivity, ArNS name resolution system, centralization analysis, and database architecture. These guides in docs/drafts/ are generally correct but should not be considered authoritative.Added origin and release information to query string parameters in outbound requests to both peer gateways and trusted gateways. Data requests now include ar-io-hops, ar-io-origin, ar-io-origin-release, ar-io-arns-record, and ar-io-arns-basename as query parameters, improving network observability and request tracing across the entire gateway network.Changed Implemented X-AR-IO header initialization for outbound peer requests while removing x-ar-io-origin and x-ar-io-origin-node-release headers from responses. This change maintains necessary header functionality for peer communication while reducing unnecessary header overhead in responses.Updated @ar.io/sdk dependency to support optional HyperBEAM URL functionality.[Release 41] - 2025-06-30 Added Added preferred chunk GET node URLs configuration via PREFERRED_CHUNK_GET_NODE_URLS environment variable to enable chunk-specific peer prioritization. Preferred URLs receive a weight of 100 for prioritization and the system selects 10 peers per attempt by default.Added hash validation for peer data fetching by including X-AR-IO-Expected-Digest header in peer requests when hash is available, validating peer responses against expected hash, and immediately rejecting mismatched data.Added DOCKER_NETWORK_NAME environment variable to configure the Docker network name used by Docker Compose.Added draft guide for running a community gateway.Added draft data verification architecture document.Changed Removed trusted node fallback for chunk retrieval. Chunks are now retrieved exclusively from peers, with the retry count increased from 3 to 50 to ensure reliability without the trusted node fallback.Fixed Fixed inverted logic preventing symlink creation in FsChunkDataStore.Fixed Content-Length header for range requests and 304 responses, properly setting header for single and multipart range requests and removing entity headers from 304 Not Modified responses per RFC 7232.Fixed MaxListenersExceeded warnings by adding setMaxListeners to read-through data cache.Fixed potential memory leaks in read-through data cache by using once instead of on for error and end event listeners.[Release 40] - 2025-06-23 This is an optional release that primarily improves caching when data is fetched from peers.Added Added experimental flush-to-stable script for manual database maintenance. This script allows operators to manually flush stable chain and data item tables, mirroring the logic of StandaloneSqliteDatabase.flushStableDataItems. WARNING: This script is experimental and directly modifies database contents. Use with caution and ensure proper backups before running.Changed Replaced yesql with custom SQL loader that handles comments better, improving SQL file parsing and maintenance.Switched to SPDX license headers to reduce LLM token usage, making the codebase more efficient for AI-assisted development.Improved untrusted data handling and hash validation in cache operations. The cache now allows caching when a hash is available for validation even for untrusted data sources, but only finalizes the cache when the computed hash matches a known trusted hash. This prevents cache poisoning while still allowing data caching from untrusted sources when the data can be validated.[Release 39] - 2025-06-17 This release enhances observability and reliability with new cache metrics, improved data verification capabilities, and automatic failover between chain data sources. The addition of ArNS-aware headers enables better data prioritization across the gateway network. This is a recommended but not urgent upgrade.Added Added filesystem cache metrics with cycle-based tracking. Two new Prometheus metrics track cache utilization: cache_objects_total (number of objects in cache) and cache_size_bytes (total cache size in bytes). Both metrics include store_type and data_type labels to differentiate between cache types (e.g., headers, contiguous_data). Metrics are updated after each complete cache scan cycle, providing accurate visibility into filesystem cache usage.Added X-AR-IO-Data-Id header to all data responses. This header shows the actual data ID being served, whether from a direct ID request or manifest path resolution, providing transparency about the content being delivered.Added automatic data item indexing when data verification is enabled. When ENABLE_BACKGROUND_DATA_VERIFICATION is set to true, the system now automatically enables data item indexing (ANS104_UNBUNDLE_FILTER) with an always: true filter if no filter is explicitly configured. This ensures bundles are unbundled to verify that data items are actually contained in the bundle associated with the Arweave transaction's data root.Added ArNS headers to outbound gateway requests to enable data prioritization. The generateRequestAttributes function now includes ArNS context headers (X-ArNS-Name, X-ArNS-Basename, X-ArNS-Record) in requests to other gateways and Arweave nodes, allowing downstream gateways to effectively prioritize ArNS data requests.Added configurable Docker Compose host port environment variables (CORE_PORT, ENVOY_PORT, CLICKHOUSE_PORT, CLICKHOUSE_PORT_2, CLICKHOUSE_PORT_3, OBSERVER_PORT) to allow flexible port mapping while maintaining container-internal port compatibility and security.Added Envoy aggregate cluster configuration for automatic failover between primary and fallback chain data sources. The primary cluster (default: arweave.net:443) uses passive outlier detection while the fallback cluster (default: peers.arweave.xyz:1984) uses active health checks. This enables zero-downtime failover between HTTPS and HTTP endpoints with configurable FALLBACK_NODE_HOST and FALLBACK_NODE_PORT environment variables.Changed Streamlined background data retrieval to reduce reliance on centralized sources. The default BACKGROUND_RETRIEVAL_ORDER now only includes chunks,s3, removing trusted-gateways and tx-data from the default configuration. This prioritizes verifiable chunk data and S3 storage for background operations like unbundling.Removed ar-io.net from default trusted gateways list and removed TRUSTED_GATEWAY_URL default value to reduce load on ar-io.net now that P2P data retrieval is re-enabled. Existing deployments with TRUSTED_GATEWAY_URL explicitly set will continue to work for backwards compatibility.[Release 38] - 2025-06-09 This release focuses on data integrity and security improvements, introducing trusted data verification and enhanced header information for data requests. Upgrading to this release is recommended but not urgent.Added Added X-AR-IO-Trusted header to indicate data source trustworthiness in responses. This header helps clients understand whether data comes from a trusted source and works alongside the existing X-AR-IO-Verified header to provide data integrity information. The system now filters peer data by requiring peers to indicate their content is either verified or trusted, protecting against misconfigured peers that may inadvertently serve unintended content (e.g., provider default landing pages) instead of actual Arweave data.Added If-None-Match header support for HTTP conditional requests enabling better client-side caching efficiency. When clients send an If-None-Match header that matches the ETag, the gateway returns a 304 Not Modified response with an empty body, reducing bandwidth usage and improving performance.Added digest and hash headers for data HEAD requests to enable client-side data integrity verification.Added EC2 IMDS (instance-profile) credential support for S3 data access, improving AWS authentication in cloud environments.Added trusted data flag to prevent caching of data from untrusted sources, ensuring only verified and reliable content is stored locally while still allowing serving of untrusted data when necessary.Changed Re-enabled ar-io-peers as fallback data source in configuration for improved data availability.Updated trusted node configuration to use arweave.net as the default trusted node URL.Updated ETag header format to use properly quoted strings (e.g., "hash" instead of hash) following HTTP/1.1 specification standards for improved compatibility with caching proxies and clients.[Release 37] - 2025-06-03 This is a recommended release due to the included observer robustness improvements. It also adds an important new feature - data verification for preferred ArNS names. When preferred ArNS names are set, the bundles containing the data they point to will be locally unbundled (verifying data item signatures), and the data root for the bundle will be compared to the data root in the Arweave chain (establishing that the data is on Arweave). To enable this feature, set your preferred ArNS names, turn on unbundling by setting ANS104_DOWNLOAD_WORKERS and ANS104_UNBUNDLE_WORKERS both to 1, and set your ANS104_INDEX_FILTER to a filter that will match the data items for your preferred names. If you don't know the filter, use {"always": true}, but be aware this will index the entire bundle for the IDs related to your preferred names.Note: this release contains migrations to data.db. If your node appears unresponsive please check core service logs to determine whether migrations are running and wait for them to finish.Added Added prioritized data verification system for preferred ArNS names, focusing computational resources on high-priority content while enabling flexible root transaction discovery through GraphQL fallback support.Added verification retry prioritization system with tracking of retry counts, priority levels, and attempt timestamps to ensure bundles do not get stuck retrying forever.Added improved observer functionality with best-of-2 observations and higher compression for more reliable network monitoring.Added MAX_VERIFICATION_RETRIES environment variable (default: 5) to limit verification retry attempts and prevent infinite loops for consistently failing data items.Added retry logic with exponential backoff for GraphQL queries to handle rate limiting (429) and server errors with improved resilience when querying trusted gateways for root bundle IDs.Changed Updated dependencies: replaced deprecated express-prometheus-middleware with the actively maintained express-prom-bundle library and updated prom-client to v15.1.3 for better compatibility and security.Updated Linux setup documentation to use modern package installation methods, replacing apt-key yarn installation with npm global install and updating Node.js/nvm versions.Improved route metrics normalization with explicit whitelist function for better granularity and proper handling of dynamic segments.Fixed Fixed docker-compose configuration to use correct NODE_MAX_OLD_SPACE_SIZE environment variable name.Fixed production TypeScript build configuration to exclude correct "test" directory path.Fixed Parquet exporter to properly handle data item block_transaction_index exports, preventing NULL value issues.Fixed bundles system to copy root_parent_offset when flushing data items to maintain data integrity.Fixed ClickHouse auto-import script to handle Parquet export not_started status properly.Fixed docker-compose ClickHouse configuration to not pass conflicting PARQUET_PATH environment variable to container scripts.Fixed verification process for data items that have not been unbundled by adding queue bundle support and removing bundle join constraint to ensure proper verification of data items without indexed root parents.[Release 36] - 2025-05-27 This is a recommended but not essential upgrade. The most important changes are the preferred ArNS caching feature for improved performance on frequently accessed content and the observer's 80% failure threshold to prevent invalid reports during network issues.Added Added preferred ArNS caching functionality that allows configuring lists of ArNS names to be cached longer via PREFERRED_ARNS_NAMES and PREFERRED_ARNS_BASE_NAMES environment variables. When configured, these names will be cleaned from the filesystem cache after PREFERRED_ARNS_CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD instead of the standard cleanup threshold (CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD). This is accomplished by maintaining an MRU (Most Recently Used) list of ArNS names in the contiguous metadata cache. When filesystem cleanup runs, it checks this list to determine which cleanup threshold to apply. This feature enables gateway operators to ensure popular or important ArNS names remain cached longer, improving performance for frequently accessed content.Added ArNS headers to responses: X-ArNS-Name, X-ArNS-Basename, and X-ArNS-Record to help identify which ArNS names were used in the resolution.Changed Updated observer to prevent report submission when failure rate exceeds 80%. This threshold helps guard against both poorly operated observers and widespread network issues. In the case of a widespread network issue, the assumption is that most gateway operators are well intentioned and will work together to troubleshoot and restore both observations and network stability, rather than submitting reports that would penalize functioning gateways.Updated default trusted gateway in docker-compose Envoy configuration to ar-io.net for improved robustness and alignment with core service configuration.Improved range request performance by passing ranges directly to getData implementations rather than streaming all data and extracting ranges.Fixed Fixed missing cache headers (X-Cache and other data headers) in range request responses to ensure consistent cache header behavior across all request types.Fixed async streaming for multipart range requests by using async iteration instead of synchronous reads, preventing potential data loss.Fixed ArNS resolution to properly exclude www subdomain from resolution logic.Fixed test reliability issues by properly awaiting stream completion before making assertions.Fixed chunk broadcasting to not await peer broadcasts, as they are best-effort operations.[Release 35] - 2025-05-19 This is a low upgrade priority release. It contains a small caching improvement and routing fix. Upgrading to help test it is appreciated but not essential.Changed Adjusted filesystem data expiration to be based on last request times rather than file access times which may be inaccurate.Adjusted CORS headers to include content-* headers.Fixed Fixed regex used to expose /api-docs when an apex ArNS name is set.[Release 34] - 2025-05-05 Given the resilience provided by adding a second trusted gateway URL, it is recommended that everyone upgrade to this release.Added Added peer list endpoints for retrieving information about Arweave peers and ar.io gateway peers.Added ar-io.net as a secondary trusted gateway to increase data retrieval resilience by eliminating a single point of failure.Added circuit breaker for Arweave peer chunk posting.Changed Created directories for DuckDB and Parquet to help avoid permission issues by the directories being created by containers.Fixed Fixed GraphQL ClickHouse error when returning block ID and timestamp.Fixed the tx-chunks-data-source to throw a proper error (resulting in a 404) when the first chunk is missing rather than streaming a partial response.[Release 33] - 2025-05-05 Added Added a [Parquet and ClickHouse usage guide]. Using ArDrive as an example, it provides step by step instructions about how to bulk load Parquet and configure continuous ingest of bundled data items into ClickHouse. This allows the ar-io-node to support performant GraphQL queries on larger data sets and facilitates sharing indexing work across gateways via distribution of Parquet files.Added support for configurable ArNS 404 pages using either:ARNS_NOT_FOUND_TX_ID: Transaction ID for custom 404 content ARNS_NOT_FOUND_ARNS_NAME: ArNS name to resolve for 404 content Added experimental /chunk/ GET route for serving chunk data by absolute offset either the local cache.Added support for AWS_SESSION_TOKEN in the S3 client configuration.Expanded ArNS OTEL tracing to improve resolution behavior observability.Added support for setting a ClickHouse username and password via the CLICKHOUSE_USERNAME and CLICKHOUSE_PASSWORD environment variable. When using ClickHouse, CLICKHOUSE_PASSWORD should always be set. However, CLICKHOUSE_USERNAME can be left unset. The username default will be used in that case.Added support for configuring the port used to connect to ClickHouse via the CLICKHOUSE_PORT environment variable.Changed Disabled ClickHouse import timing logging by default. It can be enabled via environment variable - DEBUG when running the service standalone or CLICKHOUSE_DEBUG when using Docker Compose Upgraded to ClickHouse 25.4.Fixed Ensure .env is read in clickhouse-import script.[Release 32] - 2025-04-22 Changed Reenabled parallel ArNS resolution with removal of misplaced global limit. Refer to release 30 notes for more details on configuration and rationale.Added a timeout for the last ArNS resolver in ARNS_RESOLVER_PRIORITY_ORDER. It defaults to 30 seconds and is configurable using ARNS_COMPOSITE_LAST_RESOLVER_TIMEOUT_MS. This helps prevent promise build up if the last resolver stalls.Fixed Fixed apex ArNS name handling when a subdomain is present in ARNS_ROOT_HOST.Fixed a case where fork recovery could stall due to early flushing of unstable chain data.Restored observer logs by removing unintentional default log level override in docker-compose.yaml.[Release 31] - 2025-04-11 Changed Improved peer TX header fetching by fetching from a wider range of peers and up/down weighting peers based on success/failure.Fixed Rolled back parallel ArNS resolution changes that were causing ArNS resolution to slow down over time.[Release 30] - 2025-04-04 Added Added support for filtering Winston logs with a new LOG_FILTER environment variable.Example filter: {"attributes":{"class":"ArweaveCompositeClient"}} to only show logs from that class.Use CORE_LOG_FILTER environment variable when running with docker-compose.Added parallel ArNS resolution capability.Configured via ARNS_MAX_CONCURRENT_RESOLUTIONS (default: 1).This foundation enables future enhancements to ArNS resolution and should generally not be adjusted at present.Changed Improved ClickHouse auto-import script with better error handling and continuous operation through errors.Reduced maximum header request rate per second to trusted node to load on community gateways.Optimized single owner and recipient queries on ClickHouse with specialized sorted tables.Used ID sorted ClickHouse table for ID queries to improve performance.Fixed Fixed data alignment in Parquet file name height boundaries to ensure consistent import boundaries.Removed trailing slashes from AO URLs to prevent issues when passing them to the SDK.Only prune SQLite data when ClickHouse import succeeds to prevent data loss during exports.[Release 29] - 2025-03-21 Changed Temporarily default to trusted gateway ArNS resolution to reduce CU load as much possible. On-demand CU resolution is still available as a fallback and the order can be modified by setting ARNS_RESOLVER_PRIORITY_ORDER.Remove duplicate network process call in on-demand resolver.Don't wait for network process debounces in the on-demand resolver.Slow network process dry runs no longer block fallback to next resolver.Added Added support for separate CUs URLs for the network and ANT processes via the NETWORK_AO_CU_URL and ANT_AO_CU_URL process URLs respectively. If either is missing the AO_CU_URL is used instead with a fallback to the SDK default URL if AO_CU_URL is also unspecified.Added CU URLs to on-demand ArNS resolver logs.Added circuit breakers for AR.IO network process CU dry runs. By default they use a 1 minute timeout and open after 30% failure over a 10 minute window and reset after 20 minutes.Fixed Owners in GraphQL results are now correctly retrieved from data based on offsets when using ClickHouse.[Release 28] - 2025-03-17 Changed Raised name not found name list refresh interval to 2 minutes to reduce load on CUs. This increases the maximum amount of time a user may wait for a new name to be available. Future releases will introduce other changes to mitigate this delay.Adjusted composite ArNS resolver to never timeout resolutions from the last ArNS resolver in the resolution list.Added Added support for serving a given ID or ArNS name from the apex domain of a gateway. If using an ID, set the APEX_TX_ID environment variable. If using an ArNS name, set the APEX_ARNS_NAME environment variable.Added BUNDLE_REPAIR_UPDATE_TIMESTAMPS_INTERVAL_SECONDS, BUNDLE_REPAIR_BACKFILL_INTERVAL_SECONDS, and BUNDLE_REPAIR_FILTER_REPROCESS_INTERVAL_SECONDS environment variables to control the interval for retrying failed bundles, backfilling bundle records, and reprocessing bundles after a filter change. Note: the latter two are rarely used. Queuing bundles for reprocessing via the /ar-io/admin/queue-bundle endpoint is usually preferable to automatic reprocessing as it is faster and offers more control over the reprocessing behavior.Fixed Signatures in GraphQL results are now correctly retrieved from data based on offsets when using ClickHouse.Adjusted exported Parquet file names to align with expectations of ClickHouse import script.Ensured that bundle indexing status is properly reset when bundles are manually queued after an unbundling filter change has been made.[Release 27] - 2025-02-20 Changed Set process IDs for mainnet.Increase default AO CU WASM memory limit to 17179869184 to support mainnet
process.[Release 26] - 2025-02-13 Added Added a per resolver timeout in the composite ArNS resolver. When the
composite resolver attempts resolution it is applied to each resolution
attempt. It is configurable via the ARNS_COMPOSITE_RESOLVER_TIMEOUT_MS and
defaults to 3 seconds in order to allow a fallback attempt before the default
observer timeout of 5 seconds.Added a TURBO_UPLOAD_SERVICE_URL environment variable to support
configuration of the bundler used by the observer (TurboSDK defaults are
used if not set).Added a REPORT_DATA_SINK environment variable that enables switching the
method used to post observer reports. With the default, turbo, it sends
data items via a Turbo compatible bundler. Switching it to arweave will
post base layer transactions directly to Arweave instead.Added a /ar-io/admin/bundle-status/<id> endpoint that returns the counters
and timestamps from the bundles row in data.db. This can be used for
monitoring unbundling progress and scripting (e.g., to skip requeuing already
queued bundles).Added more complete documentation for filters.Changed Use arweave.net as the default GraphQL URL for AO CUs since most gateways
will not have a complete local AO data item index.Use a default timeout of 5 seconds when refreshing Arweave peers to prevent
stalled peer refreshes.Cache selected gateway peer weights for the amount of time specified by the GATEWAY_PEERS_WEIGHTS_CACHE_DURATION_MS environment variable with a default
of 5 seconds to avoid expensive peer weight recomputation on each request.Chunk broadcasts to primary nodes occur in parallel with a concurrency limit
defaulting to 2 and configurable via the CHUNK_POST_CONCURRENCY_LIMIT environment variable.Added circuit breakers for primary chunk node POSTs to avoid overwhelming
chunk nodes when they are slow to respond.Fixed Properly cleanup timeout and event listener when terminating the data
root computation worker.Count chunk broadcast exceptions as errors in the arweave_chunk_broadcast_total metric.[Release 25] - 2025-02-07 Added Added support for indexing and querying ECDSA signed Arweave transactions.Expanded the OpenAPI specification to cover the entire gateway API and
commonly used Arweave node routes.ArNS undername record count limits are now enforced. Undernames are sorted
based on their ANT configured priority with a fallback to name comparisons
when priorities conflict or are left unspecified. Enforcement is enabled by
default but can be disabled by setting the ARNS_RESOLVER_ENFORCE_UNDERNAME_LIMIT to false.Changed Renamed the ario-peer data source to ar-io-peers for consistency and
clarity. ario-peer will continue to work for backwards compatibility but is
considered deprecated.Use AR.IO gateway peers from the ar.io gateway address registry (GAR) as the
last fallback for fetching data when responding to client data requests. This
has the benefit of making the network more resilient to trusted gateway
disruptions, but it can also result in nodes serving data from less trusted
sources if it is not found in the trusted gateway. This can be disabled by
using a custom ON_DEMAND_RETRIEVAL_ORDER that does not include ar-io-peers.Arweave data chunk requests are sent to the trusted node first with a
fallback to Arweave peers when chunks are unavailable on the trusted node.
This provides good performance by default with a fallback in case there are
issues retrieving chunks from the trusted node.Increased the observer socket timeout to 5 seconds to accommodate initial
slow responses for uncached ArNS resolutions.Disabled writing base layer Arweave signatures to the SQLite DB by default to
save disk space. When signatures are required to satisfy GraphQL requests,
they are retrieved from headers on the trusted node.Fixed Updated dependencies to address security issues.Improved reliability of failed bundle indexing retries.Fixed failure to compute data roots for verification for base layer data
larger than 2GiB.Fixed observer healthcheck by correcting node.js path in healthcheck script.[Release 24] - 2025-02-03 Added Added a ARNS_ANT_STATE_CACHE_HIT_REFRESH_WINDOW_SECONDS environment
variable that determines the number of seconds before the end of the TTL at
which to start attempting to refresh the ANT state.Added a TRUSTED_GATEWAYS_REQUEST_TIMEOUT_MS environment that defaults to
10,000 and sets the number of milliseconds to wait before timing out request
to trusted gateways.Added BUNDLE_REPAIR_RETRY_INTERVAL_SECONDS and BUNDLE_REPAIR_RETRY_BATCH_SIZE environment variables to control the time
between queuing batches of bundle retries and the number of data items
retrieved when constructing batches of bundles to retry.Added support for configuring the ar.io SDK log level via the AR_IO_SDK_LOG_LEVEL environment variable.Added a request_chunk_total Prometheus counter with status, source (a
URL) and source_type (trusted or peer) labels to track success/failure
of chunk retrieval in the Arweave network per source.Added a get_chunk_total Prometheus metric to count chunk retrieval
success/failure per chunk.Added arns_cache_hit_total and arns_cache_miss_total Prometheus counters
to track ArNS cache hits and misses for individual names respectively.Added arns_name_cache_hit_total and arns_name_cache_miss_total Prometheus
counters to track ArNS name list cache hits and misses
respectively.Added a arns_resolution_duration_ms Prometheus metric that tracks summary
statistics for the amount of time it takes to resolve ArNS names.Changed In addition to the trusted node, the Arweave network is now searched for
chunks by default. All chunks retrieved are verified against data roots
indexed from a trusted Arweave node to ensure their validity.Default to a 24 hour cache TTL for the ArNS name cache. Record TTLs still
override this, but in cases where resolution via AO CU is slow or fails, the
cache will be used. In the case of slow resolution, CU based resolution will
proceed in the background and update the cache upon completion.Switched to the ioredis library for better TLS support.Updated minor dependency minor versions (more dependencies will be updated in
the next release).Bundles imports will no longer be re-attempted for bundles that have already
been fully unbundled using the current filters if they are matched or
manually queued again.Replaced references docker-compose in the docs with the more modern docker compose.Fixed Ensure duplicate data item IDs are ignored when comparing counts to determine
if a bundle has been fully unbundled.Fixed worker threads failing to shut down properly when the main process
stopped.Ensure bundle import attempt counts are incremented when bundles are skipped
to avoid repeatedly attempting to import skipped bundles.Use observe that correctly ensure failing gateways are penalized in the AR.IO
AO process.[Release 23] - 2025-01-13 Added Added FS_CLEANUP_WORKER_BATCH_SIZE,FS_CLEANUP_WORKER_BATCH_PAUSE_DURATION, and FS_CLEANUP_WORKER_RESTART_PAUSE_DURATION environment variables to allow
configuration of number of contiguous data files cleaned up per batch, the
pause between each batch, and the pause before restarting the entire cleanup
process again.Added data_items_unbundled_total Prometheus metric that counts the total
number of data items unbundled, including those that did not match the
unbundling filter.Added a parent_type label that can be one of transaction or data_item to data item indexing metrics.Added a files_cleaned_total total Prometheus metric to enable monitoring of
contiguous data cleanup.Added support for specifying the admin API via a file specified by the ADMIN_API_KEY_FILE environment variable.Added experimental support for posting chunks in a non-blocking way to
secondary nodes specified via a comma separate list in the SECONDARY_CHUNK_POST_URLS environment variable.Changed Renamed the parent_type lable to contiguous_data_type on bundle metrics
to more accurately reflect the meaning of the label.Reduced the maximum time to refresh the ArNS name list to 10 seconds to
minimize delays in ArNS availability after a new name is registered.Changed /ar-io/admin/queue-bundle to wait for bundles rows to be written
to the DB before responding to ensure that errors that occur due to DB
contention are not silently ignored.Data items are now flushed even when block indexing is stopped. This allows
for indexing batches of data items using the admin API with block indexing
disabled.Adjust services in docker-compose to use unless-stopped as their restart
policy. This guards against missing restarts in the case where service
containers exit with a success status even when they shouldn't.Fixed Added missing created_at field in blocked_names table.Fixed broken ArNS undername resolution.[Release 22] - 2024-12-18 Added Added the ability to block and unblock ArNS names (e.g., to comply with hosting provider TOS). To block a name, POST { "name": "<name to block>" } to /ar-io/admin/block-name. To unblock a name, POST { "name": "<name to unblock>" } to /ar-io/admin/unblock-name.Changed Return an HTTP 429 response to POSTs to /ar-io/admin/queue-bundle when the bundle data import queue is full so that scripts queuing bundles can wait rather than overflowing it.Fixed Adjust ArNS length limit from <= 48 to <= 51 to match the limit enforced by the AO process.[Release 21] - 2024-12-05 Added Added a ClickHouse auto-import service. When enabled, it calls the Parquet export API, imports the exported Parquet into ClickHouse, moves the Parquet files to an imported subdirectory, and deletes data items in SQLite up to where the Parquet export ended. To use it, run Docker Compose with the clickhouse profile, set the CLICKHOUSE_URL to http://clickhouse:8123, and ensure you have set an ADMIN_KEY. Using this configuration, the core service will also combine results from ClickHouse and SQLite when querying transaction data via GraphQL. Note: if you have a large number of data items in SQLite, the first export and subsequent delete may take an extended period. Also, this functionality is considered experimental. We expect there are still bugs to be found in it and we may make breaking changes to the ClickHouse schema in the future. If you choose to use it in production (not yet recommended), we suggest backing up copies of the Parquet files found in data/parquet/imported so that they can be reimported if anything goes wrong or future changes require it.Added a background data verification process that will attempt to recompute data roots for bundles and compare them to data roots indexed from Arweave nodes. When the data roots match, all descendant data items will be marked as verified. This enables verification of data initially retrieived from sources, like other gateways, that serve contiguous data instead of verifiable chunks. Data verification can be enabled by setting the ENABLE_BACKGROUND_DATA_VERIFICATION environment variable to true. The interval between attempts to verify batches of bundles is configurable using the BACKGROUND_DATA_VERIFICATION_INTERVAL_SECONDS environment variable.Added a CHUNK_POST_MIN_SUCCESS_COUNT environment variable to configure how many Arweave nodes must accept a chunk before a chunk broadcast is considered successful.Added arweave_chunk_post_total and arweave_chunk_broadcast_total Prometheus metrics to respectively track the number of successful chunk POSTs to Arweave nodes and the number of chunks successfully broadcast.When resolving ArNS names, the entire list of names is now cached instead of individually checking whether each name exists. This reduces the load on AO CUs since the entire list can be reused across multiple requests for different names. Note: due to the default 5 minute interval between name list refreshes, newly registered may now take longer to resolver after initial registration. We intend to make further caching refinements to address this in the future.Added support for multiple prioritized trusted gateways configurable by setting the TRUSTED_GATEWAYS_URLS environment variable to a JSON value containing a mapping of gateway hosts to priorities. Data requests are sent to other gateways in ascending priority order. If multiple gateways share the same priority, all the gateways with the same priority are tried in a random order before continuing on to the next priority.Added support for caching contiguous data in S3. It is enabled by default when the AWS_S3_CONTIGUOUS_DATA_BUCKET and AWS_S3_CONTIGUOUS_DATA_PREFIX environment variables are set.Changed trusted-gateway was changed to trusted-gateways in ON_DEMAND_RETRIEVAL_ORDER and BACKGROUND_RETRIEVAL_ORDER.Renamed the S3 contiguous environment variables - AWS_S3_BUCKET to AWS_S3_CONTIGUOUS_DATA_BUCKET and AWS_S3_PREFIX to AWS_S3_CONTIGUOUS_DATA_PREFIX.[Release 20] - 2024-11-15 Added Exposed the core service chunk POST endpoint via Envoy. It accepts a Arweave data chunk and broadcasts it to either the comma separated list of URLs specified by the CHUNK_POST_URLs environment variable or, if none are specified, the /chunk path on URL specified by the TRUST_GATEWAY_URL environment variable.Added a X-AR-IO-Root-Transaction-Id HTTP header to data responses containing the root base layer transaction ID for the ID in question if it's been indexed.Added a X-AR-IO-Data-Item-Data-Offset HTTP header containing the offset of the data item relative to the root bundle base layer transaction for it. In conjunction with X-AR-IO-Root-Transaction-Id, it enables retrieving data for data item IDs from base layer data using first a HEAD request to retrieve the root ID and data offset followed by a range request into the root bundle. This greatly increases the likelihood of retriving data item data by ID since only an index into the base layer and Arweave chunk availability is needed for this access method to succeed.Added an experimental ClickHouse service to docker-compose.yaml (available via the clickhouse profile). This will be used as a supplemental GraphQL DB in upcoming releases.Added a data item indexing healthcheck that can be enabled by setting the RUN_AUTOHEAL environment variable to true. When enabled, it will restart the core service if no data items have been indexed since the value specified by the MAX_EXPECTED_DATA_ITEM_INDEXING_INTERVAL_SECONDS environment variable.[Release 19] - 2024-10-21 Fixed Adjusted data item flushing to use the bundle DB worker instead of the core DB worker to prevent write contention and failed flushes under heavy unbundling load.Added Added X-AR-IO-Digest, X-AR-IO-Stable, X-AR-IO-Verified, and ETag headers. X-AR-IO-Digest contains a base64 URL encoded representation of the SHA-256 hash of the data item data. It may be empty if the gateway has not previously cached the data locally. X-AR-IO-Stable contains either true or false depending on whether the associated Arweave transaction is more than 18 blocks old or not. X-AR-IO-Verified contains either true if the gateway has verified the data root of the L1 transaction or the L1 root parent of the data item or false if it has not. ETag contains the same value a X-AR-IO-Digest and is used to improve HTTP caching efficiency.Added support for using a different data source for on-demand and background data retrieval. Background data retrieval is used when unbundling. The background retrieval data source order is configurable using the BACKGROUND_RETRIEVAL_ORDER environment variable and defaults to chunks,s3,trusted-gateway,tx-data. Priority is given to chunk retrieval since chunks are verifiable.Added an /ar-io/admin/export-parquet/status to support monitoring of in-progress Parquet export status.Added sqlite_in_flight_ops Prometheus metric with worker (core, bundles, data, or moderation) and role (read or write) labels to support monitoring the number of in-flight DB operations.Added experimental Grafana and Prometheus based observability stack. See the "Monitoring and Observability" section of the README for more details.Changed Bundle data is now retrieved as chunks from Arweave nodes by default so that data roots can be compared against the chain (see entry about background retrieval above).Changed observer configuration to use 8 instead of 5 chosen names. These are combined with 2 names prescribed from the contract for a total of 10 names observed each epoch to provide increased ArNS observation coverage.Verification status is set on data items when unbundling a parent that has already been verified.[Release 18] - 2024-10-01 Fixed Improved performance of data attributes query that was preventing data.db WAL flushing.Added Added WAL sqlite_wal_checkpoint_pages Prometheus metric to help monitor WAL flushing.Added a POST /ar-io/admin/export-parquet endpoint that can be used to export the contents of the SQLite3 core and bundle DBs as Parquet. To trigger an export, POST JSON containing outputDir, startHeight, endHeight, and maxFileRows keys. The resulting Parquet files can then be queried directly using DuckDB or loaded into another system (e.g. ClickHouse). Scripts will be provided to help automate the latter in a future release.Added ARNS_RESOLVER_OVERRIDE_TTL_SECONDS that can be used to force ArNS names to refresh before their TTLs expire.Added a GET /ar-io/resolver/:name endpoint that returns an ArNS resolution for the given name.Changed Removed ArNS resolver service in favor of integrated resolver. If a standalone resolver is still desired, the core service can be run with the START_WRITERS environment variable set to false. This will disable indexing while preserving resolver functionality.Deduplicated writes to data.db to improve performance and reduce WAL growth rate.[Release 17] - 2024-09-09 Notes This release includes a LONG RUNNING MIGRATION. Your node may appear unresponsive while it is running. It is best to wait for it to complete. If it fails or is interrupted, removing your SQLite DBs (in data/sqlite by default) should resolve the issue, provided you are willing to lose your GraphQL index and let your node rebuild it.Fixed Use the correct environment variable to populate WEBHOOK_BLOCK_FILTER in docker-compose.yaml.Don't cache data regions retrieved to satisfy range requests to avoid unnecessary storage overhead and prevent inserting invalid ID to hash mappings into the data DB.Added Added a new ClickHouse based DB backend. It can be used in combination with the SQLite DB backend to enable batch loading of historical data from Parquet. It also opens up the possibility of higher DB performance and scalability. In its current state it should be considered a technology preview. It won't be useful to most users until we either provide Parquet files to load into it or automate flushing of the SQLite DB to it (both are planned in future release). It is not intended to be standalone solution. It supports bulk loading and efficient GraphQL querying of transactions and data items, but it relies on SQLite (or potentially another OLTP in the future) to index recent data. These limitations allow greatly simplified schema and query construction. Querying the new ClickHouse DB for transaction and data items via GraphQL is enabled by setting the CLICKHOUSE_URL environment variable.Added the ability to skip storing transaction signatures in the DB by setting WRITE_TRANSACTION_DB_SIGNATURES to false. Missing signatures are fetched from the trusted Arweave node when needed for GraphQL results.Added a Redis backed signature cache to support retrieving optimistically indexed data item signatures in GraphQL queries when writing data items signatures to the DB has been disabled.Added on-demand and composite ArNS resolvers. The on-demand resolver fetches results directly from an AO CU. The composite resolver attempts resolution in the order specified by the ARNS_RESOLVER_PRIORITY_ORDER environment variable (defaults to on-demand,gateway).Added a queue_length Prometheus metric to fasciliate monitoring queues and inform future optimizations Added SQLite WAL cleanup worker to help manage the size of the data.db-wal file. Future improvements to data.db usage are also planned to further improve WAL management.Changed Handle data requests by ID on ArNS sites. This enables ArNS sites to use relative links to data by ID.Replaced ARNS_RESOLVER_TYPE with ARNS_RESOLVER_PRIORITY_ORDER (defaults to on-demand,gateway).Introduced unbundling back pressure. When either data item data or GraphQL indexing queue depths are more than the value specified by the MAX_DATA_ITEM_QUEUE_SIZE environment variable (defaults to 100000), unbundling is paused until the queues length falls bellow that threshold. This prevents the gateway from running out of memory when the unbundling rate exceeds the indexing rate while avoiding wasteful bundle reprocessing.Prioritized optimistic data item indexing by inserting optimistic data items at the front of the indexing queues.Prioritized nested bundle indexing by inserting nested bundles at the front of the unbundling queue.[Release 16] - 2024-08-09 Fixed Fixed promise leak caused by missing await when saving data items to the DB.Modified ArNS middleware to not attempt resolution when receiving requests for a different hostname than the one specified by ARNS_ROOT_HOST.Added Added support for returning Content-Encoding HTTP headers based on user specified Content-Encoding tags.Added isNestedBundle filter enables that matches any nested bundle when indexing. This enables composite unbundling filters that match a set of L1 tags and bundles nested under them.Added ability to skip writing ANS-104 signatures to the DB and load them based on offsets from the data instead. This significantly reduces the size of the bundles DB. It can be enabled by setting the WRITE_ANS104_DATA_ITEM_DB_SIGNATURES environment variable to false.Added data_item_data_indexed_total Prometheus counter to count data items with data attributes indexed.Changed Queue data attributes writes when serving data rather than writing them syncronously.Reduced the default data indexer count to 1 to lessen the load on the data DB.Switched a number of overly verbose info logs to debug level.Removed docker-compose on-failure restart limits to ensure that services restart no matter how many times they fail.Modified the data_items_indexed_total Prometheus counter to count data items indexed for GraphQL querying instead of data attributes.Increased aggressiveness of contiguous data cleanup. It now pauses 5 seconds instead of 10 seconds per batch and runs every 4 hours instead of every 24 hours.[Release 15] - 2024-07-19 Fixed Fixed query error that was preventing bundles from being marked as fully imported in the database.Added Adjusted data item indexing to record data item signature types in the DB. This helps distinguish between signatures using different key formats, and will enable querying by signature type in the future.Adjusted data item indexing to record offsets for data items within bundles and signatures and owners within data items. In the future this will allow us to avoid saving owners and signatures in the DB and thus considerably reduce the size of the bundles DB.Added ARNS_CACHE_TTL_MS environment variable to control the TTL of ARNS cache entries (defaults to 1 hour).Added support for multiple ranges in a single HTTP range request.Added experimental chunk POST endpoint that broadcasts chunks to the comma-separate list of URLS in the CHUNK_BROADCAST_URLS environment variable. It is available at /chunk on the internal gateway service port (4000 by default) but is not yet exposed through Envoy.Added support for running an AO CU adjacent to the gateway (see README.md for details).Added X-ArNS-Process-Id to ArNS resolved name headers.Added a set of AO_... environment variables for specifying which AO URLs should be used (see docker-compose.yaml for the complete list). The AO_CU_URL is of particular use since the core and resolver services only perform AO reads and only the CU is needed for reads.Changed Split the monolithic docker-compose.yaml into docker-compose.yaml, docker-compose.bundler.yaml, and docker-compose.ao.yaml (see README for details).Replaced references to 'docker-compose' with 'docker compose' in the docs since the former is mostly deprecated.Reduce max fork depth from 50 to 18 inline to reflect Arweave 2.7.2 protocol changes.Increased the aggressiveness of bundle reprocessing by reducing reprocessing interval from 10 minutes to 5 minutes and raising reprocessing batch size from 100 to 1000.Use a patched version of Litestream to work around insufficient S3 multipart upload size in the upstream version.[Release 14] - 2024-06-26 Fixed Correctly handle manifest index after paths.[Release 13] - 2024-06-24 Added Added support for optimistically reading data items uploaded using the integrated Turbo bundler via the LocalStack S3 interface.Added X-AR-IO-Origin-Node-Release header to outbound data requests.Added hops, origin, and originNodeRelease query params to outbound data requests.Added support for fallback in v0.2 manifests that is used if no path in the manifest is matched.Changed Updated Observer to read prescribed names from and write observations to the ar.io AO network process.Updated Resolver to read from the ar.io AO network process.Fixed Modified optimistic indexing of data items to use a null parent_id when inserting into the DB instead of a placeholder value. This prevents unexpected non-null bundledIn values in GraphQL results for optimistically indexed data items.Modified GraphQl query logic to require an ID for single block GraphQL queries. Previously queries missing an ID were returning an internal SQLite error. This represents a small departure from arweave.net's query logic which returns the latest block for these queries. We recommend querying blocks instead of block in cases where the latest block is desired.Adjusted Observer health check to reflect port change to 5050.Security Modified docker-compose.yaml to only expose Redis, PostgreSQL, and LocalStack ports internally. This protects gateways that neglect to deploy behind a firewall, reverse proxy, or load balancer.[Release 12] - 2024-06-05 Added Added /ar-io/admin/queue-data-item endpoint for queuing data item headers for indexing before the bundles containing them are processed. This allows trusted bundlers to make their data items quickly available to be queried via GraphQL without having to wait for bundle data submission or unbundling.Added experimental support for retrieving contiguous data from S3. See AWS_* environment variables documentation for configuration details. In conjuction with a local Turbo bundler this allows optimistic bundle (but not yet data item) retrieval.Add experimental support for fetching data from gateway peers. It can be enabled by adding ario-peer to ON_DEMAND_RETRIEVAL_ORDER. Note: do not expect this work reliably yet! This functionality is in active development and will be improved in future releases.Add import_attempt_count to bundle records to enable future bundle import retry optimizations.Changed Removed version from docker-compose.yaml to avoid warnings with recent versions of docker-compose.Switched default observer port from 5000 to 5050 to avoid conflict on OS X. Since Envoy is used to provide external access to the observer API this should have no user visible effect.[Release 11] - 2024-05-21 Added Added arweave_tx_fetch_total Prometheus metric to track counts of transaction headers fetched from the trusted node and Arweave network peers.Changed Revert to using unnamed bind mounts due to cross platform issues with named volumes.[Release 10] - 2024-05-20 Added Added experimental support for streaming SQLite backups to S3 (and compatible services) using Litestream. Start the service using the docker-compose "litestream" profile to use it, and see the AR_IO_SQLITE_BACKUP_* environment variables documentation for further details.Added /ar-io/admin/queue-bundle endpoint for queueing bundles for import for import before they're in the mempool. In the future this will enable optimistic indexing when combined with a local trusted bundler.Added support for triggering webhooks when blocks are imported matching the filter specified by the WEBHOOK_BLOCK_FILTER environment variable.Added experimental support for indexing transactions and related data items from the mempool. Enable it by setting ENABLE_MEMPOOL_WATCHER to 'true'.Made on-demand data caching circuit breakers configurable via the GET_DATA_CIRCUIT_BREAKER_TIMEOUT_MS environment variable. This allows gateway operators to decide how much latency they will tolerate when serving data in exchange for more complete data indexing and caching.Rename cache header from X-Cached to X-Cache to mimic typical CDN practices.Add X-AR-IO-Hops and X-AR-IO-Origin headers in preparation for future peer-to-peer functionality.Upgrade to Node.js v20 and switch to native test runner.[Release 9] - 2024-04-10 Added Added experimental Farcaster Frames support, enabling simple Arweave based Frames with button navigation. Transaction and data item data is now served under /local/farcaster/frame/<ID>. /local is used as a prefix to indicate this functionality is both experimental and local to a particular gateway rather than part of the global gateway API. Both GET and POST requests are supported.Added an experimental local ArNS resolver. When enabled it removes dependence on arweave.net for ArNS resolution! Enable it by setting RUN_RESOLVER=TRUE, TRUSTED_ARNS_RESOLVER_TYPE=resolver, and TRUSTED_ARNS_RESOLVER_URL=http://resolver:6000 in your .env file.Added an X-Cached header to data responses to indicate when data is served from the local cache rather than being retrieved from an external source. This is helpful for interfacing with external systems, debugging, and end-to-end testing.Save hashes for unbundled data items during indexing. This enables reduction in data storage via hash based deduplication as well as more efficient peer-to-peer data retrieval in the future.[Release 8] - 2024-03-14 Added Added GraphQL SQL query debug logging to support trouble-shooting and performance optimization.Added support for indexing data items (not GraphQL querying) based solely on tag name. (example use case: indexing all IPFS CID tagged data items).Changes Observer data sampling now uses randomized ranges to generate content hashes.Reference gateway ArNS resolutions are now cached to improve report generation performance.Contract interactions are now tested before posting using dryWrite to avoid submitting interactions that would fail./ar-io/observer/info now reports INVALID for wallets that fail to load.Fixed Fix data caching failure caused by incorrect method name in getData circuit breakers.Fix healthcheck when ARNS_ROOT_HOST includes a subdomain.[Release 7] - 2024 - 02 - 14 Added Add support for notifying other services of transactions and data items using webhooks (see README for details).Add support for filter negation (particularly useful for excluding large bundles from indexint).Improve unbundling throughput by decoupling data fetching from unbundling.Add Envoy and core service ARM builds.Changed Improve resouce cleanup and shutdown behavior.Don't save Redis data to disk by default to help prevent memory issues on startup for small gateways.Reduce the amount of data sampled from large files by the observer.Ensure block poa2 field is not chached to reduce memory consumption.[Release 6] - 2024-01-29 Fixed Update observer to improve reliability of contract state synchronization and evaluation.[Release 5] - 2024-01-25 Added Added transaction offset indexing to support future data retrieval capabilities.Enabled IPv6 support in Envoy config.Added ability to configure observer report generation interval via the REPORT_GENERATION_INTERVAL_MS environmental variable. (Intended primarily for development and testing) Changed Updated observer to properly handle FQDN conflicts.Renamed most created_at columns to index to indexed_at for consistency and clarity.Fixed Updated LMDB version to remove Buffer workaround and fix occasional block cache errors.[Release 4] - 2024-01-11 Added Added circuit breakers around data index access to reduce impact of DB access contention under heavy requests loads.Added support for configuring data source priority via the ON_DEMAND_RETRIEVAL_ORDER environment variable.Updated observer to a version that retrieves epoch start and duration from contract state.Changed Set the Redis max memory eviction policy to allkeys-lru.Reduced default Redis max memory from 2GB to 256MB.Improved predictability and performance of GraphQL queries.Eliminated unbundling worker threads when filters are configured to skip indexing ANS-104 bundles.Reduced the default number of ANS-104 worker threads from 2 to 1 when unbundling is enabled to conserve memory.Increased nodejs max old space size to 8GB when ANS-104 workers > 1.Fixed Adjusted paths for chunks indexed by data root to include the full data root.[Release 3] - 2023-12-05 Added Support range requests (PR 61, PR 64) Note: serving multiple ranges in a single request is not yet supported.Release number in /ar-io/info response.Redis header cache implementation (PR 62).New default header cache (replaces old FS cache).LMDB header cache implementation (PR 60).Intended for use in development only.Enable by setting CHAIN_CACHE_TYPE=lmdb.Filesystem header cache cleanup worker (PR 68).Enabled by default to cleanup old filesystem cache now that Redis is the new default.Support for parallel ANS-104 unbundling (PR 65).Changed Used pinned container images tags for releases.Default to Redis header cache when running via docker-compose.Default to LMDB header cache when running via yarn start.Fixed Correct GraphQL pagination for transactions with duplicate tags.

---

# 139. ARIO Docs

Document Number: 139
Source: https://docs.ar.io/gateways/upgrading
Words: 351
Extraction Method: html

Upgrading your Gateway To ensure the optimal performance and security of your AR.IO Gateway, it's essential to regularly upgrade to the latest version. Notably, indexed data resides separate from Docker. As a result, neither upgrading the Gateway nor pruning Docker will erase your data or progress. Here's how you can perform the upgrade:Prerequisites Your Gateway should have been cloned using git. If you haven't, follow the installation instructions for windows or linux.Checking your Release Number Effective with release 3, you can view the currently implemented release on any gateway by visiting https://<gateway>/ar-io/info in a browser. Be sure to replace <gateway> with the domain of the gateway you are checking.If the release number displayed includes -pre it means that your gateway is using the develop branch of the github repo for the gateway code. Follow steps in our troubleshooting guide to switch over to the more stable main branch.Announcements will be made in our discord server showing each new release.Upgrade Steps Pull the latest changes from the repository Navigate to your cloned repository directory and execute the following command:Shut down Docker Depending on your operating system, use the respective commands:Linux Windows Prune Docker (Optional) It's a good practice to clean up unused Docker resources. Again, use the command based on your OS:NOTE: This will erase all inactive docker containers on your machine. If you use docker for anything beyond running a gateway be extremely careful using this command.Linux Windows Check for New Environmental Variables Read the update release change logs and community announcements to see if the new version includes any new environmental variables that you should set before restarting your gateway.Restart the Docker container Finally, start the Docker container again to implement the changes:Linux Windows NOTE: Effective with Release #3, it is no longer required to include the --build flag when starting your gateway. Docker will automatically build using the image specified in the docker-commpose.yaml file.That's it! Your AR.IO Gateway is now upgraded to the latest version. Ensure to test and verify that everything is functioning as expected. If you encounter any issues, reach out to the AR.IO community for assistance.

---

# 140. ARIO Docs

Document Number: 140
Source: https://docs.ar.io/gateways/windows-setup
Words: 1304
Extraction Method: html

Windows Installation Instructions Overview This guide provides step-by-step instructions for setting up the AR.IO node on a Windows computer. It covers installing necessary software, cloning the repository, creating an environment file, starting the Docker container, setting up networking, and installing and configuring NGINX Docker. No prior coding experience is required.Prerequisites Before starting the installation process, ensure you have the following:A Windows computer Administrative privileges on the computer Install Required Packages Install Docker:Download Docker Desktop for Windows from here.Run the installer and follow the prompts.During installation, make sure to select the option to use WSL (Windows Subsystem for Linux) rather than Hyper-V.Restart your PC.Update Windows Subsystem for Linux (WSL):Open the command prompt as an administrator:Press Windows Key + R.Type cmd and press Enter.Right-click on the "Command Prompt" application in the search results.Select "Run as administrator" from the context menu.Run the following commands:Restart Docker Desktop.Install Git:Download Git for Windows from here.Run the installer and use the default settings.Clone the Repository Clone the main repository:Open the command prompt:Press Windows Key + R.Type cmd and press Enter.Navigate to the directory where you want to clone the repository:Use the cd command to change directories. For example, to navigate to the Documents directory:More detailed instructions on navigating with the cd command can be found here NOTE: Your database of Arweave Transaction Headers will be created in the project directory, not Docker. So, if you are using an external hard drive to turn an old machine into a node, install the node directly to that external drive.Run the following command:Create the Environment File Create an environmental variables file:Open a text editor (e.g., Notepad):Press Windows Key and search for "Notepad".Click on "Notepad" to open the text editor.Paste the following content into the new file, replacing <your-domain> with the domain address you are using to access the node, and <your-public-wallet-address> with the public address of your Arweave wallet:The GRAPHQL values set the proxy for GQL queries to arweave.net, You may use any available gateway that supports GQL queries. If omitted, your node can support GQL queries on locally indexed transactions, but only L1 transactions are indexed by default.START_HEIGHT is an optional line. It sets the block number where your node will start downloading and indexing transactions headers. Omitting this line will begin indexing at block 0.RUN_OBSERVER turns on the Observer to generate Network Compliance Reports. This is required for full participation in the AR.IO Network. Set to false to run your gateway without Observer.ARNS_ROOT_HOST sets the starting point for resolving ARNS names, which are accessed as a subdomain of a gateway. It should be set to the url you are pointing to your node, excluding any protocol prefix. For example, use node-ar.io and not https://node-ar.io. If you are using a subdomain to access your node and do not set this value, the node will not understand incoming requests.AR_IO_WALLET is optional, and sets the wallet you want associated with your Gateway. An associated wallet is required to join the AR.IO network.OBSERVER_WALLET is the public address of the wallet used to sign Observer transactions. This is required for Observer to run, but may be omitted if you are running a gateway outside of the AR.IO network and do not plan to run Observer. You will need to supply the keyfile to this wallet in the next step.Advanced configuration options can be found at docs.ar.io Save the file with the name ".env" and make sure to select "All Files" as the file type. This helps to ensure the file saves as ".env" and not ".env.txt" Note: The .env file should be saved inside the same directory where you cloned the repository (e.g., ar-io-node).Supply Your Observer Wallet Keyfile:If you are running Observer, you need to provide a wallet keyfile in order to sign report upload transactions. The keyfile must be saved in the wallets directory in the root of the repository. Name the file <Observer-Wallet-Address>.json, replacing "<Observer-Wallet-Address>" with the public address of the wallet. This should match your OBSERVER_WALLET environmental variable.Learn more about creating Arweave wallets and obtaining keyfiles here Start the Docker Containers Start the Docker container:Open the command prompt:Press Windows Key + R.Type cmd and press Enter.Navigate to the directory where you cloned the repository (e.g., ar-io-node):Use the cd command to change directories. For example, if the repository is located in the Documents directory, you would enter:If the directory path contains spaces, enclose it in double quotation marks. For example:Use the dir command to list the contents of the current directory and verify that you're in the correct location:dir Once you are in the correct directory, run the following command to start the Docker container:Explanation of flags:up: Start the Docker containers.-d: Run the containers as background processes (detached mode).NOTE: Effective with Release #3, it is no longer required to include the --build flag when starting your gateway. Docker will automatically build using the image specified in the docker-commpose.yaml file.The gateway can be shut down using the command:If prompted by the firewall, allow access for Docker when requested.Set Up Router Port Forwarding To expose your node to the internet and use a custom domain, follow these steps:Obtain a Domain Name:Choose a domain registrar (e.g., Namecheap) and purchase a domain name.Point the Domain at Your Home Network:In your browser, go to https://www.whatsmyip.org/ to display your public ip address. It can be found at the top of the screen. Note this number down.Access your domain registrar's settings (e.g., Namecheap's cPanel).Navigate to the DNS settings for your domain. In cPanel this is under the "Zone Editor" tab.Create an A record with your registrar for your domain and wildcard subdomains, using your public IP address. For example, if your domain is "ar.io," create a record for "ar.io" and "*.ar.io." Instructions may vary depending on the domain registrar and cPanel. Consult your registrar's documentation or support for detailed steps.Obtain the Local IP Address of Your Machine:Open the command prompt:Press Windows Key + R.Type cmd and press Enter.Run the following command:ipconfig Look for the network adapter that is currently connected to your network (e.g., Ethernet or Wi-Fi).Note down the IPv4 Address associated with the network adapter. It should be in the format of 192.168.X.X or 10.X.X.X.This IP address will be used for port forwarding.Set Up Router Port Forwarding:Access your home router settings:Open a web browser.Enter your router's IP address in the address bar (e.g., 192.168.0.1).If you're unsure of your router's IP address, consult your router's documentation or contact your Internet Service Provider (ISP).Navigate to the port forwarding settings in your router configuration.The exact steps may vary depending on your router model. Consult your router's documentation or support for detailed steps.Set up port forwarding rules to forward incoming traffic on ports 80 and 443 to the local IP address of your machine where the node is installed.Configure the ports to point to the local IP address noted in the previous step.Save the settings.Install and Configure NGINX Docker Clone the NGINX Docker repository:Open the command prompt:Press Windows Key + R.Type cmd and press Enter.Navigate to the directory where you want to clone the repository (This should not be done inside the directory for the node):Use the cd command to change directories. For example, to navigate to the Documents directory:Run the following command:Note: This NGINX container was designed to easily automate many of the more technical aspects of setting up NGNIX and obtaining an ssl certificate so your node can be accessed with https. However, wildcard domain certifications cannot be universally automated due to significant security concerns. Be sure to follow the instructions in this project for obtaining wildcard domain certificates in order for your node to function properly.Follow the instructions provided in the repository for setting up NGINX Docker.Congratulations! Your AR.IO node is now running and connected to the internet. Test it by entering https://<your-domain>/3lyxgbgEvqNSvJrTX2J7CfRychUD5KClFhhVLyTPNCQ in your browser.Note: If you encounter any issues during the installation process, please seek assistance from the AR.IO community.

---

# 141. Gateway Troubleshooting  FAQ - ARIO Docs

Document Number: 141
Source: https://docs.ar.io/gateways/troubleshooting
Words: 3800
Extraction Method: html

Welcome to the unified troubleshooting and FAQ resource for AR.IO Gateway operators. Use the quick lookup table below for fast answers, or browse the detailed sections for in-depth guidance.Quick Lookup Below is a quick summary of what you should check when troubleshooting your gateway. Find more detailed information in the sections below.Issue What to Check My release number is wrong Pull the latest github updates and make sure you are on the main branch Gateway appears offline on Viewblock or ar://gateways Probably fine, but verify that your gateway is still running.'/ar-io/observer/reports/current' just says "report pending" Normal behavior, wait for the report to complete.Observer error "Cannot read properties of undefined" Normal behavior, Observer is checking for data not implemented yet.Observing my gateway shows failures Check AR_IO_WALLET and ARNS_ROOT_HOST settings.Updated.env settings not reflected on gateway Rebuild your gateway after editing.env file.Out of disk space error Check for inode exhaustion and delete files if necessary.Can't load ArNS names Check ARNS_ROOT_HOST setting in.env file, and DNS records."Your connection is not private" error Generate or renew SSL certificates.404/Nginx error when accessing domain Check Nginx settings and restart Nginx if necessary.502 error from Nginx Check for errors in your gateway.Trouble generating SSL certificates Ensure TXT records have propagated and follow certbot instructions. General Troubleshooting My Gateway Seems to be Running but...My release number doesn't match the latest version, or includes "-pre" If your release number when you go to <your-gateway>/ar-io/info is lower than the current release, you simply need to upgrade your gateway in order to reach the latest release.If your release number includes the suffix "-pre" it means you are running your gateway from the development branch of the github repository, instead of the main branch. The development branch is used for staging work that the engineering team is in the middle of. Because of this, it can be much less stable than the main branch used for production and can cause significant issues.Ensure that you are running the latest release, from the main branch, by running the below commands in your terminal:If this doesn't resolve the issue, you can also try a more extreme method of clearing out the incorrect docker images:It appears offline on Viewblock or ar://gateways Viewblock and ar://gateways use a very simple ping method for determining if a gateway is "up". There are plenty of reasons why this ping may fail while the gateway is running perfectly, so showing as down is not cause for concern. Just verify that your gateway is still running, and wait. Your gateway will show as up again soon.< gateway >/ar-io/observer/reports/current just says "report pending" This is normal. Your Observer is working to generate a report and that report will be displayed once it is complete.My Observer is showing me the error "error: Error reading interaction: Cannot read properties of undefined" This is not an issue with your observer. The short explanation is that your Observer is looking for tasks assigned to it by the AR.IO network contract, but there isnt anything there. You can safely ignore this error message.Observing my gateway shows failures When observing a gateway, there are two main pass/fail tests. "Ownership" and "ArNS Assessment" Ownership: This tests to see if the value set in your gateway AR_IO_WALLET value (in.env) matches the wallet used to join the AR.IO Network. If they don't match, update the value in your.env file and restart your gateway.ArNS Assessment: This tests to see if a gateway is able to resolve ArNS names correctly. The first thing you should check is if you have the ARNS_ROOT_HOST value set in your.env file. If not, set the value and restart your gateway. If this value is set, check to make sure you have current DNS records and SSL certificates for wildcard subdomains on your gateway.I updated my.env settings, but nothing changed on my gateway Once you edit your.env file, you need to "rebuild" your gateway for the changes to take effect. As of release 3, every time you start your gateway with docker-compose it is automatically rebuilt. So all you need to do is shut your gateway down and restart it.I am getting an out of disk space error, but I still have open storage space on my computer The most likely cause of this is inode exhaustion. Test this by running the command:If one of the lines in the output says 100%, you have run out of inodes and so your filesystem is not capable of creating new files, even if you have available space. The solution is to delete files from your data folder in order to free up inodes.This was a common issue prior to release #3, when Redis caching was introduced to reduce the number of small files created. If you are using an older version of the gateway, consider upgrading to mitigate the risk of inode exhaustion.I can't load ArNS names The first thing you should check if your gateway is not resolving ArNS names is that you have ARNS_ROOT_HOST set in your.env file. If not, set it to your domain name used for the gateway. For example, ARNS_ROOT_HOST=arweave.dev.Once this value is set, restart your gateway for the changes to take effect.If that doesn't resolve the issue, check your dns records. You need to have a wildcard subdomain ( *.< your-domain > ) set with your domain registrar so that ArNS names will actually point at your gateway. You can set this record, and generate an SSL certificate for it, in the same way you set the records for your primary domain.When I try to access my gateway in a browser I get a "Your connection is not private" error This error message means that your SSL certificates have expired. You need to renew your certificates by running the same certbot command you used when you initially started your gateway:Certbot SSL certificates expire after 90 days, and you will need to rerun this command to renew every time. If you provide an email address, you will receive an email letting you know when it is time to renew.I set my gateway up, but when I go to my domain I get a 404/Nginx error If you navigate to your domain and see a 404 error from Nginx (the reverse proxy server used in the setup guide) it means that your domain is correctly pointed at the machine running your gateway, but you have not properly configured your Nginx settings (or your gateway is not running).The Set up Networking section of the setup guide has detailed instructions on configuring your Nginx server. If all else fails, try restarting Nginx, that usually clears any issues with the server clinging to old configurations.When I visit my domain I see a 502 error from Nginx A 502 error from Nginx means that Nginx is working correctly, but it is receiving an error from your gateway when it tries to forward traffic.I am having trouble generating my SSL certificates When using the manual certbot command provided in the setup guide:You need to be sure that you are waiting after creating your TXT records for them to completely propagate. You can check propagation using a tool like dnschecker.org.If you continue to have issues, you can check the official certbot instructions guide.My gateway was working, but it just stopped Visit your gateway in a browser and see if your SSL certs are expired. This is the most common issue causing sudden stops in proper operation.I updated my SSL certs, but it still shows as bad in a browser Try restarting nginx, it sometimes has trouble looking at the new certs without a restart.My gateway won't resolve ArNS names Make sure ARNS_ROOT_HOST is properly set in your .env file. Updating this requires restarting your gateway.Make sure you have a DNS record set for *.<your-gateway-domain>. Since ArNS names are served as subdomains, you need to make sure all subdomains are pointed at your gateway.If your gateway is attempting to resolve the name, but times out, it's most likely a CU issue.I see an error in my logs, but everything appears to be working AR.IO gateways are very robust, they can handle temporary errors gracefully and not affect normal operation. You should only be concerned if the error is consistent or it is causing your gateway to not function properly.I was selected as an observer, but my logs say a report was not saved Observers generate and submit their reports at specific times throughout the epoch. This is to ensure a healthy network throughout the entire epoch, not just at the start.Your observer wallet must match the observer wallet associated with your gateway in the AR.IO contract. You can check this by navigating to your gateway in ar://gateways.I see an error in my logs that says <h"... is not valid JSON This happens when a request to a CU fails, and your gateway receives an html failure message instead of the expected JSON response. This will normally clear up on its own after congestion on that CU dies down, but if it is persistent try switching to a different CU.My gateway logs just changed, instead of importing blocks I see "polling for block" This is normal. It means you have reached the current Arweave block and need to wait for more before you can index them.Error resolving name with resolver Promise timed out This is normal. If a gateway fails to resolve an arns name within 3 seconds, it will fall back to a trusted gateway (arweave.net by default) to help resolve the name.My gateway failed an epoch There are many reasons a gateway could fail an epoch. Following these steps is usually enough to identify and correct the issue:Try to visit your gateway in a browser and see if your SSL certs are bad Try to resolve an ArNS name on your gateway. If it fails to resolve, check the console and your gateway logs for errors Look at the observation reports that failed your gateway, they will list the reason for failure  Troubleshooting Failed Epochs Overview The ARIO Network provides several tools to help troubleshoot problems with a gateway. The most powerful among these is the Observer.The Observer, which is a component of every gateway joined to the ARIO Network, checks all gateways in the network to ensure that they are functioning properly, and returning the correct data. The Observer then creates a report of the results of these checks, including the reasons why a gateway might have failed the checks.If a gateway fails the checks from more than half of the prescribed observers, the gateway is marked as failed for the epoch, and does not receive any rewards for that epoch.The first step in troubleshooting a failed gateway is always to attempt to resolve data on that gateway in a browser, but if that does not make the issue clear, the Observer report can be used to diagnose the problem.Manual Observation Manual observations may be run on a gateway at any time buy using the Network Portal   . This allows operators (or anyone with an interest in the gateway's performance) to check the gateway's performance at any time. To run a manual observation:Navigate to the Network Portal    Select the gateway you are interested in from the list of gateways Click on the "Observe" button in the top right corner of the page. Click on the "Run Observation" button in the bottom right corner of the page. Two randomly selected ArNS names will be entered automatically in the "ArNS names" field to the left of the "Run Observation" button. These can be changed, or additional ArNS names can be added to the list before running the observation.The Manual observation will run the same checks as the observer, and will display the results on the right side of the page. Accessing the Observer Report The simplest way to access an observer report is via the Network Portal   , following the steps below:Navigate to the Network Portal    Select the gateway you are interested in from the list of gateways In the Observation window, select the epoch you are interested in. This will display a list of the observers that failed the gateway for that epoch.Click on the "View Report" button to the right any observer on that list. This will display the entire report that observer generated. Locate the gateway you are interested in in the report, and click on that row. This will display the report for that gateway.Understanding the Observer Report The observer report will display a list of checked ArNS names, and a reason if the gateway failed to return the correct data for that name. There are several reasons why a gateway might fail to return the correct data for an ArNS name. Below is a list of the most common reasons, and how to resolve them.Timeout awaiting 'socket', or Timeout awaiting 'connect'   This failure means that the observer was unable to connect to the gateway when it tried to check the ArNS name. There are lots of reasons why this might happen, many of them unrelated to the gateway itself. If an observer report has a small number of these failures, among a larger number of successful checks, it is unlikely to be an issue with the gateway.If this failure occurs persistently for a large number, or all ArNS names checked, it likely means that the observer is having trouble connecting to the gateway at all. You can verify this by:Attempting to connect to the gateway in a browser Running manual observations on the gateway using the Network Portal    Using tools like curl or ping to check the gateway's connectivity If these methods consistently fail to connect to the gateway, it is likely that the gateway is not properly configured or powered on. If this is the case:Check Docker and the gateway's logs to see if the gateway is on.Ensure that the SSL certificates are valid for the gateway's domain.Check DNS records for the gateway's domain, misconfigured or conflicting DNS records can cause connectivity issues.Some gateway operators who run their gateways on their personal home networks have also reported issues with their ISP blocking, throttling, or otherwise delaying traffic to a gateway. If none of the above steps resolve the issue, it may be worth checking with your ISP to see if they are blocking or throttling traffic to the gateway.Using Grafana can also provide a visual representation of the gateway's ArNS resolution times. If this is consistently high (above 10 seconds), it is likely that the gateway is not properly configured to resolve ArNS names. Ensure that the gateway is operating on the latest Release.certificate has expired  This failure means that the gateway's SSL certificate has expired. Obtaining a new SSL certificate and updating the gateway's reverse proxy (nginx, etc) configuration to use the new certificate is the only solution to this issue.dataHashDigest mismatch  This failure means that the gateway did respond to a resolution request, but the data it returned did not match the data that was expected. This could be due to a number of reasons, including:Cached data was returned by the gateway that doesnt match the most current data on the network.The gateway is configured to operate on testnet or devnet. Gateways joined to the ARIO Network MUST operate on mainnet in order to pass observation checks.The gateway is intentionally returning fraudulent data.A gateway will not return fraudulent data unless that operator intentionally rewrote the gateway's code to do so, and a major purpose of the Observation and Incentive Protocol is to catch and prevent this behavior. A gateway may return mistaken data on occasion, usually due to a cache mismatch between the gateway and the observer's authority (usually arweavae.net). This is a relatively rare occurrence, and should only be considered an issue if it occurs persistently. If most or all of the ArNS names checked are failing for this reason, it is likely that the gateway is not operating on mainnet.Response code 502 (Bad Gateway)  This failure means that the observer was able to connect to the gateway's network, but the reverse proxy returned a 502 error. This is almost always a reverse proxy issue. Ensure that the gateway's reverse proxy is running, and that it is configured to forward requests to the gateway.Testing the validity of the reverse proxy's configuration file (sudo nginx -t on Nginx) may provide more information about the issue, and restarting the reverse proxy (sudo nginx -s reload) often resolves the issue if there are no problems with the configuration file.It is also possible that the gateway itself is not running at all. Check Docker and the gateway's logs to see if the gateway is on.Response code 503 (Service Unavailable)  This failure means that the observer was able to connect to the gateway's network, but the reverse proxy was unable to forward the request to the gateway. It differs from the 502 error in that the reverse proxy is likely able to see that the gateway is running, but is unable to communicate with it. This is often a temporary issue, caused by the gateway not being able to handle a heavy load of requests, or the gateway being in the process of restarting. If this failure occurs once or twice in a report, it is likely a temporary issue and should not be considered an issue with the gateway. However, when this failure occurs persistently, particularly for every ArNS name checked on the report, it is likely that the gateway may have crashed.Manually restarting the gateway can likely resolve the issue.connect EHOSTUNREACH  This failure means that the observer was unable to connect to the gateway at all. The connection was either refused, or the gateway was not able to find a target based on the domain name's DNS records.This is almost always an issue with DNS records or local network configuration. Ensure that the gateway domain has correct DNS records, and that the local network is set up to allow connections. Checking logs from the local network's reverse proxy (nginx, etc) may provide more information about the issue.getaddrinfo ENOTFOUND  This is another DNS related issue. Likely, the gateway does not have a valid DNS record either for the top level domain or the required wildcard subdomain. Having this failure occur once or twice in a report could mean that the DNS server being used by the observer is having temporary issues and should not be considered an issue with the gateway. However, when this failure occurs persistently, particularly for every ArNS name checked on the report, it is likely that the gateway's DNS records are not set, or are misconfigured.Hostname/IP does not match certificate's altnames: Host: <gateway-domain>. is not in the cert's altnames: DNS:<gateway-domain>  This failure means that the observer's SSL certificate does not match the gateway's domain name. This is almost always an issue with the gateway's SSL certificate. This most likely occurred because the gateway's operator did not update the gateway's SSL certificate when the gateway's domain name was changed. Obtaining a new SSL certificate and updating the gateway's reverse proxy configuration to use the new certificate is the only solution to this issue.write EPROTO <connection-id>:error:<error-code>:SSL routines:ssl3_read_bytes:tlsv1 unrecognized name:<path-to-openssl-source>:SSL alert number 112  This failure almost always means that the gateway operator did not properly obtain SSL certificates for the gateway's wildcard subdomain. Obtaining a new SSL certificate and updating the gateway's reverse proxy configuration to use the new certificate is the only solution to this issue. FAQ Why was my reward different this epoch?Show answer Gateway protocol rewards are calculated as 0.1% of the protocol balance (0.05% after August 2025) split between all gateways in the network. A change in the protocol balance or the number of gateways in the network between epochs will result in the reward for an individual gateway changing.The Observer rewards are separate from protocol rewards, and if your gateway is selected as an observer for an epoch, assuming it performs its duties well, it will receive additional rewards I have a high stake on my gateway, why am I not an observer?Show answer The observer selection process uses a weighted random selection method that considers multiple factors beyond just stake:Stake Weight (SW): Ratio of your total staked ARIO tokens (including delegated stake) to the network minimum Tenure Weight (TW): How long your gateway has been part of the network (capped at 4 after 2 years) Gateway Performance Ratio Weight (GPRW): Ratio of epochs where you correctly resolved names vs total participation Observer Performance Ratio Weight (OPRW): Ratio of epochs where you successfully submitted reports vs total observer periods A composite weight (CW) is calculated as: CW = SW × TW × GPRW × OPRW Up to 50 gateways are chosen as observers per epoch. If there are more than 50 gateways, selection is randomized based on these normalized weights. Even with a high stake, other factors like performance and tenure affect your chances of being selected.I withdrew my stake, but now I have less Show answer There is a 90 day locking period when withdrawing stake, either from delegated stake or operator stake on your gateway. This locking period can be skipped, for a fee. The fee starts at 50% of the withdrawal amount, and goes down over time. If you selected instant withdrawal, you paid the fee to skip the locking period.Why Can't I withdraw my stake?Show answer The minimum operator stake for gateways (10,000 ARIO) cannot be instantly withdrawn, it is subject to the full 90 day locking period, and withdrawal can only be started by removing your gateway from the network.I would like to move my node to a new server - how?Show answer If possible, leave your original server running while you prepare the new one Set up the new server following the same steps you used to set up the original server This includes setting up SSL certificates for the new server You must use the same gateway wallet when setting up the new server The observer wallet may be changed at any point, but requires extra steps. It is recommended you use the original observer wallet as well Once the new server is set up, change your DNS A records to point at the new server After your DNS records are set and you have verified your gateway is operating correctly, shut down the original server No changes need to be made in the network contract or on ar://gateways Can I change my nodes FQDN?Show answer Yes Configure your new domain to point at your gateway, including setting up SSL certificates Update your NGINX (or other reverse proxy) server to recognize the new domain. This usually requires a restart of NGINX Update the ARNS_ROOT_HOST variable in your .env and restart the gateway Using ar://gateways, update your gateway settings to change the FQDN in the contract Your gateway is now using the new domain name for normal operation.

---

# 142. Importing SQLite Database Snapshots - ARIO Docs

Document Number: 142
Source: https://docs.ar.io/gateways/snapshots
Words: 483
Extraction Method: html

Overview One of the challenges of running an AR.IO Gateway is the initial synchronization time as your gateway builds its local index of the Arweave network. This process can take days or even weeks, depending on your hardware and the amount of data you want to index. To accelerate this process, you can import a pre-synchronized SQLite database snapshot that contains transaction and data item records already indexed.This guide will walk you through the process of importing a database snapshot into your AR.IO Gateway.Note The below instructions are designed to be used in a linux environment. Windows and MacOS users must modify the instructions to use the appropriate package manager/ command syntax for their platform.Unless otherwise specified, all commands should be run from the root directory of the gateway.Obtaining a Database Snapshot SQLite database snapshots are very large and not easy to incrementally update. For these reasons, AR.IO is distributing them using BitTorrent. These snapshots can be downloaded using any preferred torrenting client, and below is instructions on doing so using transmission-cli from a terminal.This will download a snapshot, current to April 23, 2025, of an unbundled data set that includes all data items uploaded via an ArDrive product, including Turbo. The file will be named 2025-04-23-sqlite.tar.gz and be approximately 42.8Gb in size.Note While continuing to seed the torrent after download is not required, it is highly recommended to help ensure the continued availability of the snapshot for others, as well as the integrity of the data. Seeding this file should not cause any issues with your internet service provider.This is a compressed tarball, so it will need to be extracted before it can be used.Extracting the Database Snapshot Once the file has downloaded, you can extract it using the following command, be sure to replace the filename with the actual filename of the snapshot you are using, if not using the example above.This will extract the file into a directory matching the filename, minus the .tar.gz extension.Importing the Database Snapshot Once you have an extracted database snapshot, you can import it into your AR.IO gateway by replacing the existing SQLite database files. Follow the instructions below to do so.IMPORTANT Importing a database snapshot will delete your existing database and replace it with the snapshot you are importing.Stop your AR.IO gateway.(Optional) Backup your existing SQLite database files.Delete the existing SQLite database files.Move the snapshot files into the data/sqlite directory.Be sure to replace 2025-04-23-sqlite with the actual directory name of the extracted snapshot you are using.Start your AR.IO gateway.Verifying the Import The simplest way to verify the import is to check the gateway logs to see what block number is being imported. The 2025-04-23 snapshot was taken at block 1645229, so the gateway will start importing blocks after this height if the snapshot was imported successfully.You can also use the Grafana Sidecar to view the last block imported in a more human readable format.

---

# 143. Deploy a dApp with ArDrive web - ARIO Docs

Document Number: 143
Source: https://docs.ar.io/guides/ardrive-web
Words: 790
Extraction Method: html

ArDrive Web Deployment Guide Overview This guide will outline the simple steps needed to deploy your dApp or website onto the Arweave blockchain using the ArDrive web app and friendly UI.Simple apps and websites should work right out of the box. However, for advanced applications, this assumes you have already prepared your dApp to use hash routing and relative file paths, and built static files for any dApp in a language or framework that requires it (like React).Learn more about preparing your dApp for deployment here.Deploying Step 1: Log into ArDrive Go to the ArDrive web app and log in using the method of your choosing. If you don't already have an account, you will need to follow the instructions to set one up.Step 2: Select or Create a Drive Once logged in, navigate to the drive where you want your project to be hosted. If you haven't created a drive yet, or if you want a new one specifically for this project, click the big red "New" button at the top left and create a new drive. Remember, the drive needs to be set to public for your dApp to be accessible to others.Step 3: Upload your project With your drive selected, click the big red "New" button again, but this time, select "Upload Folder". Navigate to your project's root directory, or the built directory if required, and select it. This will upload the entire directory, maintaining your project's file structure.Step 4: Confirm Upload You'll be given a chance to review the upload and the associated cost. If everything looks right, click "Confirm". Remember, uploading to Arweave isnt free, but the cost is usually quite small and the benefits of having your dApp or website hosted on the permaweb are significant.Step 5: Create the Manifest While ArDrive displays your uploaded files as a traditional file structure, with files and folders inside other folders, thats not how they actually exist on Arweave. The manifest acts as a map to all the files your dApp needs to function. After you confirm your upload, navigate into your newly created folder by double clicking on it. Click the big red "New" button again and select "New Manifest" in the "Advanced" section. You'll be prompted to name the manifest and choose where to save it. Be sure to save it inside the folder you just created.Step 6: Get the Data TX ID Once the manifest is created, click on it to expand its details. In the "details" tab, on the bottom right, there's a line labeled "Data TX ID". This is the unique identifier for your uploaded dApp on Arweave. Copy this value.Step 7: View and Share your dApp Your dApp or website is now available on the permaweb forever! Append the Data TX ID you just copied to the end of an Arweave gateway URL, like https://arweave.net/. It might take a few minutes for all of your files to finish propagating through the network, but once they do your dApp or website will be accessible to anyone, anywhere, at any time.Step 8: Assign a Friendly Name The Data TX ID you copied in Step 6 is long and difficult to remember. To make it easier to access your dApp or website, you can assign a friendly name to it using ArNS. If you already own an ArNS name, you will be prompted during the creation of
your manifest if you want to assign one. If you do not, you can purchase one from arns.app.You can also assign an ArNS name to an existing manifest (or any other file) by clicking on the three dots on the right side of the file and selecting "Assign ArNS name".Updating your dApp Files uploaded to Arweave are permanent and immutable. They cannot be changed. However, the Arweave File System (ArFS) protocol used (and created) by ArDrive lets you "replace" them with new versions while still being able to access the old ones. You can do this with entire dApps as well. The old files won't be displayed in the ArDrive web app unless you click on a file to view its history.Once you have made changes to your dApp or website, and built the static directory for it, you can upload the entire folder again to the same location where you uploaded the original. Follow all the same steps listed above for uploading your dApp. You will need to create a new manifest to correctly point to the updated files. Give it the same name as the old manifest in order to "replace" it. Creating the new manifest will generate a new TX ID used to view the updated dApp.The old version of the dApp will always be available to anyone who has the correct TX ID.

---

# 144. ANTs on Bazar - ARIO Docs

Document Number: 144
Source: https://docs.ar.io/guides/ants-on-bazar
Words: 529
Extraction Method: html

Trading ANTs on Bazar Overview Arweave Name Tokens are Atomic Asset Spec compliant AO tokens that manage records and permission for ArNS names. Because the ANT spec is compliant with the Atomic Asset Spec, they are tradable on Bazar, which is a decentralized market place for Atomic Assets on AO. There are a few simple steps that are required in order to make an ANT available on Bazar to be traded.Bazar relies on profiles for displaying user information and tradable assets. Profiles are AO processes that contain user specified information like a name, a nickname, and images associated with the profile. Profiles also track assets held by the profile in order to provide their information to bazar.Create a Profile If you do not already have a profile associated with your wallet, you can easily create one on using the "Create your profile" button on bazar after connecting your wallet: You will be prompted to add, at a minimum, a name and handle (nickname) to associate with the profile. These values can be changed later. Click "Save" at the bottom to finish creation of your profile.Once your profile is created, you can get its ao process Id at any time by clicking on the user icon in Bazar, and then the "Copy profile address" button from the menu. Bazar profiles only track assets that are held in the profile process, not in a user wallet. In order for an ANT to be displayed and transferred on Bazar, it must first be transferred into the Bazar profile. This can be done easily using arns.app    in your manage page for a given name.    Once an ANT is transferred into the profile process, it will automatically be detected and displayed by Bazar. It can be transferred or sold just like any other atomic asset on the marketplace, with no additional steps required.Restore Controllers Optional This is an optional step that will enable updating an ANT's Target Id without transferring it back into your wallet. This step may be safely skipped without affecting the ANT's functionality or tradability on Bazar.Transferring an ANT to a new wallet or AO process resets all authorized controllers, or non-owner entities that are allowed to update some settings on the ArNS name. It does not reset the Target Id that the ArNS name is pointing to. If you want to be able to update the Target ID and undernames from your wallet using arns.app, you will need to set your wallet address as a controller for the ANT while it is in your profile. The easiest way to do this is using aos.If you have not used aos before, you can find installation instructions here    Using aos, you can log directly into your profile process with the command:Be sure to replace <profile-address> with the process Id for your profile process, and /path/to/your/keyfile with the path to the keyfile for the wallet you created the profile with.Once you are logged in with aos, you can send a message to the ANT in your profile to set your wallet as a controller:Replace <Ant-Process-ID> with the process Id of the ANT you transferred into your profile, and <Wallet-Address> with your wallet address.

---

# 145. Glossary - ARIO Docs

Document Number: 145
Source: https://docs.ar.io/glossary
Words: 1405
Extraction Method: html

Many novel terms and acronyms are used by the Arweave ecosystem as well as some new ones introduced by AR.IO. The list below is intended to serve as a non-exhaustive reference of those terms. For a comprehensive glossary of permaweb-specific terminology, check out the permaweb glossary section:AO Computer (AO):The AO Computer is an actor-oriented machine on the Arweave network, creating a unified computing environment across diverse nodes. It supports many parallel processes through an open message-passing layer, linking independent processes into a cohesive system, similar to how websites are interconnected via hyperlinks.Arweave Name System (ArNS):a decentralized and censorship-resistant naming system enabled by AR.IO gateways which connects friendly names to permaweb applications, pages, data or identities.Arweave Name Token (ANT), "Name Token":A an AO Computer based token, that is connected to each registered ArNS Name. Each ANT gives the owner the ability to update the subdomains and Arweave Transaction IDs used by the registered name as well as transfer ownership and other functions.Arweave Network Standards (ANS):Drafts and finalized standards for data formats, tag formats, data protocols, custom gateway features and anything that is built on top the Arweave Network. Specific standards are denoted by an associated number, e.g., ANS-###.Base Layer Transaction:refers to one of up to 1,000 transactions that make up a single Arweave block. A base layer transaction may contain bundled data items.Bundle, bundling:an Arweave concept introduced in ANS-104 that allows for a way of writing multiple independent data transactions into one base layer transaction. Bundled transactions contain multiple independent transactions, called data items, wrapped into one larger transaction. This offers two major network benefits:A scaling solution for increasing the throughput of uploads to the Arweave network,Allows delegation of payment for an upload to a third party, while maintaining the identity and signature of the person who created the upload, without them needing to have a wallet with funds.Bundled Data Item (BDI):A data item / transaction nested within an ANS-104 bundled transaction.Bundler:A third-party service and gateway feature that bundles data files on a user's behalf.Chunk:A chunk is a unit of data that is stored on the Arweave network. It represents a piece of a larger file that has been split into smaller, manageable segments for efficient storage and retrieval.Decentralized, decentralization, etc:A nonbinary, many axis scale enabling a system or platform to be: permissionless, trustless, verifiable, transparent, open-source, composable, resilient, and censorship resistant. Ultimately, something that is decentralized is not prone to single points of failure or influence.Epoch:a specific duration (e.g., one day) during which network activities and evaluations are conducted. It serves as a key time frame for processes such as observation duties, performance assessments, and reward distributions within the network's protocols.Gateway:A node operating on the Arweave network that provides services for reading from, writing to, and indexing the data stored on the permaweb. Sometimes referred to as "permaweb nodes".Gateway Address Registry (GAR):a decentralized directory maintained in the AR.IO smart contract. It serves as the authoritative list of all registered gateways on the AR.IO Network. The registry provides detailed metadata about each gateway to facilitate discovery, health monitoring, and data sharing among apps, users and other infrastructure. The GAR is designed to be easily queryable, sortable, and filterable by end users and clients, allowing for tailored selections based on various criteria to meet specific use cases.Indexing:The act of organizing transaction data tags into queryable databases.Layer 2 Infrastructure:Layer 2 refers to the technology / infrastructure stack built "above" a base layer. In this use, the AR.IO Network would be considered Layer 2 infrastructure to the base Arweave protocol.Manifest (aka Path Manifest, Arweave Manifest):Special "aggregate" files uploaded to Arweave that map user-definable sub-paths with other Arweave transaction IDs. This allows users to create logical groups of content, for example a directory of related files, or the files and assets that make up a web page or application. Instead of having to manually collate these assets, manifests group them together so that an entire website or app can be launched from a single manifest file. Gateways can interpret this structure, so that users can then reference individual transactions by their file name and/or path.Mempool:Short for "memory pool," is a component of Arweave mining nodes that temporarily stores valid transactions that have been broadcasted to the network but have not yet been added to a block.Message:An interaction with an AO Process, including action and tags. Every interaction with AO takes the form of a message.Miner (aka Arweave Node):A node operating on the Arweave network responsible for data storage and recall.Native Address:The way public addresses are commonly (or by spec) represented in their native blockchain. Arweave keys are 43 character base64url representations of the public key, while Ethereum keys use a different hashing algorithm and start with 0x etc.Normalized Address:43 character base64url representation of the sha256 hash of a public key. Public keys for other chains can be normalized by this representation.Observer:A gateway selected to evaluate the performance of peer gateways in resolving ArNS names. Observers assess and report on the operational efficacy of other gateways.Optimistic Indexing:Indexing transaction or data item headers before the associated L1 transaction has been accepted and confirmed in a chain block.Owner:Generally, the public key of the signer.Owner Address:The normalized address of the owner Period:Refers to a predefined time span (e.g., a day) that serves as a cycle for network activities such as dynamic pricing. It is a fundamental unit of time for operational and protocol processes within the network.Permanent Cloud Network:A decentralized network that securely stores, distributes, and serves data and applications in a timeless, tamper-proof, and universally accessible way. Unlike traditional clouds, it ensures data permanence and user sovereignty by eliminating reliance on centralized providers and creating a resilient, censorship-resistant infrastructure.Permaweb:The permaweb is the permanent and decentralized web of files and applications built on top of Arweave.Process:Process: A decentralized computation unit in the AO framework, enabling scalable, parallel execution via message-passing. Each process maintains its own state, interacts asynchronously, and is permanently stored on Arweave for transparency and immutability.Process ID (PID):Every process in AO is assigned a unique immutable identifier code.Protocol Balance:The primary sink and source of ARIO tokens circulating through the AR.IO Network. This balance is akin to a central vault or wallet programmatically encoded into the network's smart contract from which ArNS revenue is accumulated and incentive rewards are distributed.Protocol Rewards:ARIO Token incentive rewards distributed by the protocol to the network's eligible users and gateway operators.Public Key:The publicly known keys for a signer (wallet). Public keys are different byte lengths depending on the signer type (e.g. Arweave vs. Ethereum (ECDSA), vs Solana, etc.) Seeding:Refers to the act of propagating new data throughout the network. Miner nodes seed Arweave base layer transaction data to other miners, while gateways ensure that the transactions they receive reach the Arweave nodes. Both gateways and Arweave nodes seed base layer transactions and data chunks.Staking (of tokens):Refers to the process of locking ARIO tokens into a protocol-facilitated vault, temporarily removing them from circulation until unlocked. This action represents an opportunity cost for the gateway operator and serves as a motivator to prioritize the network's collective interests.Stake Redelegation:The process by which stakers move their delegated tokens from one gateway to another.Stake Redemption:A feature allowing stakers to use their staked tokens for ArNS-related activities, such as purchasing names, extending leases, or increasing undername capacity.Transaction ID (txID):Every transaction and data file uploaded to Arweave is assigned a unique identifier code known as the Transaction ID. These txID's can be referenced by users to easily locate and retrieve files.Trust-minimization:Relates to enacting network security by minimizing the number of entities and the degree to which they must be trusted to achieve reliable network interactions. A network with trust-minimizing mechanisms means that it has reduced exposure to undesirable third-party actions and built-in incentives to reward good behavior while punishing bad behavior.Vault:Token vaults are protocol level mechanisms used to contain staked tokens over time. Each vault contains a starting timestamp, ending timestamp (if applicable), along with a balance of tokens.Wayfinder Protocol:The Wayfinder protocol provides applications with a pattern for dynamically switching / routing between network gateways. It also allows for abstraction of top level domain names from Arweave data and verifies the responses from AR.IO Gateways. It forms the basis of the ar:// schema, so users can seamlessly access ArNS names, Arweave base layer transactions, and bundled data items without the user providing a top-level domain.Permaweb Glossary For a more comprehensive glossary of terms used in the permaweb, try the Permaweb Glossary. Or use it below:

---

# 146. Arlink Deploy - ARIO Docs

Document Number: 146
Source: https://docs.ar.io/guides/arlink
Words: 462
Extraction Method: html

Overview Arlink is a third party tool that allows you to permanently deploy and manage web apps on the permaweb with ease.How it works Users can link their Github or Protocol.land repositories to their Arlink account through the Arlink dashboard. When a new project or build is deployed,
Arlink will take the repository, build it, and upload the build folder to Arweave.Arlink also allows users to connect their project to an ArNS name they own, or an undername of the ArNS name ar://arlink.Dashboard After connecting your wallet to the Arlink web app using the button at the top right, you will be taken to your dashboard. This page will display any deployments associated with your wallet, and includes a "+ New Deployment" button
in order to start the process of deploying a new project. New Deployment After clicking on the new deployment button, you will be prompted to import a repository from either Github or Protocol.land. Authorize Github If this is your first time importing from Github, you will be prompted to authorize Arlink to access your Github repositories. You can authorize all repositories, or limit authorization to any number of specific ones. Select Repository Once authorization is approved, select which repository and branch you want to deploy. Define Build and Output Steps Once you select what you want to deploy, you need to specify how the project needs to be built to get it ready. Arlink prompts for five inputs:Project Name: This is the name of your project.Install Command: The command for installing dependencies for your project. Usually npm install or yarn install Build Command: This is the command to run your build script. Usually npm run build or yarn build Sub Directory: If the front end for your project lives in a sub directory of your selected repository, you can specify that here.Output Directory: This is the path to the build folder being deployed. This will be different depending on the framework your project uses. Select ArNS The last thing to do is select an ArNS name to deploy your project to. If you own your own name, you can connect to it here with the "Use existing ArNS" toggle. Otherwise, you can select an undername of the ArNS name arlink to deploy to.
Duplicate undernames cannot exist, so you can only select an undername that is not already being used. Logs Once you select your ArNS name and click "Deploy", your project will be deployed. Logs from the build and deploy process will be displayed so you can monitor for errors. Updates To deploy a new build of your project, select it from the dashboard. The project page gives you the option to update any settings or configurations, and has a "Deploy Latest" button which will redeploy your project.

---

# 147. ARIO Docs

Document Number: 147
Source: https://docs.ar.io/guides/permaweb-deploy
Words: 1060
Extraction Method: html

Deploy a Website or Application Overview With the growing popularity of permanently deployed apps, hosted on Arweave, along with the growing list of tools offered by AR.IO, several methods have been developed to automate the process of deploying a website and updating the ArNS name pointed at it. A particularly useful tool for this is permaweb-deploy from Forward Research.permaweb-deploy is a cli tool that handles uploading a build folder to Arweave using Turbo, creating a manifest, and then updating an ArNS name to point at the new manifest. It being a cli tool makes it very easy to incorporate into a github actions flow. Setting up an automated deployment with permaweb-deploy is simple, but does require a few steps.ENV Security Before automating your deployments, be sure to build your app and check for exposed environmental secrets. Some app frameworks or build flows will build your app with the secrets exposed, and if you are using a tool like permaweb-deploy, those secrets will be uploaded to Arweave. Since the permaweb is permanent, this could pose a security risk, especially with a copy of your wallet keyfile required for the deployment automation.Getting Started Installing package permaweb-deploy is an npm package, and must be installed in any project before it can be used. If you are using npm, you can install the package with the below command:If you prefer yarn for your package installations, the process is slightly more involved. permaweb-deploy is not designed for installation with yarn, so you must provide the additional argument ignore-engines in order to skip over the yarn version error you would normally get with installation. There are two methods for doing so:Directly in the install command In a .yarnc file You can provide a file, named .yarnc in the same directory as your package.json in order to assign specific instructions to all of your yarn commands. Creating a .yarnc file with the line will have the same effect as providing the flag directly in your yarn command Adding a Deploy Script The simplest way to utilize the permaweb-deploy tool is to build it into a script in your package.json. Here you will provide all of the variables that permaweb-deploy needs in order to function properly, as well as ensure that your app is statically built before being uploaded.Be sure to replace <YOUR_ARNS_NAME> with the name of the ArNS name you want to deploy to.The above example shows a build script for a vuepress app, which will build the app into a static folder for deployment, and a deploy script which runs build and then permaweb-deploy. Your build script will look different depending on the framework you are using, but most will provide that for you when you create your app.The permaweb-deploy command has two required arguments:--deploy-folder This is the relative path (from your package.json) to the build folder you want to upload. In a vuepress app, that will be ./src/.vuepress/dist unless you manually specify otherwise in your vuepress configuration. It will be different depending on your chosen framework and if you have modified the default location.--arns-name This is the ArNS name you want to deploy to. It must be an ArNS name that the wallet used to authenticate has ownership or controller privileges over, otherwise the deployment will fail at authentication in the ao process that controls the ArNS name.Undernames The --arns-name flag MUST be the top level name, not and undername. That is, if you want to deploy to undername_arnsname you must set --arns-name arnsname and not --arns-name undername_arnsname.There is the additional, optional flag --undername. If you want to deploy your app to an undername on an ArNS name, provide that name with this flag.--arns-name arnsname --undername undername Testnet Permaweb-deploy supports both Mainnet and Testnet deployments. By default, it will deploy to Mainnet. To deploy to Testnet, you can provide the --ario-process flag as "testnet". If not provided, deployments will default to Mainnet.Providing Arweave Wallet Keys While using permaweb-deploy, you will be uploading data to Arweave using Turbo, as well as performing protected actions on an Arweave Name Token. Because of this, you will need to provide the keys to an Arweave wallet in order for the actions to be successful. The wallet must contain Turbo Credits to pay for the upload, and it must either be a controller or the owner of the ArNS name you are trying to update.permaweb-deploy requires your wallet keyfile be encoded in base64 format. You can convert a local keyfile to base64, and copy the new value to your clipboard by using one of the below commands, depending on your operating system:Linux Mac Windows (CMD) Be sure to replace wallet.json with the path to your chosen wallet keyfile. Once you have this value saved to your clipboard, you can move on to the next step.Create Github Secrets Anyone who has your wallet keyfile (including the base64 formatted keyfile) has full control over your wallet and any of its assets. Because of this, you do not want to include it directly in your package.json script. Instead, keep the value safe by storing it in a github secret. You will create the secrets in the settings tab on your github repo, and the secrets will act as environmental variables in the github actions workflow.You will need to create 1 secret DEPLOY_KEY: This is the base64 encoded version of your Arweave wallet keyfile.Create Action Workflow Github Actions allow you to perform specific actions whenever you push code to github. They are handled by using .yaml files provided in <root-of-project>/.github/workflows.To get started, create a new file named deploy.yaml in the workflows directory, then paste the below inside of it:The above tells github to perform these actions when you push new code to the branch main It then sets up a vps with nodejs v 20. When that is complete, it installs dependencies for your project using npm (You will need to add a step to install yarn if that is your preferred package manager), and runs your deploy script, which builds your static folder and then runs permaweb-deploy. It also loads your github secrets into environmental variables that can be used by your deploy script.Deploying App With the above setup complete, the only thing you need to do to deploy a new version of a permasite app to Arweave is push the updated code to branch main on github. Everything else is fully automated.

---

# 148. Managing Undernames - ARIO Docs

Document Number: 148
Source: https://docs.ar.io/guides/managing-undernames
Words: 302
Extraction Method: html

Overview ArNS undernames are subdomains of top level ArNS domains. They are separated from the main ArNS domain using an underscore "_" in place of the more typically used dot ".".Records for undernames can be set using the setRecord method on the AR.IO SDK, or removed by using the removeRecord method.
The process for setting/removing a record for an undername vs. a top level ArNS domain is nearly identical, the only difference being the undername parameter. When managing a record on a top level ArNS domain, this must be set to @, while updates to an undername should provide the undername being updated.Chaining Undernames Undernames can be created on other undernames, for example ar://og_logo_ardrive. In this example the undername og exists under the undername logo on the ArNS name ardrive.For the purpose of the undername parameter in the AR.IO SDK, this should be written as a single undername, including the separating underscores:og_logo Creating an Undername There are no special steps required to create an undername (provided the selected ArNS name has available undername space). Simply setting a record for an undername that does not exist will create the undername.Updating an Undername If an undername already exists, its record can easily be updated using the same setRecord method.Removing an Undername An existing undername can be removed by using the removeRecord method on the AR.IO SDK.
The undername parameter should be set to the undername being removed.Increasing Undername Support By default, ArNS names support up to 10 undernames. This number can be increased, for a fee. This is done using the increaseUndernameLimit method on the ARIO class of the AR.IO SDK, rather than the ANT class.
The quantity (qty) parameter specifies the number of ADDITIONAL undernames to be supported. i.e. increasing from 10 undernames to 15 would require the qty parameter set to 5.

---

# 149. ARIO Docs

Document Number: 149
Source: https://docs.ar.io/guides/gql
Words: 1295
Extraction Method: html

GraphQL Overview GraphQL is a powerful query language designed for modern web applications to efficiently fetch data. It enables precise queries, allowing users to specify exactly which data they need and in what format, significantly reducing the amount of unnecessary data transferred. This approach is ideal for dealing with complex systems and large datasets, as it minimizes bandwidth and improves performance. GraphQL operates through a single endpoint, streamlining the way applications communicate with databases.The integration of GraphQL with Arweave introduces a refined method for interacting with decentralized data storage. Arweave allows for the tagging of uploaded data, facilitating enhanced searchability and retrievability within its blockchain network. Utilizing GraphQL, users can perform targeted queries that leverage these tags, ensuring the retrieval of specific data swiftly and efficiently. This capability is particularly beneficial for the development of decentralized applications (dApps), the archival of content in a permanent and unalterable form, and the establishment of data marketplaces where precision and efficiency in data access are paramount.Together, GraphQL and Arweave form a compelling combination, offering developers and users a robust framework for managing and querying data in a decentralized environment. This integration not only promotes the efficient and scalable retrieval of data but also supports the creation of more sophisticated and data-intensive applications on the decentralized web, maintaining a balance between technical depth and accessibility.Constructing a Query Basic Syntax In GraphQL, you start with a root field and use braces to outline the fields you want to retrieve, allowing for precise, hierarchical data requests. For instance:This query demonstrates fetching transactions and their tags, illustrating the hierarchical nature of GraphQL queries.Customizing Searches with Tags Arweave utilizes a tagging system for transactions, enabling intricate search capabilities. You can filter queries using these tags:This example filters transactions by a specific application name, and returns the id, size, and type of the transaction, showcasing how to customize queries for targeted data retrieval.NOTE: Tags are not the only option for filtering results, but are extremely useful due to the ability to add custom tags during the upload process.Understanding Edges and Nodes In the realm of GraphQL queries, especially when interfacing with Arweave, grasping the concept of edges and nodes is pivotal for constructing efficient and effective queries. This structure is not unique to Arweave but is particularly relevant due to the decentralized and interconnected nature of the data stored on its blockchain.Nodes: At the heart of GraphQL's query structure, nodes represent individual data points or entities. In the context of Arweave, a node could be a transaction, a block, or any piece of data stored within the network. Nodes are the primary targets of your query, containing the data you wish to retrieve, such as transaction IDs, tags, or the content of data transactions.Edges: Serving as the glue between nodes, edges are constructs that outline the relationship between different nodes. They can contain metadata about the connection, such as the nature of the relationship or additional attributes that describe how nodes are linked. In many GraphQL implementations, including those that interact with Arweave, edges are used to navigate through collections of related data, making them crucial for understanding the data's structure and lineage.This hierarchical model is especially useful for querying complex and relational data sets, allowing for detailed navigation and efficient data retrieval within Arweave's decentralized storage system. By effectively utilizing the edges and nodes structure, you can precisely target the data you need, whether it's filtering transactions by tags, fetching related transactions, or exploring the blockchain's structure.Pagination To add pagination to your GraphQL queries, you can use the first, last, before, and after parameters. These parameters control the slice of data you're querying, making data retrieval more efficient and manageable.first: Specify the number of items to retrieve from the start of the list or dataset.last: Specify the number of items to retrieve from the end of the list or dataset.This query fetches the first 10 transactions.To navigate through your dataset, you can use after and before in conjunction with first or last. These parameters accept cursors, which are typically provided in the response of your initial query.after: Fetch items after the specified cursor, used with first.before: Fetch items before the specified cursor, used with last.This query fetches the next 10 transactions following the transaction with the cursor "cursorOfLastItem".If no pagination terms are set, GraphQL servers may apply default limits to prevent excessively large datasets from being returned in a single query, potentially impacting performance. The default behavior can vary based on the server's configuration but often involves returning a predefined maximum number of items.For instance, without specifying first or last, a query to the transactions field might return the first 5-10 transactions by default, depending on the server settings.This behavior ensures that server resources are not overwhelmed by large requests and that client applications receive data in manageable chunks.General Tips for Optimizing Queries To optimize your GraphQL queries in Arweave, follow these general guidelines:Specificity: Query with the most precise tags possible to narrow the search scope and enhance performance.Minimalism: Limit your query to the essential set of tags to reduce processing time and data transfer.Schema Design: Design your app's schema to reflect query patterns, possibly introducing tags that encapsulate frequent combinations of criteria.Include Non-tag Fields: Adding fields like owner can refine your search, making your queries more efficient.Order Your Tags: Arrange tags from most specific to most general to leverage Arweave's indexing more effectively.By incorporating these strategies, developers can achieve faster and more precise data access from Arweave, enhancing the performance and responsiveness of decentralized applications. This balanced approach to query construction and optimization is key to navigating the expansive and decentralized storage landscape Arweave provides.Making a Query Executing GraphQL queries within the Arweave ecosystem offers flexibility and multiple avenues for developers and users alike. Whether you prefer a hands-on, manual approach to constructing and testing queries, or you aim for automation and integration within your applications, Arweave provides the tools necessary to interact with its decentralized data storage seamlessly.GraphQL Playground For those new to GraphQL or seeking to fine-tune their queries before implementation, the GraphQL playground offers an invaluable resource. This interactive interface allows users to manually construct queries, explore the schema, and immediately see the results of their queries. Accessible via web browsers, the playground can be found at the /graphql endpoint of most Arweave indexing services, such as https://arweave.dev/graphql. Here, you can experiment with different queries, understand the structure of the data, and refine your approach without writing a single line of code in your application.Steps for Accessing the GraphQL Playground:Navigate to https://arweave.dev/graphql, or the graphql endpoint of any AR.IO gateway, in your web browser.Enter your GraphQL query in the provided interface.Press the "play" button to execute the query to see real-time results and debug as needed.Using an API For application development and automation, making GraphQL queries programmatically is essential. You can send POST requests directly to the GraphQL endpoint of any indexing service that supports it, such as arweave.net or any AR.IO gateway. These requests should contain your query in the body, allowing for dynamic and automated data retrieval within your application.When selecting an indexing service, consider the data coverage and reliability of the gateway to ensure it meets your application's needs. Different gateways might have varying degrees of indexed data available, so choosing one that is consistently up-to-date and comprehensive is key.Example of making a programmatic query:Using an SDK For an even more integrated experience, some Software Development Kits (SDKs) offer direct methods for executing GraphQL queries. The Arweave SDK, for example, provides built-in functionalities to interact with the blockchain, simplifying the process of making queries. By leveraging these SDKs, developers can bypass the intricacies of manual HTTP request construction, focusing instead on the logic and design of their applications.Example of using the Arweave SDK for GraphQL queries:

---

# 150. Uploading to Arweave - ARIO Docs

Document Number: 150
Source: https://docs.ar.io/guides/uploading-to-arweave
Words: 341
Extraction Method: html

Uploading to Arweave Overview While AR.IO provides powerful tools for accessing and interacting with data on Arweave, that data must first be uploaded to the network. This guide will walk you through the process of uploading data to Arweave using the Turbo SDK, which provides a streamlined experience for data uploads.Installing Turbo SDK Authentication Node.js Environment Browser Environment Purchasing Turbo Credits Turbo Credits are the payment medium used by the Turbo Upload Service. Each Credit represents a 1:1 conversion from the upload power of the Arweave native token (AR). Turbo Credits can be purchased with fiat currency via the Turbo Top Up App, or with supported cryptocurrencies via the Turbo SDK. Learn more about Turbo Credits and available methods for purchasing them here.Node.js Environment Browser Environment In a browser environment, the topUpWithTokens method is not available. Instead, you'll need to manually send tokens to the Turbo wallet address and then submit the transaction for processing. Here are detailed examples for each supported chain:Browser Top-Up Examples Note: The wait times for chain settlement are approximate and may need adjustment based on network conditions:Ethereum: ~15 minutes Solana: ~400-600 milliseconds Arweave: ~30-36 minutes Polygon: ~2-3 seconds Base: ~2-5 seconds KYVE: ~5 minutes Once you have purchased Turbo credits, you can upload files and folders to Arweave. The process is the same regardless of which token type you used for authentication, but differs between Node.js and browser environments.Node.js Environment Node.js Upload Examples Browser Environment Browser Upload Examples Important Notes:For single file uploads, always include a Content-Type tag to ensure proper file viewing The fileStreamFactory must return a NEW stream each time it's called Folder uploads automatically detect and set Content-Type tags for all files You can specify additional tags in dataItemOpts for both file and folder uploads The maxConcurrentUploads option controls how many files are uploaded simultaneously Use throwOnFailure: true to ensure all files are uploaded successfully Complete Examples Here are complete examples showing how to authenticate, check balances, and handle lazy funding for uploads. These examples demonstrate the full workflow from start to finish.

---

# 151. Managing Primary Names - ARIO Docs

Document Number: 151
Source: https://docs.ar.io/guides/primary-names
Words: 716
Extraction Method: html

Overview Primary names allow users to set a user-friendly alias for their Arweave wallet address, simplifying how addresses are displayed across applications. This process involves interaction between two separate smart contracts:The AR.IO Contract - which manages the primary name registry and requests The ANT Contract - which controls the specific ArNS name and must approve any primary name requests The process requires two steps because these are separate contracts:First, a request must be submitted to the AR.IO contract to set a specific ArNS name as the primary name for a wallet Then, the ANT owner must approve this request, confirming that this wallet can use the name as its primary identifier This two-step verification ensures that both the wallet owner and the ANT owner have authorized the connection.Think of this like setting a username on a social platform - where the
platform (AR.IO contract) maintains the registry of usernames, and the name
owner (ANT) must approve who can claim their name as an identifier. Setting a Primary Name with arns.app arns.app is the official ArNS portal from AR.IO. It allows you to manage your ArNS names and set primary names for your wallet addresses.To set a primary name using arns.app, connect your wallet and navigate to the ArNS name management page. Simply locate the ArNS name you want to set as primary and click the star icon at the right of the entry. You will then be prompted to accept the cost of setting the name, and the location of the funds to pay for the transaction. Once the transaction is confirmed, you will be prompted to sign the transaction with your connected wallet. When this is completed, the name will be set as primary for your wallet address, and apps that support primary names will display the name instead of the wallet address. Setting a Primary Name With the AR.IO SDK The process of setting a primary name using the AR.IO SDK involves two steps: requesting and approval. This two-step process ensures proper authorization from both the wallet owner and the ANT owner.Requesting a Primary Name When requesting a primary name, you're asking to use an ArNS name as the identifier for your wallet address. This requires:The ArNS name to exist Your wallet to submit the request using the requestPrimaryName method The ANT owner's approval Check Primary Name Requests The getPrimaryNameRequest method allows you to verify if a primary name request exists and its status. Use this to:Verify if your request is pending Check if someone has requested to use your ANT's name Build UI flows around the request/approval process Approving a Primary Name Request The ANT owner must approve any requests to use their name as a primary name using the approvePrimaryNameRequest method. This gives ANT owners control over how their names are used as identifiers.Querying Primary Names The AR.IO SDK provides several methods to query primary names, each serving different use cases:Get a Single Primary Name Use getPrimaryName when you need to find the primary name for a specific wallet address. This is particularly useful in applications where you want to display a user-friendly identifier instead of their wallet address.Common use cases:Displaying a user's primary name in a profile or dashboard Showing who authored a piece of content Making transaction histories more readable List All Primary Names Use getPrimaryNames when fetching all primary names. This is useful when you need to:Build a directory of users Create search functionality Display multiple users in a more readable format Map multiple wallet addresses to their friendly names at once The method supports pagination through a cursor-based system, where the cursor is the last name from your previous request.The response includes:items: Array of primary names for the current page cursor: The last name from the current request, used for getting the next page hasMore: Boolean indicating if there are more results available totalItems: Total number of primary names matching your query Best Practices Always verify ownership of both the ArNS name and ANT before attempting to set a primary name Check if a primary name request already exists before submitting a new one Consider implementing error handling for cases where the name or ANT doesn't exist When displaying primary names in your application, always have a fallback to show the wallet address if no primary name exists

---

# 152. The ARIO Token - ARIO Docs

Document Number: 152
Source: https://docs.ar.io/token
Words: 251
Extraction Method: html

Overview ARIO is the multifunction AO Computer based token that powers the AR.IO Network and its suite of permanent cloud applications. The ARIO Token uses include:Gateway Participation: Gateway operators must stake ARIO tokens to join and actively participate in the network.Eligibility for Protocol Rewards: Both individuals who stake tokens as gateway operators and those who delegate tokens to a gateway are positioned to receive protocol rewards.ArNS Name Purchases: Acquiring friendly names through the Arweave Name System (ArNS) requires ARIO tokens. These transactions directly contribute to the protocol, with the proceeds being redistributed through the Observation and Incentive Protocol.Universal Currency: Within the AR.IO ecosystem, ARIO tokens serve as a versatile currency, enabling network participants to make purchases and exchange value.Moreover, ARIO tokens play a crucial role in driving ecosystem growth, fueling incentive programs, investments, bounties, and grants designed for active participants.Adding ARIO Token to Wander To view your ARIO token balance in Wander, formerly ArConnect, follow these steps to add the token to your wallet:Open your Wander wallet (available on both desktop and mobile) Access Settings:Mobile: Click the 3 vertical dots in the top right, then select "Settings" Desktop: Click the hamburger menu icon in the top left Select "Tokens" Click "Import Token" For Desktop users: Ensure "Token Type" is set to "ao Token" Enter the AO process ID:qNvAoz0TgcH7DMg8BCVn8jF32QH5L6T29VjHxhHqqGE The token ticker "ARIO" and name "AR.IO Network" will appear automatically Click "Import Asset" to complete the process Once imported, you'll be able to view your total ARIO balance in your Wander wallet.

---

# 153. ARIO Docs

Document Number: 153
Source: https://docs.ar.io/wayfinder
Words: 351
Extraction Method: html

Wayfinder Wayfinder is a client-side routing and verification protocol that provides decentralized, cryptographically verified access to data stored on Arweave via the AR.IO Network. It automatically selects optimal gateways and ensures data integrity for seamless permaweb experiences.What is Wayfinder?Wayfinder solves the challenge of reliable data access on the permaweb by:Intelligent Routing: Automatically selects the best gateway for each request based on performance, availability, and user preferences Data Verification: Cryptographically verifies data integrity to ensure you're getting authentic, unmodified content Decentralized Access: Eliminates single points of failure by distributing requests across multiple AR.IO gateways Seamless Integration: Works behind the scenes to provide fast, reliable access without requiring users to understand the underlying infrastructure Who is Wayfinder For?Builders People who build dApps on the permaweb Wayfinder enables developers to build robust decentralized applications with:Reliable Data Access: Never worry about gateway downtime or slow responses Built-in Verification: Ensure data integrity without implementing complex verification logic Developer-Friendly APIs: Simple JavaScript/TypeScript libraries and React components Performance Monitoring: Built-in telemetry to track and optimize application performance Flexible Configuration: Choose routing strategies and verification methods that fit your use case Browsers People who browse the permaweb Wayfinder provides end users with:Fast Loading: Automatically routes to the fastest available gateway for optimal performance Reliable Access: Seamlessly switches between gateways if one becomes unavailable Data Integrity: Verifies that content hasn't been tampered with or corrupted Transparent Operation: Works invisibly in the background without requiring user interaction No Tokens Required: Access permaweb content without needing AR tokens or wallet connections Operators People who operate AR.IO gateways Wayfinder helps gateway operators by:Performance Insights: Provides telemetry data to help optimize gateway performance Network Participation: Enables gateways to participate in the decentralized routing ecosystem Load Distribution: Intelligently distributes traffic based on gateway capabilities and performance Quality Monitoring: Tracks gateway reliability and performance metrics Network Health: Contributes to overall AR.IO network resilience and performance Available Packages @ar.io/wayfinder-core: Core JavaScript/TypeScript library for any web application @ar.io/wayfinder-react: React components, hooks, and providers for React applications Getting Started Ready to integrate Wayfinder into your project? Check out our Getting Started Guide for installation instructions and basic configuration examples.

---

# 154. ARIO Docs

Document Number: 154
Source: https://docs.ar.io/wayfinder/core/gateway-providers/local-storage
Words: 142
Extraction Method: html

LocalStorageGatewaysProvider The LocalStorageGatewaysProvider is a gateway provider that caches gateway lists in the browser's localStorage. This allows gateway data to persist across page reloads and browser sessions, making it ideal for web applications that require fast access to gateway information without repeated network requests. The provider automatically manages cache expiration based on a configurable TTL (time-to-live), ensuring that gateway data remains fresh while minimizing network usage. Use this provider when you want persistent, client-side caching of gateway lists in browser environments.Note: If you are building a React-based application, consider using @ar.io/wayfinder-react for seamless integration with React components, hooks, and context providers. This package is designed to work hand-in-hand with gateway providers like LocalStorageGatewaysProvider for optimal developer experience.Basic Usage Configuration Options Related Documentation Gateway Providers Overview: Compare all gateway providers NetworkGatewaysProvider: Dynamic network discovery StaticGatewaysProvider: Static gateway configuration Wayfinder Configuration: Main wayfinder setup

---

# 155. Telemetry - ARIO Docs

Document Number: 155
Source: https://docs.ar.io/wayfinder/core/telemetry
Words: 195
Extraction Method: html

Telemetry Configuration Wayfinder includes optional telemetry support built on the OpenTelemetry standard. Telemetry is completely opt-in and disabled by default.Overview Wayfinder's telemetry system:100% Opt-in: Disabled by default, only enabled when you explicitly configure it OpenTelemetry Standard: Built on the industry-standard OpenTelemetry framework Your Choice of Destination: Send data to your own servers Configurable: Full control over what data is collected and where it goes All telemetry is disabled by default. It only activates when you
explicitly enable it in your configuration.Basic Configuration Minimal Setup Send to Your Own Infrastructure Wayfinder uses Honeycomb OTLP by default (exporter URL and Honeycomb headers).
Using other endpoints may require an OTLP-compatible collector or intermediary
that accepts Honeycomb headers. The dataset is currently fixed to wayfinder-core.Frequently Asked Questions Q: Is telemetry enabled by default?A: No, telemetry is completely disabled by default and only activates when you explicitly configure it.Q: Can I use my own telemetry backend?A: Yes, specify your own exporterUrl to send data to any OpenTelemetry-compatible backend.Q: Does this affect performance?A: Minimal impact when using appropriate sampling rates (1-10% for production).Q: Can I disable telemetry after enabling it?A: Yes, simply set enabled: false or remove the telemetrySettings configuration entirely.

---

# 156. Signature Verification Strategy - ARIO Docs

Document Number: 156
Source: https://docs.ar.io/wayfinder/core/verification-strategies/signature-verification
Words: 148
Extraction Method: html

SignatureVerificationStrategy Overview The SignatureVerificationStrategy validates Arweave transaction signatures to ensure data authenticity and ownership. This strategy provides cryptographic proof that the data was created by the claimed wallet address and hasn't been tampered with since signing.Important SignatureVerificationStrategy requires that the trusted gateway has the relevant
transaction data indexed locally. Gateways cannot proxy out verification
requests to other sources, as this would compromise the security and
reliability of the verification process. If a gateway doesn't have the
required data indexed, verification will fail.How It Works Fetch Metadata: Retrieve transaction metadata from trusted gateways Reconstruct Signature Data: Build the signature data using the received content Verify Signature: Validate the signature matches the claimed owner's public key Check Ownership: Confirm the transaction was signed by the claimed wallet Result: Pass or fail based on signature validation Basic Usage Related Hash Verification: Learn about fast integrity checking Signature Verification: Understand authenticity validation

---

# 157. Wayfinder Core Release Notes - ARIO Docs

Document Number: 157
Source: https://docs.ar.io/wayfinder/release-notes/core
Words: 514
Extraction Method: html

Wayfinder Core Changelog Overview Welcome to the documentation page for the Wayfinder Core release notes. Here, you will find detailed information about each version of the Wayfinder Core library, including the enhancements, bug fixes, and any other changes introduced in every release. This page serves as a comprehensive resource to keep you informed about the latest developments and updates in the Wayfinder Core library. For those interested in exploring the source code, each release's code is readily accessible at our GitHub repository: Wayfinder Core change logs. Stay updated with the continuous improvements and advancements in the Wayfinder Core library by referring to this page for all release-related information.1.2.0 (2025-08-04) Minor Changes 2d5970f: Add PingRoutingStrategy that performs a HEAD check on gateway returned from provided routing strategy 1.2.0-alpha.0 (2025-08-04) Minor Changes 2d5970f: Add PingRoutingStrategy that performs a HEAD check on gateway returned from provided routing strategy 1.1.1 (2025-07-30) Patch Changes b246f78: Provide gateways when calling selectGateway in resolveUrl 1.1.0 (2025-07-30) Minor Changes 69ddbfb: Add runtime configuration methods for routing and verification strategies 1.1.0-alpha.0 (2025-07-23) Minor Changes 69ddbfb: Add runtime configuration methods for routing and verification strategies 1.0.7 (2025-07-23) Patch Changes 658c5f6: Fix SimpleCacheRoutingStrategy to avoid duplicate requests to routingStrategy 1.0.6 (2025-07-23) Patch Changes a42d57c: Allow gatewaysProvider to be optional, use StaticGatewaysProvider by default 1.0.6-alpha.0 (2025-07-23) Patch Changes a42d57c: Allow gatewaysProvider to be optional, use StaticGatewaysProvider by default 1.0.5 (2025-07-22) Patch Changes 73aa1b9: Adds RemoteVerificationStrategy and modifies verifyData interface to support optional response headers to use when verifying data b7299cc: Remove unused parameters from various routing strategies b81b54e: Remove extra gateways arg in RoundRobinRoutingStrategy 1.0.5-alpha.2 (2025-07-22) Patch Changes b7299cc: Remove unused parameters from various routing strategies 1.0.5-alpha.1 (2025-07-22) Patch Changes b81b54e: Remove extra gateways arg in RoundRobinRoutingStrategy 1.0.5-alpha.0 (2025-07-21) Patch Changes 73aa1b9: Adds RemoteVerificationStrategy and modifies verifyData interface to support optional response headers to use when verifying data 1.0.4 (2025-07-17) Patch Changes 719acbd: Add require and default exports to wayfinder-core 1.0.3 (2025-07-16) Patch Changes 86bdc2f: Prevent duplicate requests in LocalStorageGatewaysProvider and SimpleCacheGatewaysProvider 226f3af: Fix defaultTtlSeconds in gateway caches 1.0.3-alpha.1 (2025-07-16) Patch Changes 226f3af: Fix defaultTtlSeconds in gateway caches 1.0.3-alpha.0 (2025-07-16) Patch Changes 86bdc2f: Prevent duplicate requests in LocalStorageGatewaysProvider and SimpleCacheGatewaysProvider 1.0.2 (2025-07-15) Patch Changes 8f79caf: Fix import of zone.js file, only load once and in browsers if not already available a3e69af: Add support for clientName and clientVersion on telemetry settings cfcfb66: Default Wayfinder to use RandomRoutingStrategy 1.0.2-alpha.2 (2025-07-15) Patch Changes 8f79caf: Fix import of zone.js file, only load once and in browsers if not already available 1.0.2-alpha.1 (2025-07-15) Patch Changes cfcfb66: Default Wayfinder to use RandomRoutingStrategy 1.0.2-alpha.0 (2025-07-15) Patch Changes a3e69af: Add support for clientName and clientVersion on telemetry settings 1.0.1 (2025-07-14) Patch Changes aa5700e: Improve telemetry configuration for browsers and chrome extensions 2c170be: Adds additional telemetry support when calling resolveUrl c78effa: Add LocalStorageGatewaysProvider cache as main export from wayfinder-core 1.0.1-alpha.2 (2025-07-14) Patch Changes c78effa: Add LocalStorageGatewaysProvider cache as main export from wayfinder-core 1.0.1-alpha.1 (2025-07-10) Patch Changes 2c170be: Adds additional telemetry support when calling resolveUrl 1.0.1-alpha.0 (2025-07-09) Patch Changes aa5700e: Improve telemetry configuration for browsers and chrome extensions 1.0.0 (2025-07-01) Major Changes 89c0efe: Initial wayfinder-core release

---

# 158. ArNS - Arweave Name System  Cooking with the Permaweb

Document Number: 158
Source: https://cookbook.arweave.net/fundamentals/accessing-arweave-data/arns.html
Words: 523
Extraction Method: html

ArNS - Arweave Name System Overview The Arweave Name System (ArNS) is the phonebook of the Permaweb.It is a decentralized and censorship-resistant naming system that is enabled by AR.IO Gateways and used to connect friendly names to Permaweb apps, pages and data.This system works similarly to traditional DNS, where a user can purchase a name in a registry and DNS Name servers resolve these names to IP addresses.With ArNS, the registry is decentralized, permanent and stored on Arweave (via AO), and each AR.IO gateway acts as both cache and name resolver. Users can register a name within the ArNS Registry, like "my-name" and set a pointer to any Arweave Transaction ID.AR.IO Gateways will resolve that name as one of their own subdomains, eg. https://laserilla.arweave.net and proxy all requests to the associated Arweave Transaction ID. Each registered name can also have under names associated with it that each point to an Arweave Transaction ID, like https://v1_laserilla.arweave.net, giving even more flexibility and control to its owner.The ArNS Registry ArNS uses AO to manage its name records. Each record, or name, is leased by a user or bought permanently and tied to an ANT token. You can register multiple ArNS names to a single ANT, but you cannot register multiple ANTs to a single ArNS name - the gateways wouldn't know where to point the routing ID.ArNS names can be up to 32 characters, including numbers [0-9], letters [a-z], and dashes [-]. The dashes cannot be trailing dashes, e.g. -myname.ANTs (Arweave Name Tokens) ANTs are a crucial part of the ArNS ecosystem - they are the actual key to owning an ArNS name. When you register an ArNS name to an ANT, the ANT then becomes the transfer method for that name. The ArNS registry does not care who owns the ANT, it simply knows what name ANT it belongs to.Within ANTs you can build out whatever functionality you wish, within the scope ArNS registry approved source code transaction list.Under_Names Undernames are records held and managed by your ANT (Arweave Name Token). These records can be created and managed without even owning an ARNS name, and will be transferred along with the ant when sent to a new owner. Likewise if your ArNS name expires, and you register your ANT to a new ArNS name, all your undername will remain intact.Example: you own oldName.arweave.net.then: You create the undername "my" - my_oldName.arweave.net.then: oldName.arweave.net expires, and you register newName.arweave.net to your ANT.now: my_ undername is accessable on newName - my_newName.arweave.net.Below is an example of an ANT contract State:{
  balances:{ QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ : 1 },
  controller: "QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ",
  evolve: null,
  name: "ArDrive OG Logo",
  owner: "QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ",
  records:{
    @:{ transactionId: "xWQ7UmbP0ZHDY7OLCxJsuPCN3wSUk0jCTJvOG1etCRo" },
    undername1:{ transactionId: "usOLUmbP0ZHDY7OLCxJsuPCN3wSUk0jkdlvOG1etCRo" }
  },
  ticker:"ANT-ARDRIVE-OG-LOGO"
} the base "@" record is the initial routing id for the ANT. if you registered 'my-name' to this ANT, and tried to access it via my-name.arweave.net, you would be redirected to the @ record's transactionId.if you tried to access undername1_my-name.arweave.net, you would get 'undername1's transactionId.ANT's, in theory, have an UNLIMITED number of undernames. However, how many will be served depends on which tier is used with your ArNS name.Resources ArNS App ArNS Docs

---

# 159. Accessing Arweave Data  Cooking with the Permaweb

Document Number: 159
Source: https://cookbook.arweave.net/fundamentals/accessing-arweave-data/index.html
Words: 401
Extraction Method: html

Accessing Arweave Data One of Arweave's core strengths is permanent data storage, but stored data is only valuable if it can be efficiently discovered and retrieved. Arweave provides multiple methods for accessing data, each optimized for different use cases and requirements.Data Access Fundamentals All data on Arweave is accessible through gateways - HTTP endpoints that serve as bridges between users and the Arweave network. These gateways expose various interfaces and methods for retrieving data:Direct data retrieval - Access data by transaction ID Query interfaces - Search and filter data based on metadata Name resolution - Access data through human-readable names Path-based access - Navigate data collections like websites Access Methods 1. HTTP API The simplest way to access Arweave data is through direct HTTP requests using a transaction ID:https://arweave.net/{transaction-id} This method is ideal for:Retrieving known data by ID Simple integrations Direct data access without queries Learn more about Gateways & Access → 2. GraphQL Queries For more complex data discovery and filtering, Arweave gateways provide GraphQL endpoints that enable sophisticated queries based on:Owner addresses Transaction tags Block heights Time ranges GraphQL is the preferred method for:Searching large datasets Building applications that need to discover data Complex filtering based on metadata Learn more about GraphQL → 3. Path Manifests Path manifests enable organizing multiple files into navigable collections, similar to traditional websites:https://arweave.net/{manifest-id}/{path/to/file} Use manifests for:Hosting websites and web applications Creating browsable file collections Organizing related content Learn more about Manifests → 4. ArNS (Arweave Name System) ArNS provides human-readable names that resolve to Arweave transaction IDs:https://{name}.arweave.dev ArNS is perfect for:User-friendly URLs Updatable references to content Building permanent web applications Learn more about ArNS → Choosing the Right Method Method Best For Example Use Case HTTP API Direct access by ID Loading a specific image or file GraphQL Data discovery and filtering Finding all transactions with specific tags Manifests Multi-file collections Hosting a website or application ArNS Human-readable addressing Creating a permanent blog or dApp Gateway Infrastructure All these access methods are provided by Arweave gateways. Gateways can be:Public gateways like arweave.net - Open for anyone to use Private gateways - Run by individuals or organizations for their own use AR.IO gateways - Decentralized gateway network providing enhanced features Set up gateway access for your application Query data with GraphQL to build dynamic applications Create path manifests for organizing content Register ArNS names for user-friendly addressing Additional Resources Querying Arweave Guide GraphQL Reference

---

# 160. Gateways in the Arweave Network  Cooking with the Permaweb

Document Number: 160
Source: https://cookbook.arweave.net/fundamentals/accessing-arweave-data/gateways.html
Words: 275
Extraction Method: html

Gateways in the Arweave Network Gateways serve as the interface between the Arweave network and end-users, making permaweb data easily accessible through standard web browsers. Often described as the "front door to the permaweb," these services allow users to interact with blockchain-stored content in a familiar web-like experience.When you access content on Arweave, you typically use a URL structure like:https://<gateway>/<tx> This allows HTML files to render as web pages, images to display properly, and other data types to be served appropriately—creating an experience similar to the traditional web despite the content being stored on a decentralized network.Key Functions of Gateways Gateways provide several critical services beyond basic content delivery:Content Caching: Store frequently accessed transactions to improve performance Data Indexing: Provide GraphQL interfaces for querying transactions by tags and metadata Network Seeding: Help distribute transactions throughout the Arweave network Content Moderation: Apply content policies to determine which data is served Relationship to Core Protocol It's important to understand that gateways are not part of the core Arweave protocol. This distinction has several implications:Operating a gateway is separate from running a node that secures the network There is no built-in protocol-level incentive structure for gateway operators Gateway services can implement their own economic models and incentives Applications can operate their own gateways for improved performance This separation allows for a more flexible and decentralized ecosystem where different gateway operators can experiment with various service models.Several gateway services currently serve the Arweave ecosystem:arweave.net - Operated by the Arweave team arweave.world arweave.asia arweave.live g8way.io The AR.IO project is working to make gateway operation more accessible, potentially increasing the decentralization of access points to the network.ArWiki Gateway Documentation AR.IO Project

---

# 161. Path Manifests  Cooking with the Permaweb

Document Number: 161
Source: https://cookbook.arweave.net/fundamentals/accessing-arweave-data/manifests.html
Words: 370
Extraction Method: html

Path Manifests Overview When uploading files to Arweave each file is assigned its own unique transaction ID. By default these ID's aren't grouped or organized in any particular manner.One picture of your cat might be stored with a transaction ID of bVLEkL1SOPFCzIYi8T_QNnh17VlDp4RylU6YTwCMVRw, while another with FguFk5eSth0wO8SKfziYshkSxeIYe7oK9zoPN2PhSc0 as its transaction ID.Cat1 Cat2   bVLEkL1SOPFCzIYi8T_QNnh17VlDp4...FguFk5eSth0wO8SKfziYshkSxeIYe7oK9zoPN2PhSc0 These transaction ID's are a bit unwieldy and make it difficult to find all of your relevant files. Without a path manifest, if you uploaded 100 pictures of your cat you would need to keep track of 100 different IDs and links!Path Manifests are a way to link multiple transactions together under a single base transaction ID and give them human readable file names. In relation to the cat example, you could have one base transaction ID to remember and use it like a folder - accessing your cat pictures with more memorable filenames like {base id}/cat1.jpg, {base id}/cat2.jpg, etc.Creating grouped sets of readable file names is essential for creating practical applications on Arweave, and unlocks the ability to host websites or other file collections as explored in the examples below.What Can You Use Manifests For? Any time you need to group files in a hierarchical way, manifests can be useful. For example:Storing NFT collections:https://arweave.net/X8Qm…AOhA/0.png https://arweave.net/X8Qm…AOhA/1.png This mirrors the common base path approach used by NFT collections when linking to NFT images and metadata on a storage API or IPFS.Hosting websites:https://arweave.net/X8Qm…AOhA/index.html https://arweave.net/X8Qm…AOhA/styles.css https://arweave.net/X8Qm…AOhA/public/favicon.png Manifest Structure  Path Manifests are a special format of transaction created and posted to Arweave using the Tags:{ name: "Content-type", value: "application/x.arweave-manifest+json" } and having JSON formatted transaction data that matches the example below.{
  "manifest": "arweave/paths",
  "version": "0.2.0",
  "index": {
    "path": "index.html"
  },
  "fallback": {
    "id": "cG7Hdi_iTQPoEYgQJFqJ8NMpN4KoZ-vH_j7pG4iP7NI"
  },
  "paths": {
    "index.html": {
      "id": "cG7Hdi_iTQPoEYgQJFqJ8NMpN4KoZ-vH_j7pG4iP7NI"
    },
    "js/style.css": {
      "id": "fZ4d7bkCAUiXSfo3zFsPiQvpLVKVtXUKB6kiLNt2XVQ"
    },
    "css/style.css": {
      "id": "fZ4d7bkCAUiXSfo3zFsPiQvpLVKVtXUKB6kiLNt2XVQ"
    },
    "css/mobile.css": {
      "id": "fZ4d7bkCAUiXSfo3zFsPiQvpLVKVtXUKB6kiLNt2XVQ"
    },
    "assets/img/logo.png": {
      "id": "QYWh-QsozsYu2wor0ZygI5Zoa_fRYFc8_X1RkYmw_fU"
    },
    "assets/img/icon.png": {
      "id": "0543SMRGYuGKTaqLzmpOyK4AxAB96Fra2guHzYxjRGo"
    }
  }
} fallback:Manifest version 0.2.0 introduced the fallback attribute. fallback is an object that accepts the sub attribute id, which defines an Arweave data item transaction id for the resolver to fall back to if it fails to correctly resolve a requested path.Source and Further Reading in the official Arweave Path Manifest docs: Arweave Docs

---

# 162. Decentralized Computing  Cooking with the Permaweb

Document Number: 162
Source: https://cookbook.arweave.net/fundamentals/decentralized-computing/index.html
Words: 921
Extraction Method: html

Decentralized Computing The Permaweb enables a new paradigm of decentralized computing that goes beyond simple data storage. Through technologies like AO and HyperBEAM, developers can build sophisticated applications that run permanently and trustlessly on a global, decentralized computer.Overview Traditional computing relies on centralized servers and cloud providers, creating single points of failure and control. Decentralized computing on the Permaweb distributes computation across a network of nodes, ensuring permanence, censorship resistance, and trustless execution.Key Components AO Processes - Autonomous computational units that maintain state and execute logic permanently on Arweave HyperBEAM - The production implementation of AO-Core that provides HTTP access to decentralized computing resources Decentralized Computing Architecture:User/Application
       ↓ (HTTP Request)
   HyperBEAM Node ←──────→ HTTP Response
       ↓ (Route Message)
   AO Process ←──────→ Other AO Process
       ↓ (Execute Logic)      (Messages)
 Lua/WASM Runtime
       ↓ (Update State)
   Arweave Storage

Decentralized Computing Network:
┌─────────────────────────────────────┐
│ Node 1  Node 2  Node 3  ... Node N │
│   ↑       ↑       ↑          ↑     │
│   └───────┼───────┼──────────┘     │
│           └───────┘                │
│     (Distributed across network)   │
└─────────────────────────────────────┘ This system enables permanent, trustless computation where HyperBEAM nodes provide HTTP access to AO processes that execute on various runtimes and persist state to Arweave.Core Concepts Permanent Computation Unlike traditional serverless functions that can be shut down or modified, AO processes run permanently on the Arweave network. Once deployed, they continue executing indefinitely without requiring maintenance or hosting fees.Trustless Execution All computation is cryptographically verifiable. Results can be independently verified by anyone, eliminating the need to trust centralized providers or intermediaries.Message-Based Architecture AO processes communicate through asynchronous message passing, enabling sophisticated distributed computing patterns while maintaining consistency and ordering.Device Modularity HyperBEAM's device architecture allows different computational engines (Lua, WebAssembly, custom modules) to be plugged in as needed, creating a flexible and extensible computing environment.Why Decentralized Computing Matters Permanence Applications never go offline or disappear No vendor lock-in or platform dependencies Permanent accessibility for users worldwide Censorship Resistance No single authority can shut down applications Global network distribution prevents blocking Permissionless participation in the network Economic Efficiency Pay once for permanent deployment No ongoing hosting or maintenance costs Competitive pricing through decentralized markets Trust Minimization Cryptographically verifiable execution Open source and auditable code Mathematical guarantees instead of institutional trust Architecture Comparison Aspect Traditional Cloud Decentralized Computing Uptime 99.9% SLA 100% (permanent) Control Platform controlled User controlled Costs Monthly recurring One-time deployment Scalability Manual scaling Automatic network scaling Verification Trust-based Cryptographically provable Censorship Platform policies Censorship resistant Learning Path 1. Start with AO Processes Understand the fundamental building blocks of decentralized computing:What are AO Processes - Learn the core concepts and architecture Process Communication - Master message passing and inter-process communication State Management - Understand persistent state and data consistency 2. Explore HyperBEAM Learn how to interact with and leverage the AO Computer:HyperBEAM Introduction - Understand the HTTP gateway to AO Querying AO Process State - Master the HTTP API for data access Lua Serverless Functions - Build serverless functions with permanent availability HyperBEAM Devices - Understand the modular device architecture 3. Build Applications Apply your knowledge to real-world projects:Builder's Journey - End-to-end development workflow Zero-deployed Full Stack App - Quick start guide Advanced Patterns - Sophisticated application architectures Use Cases Decentralized Applications (dApps) Token contracts and DeFi protocols Voting and governance systems Social media and content platforms Gaming and virtual worlds Serverless Computing API endpoints with permanent availability Data processing and transformation Scheduled tasks and automation Microservices architecture Data Processing ETL pipelines and analytics Machine learning inference Content delivery and caching Database and storage systems Integration and Automation Cross-chain bridges and oracles Webhook endpoints and notifications Workflow automation Third-party API integration Technical Benefits Developer Experience Familiar interfaces - HTTP APIs and standard programming languages No infrastructure management - Deploy and forget Instant scaling - Network automatically handles load Built-in persistence - State management included Performance Characteristics Low latency - Global edge network High availability - No single points of failure Elastic scaling - Resources scale with demand Predictable costs - One-time deployment fees Security Model Cryptographic verification - All execution is provable Isolated execution - Sandboxed environments Immutable code - Deployed logic cannot be changed Transparent operations - All activity is publicly auditable Getting Started Prerequisites Basic understanding of blockchain concepts Familiarity with HTTP APIs Programming experience (JavaScript/Lua preferred) Quick Start Deploy your first process - Zero-deployed App Guide Query process state - HyperBEAM Querying Build serverless functions - Lua Functions Guide Development Tools AOS - AO Studio development environment ao-connect - JavaScript SDK for AO interaction HyperBEAM nodes - HTTP gateways to the AO Computer Future of Decentralized Computing Emerging Capabilities AI/ML integration - Permanent machine learning models Advanced cryptography - Zero-knowledge proofs and privacy Cross-chain bridges - Seamless blockchain interoperability IoT integration - Edge computing with global state Network Effects As the decentralized computing network grows:More computational resources become available Specialized devices and capabilities emerge Costs decrease through competition Innovation accelerates through composability Industry Impact Decentralized computing enables:Platform independence - Applications that outlive their creators Global accessibility - Permanent availability worldwide Economic inclusion - Lower barriers to application deployment Innovation freedom - Censorship-resistant development Community and Resources Documentation AO Computer Docs - ao.arweave.net HyperBEAM Documentation - hyperbeam.arweave.net Arweave Developer Docs - docs.arweave.org Community Discord - discord.gg/arweave GitHub - github.com/permaweb Twitter - @ArweaveEco Tools and SDKs AO Connect - JavaScript SDK for AO processes AOS - Local development environment Permaweb Deploy - Deployment tools and utilities  Ready to build? Start with What are AO Processes to understand the fundamentals, then move to HyperBEAM Introduction to learn how to interact with the AO Computer.

---

# 163. Transaction Metadata (Tags)  Cooking with the Permaweb

Document Number: 163
Source: https://cookbook.arweave.net/fundamentals/transactions/tags.html
Words: 559
Extraction Method: html

Arweave can be thought of as a permanent append-only hard drive where each entry on the drive is its own unique transaction. Transactions have a unique ID, signature, and owner address for the address that signed and paid for the transaction to be posted. Along with those header values, the Arweave protocol allows users to tag transactions with custom tags. These are specified as a collection name value pairs appended to the transaction. These tags make it possible to query Arweave and find all the Transactions that include a particular tag or tags. The ability to query and filter transactions is critical to supporting apps built on Arweave.What are Transaction Tags?Transaction tags are key-value pairs, where the combination of base64URL keys and values must be less than the maximum of 2048 bytes for an arweave native transaction.Some common examples of transaction tags include:Content-Type: Used to specify the MIME type of content for render on the permaweb.App-Name: This tag describes the app that is writing the data App-Version: This tag is the version of the app, paired with App-Name Unix-Time: This tag is the a unix timestamp, seconds since epoch.Title: Used to give a name or brief description of the content stored in the transaction.Description: Used to provide a longer description of the content.Transaction tags can be used for a variety of purposes, such as indexing transactions for search, organizing transactions into categories, or providing metadata about the content stored in a transaction.Some good things to know about Transaction Tags Transaction tags are encoded as Base64URL encoded strings for both the key and value. This makes it possible to post arrays of bytes as keys or values and transfer them safely over http. While it's not human readable without decoding, it shouldn't be considered encryption.The max total size of Transaction tags for transaction posted directly to Arweave is 2048 bytes. This size is determined by the concatenation of all keys and all values of the transaction tags.Transaction tags can be used in GraphQL queries to return a filtered set of transaction items.Common Tags used in the community Tag Name Description Use Cases App-Name Most commonly used to identify applications using Arweave Common uses are the project's name, sometimes also used in specific ANS transactions App-Version The version of this data, it may represent the app consuming this information E.g. 0.3.0 Content-Type MIME Type to identify the data contained in the transaction text/html, application/json, image/png Unix-Time This tag is the a unix timestamp, seconds since epoch The time the transaction is submitted Title ANS-110 Standard for describing content Providing a name for an Atomic Asset Type ANS-110 Standard for categorization of data a type can classify a permaweb asset Examples const tx = await arweave.createTransaction({ data: mydata });
tx.addTag("Content-Type", "text/html");
tx.addTag("Title", "My incredible post about Transaction Tags");
tx.addTag("Description", "This is one post you do not want to miss!");
tx.addTag("Topic:Amazing", "Amazing");
tx.addTag("Type", "blog-post");

await arweave.transactions.sign(tx, jwk);
await arweave.transactions.post(tx);Summary Understanding how Transaction Tags factor into the Arweave tech stack can provide context on how to solve problems using the Permaweb as an application platform. Tags provide a tool to consume and create common data standards and patterns to encourage a non-rivalous data experience on the Permaweb. The result gives users of the ecosystem the choice of applications to consume and create content as their data is always with the user not the application.

---

# 164. Posting Transactions  Cooking with the Permaweb

Document Number: 164
Source: https://cookbook.arweave.net/fundamentals/transactions/post-transactions.html
Words: 635
Extraction Method: html

Posting Transactions There are several ways to post transactions to Arweave. Each has its own unique affordances and constraints. The diagram below illustrates the four main approaches to posting transactions.Direct to Peer,Direct to Gateway, Bundled, and Dispatched. Guaranteed Transactions When posting a large quantity of transactions or when fast settlement time is desireable consider using a bundling service. Bundlers settle large volumes of transactions immediately and make the transaction data available within milliseconds. The bundling service holds onto posted transactions until they are confirmed on-chain. If the transactions are not included in the most recent block the bundling service re-posts them with each new block until they are recorded on chain with a sufficient number of confirmations.Direct Transactions Transactions posted directly to Arweave come in two varieties wallet-to-wallet transactions and data transactions. The first transfers AR tokens between wallet addresses. The second posts data to Arweave and pays the associated storage costs.Interestingly, data transactions may also transfer AR tokens to a wallet address while paying storage costs at the same time.All transactions allow the user to specify up to 2KB worth of metadata in the form of custom tags.Direct to Peer Transactions may be posted directly to an Arweave peer (mining node). This is perhaps the most decentralized means of posting a transaction as clients can choose what peer they wish to post to.This approach is not without drawbacks. Peers may come and go making it difficult to reliably post transactions from an app. While it's possible to query a list of active peers and choose one before posting it adds overhead and friction to the process. Additionally, transactions posted to peers are only queryable at the gateway after being mined in a block. This introduces a 1-2 minute delay between posting the transaction to a peer and it being available to read in a browser from a gateway.For the above reasons, developers tend to configure arweave-js to point to a gateway when posting direct transactions as the optimistic cache at the gateway makes the transaction available almost immediately.Direct to Gateway Gateways sit between clients and Arweave's network of peers. One of the primary functions of the gateway is to index transactions and optimistically cache the data posted to the network while waiting for it to be included in a block. This makes the transaction queryable in a "Pending" state almost instantly which allows applications built on top of a gateway to be more responsive. There is still a risk of transactions dropping out of the optimistic cache if they are not mined in a block by the peers.An example of how to post a direct transaction using arweave-js can be found in this guide.Bundled Transactions Services built on top of Arweave that provide additional utility for Permaweb builders are sometimes called Permaweb Services. A bundler is one such service. Bundlers take multiple individual transactions and bundle them together into a single transaction that is posted directly to Arweave. In this way a single transaction at the protocol level can contain tens of thousands of bundled transactions. There is one restriction, however, only data transactions can be included in a bundle. Wallet-to-wallet transactions (that transfer AR tokens between wallet addresses) must be done as individual transactions posted directly to Arweave.Dispatched Transactions Another way to post bundled transactions is from the browser. While browsers enforce some constraints around the size of data that can be uploaded, browser based wallets are able to post transactions to bundlers. Arweave browser wallets implement a dispatch() API method. If you are posting small transactions (100KB or less) you can use the wallets dispatch() method to take advantage of bundled transactions.An example of how to post a 100KB or less bundled transaction with an Arweave wallets dispatch() method can be found in this guide.Resources arweave-js example dispatch example Turbo SDK example

---

# 165. Cooking with the Permaweb

Document Number: 165
Source: https://cookbook.arweave.net/fundamentals/wallets-and-keyfiles/creating-a-wallet.html
Words: 167
Extraction Method: html

Creating a Wallet Users can create Arweave and AO wallets without requiring any technical knowledge by using wallets like Wander open in new window or Beacon open in new window.These are third-party applications, so as with most of crypto DYOR before choosing a wallet.Generating a wallet programmatically Arweave wallets can also be generated programatically.Creating a wallet with arweave-js npm install arweave arweave.wallets.generate().then((key) => {
    console.log(key);
    // {
    //     "kty": "RSA",
    //     "n": "3WquzP5IVTIsv3XYJjfw5L-t4X34WoWHwOuxb9V8w...",
    //     "e": ...
});Creating a wallet from the command line If you would prefer to create an Arweave wallet through a command-line application, you can use the ArDrive CLI.npm install -g ardrive-cli You can generate a seed phrase with the command generate-seedphrase:# Generate seed-phrase
ardrive generate-seedphrase
"this is an example twelve word seed phrase that you could use" Or, you can generate a wallet file using generate wallet:# Generate a wallet and store it in a chosen output file
ardrive generate-wallet > /path/to/wallet/file.json Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 166. Wallets and Keys  Cooking with the Permaweb

Document Number: 166
Source: https://cookbook.arweave.net/fundamentals/wallets-and-keyfiles/index.html
Words: 515
Extraction Method: html

Wallets and Keys Arweave wallets serve as the gateway to interact with the Arweave blockchain network. They don't physically store tokens but instead manage the cryptographic keys needed to access and control your on-chain assets and data.What is an Arweave wallet?A wallet on Arweave is a cryptographic tool that secures your unique blockchain address. This address tracks your $AR token balance and enables network interactions such as sending transactions or working with AO Processes.It's important to understand that wallets don't actually "hold" tokens. Instead, they store the cryptographic public-private key pair that allows you to sign transactions and manage your on-chain assets. Token balances exist on the blockchain itself, linked to your wallet's address.Key Points Wallets contain the cryptographic keys needed to sign transactions and access funds on the Arweave network Only the wallet owner (with access to the private key) can authorize transactions for their address Arweave uses 4096-bit RSA-PSS key-pairs stored in JWK (JSON Web Keys) format Wallet addresses are derived from the public key using SHA-256 hashing and Base64URL encoding Private keys must be kept secure at all times, as they control access to your funds Keypair and Wallet Format Arweave utilizes 4096-bit RSA-PSS key-pairs stored in the JWK (JSON Web Keys) format. A typical JWK file for an Arweave wallet looks like this (with abbreviated values):{
    "d": "cgeeu66FlfX9wVgZr5AXKlw4MxTlxSuSwMtTR7mqcnoE...",
    "dp": "DezP9yvB13s9edjhYz6Dl...",
    "dq": "SzAT5DbV7eYOZbBkkh20D...",
    "e": "AQAB",
    "ext": true,
    "kty": "RSA",
    "n": "o4FU6y61V1cBLChYgF9O37S4ftUy4newYWLApz4CXlK8...",
    "p": "5ht9nFGnpfW76CPW9IEFlw...",
    "q": "tedJwzjrsrvk7o1-KELQxw...",
    "qi": "zhL9fXSPljaVZ0WYhFGPU..."
} In this JWK file:The n value represents your wallet's public key, which can be safely shared The d value (along with other fields) comprises your wallet's private key, which must be kept confidential These JWK files can be created and exported from wallet applications like Arweave.app or generated programmatically using arweave-js When using certain wallet applications, your private key may also be represented as a mnemonic seed phrase, which can be used to sign transactions or recover your wallet.Wallet Addresses Arweave wallet addresses are derived from the public key through a deterministic process:The SHA-256 hash of the public key is calculated This hash is then Base64URL encoded The result is a 43-character wallet address that's more convenient to use than the full 4096-bit public key This process creates a secure and verifiable link between your wallet address and public key, while providing a more human-readable format for everyday use.Wallet Security Your private key grants complete control over your wallet and funds. Anyone with access to your private key can transfer tokens from your address. As a developer, exercise extreme caution:Never include your keyfile in public GitHub repositories Don't store your private key on unsecured devices or cloud services Back up your private key or seed phrase securely Consider using hardware wallets for significant holdings Available Wallets Several wallet options are available for interacting with the Arweave network:Wander - Browser extension and mobile wallet for Arweave and AO Beacon - Browser extension and mobile wallet for Arweave and AO Arweave.app - Web wallet for deploying permanent data, connecting to dApps, and navigating the weave (Limited AO/HyperBEAM support) Arweave Docs JSON Web Key Format (RFC 7517)

---

# 167. Developing on the Permaweb  Cooking with the Permaweb

Document Number: 167
Source: https://cookbook.arweave.net/getting-started/welcome.html
Words: 203
Extraction Method: html

Developing on the Permaweb Welcome to the Permaweb Creating applications on the Permaweb, which is built on the Arweave protocol, is similar to building traditional web applications but with some key differences.One major difference is that data is stored on the Permaweb permanently, as the name suggests, rather than on a centralized server. This means that once data is uploaded to the Permaweb, it cannot be deleted or altered. This can be beneficial for applications that require tamper-proof data storage, such as supply chain management or voting systems.Another difference is that the Permaweb is decentralized, meaning there is no central point of control or failure. This can provide increased security and reliability for applications.Additionally, the Permaweb uses a unique token, called AR, to pay for the storage of data on the network. This can add a new layer of complexity to application development, as developers need to consider how to integrate AR into their applications and handle payments.Overall, the experience of creating applications on the Permaweb can be challenging, but it can also be rewarding as it offers unique benefits over traditional web development.Hello Worlds Hello World (No Code) Hello World (CLI) Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 168. Hello World (Code)  Cooking with the Permaweb

Document Number: 168
Source: https://cookbook.arweave.net/getting-started/quick-starts/hw-code.html
Words: 330
Extraction Method: html

Hello World (Code) This guide walks you through a quick way to get a static HTML, CSS and JavaScript webpage onto the Permaweb using a few lines of code and a command-line interface (CLI).Requirements NodeJS LTS or greater Basic knowledge of HTML, CSS and JavaScript A text editor (VS Code, Sublime, or similar) Description Using a terminal/console window create a new folder called hello-world.Setup cd hello-world
npm init -y
mkdir src && cd src
touch index.js index.html style.css Next open your text editor and import the hello-world directory.Generate a wallet node -e "require('arweave').init({}).wallets.generate().then(JSON.stringify).then(console.log.bind(console))" > wallet.json The wallet.json file must be in the root of the hello-world folder and not inside of your src folder.Create a webpage This webpage is using basic HTML, CSS and JavaScript to create a styled button that when you click it the header text changes color. Once finished, we will be using permaweb-deploy and our previously generated wallet to deploy a fully functioning, static and permanent webpage to Arweave.Paste the code from the following code blocks into their files:index.html Click to view HTML <!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <script src="index.js"></script>
    <title>Cookbook Hello World!</title>
  </head>

  <body>
    <button onclick="changeColor()" class="button">Click Me!</button>
    <h1 id="main">Hello World!</h1>
  </body>
</html>  style.css Click to view CSS.button {
  padding: "10px";
  background-color: #4caf50;
}  index.js Click to view JS  Now that there is a static site to deploy, it can be checked to ensure it all functions properly by typing open src/index.html in your console/terminal. If everything is working as expected it is time to deploy to Arweave!Upload using permaweb-deploy Install and configure permaweb-deploy for deployment:npm install --save-dev permaweb-deploy Add a deployment script to your package.json:{
  "scripts": {
    "deploy": "permaweb-deploy --arns-name my-hello-world --deploy-folder src"
  }
} Deploy your application:DEPLOY_KEY=$(base64 -i wallet.json) npm run deploy For detailed deployment instructions, see Permaweb Deploy.Congrats!!You just published a static site on Arweave using a few commands and a few lines of code!

---

# 169. Svelte Starter Kits  Cooking with the Permaweb

Document Number: 169
Source: https://cookbook.arweave.net/kits/svelte/index.html
Words: 115
Extraction Method: html

Svelte Starter Kits Svelte is a framework that compiles to a JavaScript bundle and in the process removes the framework from the distribution of the app. This results in a much smaller footprint than other frameworks. Svelte is the perfect framework for Permaweb Applications. A Permaweb Application is built on the principles of a Single Page Application, but lives on the Arweave network and is distributed by Permaweb gateways.Svelte Starter Kit Guides:Minimal - the minimum required to build a svelte permaweb app Vite - Svelte, Typescript and Vite Permaweb Application Constraints 100% Front-end application (No Server-Side Backend) Applications are served from a sub-path (https://[gateway]/[TX_ID]) Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 170. React Starter Kits  Cooking with the Permaweb

Document Number: 170
Source: https://cookbook.arweave.net/kits/react/index.html
Words: 100
Extraction Method: html

React Starter Kits React is a popular library used for building user interfaces. Alongside other popular tools such as create-react-app, a React project can be compiled into a bundle. This bundle can be uploaded as a transaction to the permaweb where it will serve as a single page application.React Starter Kit Guides:Vite - React + Vite, publish with permaweb-deploy Create React App - utilize Create React App to build a React permaweb app Permaweb Application Constraints 100% Front-end application (No Server-Side Backend) Applications are served from a sub-path (https://[gateway]/[TX_ID]) Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 171. Vue Starter Kits  Cooking with the Permaweb

Document Number: 171
Source: https://cookbook.arweave.net/kits/vue/index.html
Words: 117
Extraction Method: html

Vue Starter Kits Vue.js is a progressive JavaScript framework that allows building user interfaces. Unlike other frameworks, it compiles the template into JavaScript during runtime, resulting in a smaller file size and faster performance. Vue is ideal for building performant and scalable single-page applications, making it a popular choice among front-end developers.Vue Starter Kit Guides:Note: - Since npm init vue@latest alredy uses vite, we have not included a vite guide for Vue.Create Vue App - Use Create Vue to efficiently build a Vue.js-based with TypeScript and Vite modern permaweb application Permaweb Application Constraints 100% Front-end application (No Server-Side Backend) Applications are served from a sub-path (https://[gateway]/[TX_ID]) Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 172. SvelteVite Starter Kit  Cooking with the Permaweb

Document Number: 172
Source: https://cookbook.arweave.net/kits/svelte/vite.html
Words: 625
Extraction Method: html

Svelte/Vite Starter Kit Svelte is the framework that compiles out of the way, that results is small packages, which is perfect for the permaweb. As developers, we value Dev Experience as much as we value User Experience. This kit uses the vite bundle system to give developers a great DX experience.Installing vite with svelte and typescript npm create vite@latest my-perma-app --template svelte-ts npm create vite@latest my-perma-app -- --template svelte-ts yarn create vite my-perma-app --template svelte-ts pnpm create vite my-perma-app --template svelte-ts Project Info The vite build system places your index.html file in the root directory, this is where you would include any css or global script dependencies if needed. For more information about the vite project layout check out the vite documentation Setup hash-router To setup the hash-router we will use tinro. tinro is a tiny declarative routing library, that is similar to React Router.npm install --save-dev tinro yarn add -D tinro Telling Svelte to use hash routing In the src/App.svelte file, you want to configure the router to use the hash routing mode.The router.mode.hash function turns on hash router mode. The router.subscribe callback is nice to reset the page to the top on page transfers Adding some transition components These component will manage the transition between one page to another page when routing.Create a directory under the src directory called components and add these two files:announcer.svelte This component is for screen readers announcing when a page changes transition.svelte <script>
  import { router } from "tinro";
  import { fade } from "svelte/transition";
</script>

{#key $router.path}
  <div in:fade={{ duration: 700 }}>
    <slot />
  </div>
{/key} This component adds a fade to the page transition Adding Routes to the app <script lang="ts">
    ...
    import Announcer from "./components/announcer.svelte";
    import Transition from "./components/transition.svelte";
    ...
</script>
<Announcer />
<Transition>
    <Route path="/">
        <Home />
    </Route>
    <Route path="/about">
        <About />
    </Route>
</Transition> Adding the Announcer and Transition components to our routing system will handle announcing page transitions as well as animating the transition.Create some pages home.svelte <script lang="ts">
    let count = 0;

    function inc() {
        count += 1;
    }
</script>
<h1>Hello Permaweb</h1>
<button on:click="{inc}">Inc</button>
<p>Count: {count}</p>
<a href="/about">About</a> about.svelte <h1>About Page</h1>
<p>Svelte/Vite About Page</p>
<a href="/">Home</a> Modify App.svelte <script lang="ts">
    ...
    import Home from './home.svelte'
    import About from './about.svelte'
</script>
...Deploy Permanently Generate Wallet We need the arweave package to generate a wallet npm install --save arweave yarn add arweave -D then run this command in the terminal node -e "require('arweave').init({}).wallets.generate().then(JSON.stringify).then(console.log.bind(console))" > wallet.json Fund Wallet You will need to fund your wallet with ArDrive Turbo credits. To do this, enter ArDrive and import your wallet. Then, you can purchase turbo credits for your wallet.Setup Permaweb-Deploy npm install --global permaweb-deploy yarn global add permaweb-deploy Update vite.config.ts import { defineConfig } from 'vite'
import { svelte } from '@sveltejs/vite-plugin-svelte'

export default defineConfig({
  plugins: [svelte()],
  base: './'
}) Update package.json Update package.json {
  ...
  "scripts": {
    ...
    "deploy": "DEPLOY_KEY=$(base64 -i wallet.json) permaweb-deploy --ant-process << ANT-PROCESS >> --deploy-folder build"
  }
  ...
} Replace << ANT-PROCESS >> with your ANT process id.Run build Now it is time to generate a build, run npm run build yarn build Run deploy Finally we are good to deploy our first Permaweb Application npm run deploy yarn deploy ERROR If you receive an error Insufficient funds, make sure you remembered to fund your deployment wallet with ArDrive Turbo credits.Response You should see a response similar to the following:Deployed TxId [<<tx-id>>] to ANT [<<ant-process>>] using undername [<<undername>>] Your Svelte app can be found at https://arweave.net/<< tx-id >>.SUCCESS You should now have a Svelte Application on the Permaweb! Great Job!Summary This is a minimal version of publishing a Svelte application on the permaweb, but you may want more features, like hot-reloading and tailwind, etc. Check out hypar for a turnkey starter kit. HypAR

---

# 173. arkb  Cooking with the Permaweb

Document Number: 173
Source: https://cookbook.arweave.net/tooling/deployment/arkb.html
Words: 243
Extraction Method: html

arkb Requirements An Arweave wallet is required to deploy using arkb for covering the data transaction costs.Installation To install arkb run npm install -g arkb yarn add ar-gql Deploying When uploading a directory of files or a Permaweb application, by default arkb deploys each file separately as an L1 transaction, with the option to bundle the transactions using Bundlr.Static Build Permaweb applications are statically generated, meaning that the code and content are generated ahead of time and stored on the network.Below is an example of a static site. To deploy this to the Permaweb, the build directory will be passed in as the argument for the deploy flag.|- build
    |- index.html
    |- styles.css
    |- index.js Default Deployment Deploying as an L1 transaction can take longer to confirm as it is directly uploaded to the Arweave network.arkb deploy [folder] --wallet [path to wallet]   Bundled Deployment To deploy using Bundlr you will need to fund a Bundlr node.Bundlr node2 allows free transactions under 100kb.You can add custom identifiable tags to the deployment using tag-name/tag-value syntax.arkb deploy [folder] --use-bundler [bundlr node] --wallet [path to wallet] --tag-name [tag name] --tag-value [tag value]   Other Commands Fund Bundlr arkb fund-bundler [amount] --use-bundler [bundlr node] * Funding a Bundlr instance can take up to 30 minutes to process Saving Keyfile arkb wallet-save [path to wallet] After saving your key you can now run commands without the --wallet-file option, like this arkb deploy [path to directory] Check Wallet Balance arkb balance

---

# 174. Permaweb Deploy  Cooking with the Permaweb

Document Number: 174
Source: https://cookbook.arweave.net/tooling/deployment/permaweb-deploy.html
Words: 707
Extraction Method: html

Permaweb Deploy permaweb-deploy is a Node.js command-line tool that streamlines deployment of web applications and files to the Permaweb using Arweave. It uploads your build folder or single files, creates Arweave manifests, and automatically updates ArNS (Arweave Name Service) records with the new transaction ID.Features Turbo SDK Integration: Fast, reliable file uploads to Arweave Arweave Manifest v0.2.0: Creates manifests with fallback support for SPAs ArNS Updates: Automatically updates ArNS records via ANT with new transaction IDs Automated Workflow: Integrates seamlessly with GitHub Actions Git Hash Tagging: Automatically tags deployments with Git commit hashes 404 Fallback Detection: Automatically detects and sets 404.html as fallback Network Support: Supports mainnet, testnet, and custom ARIO process IDs Flexible Deployment: Deploy folders or single files Installation npm install permaweb-deploy --save-dev For Yarn users:yarn add permaweb-deploy --dev --ignore-engines Prerequisites Wallet Configuration For Arweave signer (default): Encode your Arweave wallet key in base64 format:base64 -i wallet.json | pbcopy Set the encoded wallet as the DEPLOY_KEY environment variable.For Ethereum/Polygon/KYVE signers: Use your raw private key (no encoding needed) as the DEPLOY_KEY.Security Best Practice Use a dedicated wallet for deployments to minimize security risks. Ensure your wallet has sufficient Turbo Credits for uploads.Basic Usage Add deployment scripts to your package.json:{
  "scripts": {
    "build": "vite build",
    "deploy": "npm run build && permaweb-deploy --arns-name my-app"
  }
} Deploy your application:npm run deploy CLI Options Option Alias Description Default --arns-name -n Required. The ArNS name to update - --ario-process -p ARIO process (mainnet, testnet, or process ID) mainnet --deploy-folder -d Folder to deploy./dist --deploy-file -f Deploy a single file instead of a folder - --undername -u ANT undername to update @ --ttl-seconds -t TTL in seconds for the ANT record (60-86400) 3600 --sig-type -s Signer type (arweave, ethereum, polygon, kyve) arweave Examples Deploy Application DEPLOY_KEY=$(base64 -i wallet.json) npx permaweb-deploy --arns-name my-app Deploy Specific Folder DEPLOY_KEY=$(base64 -i wallet.json) npx permaweb-deploy --arns-name my-app --deploy-folder ./build Deploy Single File DEPLOY_KEY=$(base64 -i wallet.json) npx permaweb-deploy --arns-name my-app --deploy-file ./script.lua Deploy to Undername DEPLOY_KEY=$(base64 -i wallet.json) npx permaweb-deploy --arns-name my-app --undername staging Deploy with Custom TTL DEPLOY_KEY=$(base64 -i wallet.json) npx permaweb-deploy --arns-name my-app --ttl-seconds 7200 Deploy with Ethereum Signer DEPLOY_KEY=<ETH_PRIVATE_KEY> npx permaweb-deploy --arns-name my-app --sig-type ethereum Network Configurations Mainnet (Default) {
  "scripts": {
    "deploy": "npm run build && permaweb-deploy --arns-name my-app"
  }
} Testnet {
  "scripts": {
    "deploy:test": "npm run build && permaweb-deploy --arns-name my-app --ario-process testnet"
  }
} Custom Process ID {
  "scripts": {
    "deploy:custom": "npm run build && permaweb-deploy --arns-name my-app --ario-process PROCESS_ID"
  }
} GitHub Actions Integration Create .github/workflows/deploy.yml:name: Deploy to Permaweb
on:
  push:
    branches:
      - main
jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 20
      - run: npm install
      - run: npm run deploy
        env:
          DEPLOY_KEY: ${{ secrets.DEPLOY_KEY }} Deployment Output After successful deployment, you'll see output similar to:-------------------- DEPLOY DETAILS --------------------
Tx ID: abc123def456ghi789jkl012mno345pqr678stu901v
ArNS Name: my-app
Undername: @
ANT: xyz789abc012def345ghi678jkl901mno234pqr567s
AR IO Process: bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM
TTL Seconds: 3600
--------------------------------------------------------
Deployed TxId [abc123def456ghi789jkl012mno345pqr678stu901v] to name [my-app] for ANT [xyz789abc012def345ghi678jkl901mno234pqr567s] using undername [@] Security Best Practices Use dedicated wallets: Create deployment-specific wallets to minimize security risks Secure secret management: Never commit your DEPLOY_KEY to version control Build verification: Always check your build for exposed secrets before deployment Sufficient credits: Ensure your wallet has enough Turbo Credits before deployment Base64 encoding: Arweave wallets must be base64 encoded for the deployment script Troubleshooting "ARNS_NAME not configured" Ensure you're passing the --arns-name flag with a valid ArNS name "DEPLOY_KEY not configured" Verify your base64 encoded wallet is set as the DEPLOY_KEY environment variable "deploy-folder does not exist" Check that your build folder exists and the path is correct Run your build command first "ARNS name does not exist" Verify the ArNS name is correct and exists in the specified network "Upload timeouts" Files have a 10-second upload timeout Large files may fail and require optimization "Insufficient Turbo Credits" Check your wallet balance and add more credits if needed Debug Information Enable verbose logging by setting the DEBUG environment variable:DEBUG=permaweb-deploy* npm run deploy Dependencies @ar.io/sdk: ANT operations and ArNS management @ardrive/turbo-sdk: Fast file uploads to Arweave @permaweb/aoconnect: AO network connectivity yargs: CLI argument parsing ArNS Setup: ArNS Names Turbo Credits: Turbo SDK GitHub Actions: CI/CD Integration Resources GitHub Repository: permaweb/permaweb-deploy Turbo SDK Documentation: docs.ardrive.io/turbo ArNS Documentation: ar.io/arns Arweave Ecosystem: arweave.org

---

# 175. ANS-104 Bundled Data v20 - Binary Serialization  Cooking with the Permaweb

Document Number: 175
Source: https://cookbook.arweave.net/tooling/specs/ans/ANS-104.html
Words: 1589
Extraction Method: html

ANS-104: Bundled Data v2.0 - Binary Serialization Status: Standard Abstract This document describes the data format and directions for reading and writing bundled binary data. Bundled data is a way of writing multiple independent data transactions (referred to as DataItems in this document) into one top level transaction. A DataItem shares many of the same properties as a normal data transaction, in that it has an owner, data, tags, target, signature, and id. It differs in that is has no ability to transfer tokens, and no reward, as the top level transaction pays the reward for all bundled data.Motivation Bundling multiple data transactions into one transaction provides a number of benefits:Allow delegation of payment for a DataItem to a 3rd party, while maintaining the identity and signature of the person who created the DataItem, without them needing to have a wallet with funds Allow multiple DataItems to be written as a group Increase the throughput of logically independent data-writes to the Arweave network Reference Implementation There is a reference implementation for the creation, signing, and verification of DataItems and working with bundles in TypeScript Specification 1. Transaction Format 1.1 Transaction Tags A bundle of DataItems MUST have the following two tags present:Bundle-Format a string describing the bundling format. The format for this standard is binary Bundle-Version a version string. The version referred to in this standard is 2.0.0 Version changes may occur due to a change in encoding algorithm in the future 1.2 Transaction Body Format This format for the transaction body is binary data in the following bytes format N = number of DataItems Bytes Purpose 32 Numbers of data items N x 64 Pairs of size and entry ids [size (32 bytes), entry ID (32 bytes)] Remaining bytes Binary encoded data items in bundle 1.3 DataItem Format A DataItem is a binary encoded object that has similar properties to a transaction Field Description Encoding Length (in bytes) Optional signature type Type of key format used for the signature Binary 2 ❌ signature A signature produced by owner Binary Depends on signature type ❌ owner The public key of the owner Binary 512 ❌ target An address that this DataItem is being sent to Binary 32 (+ presence byte) ✔️ anchor A value to prevent replay attacks Binary 32 (+ presence byte) ✔️ number of tags Number of tags Binary 8 ❌ number of tag bytes Number of bytes used for tags Binary 8 ❌ tags An avro array of tag objects Binary Variable ❌ data The data contents Binary Variable ❌ All optional fields will have a leading byte which describes whether the field is present (1 for present, 0 for not present). Any other value for this byte makes the DataItem invalid.A tag object is an Apache Avro encoded stream representing an object { name: string, value: string }. Prefixing the tags objects with their bytes length means decoders may skip them if they wish.The anchor and target fields in DataItem are optional. The anchor is an arbitrary value to allow bundling gateways to provide protection from replay attacks against them or their users.1.3.1 Tag format Parsing the tags is optional, as they are prefixed by their bytes length.To conform with deployed bundles, the tag format is Apache Avro with the following schema:{
  "type": "array",
  "items": {
    "type": "record",
    "name": "Tag",
    "fields": [
      { "name": "name", "type": "bytes" },
      { "name": "value", "type": "bytes" }
    ]
  }
} Usually the name and value fields are UTF-8 encoded strings, in which case "string" may be specified as the field type rather than "bytes", and avro will automatically decode them.To encode field and list sizes, avro uses a long datatype that is first zig-zag encoded, and then variable-length integer encoded, using existing encoding specifications. When encoding arrays, avro provides for a streaming approach that separates the content into blocks.1.3.1.1 ZigZag coding ZigZag is an integer format where the sign bit is in the 1s place, such that small negative numbers have no high bits set. In surrounding code, normal integers are almost always stored in a twos-complement manner instead, which can be converted as below.Converting to ZigZag:zigzag = twos_complement << 1;
if (zigzag < 0) zigzag = ~zigzag;Converting from ZigZag:if (zigzag & 1) zigzag = ~zigzag;
twos_complement = zigzag >> 1;1.3.1.2 Variable-length integer coding Variable-length integer is a 7-bit little-endian integer format, where the 8th bit of each byte indicates whether another byte (of 7 bits greater significance) follows in the stream.Converting to VInt:// writes 'zigzag' to 'vint' buffer
offset = 0;
do {
  vint_byte = zigzag & 0x7f;
  zigzag >>= 7;
  if (zigzag)
    vint_byte |= 0x80;
  vint.writeUInt8(vint_byte, offset);
  offset += 1;
} while(zigzag);Converting from VInt:// constructs 'zigzag' from 'vint' buffer
zigzag = 0;
offset = 0;
do {
  vint_byte = vint.readUInt8(offset);
  zigzag |= (vint_byte & 0x7f) << (offset*7);
  vint_byte &= 0x80;
  offset += 1;
} while(vint_byte);1.3.1.3 Avro tag array format Avro arrays may arrive split into more than one sequence of items. Each sequence is prefixed by its length, which may be negative, in which case a byte length is inserted between the length and the sequence content. This is used in schemas of larger data to provide for seeking. The end of the array is indicated by a sequence of length zero.The complete tags format is a single avro array, consisting solely of blocks of the below format. The sequence is terminated by a block with a count of 0. The size field is only present if the count is negative, in which case its absolute value should be used.Field Description Encoding Length Optional count Number of items in block ZigZag VInt Variable ❌ size Number of bytes if count<0 ZigZag VInt Variable ✔️ block Concatenated tag items Binary size ❌ 1.3.1.4 Avro tag item format Each item of the avro array is a pair of avro strings or bytes objects, a name and a value, each prefixed by their length.Field Description Encoding Length Optional name_size Number of bytes in name ZigZag VInt Variable ❌ name Name of the tag Binary name_size ❌ value_size Number of bytes in value ZigZag VInt Variable ❌ value Value of the tag Binary value_size ❌ 2. DataItem signature and id The signature and id for a DataItem is built in a manner similar to Arweave 2.0 transaction signing. It uses the Arweave 2.0 deep-hash algorithm. The 2.0 deep-hash algorithm operates on arbitrarily nested arrays of binary data, i.e a recursive type of DeepHashChunk = Uint8Array | DeepHashChunk[].There are reference implementations for the deep-hash algorithm in TypeScript and Erlang To generate a valid signature for a DataItem, the contents of the DataItem and static version tags are passed to the deep-hash algorithm to obtain a message. This message is signed by the owner of the DataItem to produce the signature. The id of the DataItem, is the SHA256 digest of this signature.The exact structure and content passed into the deep-hash algorithm to obtain the message to sign is as follows:[
  utf8Encoded("dataitem"),
  utf8Encoded("1"),
  owner,
  target,
  anchor,
  [
    ... [ tag.name, tag.value ],
    ... [ tag.name, tag.value ],
    ...
  ],
  data
] 2.1 Verifying a DataItem DataItem verification is a key to maintaining consistency within the bundle standard. A DataItem is valid iff.1:id matches the signature (via SHA-256 of the signature) signature matches the owner's public key tags are all valid an anchor isn't more than 32 bytes A tag object is valid iff.:there are <= 128 tags each key is <= 1024 bytes each value is <= 3072 bytes only contains a key and value both the key and value are non-empty strings 3. Writing a Bundle of DataItems To write a bundle of DataItems, each DataItem should be constructed, signed, encoded, and placed in a transaction with the transaction body format and transaction tags specified in Section 1.3.1 Nested bundle Arweave Transactions and DataItems have analogous specifications for tagging and bearing of a binary payload. As such, the ANS-104 Bundle Transaction tagging and binary data format specification can be applied to the tags and binary data payload of a DataItem. Assembling a DataItem this way provides for the nesting of ANS-104 Bundles with one-to-many relationships between "parent" and "child" bundles and theoretically unbounded levels of nesting. Additionally, nested DataItem Bundles can be mixed heterogeneously with non-Bundle DataItems at any depth in the Bundle tree.To construct an ANS-104 DataItem as a nested Bundle:Add tags to the DataItem as described by the specification in section 1.1 Provide a binary payload for the DataItem matching the Bundle Transaction Body Format described in section 1.2, i.e. the Bundle header outlining the count, size, and IDs of the subsequent, nested DataItems, each of which should be verifiable using the method described in section 2.1.Gateway GQL queries for DataItem headers should, upon request, contain a bundledIn field whose value indicates the parent-child relationship of the DataItem to its immediate parent. Any nested bundle should be traceable to a base layer Arweave Transaction by recursively following the bundledIn field up through the chain of parents.4. Reading a Bundle of DataItems To read a bundle of DataItems, the list of bytes representing the DataItems can be partitioned using the offsets in each pair. Subsequently, each partition can be parsed to a DataItem object (struct in languages such as Rust/Go etc. or JSON in TypeScript).This allows for querying of a singleton or a bundle as a whole.4.1 Indexing DataItems This format allows for indexing of specific fields in O(N) time. Some form of caching or indexing could be performed by gateways to improve lookup times.1 - if and only if

---

# 176. ANS-102 Bundled Data - JSON Serialization  Cooking with the Permaweb

Document Number: 176
Source: https://cookbook.arweave.net/tooling/specs/ans/ANS-102.html
Words: 708
Extraction Method: html

ANS-102: Bundled Data - JSON Serialization Status: Standard Abstract This document describes the data format and directions for reading and writing bundled data. Bundled data is a way of writing multiple logical data transactions (referred to as DataItems in this document) into one top level transaction. A DataItem shares many of the same properties as normal data transaction, in that it has an owner, data, tags, target, signature, and id. It differs in that it has no ability to transfer tokens, and no reward, as the top level transaction pays the reward for all bundled data.Motivation Bundling multiple logical data transactions into one transaction provides a number of benefits:To allow delegation of payment for a DataItem to a 3rd party, while maintaining the identity and signature of person who created the DataItem, without them needing to have a wallet with funds.Allow multiple DataItems to be written as a group.To increase the throughput of logically independent data writes to the Arweave network Reference Implementation There is a reference implementation for the creation, signing, and verification of DataItems and working with bundles in TypeScript Specification 1. Transaction Format 1.1 Transaction Tags A bundle of DataItems MUST have the following two tags present Bundle-Format a string describing the bundling format. The format for this standard is json Bundle-Version a version string. The version referred to in this standard is 1.0.0 1.2 Transaction Body Format This format for the transaction body is a JSON object in the following format {
  items: [
    { DataItem },
    { DataItem }
  ]
} 1.3 DataItem Format A DataItem is a JSON object that has similar properties to a transaction:B64U Encoding indicates the field is Base64Url encoded binary.All properties MUST be present, for optional values the value in 'Empty Value' MUST be used.Field Description Encoding Empty Value owner The public key of the owner B64U  target An address that this DataItem is being sent to B64U Empty String nonce A value to prevent replay attacks B64U Empty String tags An array of tag objects Json Array Empty Json Array data The data contents B64U  signature A signature produced by owner B64U  id The id the item B64U  A tag object is a JSON object with the following two keys. A tag object MUST NOT have any other keys.Field Description Encoding Empty Value name Name of the tag B64U  value Value of the tag B64U  The fields in the DataItem and Tags objects can be handled in an identical way as their counterpart in a regular Arweave transaction.The nonce field in DataItem is optional, and is an arbitrary value to allow bundling gateways to provide protection from replay attacks against them or their users.2. DataItem signature and id The signature, and id for a DataItem is built in a manner similar to Arweave 2.0 transaction signing. It uses the Arweave 2.0 deep-hash algorithm. The 2.0 deep-hash algorithm operates on arbitrarily nested arrays of binary data, i.e a recursive type of DeepHashChunk = Uint8Array | DeepHashChunk[].There is reference implementations for the deep-hash algorithm in TypeScript and Erlang To generate a valid signature for a DataItem, the contents of the DataItem and static version tags, are passed to the deep-hash algorithm to obtain a message. This message is signed by the owner of the DataItem to produce the signature. The id of the DataItem, is the SHA256 digest of this signature.The exact structure and content passed into the deep-hash algorithm to obtain the message to sign is as follows:[
  utf8Encoded("dataitem"),
  utf8Encoded("1"),
  owner,
  target,
  nonce,
  [
    ... [ tag.name, tag.value ],
    ... [ tag.name, tag.value ],
    ...
  ],
  data
] 3. Expanding a bundle of DataItems To read and expand a bundle of DataItems, each DataItem in the items should be verified using the verification algorithm. Individual items that fail verification MUST be discarded.In rare cases, an identical DataItem may exist in more that one transaction. That is, the contents and id of the DataItem are identical but exist in multiple Arweave transactions. Since they are identical, any of the copies can be discarded.4. Writing a bundle of DataItems To write a bundle of DataItems, each DataItem should constructed and signed, and placed in a transaction with the transaction body format and transaction tags specified in Section 1. Transaction Format.

---

# 177. ANS-103 Succinct Proofs of Random Access  Cooking with the Permaweb

Document Number: 177
Source: https://cookbook.arweave.net/tooling/specs/ans/ANS-103.html
Words: 1126
Extraction Method: html

ANS-103: Succinct Proofs of Random Access Status: Draft Authors: Sam Williams sam@arweave.org, Lev Berman ldmberman@protonmail.com Abstract This document describes the new consensus mechanism for the Arweave network based on the competition to find a chunk of the past data in a set of historical chunks inferred from the latest agreed-upon blockweave state.Motivation At the time when this specification is written, the consensus mechanism employed by the Arweave network is a classical Proof of Work with an additional requirement of including a chunk (up to 256 KiB) of the past data into the hash preimage where the chunk is deterministically determined by the latest state of the blockweave.While this approach incentivizes the network to keep historical data, it does not impose any significant restrictions on the speed of access to data a miner needs to be competitive. Specifically, miners can benefit from using a remote storage pool. Moreover, combined with a computation pool, it can serve Proof of Work preimages to millions of clients per second via a Gbit Internet link. A storage and computation pool has been evidenced in the Arweave network by a decreased number of public nodes and a simultaneous increase of the network hashpower and people claiming to be part of the pool.Therefore, the new consensus algorithm's first goal is to make the mining advantage grow sharply with the growing speed of access to data, promoting replication more aggressively.The second but not less important goal is to reduce the energy consumed by the network. Proof of Work networks are known for their energy-intensity. Reduced energy consumption reduces the carbon footprint, widely believed to be very harmful to the planet. We believe the environment-friendliness of the consensus mechanism is crucial for the long-term sustainability of the Arweave platform.To sum up, the consensus algorithm described here aims at two major goals.Disincentivize miners from retrieving data on demand from the network. In other words, incentivize miners to store data as close to the mining machine as possible.Reduce network's energy consumption.Reference Implementation Link.Specification Prerequsites Indexed Dataset The core of the new mechanism - continuous retrieval of chunks of the past data. Every chunk is identified by a global offset, as we want to make the possession of any byte equally incentivized. Therefore, the whole weave has to be indexed so that every chunk is quickly accessible by its global offset.Starting from the release 2.1, the Arweave Erlang client maintains such an index.Slow Hash The consensus mechanism needs a deterministic but unpredictable way to choose candidate chunks, to make the miners continuously access storage. However, choosing candidate chunks cannot be too easy as it would allow miners to reduce the number of chunks they ever work with. There are two threats associated with that. One threat is the lower cost of the computation expenditure, as compared to the cost of storing extra data required for the same probability of a reward. The second threat is the existing computation technology that, although being more expensive than storing the entire weave, is so efficient that it outperforms even the most efficient data retrieval based client.Starting from release 1.7, the Arweave protocol relies on RandomX, a proof-of-work algorithm optimized for general-purpose CPUs.Algorithm Description Every block uniquely determines a Search Space - a set of offsets (Recall Bytes) on the [0, weave size at a certain block (Search Space Upper Bound)] interval.Miner Steps Generate a random nonce and generate a slow hash (H0) of the hash of a Merkle tree containing the current state, the candidate block, and the nonce.Compute the unique recall byte from H0, the hash of the previous block (PrevH), and Search Space Upper Bound.Search the local storage for the chunk containing the computed Recall Byte. If not found, repeat from the first step.Compute a fast hash of the hash of the Merkle tree containing the slow hash computed at the first step and the located chunk.Check whether the computed hash is bigger than the current mining difficulty (the number computed from its binary digit big-endian representation is bigger). If not, repeat from the first step. If yes, pack and distribute the block (the block contains the nonce and the chunk).The solution chunk together with the Merkle proofs of its inclusion in the blockweave we call Succinct Proof of Random Access (or SPoRA) and use as a name for the new consensus mechanism.Verifier Steps Perform one iteration over the miner steps where the nonce and the chunk are found in the verified block.Rationale Search Space Constraints Search Space needs to be big enough for two reasons:make it prohibitively expensive to download the entire search space on demand; note that the prior consensus mechanism can be viewed as an edge case of SPoRA where the search space consists of a single chunk; the efficiency of serving data to the computing agents on demand depends on the network bandwidth, which grows over time.make it prohibitively expensive to compensate for the lack of data by hashing.On the other hand, Search Space needs to be small enough to incentivize miners to replicate the rarer parts of the weave, which would give them and advantage over miners who replicated less of the corresponding area at the corresponding blocks.We choose the Search Space size to be 10% of the weave. In this case, 10% of the miners storing unique 10% of the weave in the network where everybody stores 90% of the weave are roughly 1.2 times more efficient than the rest of the miners. It holds for various ratios of the time it takes to compute a RandomX hash and the time it takes to look up a chunk.Pseudocode The ar_deep_hash definition.mine(Nonce, BlockPreimage, PrevH, SearchSpaceUpperBound):
    // Compute a slow hash.
    H0 := randomx_hash(concat(Nonce, BlockPreimage))
    RecallByte := pick_recall_byte(H0, PrevH, SearchSpaceUpperBound)
    // Search the local storage for the chunk containing Recall Byte.
    SPoA := get_spoa_by_byte(RecallByte)
    if SPoA == not_found
        mine(random_nonce(), BlockPreimage, PrevH, SearchSpaceUpperBound)
    Chunk := get_chunk(SPoA)
    SolutionHash := randomx_hash(concat(H0, PrevH, Timestamp, Chunk))
    if validate(Candidate, Diff)
        return
    mine(random_nonce(), BlockPreimage, PrevH, SearchSpaceUpperBound)

pick_recall_byte(H0, PrevH, SearchSpaceUpperBound):
    SubspaceNumber := remainder(hash_to_number(H0), SUBSPACES_COUNT)
    SearchSpaceSize := integer_division(SearchSpaceUpperBound, 10)
    EvenSubspaceSize := integer_division(SearchSpaceUpperBound, SUBSPACES_COUNT)
    SearchSubspaceSize := integer_division(SearchSpaceSize, SUBSPACES_COUNT)
    SubspaceStart := SubspaceNumber * EvenSubspaceSize
    SubspaceSize := min(SearchSpaceUpperBound - SubspaceStart, EvenSubspaceSize)
    EncodedSubspaceNumber := number_to_binary(SubspaceNumber)
    SearchSubspaceSeed := hash_to_number(sha256(concat(PrevH, EncodedSubspaceNumber)))
    SearchSubspaceStart := remainder(SearchSubspaceSeed, SubspaceSize)
    SearchSubspaceByteSeed := hash_to_number(sha256(H))
    SearchSubspaceByte := remainder(SearchSubspaceByteSeed, SearchSubspaceSize)
    return AbsoluteSubspaceStart + remainder(SearchSubspaceStart + SearchSubspaceByte, SubspaceSize) The work was heavily inspired by Permacoin: Repurposing Bitcoin Work for Data Preservation by Andrew Miller, Ari Juels, Elaine Shi, Bryan Parno, and Jonathan Katz from Microsoft Research.We suggest to use the existing, growing, and always accessible Arweave dataset instead of one pre-generated by a trusted dealer. We use a slow specialized hardware-resistant hash to make it prohibitively expensive to compensate for the lack of local data with computation. Finally, we provide the network with the incentive to replicate data uniformly.

---

# 178. ArFS Protocol A Decentralized File System on Arweave  Cooking with the Permaweb

Document Number: 178
Source: https://cookbook.arweave.net/tooling/specs/arfs/arfs.html
Words: 404
Extraction Method: html

ArFS Protocol: A Decentralized File System on Arweave Arweave File System, or “ArFS” is a data modeling, storage, and retrieval protocol designed to emulate common file system operations and to provide aspects of mutability to your data hierarchy on Arweave 's otherwise permanent, immutable data storage blockweave.Due to Arweave's permanent, immutable and public nature traditional file system operations such as permissions, file/folder renaming and moving, and file updates cannot be done by simply updating the on-chain data model.ArFS works around this by implementing a privacy and encryption pattern and defining an append-only transaction data model using tags within Arweave Transaction headers.Key Features File Structure ArFS organizes files and folders using a hierarchical structure. Files are stored as individual transactions on the Arweave blockchain, while folders are metadata that reference these file transactions.Each file and folder has associated metadata, such as the name, type, size, and modification timestamp. ArFS leverages Arweave's tagging system to store this metadata in a standardized format, which allows for easy querying and organization.File Permissions ArFS supports public and private file permissions. Public files can be accessed by anyone on the network, while private files are encrypted using the owner's private key, ensuring only they can decrypt and access the content.File Versioning ArFS supports versioning of files, allowing users to store multiple versions of a file and access previous versions at any time. This is achieved by linking new file transactions to previous versions through the use of metadata tags.Data Deduplication To minimize storage redundancy and costs, ArFS employs data deduplication techniques. If a user tries to store a file that already exists on the network, the protocol will simply create a new reference to the existing file instead of storing a duplicate copy.Search and Discovery ArFS enables users to search and discover files based on their metadata, such as file names, types, and tags. This is made possible by indexing the metadata stored within the Arweave blockchain.Interoperability ArFS is designed to be interoperable with other decentralized applications and services built on the Arweave network. This allows for seamless integration and collaboration between different applications and users.Getting Started To start using ArFS, you'll need to familiarize yourself with the Arweave ecosystem, acquire AR tokens to cover storage costs, and choose a compatible client or library to interact with the ArFS protocol.Resources For more information, documentation, and community support, refer to the following resources:Arweave Official Website Arweave Developer Documentation Arweave Community Forums

---

# 179. Privacy  Cooking with the Permaweb

Document Number: 179
Source: https://cookbook.arweave.net/tooling/specs/arfs/privacy.html
Words: 700
Extraction Method: html

Privacy The Arweave blockweave is inherently public. But with apps that use ArFS, like ArDrive, your private data never leaves your computer without using military grade (and quantum resistant) encryption. This privacy layer is applied at the Drive level, and users determine whether a Drive is public or private when they first create it. Private drives must follow the ArFS privacy model.Every file within a Private Drive is symmetrically encrypted using AES-256-GCM. Every Private drive has a master "Drive Key" which uses a combination of the user's Arweave wallet signature, a user defined drive password, and a unique drive identifier (uuidv4). Each file has its own "File Key" derived from the "Drive Key". This allows for single files to be shared without exposing access to the other files within the Drive.Once a file is encrypted and stored on Arweave, it is locked forever and can only be decrypted using its file key.Deriving Keys Private drives have a global drive key, D, and multiple file keys F, for encryption. This enables a drive to have as many uniquely encrypted files as needed. One key is used for all versions of a single file (since new file versions use the same File-Id) D is used for encrypting both Drive and Folder metadata, while F is used for encrypting File metadata and the actual stored data. Having these different keys, D and F, allows a user to share specific files without revealing the contents of their entire drive.D is derived using HKDF-SHA256 with an unsalted RSA-PSS signature of the drive's id and a user provided password.F is also derived using HKDF-SHA256 with the drive key and the file's id. Other wallets (like Wander) integrate with this Key Derivation protocol just exposing an API to collect a signature from a given Arweave Wallet in order to get the SHA-256 signature needed for the HKDF to derive the Drive Key.An example implementation, using Dart, is available here, with a Typescript implementation here.Private Drives Drives can store either public or private data. This is indicated by the Drive-Privacy tag in the Drive entity metadata.If a Drive entity is private, an additional tag Drive-Auth-Mode must also be used to indicate how the Drive Key is derived. ArDrive clients currently leverage a secure password along with the Arweave Wallet private key signature to derive the global Drive Key.Drive-Auth-Mode?: 'password' On every encrypted Drive Entity, a Cipher tag must be specified, along with the public parameters for decrypting the data. This is done by specifying the parameter with a Cipher-* tag. eg. Cipher-IV. If the parameter is byte data, it must be encoded as Base64 in the tag.ArDrive clients currently leverage AES256-GCM for all symmetric encryption, which requires a Cipher Initialization Vector consisting of 12 random bytes.Cipher?: "AES256-GCM"
Cipher-IV?: "<12 byte initialization vector as Base64>" Additionally, all encrypted transactions must have the Content-Type tag application/octet-stream as opposed to application/json Private Drive Entities and their corresponding Root Folder Entities will both use these keys and ciphers generated to symmetrically encrypt the JSON files that are included in the transaction. This ensures that only the Drive Owner (and whomever the keys have been shared with) can open the drive, discover the root folder, and continue to load the rest of the children in the drive.Private Files When a file is uploaded to a private drive, it by default also becomes private and leverages the same drive keys used for its parent drive. Each unique file in a drive will get its own set of file keys based off of that file's unique FileId. If a single file gets a new version, its File-Id will be reused, effectively leveraging the same File Key for all versions in that file's history.These file keys can be shared by the drive's owner as needed.Private File entities have both its metadata and data transactions encrypted using the same File Key, ensuring all facets of the data is truly private. As such, both the file's metadata and data transactions must both have a unique Cipher-IV and Cipher tag:Cipher?: "AES256-GCM"
Cipher-IV?: "<12 byte initialization vector as Base64>" Just like drives, private files must have the Content-Type tag set as application/octet-stream in both its metadata and data transactions:Content-Type: "application/octet-stream"

---

# 180. Content Types  Cooking with the Permaweb

Document Number: 180
Source: https://cookbook.arweave.net/tooling/specs/arfs/content-types.html
Words: 197
Extraction Method: html

Content Types All transaction types in ArFS leverage a specific metadata tag for the Content-Type (also known as mime-type) of the data that is included in the transaction. ArFS clients must determine what the mime-type of the data is, in order for Arweave gateways and browswers to render this content appropriately.All public drive, folder, and file (metadata only) entity transactions all use a JSON standard, therefore they must have the following content type tag:Content-Type: '<application/json>' However, a file's data transaction must have its mime-type determined. This is stored in the file's corresponding metadata transaction JSON's dataContentType as well as the content type tag in the data transaction itself.Content-Type: "<file's mime-type>" All private drive, folder, and file entity transactions must have the following content type, since they are encrypted:Content-Type: '<application/octet-stream>' ArDrive-Core open in new window includes methods to determine a file's content type.Other Tags ArFS enabled clients should include the following tags on their transactions to identify their application App-Name: "<defined application name eg. ArDrive"
App-Version: "<defined version of the app eg. 0.5.0"
Client?: "<if the application has multiple clients, they should be specified here eg. Web" Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 181. Data Model  Cooking with the Permaweb

Document Number: 181
Source: https://cookbook.arweave.net/tooling/specs/arfs/data-model.html
Words: 344
Extraction Method: html

Data Model Because of Arweave's permanent and immutable nature, traditional file structure operations such as renaming and moving files or folders cannot be accomplished by simply updating on-chain data. ArFS works around this by defining an append-only transaction data model based on the metadata tags found in the Arweave Transaction Headers.This model uses a bottom-up reference method, which avoids race conditions in file system updates. Each file contains metadata that refers to the parent folder, and each folder contains metadata that refers to its parent drive. A top-down data model would require the parent model (i.e. a folder) to store references to its children.These defined entities allow the state of the drive to be constructed by a client to look and feel like a file system Drive Entities contain folders and files Folder Entities contain other folders or files File Entities contain both the file data and metadata Snapshot entities contain a state rollups of all files and folder metadata within a drive Entity relationships The following diagram shows the high level relationships between drive, folder, and file entities, and their associated data. More detailed information about each Entity Type can be found here. Entity Relationship Diagram As you can see, each file and folder contains metadata which points to both the parent folder and the parent drive. The drive entity contains metadata about itself, but not the child contents. So clients must build drive states from the lowest level and work their way up.Metadata stored in any Arweave transaction tag will be defined in the following manner:{ "name": "Example-Tag", "value": "example-data" } Metadata stored in the Transaction Data Payload will follow JSON formatting like below:{
    "exampleField": "exampleData"
} fields with a ? suffix are optional.{
  "name": "My Project",
  "description": "This is a sample project.",
  "version?": "1.0.0",
  "author?": "John Doe"
} Enumerated field values (those which must adhere to certain values) are defined in the format "value 1 | value 2".All UUIDs used for Entity-Ids are based on the Universally Unique Identifier standard.There are no requirements to list ArFS tags in any specific order.

---

# 182. Entity Types  Cooking with the Permaweb

Document Number: 182
Source: https://cookbook.arweave.net/tooling/specs/arfs/entity-types.html
Words: 1325
Extraction Method: html

Entity Types Overview Arweave transactions are composed of transaction headers and data payloads.ArFS entities, therefore, have their data split between being stored as tags on their transaction header and encoded as JSON and stored as the data of a transaction. In the case of private entities, JSON data and file data payloads are always encrypted according to the protocol processes defined below.Drive entities require a single metadata transaction, with standard Drive tags and encoded JSON with secondary metadata.Folder entities require a single metadata transaction, with standard Folder tags and an encoded JSON with secondary metadata.File entities require a metadata transaction, with standard File tags and an encoded Data JSON with secondary metadata relating to the file.File entities also require a second data transaction, which includes a limited set of File tags and the actual file data itself.Snapshot entities require a single transaction. which contains a Data JSON with all of the Drive’s rolled up ArFS metadata and standard Snapshot GQL tags that identify the Snapshot.Drive A drive is the highest level logical grouping of folders and files. All folders and files must be part of a drive, and reference the Drive ID of that drive.When creating a Drive, a corresponding folder must be created as well. This will act as the root folder of the drive. This separation of drive and folder entity enables features such as folder view queries, renaming, and linking.Drive Entity Transaction Example Folder A folder is a logical grouping of other folders and files. Folder entity metadata transactions without a parent folder id are considered the Drive Root Folder of their corresponding Drives. All other Folder entities must have a parent folder id. Since folders do not have underlying data, there is no Folder data transaction required.ArFS: "0.13"
Cipher?: "AES256-GCM"
Cipher-IV?: "<12 byte initialization vector as Base64>"
Content-Type: "<application/json | application/octet-stream>"
Drive-Id: "<drive uuid>"
Entity-Type: "folder"
Folder-Id: "<uuid>"
Parent-Folder-Id?: "<parent folder uuid>"
Unix-Time: "<seconds since unix epoch>"

Data JSON {
    "name": "<user defined folder name>"
} Folder Entity Transaction Example File A File contains uploaded data, like a photo, document, or movie.In the Arweave File System, a single file is broken into 2 parts - its metadata and its data.In the Arweave File System, a single file is broken into 2 parts - its metadata and its data.A File entity metadata transaction does not include the actual File data. Instead, the File data must be uploaded as a separate transaction, called the File Data Transaction. The File JSON metadata transaction contains a reference to the File Data Transaction ID so that it can retrieve the actual data. This separation allows for file metadata to be updated without requiring the file itself to be reuploaded. It also ensures that private files can have their JSON Metadata Transaction encrypted as well, ensuring that no one without authorization can see either the file or its metadata.ArFS: "0.13"
Cipher?: "AES256-GCM"
Cipher-IV?: "<12 byte initialization vector as Base64>"
Content-Type: "<application/json | application/octet-stream>"
Drive-Id: "<drive uuid>"
Entity-Type: "file"
File-Id: "<uuid>"
Parent-Folder-Id: "<parent folder uuid>"
Unix-Time: "<seconds since unix epoch>"

Data JSON {
    "name": "<user defined file name with extension eg. happyBirthday.jpg>",
    "size": "<computed file size - int>",
    "lastModifiedDate": "<timestamp for OS reported time of file's last modified date represented as milliseconds since unix epoch - int>",
    "dataTxId": "<transaction id of stored data>",
    "dataContentType": "<the mime type of the data associated with this file entity>",
    "pinnedDataOwner": "<the address of the original owner of the data where the file is pointing to>" # Optional
}  Pin Files  Since the version v0.13, ArFS suports Pins. Pins are files whose data may be any transaction uploaded to Arweave, that may or may not be owned by the wallet that created the pin.When a new File Pin is created, the only created transaction is the Metadata Transaction. The dataTxId field will point it to any transaction in Arweave, and the optional pinnedDataOwner field is gonna hold the address of the wallet that owns the original copy of the data transaction.File Data Transaction Example The File Data Transaction contains limited information about the file, such as the information required to decrypt it, or the Content-Type (mime-type) needed to view in the browser.Cipher?: "AES256-GCM"
Cipher-IV?: "<12 byte initialization vector as Base64>"
Content-Type: "<file mime-type | application/octet-stream>"
 { File Data - Encrypted if private } File Metadata Transaction Example The the File Metadata Transaction contains the GQL Tags necessary to identify the file within a drive and folder.Its data contains the JSON metadata for the file. This includes the file name, size, last modified date, data transaction id, and data content type.ArFS: "0.13"
Cipher?: "AES256-GCM"
Cipher-IV?: "<12 byte initialization vector as Base64>"
Content-Type: "<application/json | application/octet-stream>"
Drive-Id: "<drive uuid>"
Entity-Type: "file"
File-Id: "<uuid>"
Parent-Folder-Id: "<parent folder uuid>"
Unix-Time: "<seconds since unix epoch>"
 { File JSON Metadata - Encrypted if private } Snapshot ArFS applications generate the latest state of a drive by querying for all ArFS transactions made relating to a user's particular Drive-Id. This includes both paged queries for indexed ArFS data via GQL, as well as the ArFS JSON metadata entries for each ArFS transaction.For small drives (less than 1000 files), a few thousand requests for very small volumes of data can be achieved relatively quickly and reliably. For larger drives, however, this results in long sync times to pull every piece of ArFS metadata when the local database cache is empty. This can also potentially trigger rate-limiting related ArWeave Gateway delays.Once a drive state has been completely, and accurately generated, in can be rolled up into a single snapshot and uploaded as an Arweave transaction. ArFS clients can use GQL to find and retrieve this snapshot in order to rapidly reconstitute the total state of the drive, or a large portion of it. They can then query individual transactions performed after the snapshot.This optional method offers convenience and resource efficiency when building the drive state, at the cost of paying for uploading the snapshot data. Using this method means a client will only have to iterate through a few snapshots instead of every transaction performed on the drive.Snapshot Entity Tags Snapshot entities require the following tags. These are queried by ArFS clients to find drive snapshots, organize them together with any other transactions not included within them, and build the latest state of the drive.ArFS: "0.13"
Drive-Id: "<drive uuid that this snapshot is associated with>"
Entity-Type: "snapshot"
Snapshot-Id: "<uuid of this snapshot entity>"
Content-Type: "<application/json>"
Block-Start: "<the minimum block height from which transactions were searched for in this snapshot, eg. 0>"
Block-End: "<the maximum block height from which transactions were searched for in this snapshot, eg 1007568>"
Data-Start: "<the first block in which transaction data was found in this snapshot, eg 854300"
Data-End: "<the last block in which transaction was found in this snapshot, eg 1001671"
Unix-Time: "<seconds since unix epoch>" Snapshot Transaction GQL tags example Snapshot Entity Data A JSON data object must also be uploaded with every ArFS Snapshot entity. THis data contains all ArFS Drive, Folder, and File metadata changes within the associated drive, as well as any previous Snapshots. The Snapshot Data contains an array txSnapshots. Each item includes both the GQL and ArFS metadata details of each transaction made for the associated drive, within the snapshot's start and end period.A tsSnapshot contains a gqlNode object which uses the same GQL tags interface returned by the Arweave Gateway. It includes all of the important block, owner, tags, and bundledIn information needed by ArFS clients. It also contains a dataJson object which stores the correlated Data JSON for that ArFS entity.For private drives, the dataJson object contains the JSON-string-escaped encrypted text of the associated file or folder. This encrypted text uses the file's existing Cipher and Cipher-IV. This ensures clients can decrypt this information quickly using the existing ArFS privacy protocols.Snapshot Transaction JSON data example Schema Diagrams The following diagrams show complete examples of Drive, Folder, and File entity Schemas.Public Drive  Public Drive Schema Private Drive  Private Drive Schema

---

# 183. AR  WAO

Document Number: 183
Source: https://docs.wao.eco/api/ar
Words: 654
Extraction Method: html

AR handles operations on the base Arweave Storage layer as well as wallet connections.Instantiate import { AR } from "wao"

const ar = new AR() host, port, and protocol can be set to access a specific gateway rather than https://arweave.net.const ar = new AR({ host: "localhost", port: 4000, protocol: "http" }) In the case of local gateways, you can only set port and the rest will be automatically figured out.const ar = new AR({ port: 4000 }) AO class auto-instantiates AR internally.import { AO } from "wao"

const ao = new AO()

const ar = ao.ar Set or Generate Wallet You can initialize AR with a wallet JWK or ArConnect.const ar = await new AR().init(jwk || arweaveWallet) Or you can generate a new wallet. In case of ArLocal, you can mint AR at the same time.const { jwk, addr, pub, balance } = await ar.gen("100") // mint 100 AR Once a wallet is set in one of these 3 ways, you cannot use the instance with another wallet unless you re-initialize it with another wallet. This is to prevent executing transactions with the wrong wallet when the browser connected active address has been changed unknowingly.You can go on without calling init or gen, in this case, AR generates a random wallet when needed, and also using different wallets will be allowed. This is useful, if you are only calling dryrun with AO, since AO requires a signature for dryrun too, but you don't want to bother the user by triggering the browser extension wallet for read only calls.Once a wallet is set, ar.jwk and ar.addr will be available.Token Related Methods toAddr Convert a jwk to the corresponding address.const addr = await ar.toAddr(jwk) mine Mine pending blocks (only for arlocal).await ar.mine() balance | toAR | toWinston Get the current balance of the specified address in AR. addr will be ar.addr if omitted.const balance_AR = await ar.balance() // get own balance

const balance_Winston = ar.toWinston(balance_AR)

const balance_AR2 = ar.toAR(balance_Winston)

const balance_AR3 = await ar.balance(addr) // specify wallet address transfer Transfer AR token. amount is in AR, not in winston for simplicity.const { id } = await ar.transfer(amount, to) You can set a jwk to the 3rd parameter as a sender. Otherwise, the sender is ar.jwk.const { id } = await ar.transfer(amount, to, jwk) For most write functions, jwk can be specified as the last parameter or a field like { data, tags, jwk }.checkWallet checkWallet is mostly used internally, but it returns this.jwk if a wallet has been assigned with init, or else it generates a random wallet to use. The following pattern is used in many places. With this pattern, if a wallet is set with init and the jwk the user is passing is different, checkWallet produces an error to prevent the wrong wallet. If no wallet has been set with init or gen and the jwk is not passed, it generates and returns a random wallet.some_class_method({ jwk }){

  let err = null

  ;({ err, jwk } = await ar.checkWallet({ jwk }))

  if(!err){

    // do something with the jwk

  }

} Storage Related Methods post Post a data to Arweave.const { err, id } = await ar.post({ data, tags }) tags are not an Array but a hash map Object for brevity.const tags = { "Content-Type": "text/markdown", Type: "blog-post" } If you must use the same name for multiple tags, the value can be an Array.const tags = { Name: [ "name-tag-1", "name-tag-2" ] } tx Get a transaction.const tx = await ar.tx(txid) data Get a data.const data = await ar.data(txid, true) // true if string bundle Bundle ANS-104 dataitems.const { err, id } = await ar.bundle(dataitems) dataitems are [ [ data, tags ], [ data, tags ], [ data, tags ] ].const { err, id } = await ar.bundle([

  [ "this is text", { "Content-Type": "text/plain" }],

  [ "# this is markdown", { "Content-Type": "text/markdown" }],

  [ png_image, { "Content-Type": "image/png" }]

])

---

# 184. ArMem  WAO

Document Number: 184
Source: https://docs.wao.eco/api/armem
Words: 209
Extraction Method: html

ArMem stands for Arweave in memory and is a class to emulate an Arweave node and AO units in memory, which is internally used in the WAO testing framework. You can instantiate ArMem and control multiple emulators by passing it between other classes.Instantiate Instantiate When you instantiate WAO connect or AO from wao/test, it automatically and internally instantiates ArMem.import { connect } from "wao/test"

const { spawn, message, dryrun, assign, result, mem } = connect() // aoconnect APIs import { AO } from "wao/test"

const ao = new AO() // ao.mem Shared Memory You can instantiate ArMem and pass it to other classes.import { ArMem, AO, AR, connect } from "wao/test"

const mem = new ArMem()

const { spawn, message, dryrun, assign, result } = connect(mem)

const ao = new AO({ mem })

const ar = new AR({ mem }) If you don't pass the same ArMem instance, the two AO instances will have different environments.import { AO } from "wao/test"

const ao = new AO() // ao.mem

const ao2 = new AO() // ao2.mem ao.mem and ao2.mem are not connected. They are on different networks.import { AO } from "wao/test"

const ao = new AO()

const ao2 = new AO({ mem: ao.mem }) This will connect the two.

---

# 185. HyperBEAM  WAO

Document Number: 185
Source: https://docs.wao.eco/api/hyperbeam
Words: 668
Extraction Method: html

HperBEAM class can start and manage a HyperBEAM node from within JS code for testing.You should first install HyperBEAM on your local machine by following the official docs.Basic Usage import { HyperBEAM } from "wao/test"

import { describe, it, before, after } from "node:test"

 

describe("HyperBEAM", function () {

  let hbeam

  before(async () => {

    hbeam = await new HyperBEAM({ 

      port: 10001,

      wallet: ".wallet.json",

      cwd: "./HyperBEAM", 

      reset: true,

      bundler: 4001,

      gateway: 4000,

      logs: true,

      shell: true,

      as: [ "genesis_wasm" ],

      devices: [ "flat", "structured", "httpsig", "json", "meta" ],

      faff: [ addr, addr2, addr3 ],

      simple_pay: true,

      simple_pay_price: 3,

      p4_lua: { processor: pid, client: cid },

      operator: addr

    }).ready()

  })

  after(() => hbeam.kill())

  it("should run", async () => {

    // run some tests

  })

})

 

hbeam.kill() Parameters Name Default Description port 10001 Port number for the HyperBEAM node cwd../HyperBEAM node directory relative to current working directory wallet.wallet.json node operator jwk location relative to cwd reset false clear storage to start fresh bundler - bundler service port gateway - gateway service port logs true set false to disable HyperBEAM logs shell true false to not auto-start rebar3 shell as [] rocksdb, genesis_wasm, http3 devices - array of preloaded devices, undefined to load everything faff - array of addresses (likely for funding/faucet) simple_pay false enable simple-pay@1.0 simple_pay_price - base price for transactions p4_lua - { processor: pid, client: cid } operator - payment operator address Environment Variables If your installation requires environment variables to run rebar3, define then in .env.hyperbeam.cwd can also be set in .env.hyperbeam to apply globally across all test files.File .env.hyperbeam CWD=./HyperBEAM

CC=gcc-12

CXX=g++-12

CMAKE_POLICY_VERSION_MINIMUM=3.5 Preloaded Devices Key Name Module meta meta@1.0 dev_meta json json@1.0 dev_codec_json flat flat@1.0 dev_codec_flat httpsig httpsig@1.0 dev_codec_httpsig structured structured@1.0 dev_codec_structured process process@1.0 dev_process message message@1.0 dev_message scheduler scheduler@1.0 dev_scheduler delegated-compute delegated-compute@1.0 dev_delegated_compute genesis-wasm genesis-wasm@1.0 dev_genesis_wasm lua lua@5.3a dev_lua wasi wasi@1.0 dev_wasi wasm-64 wasm-64@1.0 dev_wasm json-iface json-iface@1.0 dev_json_iface test-device test-device@1.0 dev_test patch patch@1.0 dev_patch push push@1.0 dev_push stack stack@1.0 dev_stack multipass multipass@1.0 dev_multipass faff faff@1.0 dev_faff p4 p4@1.0 dev_p4 node-process node-process@1.0 dev_node_process simple-pay simple-pay@1.0 dev_simple_pay cron cron@1.0 dev_cron relay relay@1.0 dev_relay router router@1.0 dev_router cache cache@1.0 dev_cache local-name local-name@1.0 dev_local_name lookup lookup@1.0 dev_lookup name name@1.0 dev_name compute compute@1.0 dev_cu dedup dedup@1.0 dev_dedup manifest manifest@1.0 dev_manifest monitor monitor@1.0 dev_monitor snp snp@1.0 dev_snp volume volume@1.0 dev_volume poda poda@1.0 dev_poda greenzone greenzone@1.0 dev_green_zone hyperbuddy hyperbuddy@1.0 dev_hyperbuddy ans104 ans104@1.0 dev_codec_ans104 cacheviz cacheviz@1.0 dev_cacheviz wao wao@1.0 dev_wao If the device is not listed, you need to define it yourself.device = { name, module } const hbeam = await new HyperBEAM({

  devices: [

    "flat",

    "structured",

    "httpsig",

    "json",

    "meta",

    { name: "mydev@1.0", module: "dev_mydev" }

  ]

}).ready() Node Operator Address You can use HyperBEAM.OPERATOR as a placeholder for the node operator address before instantiation.const hbeam = await new HyperBEAM({

  operator: HyperBEAM.OPERATOR,

  faff: [ HyperBEAM.OPERATOR, addr2, addr3 ]

}).ready() HyperBEAM.OPERATOR will be replaced with the actual node operator address on instantiation.Eunit Testing You can run Erlang eunit tests from JS.import assert from "assert"

import { after, describe, it, before, beforeEach } from "node:test"

import HyperBEAM from "../../src/hyperbeam.js"

 

describe("Hyperbeam Eunit", function () {

  let hbeam

  before(async () => {

    hbeam = new HyperBEAM({ reset: true, shell: false }))

  })

  beforeEach(async () => (hb = hbeam.hb))

  

  it("should run a single module test", async () => {

    await hbeam.eunit("dev_message")

  })

  

  it("should run multiple module tests", async () => {

    await hbeam.eunit([ "dev_message", "dev_process", "dev_schduler" ])

  })

  

  it("should run a specific test", async () => {

    await hbeam.eunit("dev_message", "verify_test")

  })

 

  it("should run multiple tests", async () => {

    await hbeam.eunit([

      "dev_message:verify_test", 

      "dev_process:persistent_process_test"

    )

  })

}) Reading Local Files You can read local files under the HyperBEAM cwd directory with the file method.The following is reading lua scripts and caching them to the HyperBEAM node, then starting another node process on port 10002 with the p4 payment service using the cached Lua scripts.const process = hbeam.file("scripts/p4-payment-process.lua")

const pid = await hb.cacheScript(process)

const client = hbeam.file("scripts/p4-payment-client.lua")

const cid = await hb.cacheScript(client)

const hbeam2 = await new HyperBEAM({

  port: 10002,

  operator: hb.addr,

  p4_lua: { processor: pid, client: cid },

}).ready()

---

# 186. Structured Codec  WAO

Document Number: 186
Source: https://docs.wao.eco/hyperbeam/codec-structured
Words: 411
Extraction Method: html

structured@1.0 turns complex objects into strings with extended ao-types according to the HTTP Structured Field Values (RFC-9651) specification.This object type is called TABM (Type Annotated Binary Message) in HyperBEAM.ao-types has the following types:integer: 123 float: 3.14 binary: "abc" | Buffer.from([1,2,3]) atom: true | false | null | Symbol("abc") list: [1, 2, 3] empty-binary: "" | Buffer.from([]) empty-list: [] empty-message: {} File  /HyperBEAM/src/dev_mydev.erl -export([ structured_to/3, structured_from/3 ]).

 

structured_to(Msg1, Msg2, Opts) ->

    Body = maps:get(<<"body">>, Msg1),

    OBJ = dev_codec_json:from(Body),

    TABM = dev_codec_structured:to(OBJ),

    JSON = dev_codec_json:to(TABM),

    {ok, JSON}.

 

structured_from(Msg1, Msg2, Opts) ->

    Body = maps:get(<<"body">>, Msg1),

    TABM = dev_codec_json:from(Body),

    OBJ = dev_codec_structured:from(TABM),

    JSON = dev_codec_json:to(OBJ),

    {ok, JSON}.Encode JSON with dev_codec_structured:from.File  /test/codec-structured.test.js const cases = [

  { list: [1, true, "abc"] },

  { nested_list: [1, [2, 3]] },

  { a: { b: [1, 2, 3] } },

  { a: [1, 2], b: [3, 4] },

  { empty_list: [], empty_binary: "", empty_message: {} },

]

for (const v of cases) {

  const { out } = await hb.post({

    path: "/~mydev@1.0/structured_from",

    body: JSON.stringify(v),

  })

  console.log(JSON.parse(out))

} { list: [1, true, "abc"] } {

  'ao-types': 'list="list"',

  list: '"(ao-type-integer) 1", "(ao-type-atom) \\"true\\"", "abc"'

} { nested_list: [1, [2, 3]] } {

  'ao-types': 'nested_list="list"',

  nested_list: '"(ao-type-integer) 1", "(ao-type-list) \\"(ao-type-integer) 2\\", \\"(ao-type-integer) 3\\""'

} { a: { b: [1, 2, 3] } } {

  a: {

    'ao-types': 'b="list"',

    b: '"(ao-type-integer) 1", "(ao-type-integer) 2", "(ao-type-integer) 3"'

  }

} { a: [1, 2], b: [3, 4] } {

  a: '"(ao-type-integer) 1", "(ao-type-integer) 2"',

  'ao-types': 'a="list", b="list"',

  b: '"(ao-type-integer) 3", "(ao-type-integer) 4"'

} { empty_list: [], empty_binary: "", empty_message: {} } {

  'ao-types': 'empty_binary="empty-binary", empty_list="empty-list", empty_message="empty-message"'

} You can specify ao-types of the values at the same level, annotate keys with (ao-type-[type]), and join multiple entries with , .Let's decode the encoded values.File  /test/codec-structured.test.js const cases = [

  {

    'ao-types': 'list="list"',

    list: '"(ao-type-integer) 1", "(ao-type-atom) \\"true\\"", "abc"'

  },

  {

    'ao-types': 'nested_list="list"',

    nested_list: '"(ao-type-integer) 1", "(ao-type-list) \\"(ao-type-integer) 2\\", \\"(ao-type-integer) 3\\""'

  },

  {

    a: {

      'ao-types': 'b="list"',

      b: '"(ao-type-integer) 1", "(ao-type-integer) 2", "(ao-type-integer) 3"'

    }

  },

  {

    a: '"(ao-type-integer) 1", "(ao-type-integer) 2"',

    'ao-types': 'a="list", b="list"',

    b: '"(ao-type-integer) 3", "(ao-type-integer) 4"'

  },

  {

    'ao-types': 'empty_binary="empty-binary", empty_list="empty-list", empty_message="empty-message"'

  }

]

for (const v of cases) {

  const { out } = await hb.post({

    path: "/~mydev@1.0/structured_to",

    body: JSON.stringify(v),

  })

  console.log(JSON.parse(out))

} Running Tests You can find the working test file for this chapter here:codec-structured.test.js Run tests:Terminal   Terminal yarn test test/codec-structured.test.js References Specs Structured Field Values for HTTP [RFC-9651] Device API dev_codec_structured.erl WAO API HyperBEAM Class API HB Class API

---

# 187. Flat Codec  WAO

Document Number: 187
Source: https://docs.wao.eco/hyperbeam/codec-flat
Words: 363
Extraction Method: html

flat@1.0 is a simple codec device to flatten/unflatten object paths since HTTP headers and body cannot handle nested object structures. It's internally used by httpsig@1.0 to resolve object paths.Codec devices have to and from methods to encode and decode, but they are not exposed to external URLs. We can create custom methods for our custom device to expose them.File  /HyperBEAM/src/dev_mydev.erl -export([ flat_to/3, flat_from/3 ]).

 

flat_to(Msg1, Msg2, Opts) ->

    Body = maps:get(<<"body">>, Msg1),

    OBJ = dev_codec_json:from(Body),

    FLAT = dev_codec_flat:to(OBJ),

    JSON = dev_codec_json:to(FLAT),

    {ok, JSON}.

 

flat_from(Msg1, Msg2, Opts) ->

    Body = maps:get(<<"body">>, Msg1),

    OBJ = dev_codec_json:from(Body),

    FLAT = dev_codec_flat:from(OBJ),

    JSON = dev_codec_json:to(FLAT),

    {ok, JSON}.One thing to note is that HTTP header keys cannot contain /, so if you ever need to send keys with / you need to push them into multipart body. We will handle this in the next chapter with Httpsig Codec. Flat codec only handles map structures. List structures are handled by Structured Codec. Also Flat Codec is an intermediary step used by Httpsig Codec, so values also have to be strings with dev_codec_flat:to.File  /test/codec-flat.test.js const cases = [

  { a: { b: "v" } },

  { a: "v", b: { c: "v2", d: "v3" } },

  { a: { b: { c: { d: "v" } } } },

]

for (const v of cases) {

  const { body } = await hb.post({

    path: "/~mydev@1.0/flat_to",

    body: JSON.stringify(v),

  })

  console.log(JSON.parse(body))

} { a: { b: "v" } } -> { "a/b": "v" } { a: "v", b: { c: "v2", d: "v3" } } -> { a: "v", "b/c": "v2", "b/d": "v3" } { a: { b: { c: { d: "v" } } } } -> { "a/b/c/d": "v" } File  /test/codec-flat.test.js const cases = [

  { "a/b": "v" },

  { a: "v", "b/c": "v2", "b/d": "v3" },

  { "a/b/c/d": "v" },

]

for (const v of cases) {

  const { body } = await hb.post({

    path: "/~mydev@1.0/flat_from",

    body: JSON.stringify(v),

  })

  console.log(JSON.parse(body))

} Running Tests You can find the working test file for this chapter here:codec-flat.test.js Run tests:Terminal   Terminal yarn test test/codec-flat.test.js References General HyperBEAM Class API HB Class API Device API dev_codec_flat.erl WAO API HyperBEAM Class API HB Class API

---

# 188. Creating Custom HyperBEAM Devices  WAO

Document Number: 188
Source: https://docs.wao.eco/tutorials/creating-devices
Words: 341
Extraction Method: html

You can create your own HyperBEAM devices with Erlang, Rust or C++, and test them using WAO JS SDK.You should write unit tests in device's own language such as Erlang with eunit.Minimum Viable Device You can add an arbitrary device in the HyperBEAM/src directory.-module(dev_foo).

-export([ info/3 ]).

-include_lib("eunit/include/eunit.hrl").

-include("include/hb.hrl").

 

info(Msg, _, Opts) ->

    {ok, hb_ao:set(Msg, #{ <<"version">> => <<"1.0">> }, Opts)}.Then add your device to the preloaded_devices list in HyperBEAM/src/hb_ops.preloaded_devices => [

  #{<<"name">> => <<"ans104@1.0">>, <<"module">> => dev_codec_ans104},

  #{<<"name">> => <<"compute@1.0">>, <<"module">> => dev_cu},

  ...

  #{<<"name">> => <<"foo@1.0">>, <<"module">> => dev_foo}

],Now you can execute the functions using the WAO HB class.import assert from "assert"

import { describe, it, before, after, beforeEach } from "node:test"

import { HyperBEAM } from "wao"

 

const cwd = "../HyperBEAM" // HyperBEAM directory

 

describe("Hyperbeam Legacynet", function () {

  let hbeam, hb

  before(async () => {

    hbeam = await new HyperBEAM({ cwd, reset: true }).ready()

  })

  beforeEach(async () => (hb = hbeam.hb))

  after(async () => hbeam.kill())

 

  it("should query a custome device", async () => {

    const { version } = await hb.g("/~foo@1.0/info")

    assert.equal("1.0", version)

  })

}) Execution Device for AO Process To create an execution device for AO process, you need to implement at least init/3, normalize/3, compute/3 and snapshot/3.-module(dev_foo).

-export([ compute/3, init/3, snapshot/3, normalize/3 ]).

-include_lib("eunit/include/eunit.hrl").

-include("include/hb.hrl").

 

compute(Msg1, Msg2, Opts) ->

    case hb_ao:get([<<"body">>,<<"Action">>], Msg2, Opts) of

    Other ->

        {ok, hb_ao:set( Msg1, #{  }, Opts )}

    end.

 

init(Msg, Msg2, Opts) -> 

    {ok, hb_ao:set(Msg, #{ }, Opts)}.

 

snapshot(Msg, _Msg2, _Opts) -> {ok, Msg}.

 

normalize(Msg, _Msg2, _Opts) -> {ok, Msg}.Now you can spawn a process by specifying execution-device.import assert from "assert"

import { describe, it, before, after, beforeEach } from "node:test"

 

const cwd = "../HyperBEAM" // HyperBEAM directory

 

describe("Hyperbeam Legacynet", function () {

  let hbeam, hb

  before(async () => {

    hbeam = await new HyperBEAM({ cwd, reset: true }).ready()

  })

  beforeEach(async () => (hb = hbeam.hb))

  after(async () => hbeam.kill())

 

  it("should query a custom device", async () => {

    const { pid } = await hb.spawn({"excecution-device": "foo@1.0"})

    const { slot, res } = await hb.message({ pid })

  })

})

---

# 189. Custom Devices in C  WAO

Document Number: 189
Source: https://docs.wao.eco/tutorials/devices-cpp
Words: 375
Extraction Method: html

Erlang can natively execute C++ functions with almost no overhead via NIF (Native Implemented Function).Creating a C++ Device Go to HyperBEAM/native directory and create a new directory for your C++ device.mkdir -p dev_mul_nif/include && cd dev_mul_nif Step 1: Create the C++ Header File Create include/dev_mul.h:#pragma once

 

extern "C" {

  int multiply(const int a, const int b);

} The extern "C" linkage is crucial for making C++ functions callable from C/Erlang NIFs.Step 2: Implement the C++ Logic Create dev_mul.cpp:#include "include/dev_mul.h"

 

int multiply(const int a, const int b) {

  return a * b;

} Step 3: Create the NIF Wrapper Create dev_mul_nif.cpp:#include <erl_nif.h>

#include "include/dev_mul.h"

 

static int load(ErlNifEnv* env, void** priv_data, ERL_NIF_TERM load_info) {

    return 0;

}

 

static void unload(ErlNifEnv* env, void* priv_data) {}

 

static ERL_NIF_TERM mul_nif(ErlNifEnv* env, int argc, const ERL_NIF_TERM argv[]) {

    int a, b;

    if (!enif_get_int(env, argv[0], &a) || !enif_get_int(env, argv[1], &b)) {

        return enif_make_badarg(env);

    }

 

    int result = multiply(a, b);

    return enif_make_int(env, result);

}

 

static ErlNifFunc nif_funcs[] = {

    {"multiply", 2, mul_nif}

};

 

ERL_NIF_INIT(dev_mul_nif, nif_funcs, load, NULL, NULL, unload) Step 4: Configure Build in rebar.config Add the C++ compilation settings to HyperBEAM/rebar.config:{port_env, [

    {"(linux|darwin|solaris)", "CXX", "g++"},

    {"(linux|darwin|solaris)", "CXXFLAGS", 

        "$CXXFLAGS -std=c++17 -I${REBAR_ROOT_DIR}/native/dev_mul_nif/include -I/usr/local/lib/erlang/usr/include/"},

    {"(linux|darwin|solaris)", "LDFLAGS", 

        "$LDFLAGS -lstdc++"}

]}.Add the port specification:{port_specs, [

    ...

    {"./priv/dev_mul.so", [

        "./native/dev_mul_nif/dev_mul_nif.cpp",

        "./native/dev_mul_nif/dev_mul.cpp"

    ]}

    ...

]}.Add cleanup hooks:{post_hooks, [

    ...

    { compile, "rm -f native/dev_mul_nif/*.o native/dev_mul_nif/*.d"}

    ...

]}.Step 5: Create the Erlang NIF Module Create HyperBEAM/src/dev_mul_nif.erl:-module(dev_mul_nif).

-export([multiply/2]).

 

-include("include/hb.hrl").

-include_lib("eunit/include/eunit.hrl").

 

-on_load(init/0).

 

-define(NOT_LOADED, not_loaded(?LINE)).

not_loaded(Line) ->

    erlang:nif_error({not_loaded, [{module, ?MODULE}, {line, Line}]}).

 

init() ->

    PrivDir = code:priv_dir(hb),

    Path = filename:join(PrivDir, "dev_mul"),

    case erlang:load_nif(Path, 0) of

        ok -> ok;

        {error, Reason} -> exit({load_failed, Reason})

    end.

 

multiply(_A, _B) ->

    not_loaded(?LINE).Step 6: Create the Erlang Device Module Create HyperBEAM/src/dev_mul.erl:-module(dev_mul).

-export([mul/3]).

-include("include/hb.hrl").

-include_lib("eunit/include/eunit.hrl").

 

mul(_, M2, Opts) ->

    A = hb_ao:get(<<"a">>, M2, Opts),

    B = hb_ao:get(<<"b">>, M2, Opts),

 

    Product = dev_mul_nif:multiply(A, B),

 

    {ok, #{ <<"product">> => Product, <<"a">> => A, <<"b">> => B }}.

 

multiply_test() ->

    M1 = #{<<"device">> => <<"mul@1.0">>},

    M2 = #{

        <<"path">> => <<"mul">>,

        <<"a">> => 2,

        <<"b">> => 3

    },

    {ok, Product} = hb_ao:resolve(M1, M2, #{}),

    ?assertEqual(6, maps:get(<<"product">>, Product)).Step 7: Register the Device Add the device to HyperBEAM/hb_opt.erl:preloaded_devices => [

  ...

  #{<<"name">> => <<"mul@1.0">>, <<"module">> => dev_mul},

  ...

],Step 8: Build and Test Run the unit tests:rebar3 eunit --module=dev_mul Test the device with WAO:

---

# 190. getArNSReservedNames - ARIO Docs

Document Number: 190
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-reserved-names
Words: 95
Extraction Method: html

getArNSReservedNames is a method on the ARIO class that retrieves all reserved ArNS names, with support for pagination and custom sorting. Reserved names are names that are protected and cannot be registered by users.getArNSReservedNames does not require authentication.Examples Parameters Parameter Type Description Optional Default cursor string The name to use as the starting point for the next page of results true None limit number The maximum number of records to return (max: 1000) true 100 sortBy string The property to sort results by true name sortOrder string The sort direction ('desc' or 'asc') true asc

---

# 191. upgradeRecord - ARIO Docs

Document Number: 191
Source: https://docs.ar.io/ar-io-sdk/ario/arns/upgrade-record
Words: 94
Extraction Method: html

upgradeRecord is a method on the ARIO class that upgrades an existing ArNS record from a lease to a permanent ownership (permabuy). This allows converting a leased name to permanent ownership.upgradeRecord requires authentication.Examples Parameters Parameter Type Description Optional name string The ArNS name to upgrade to permanent ownership false fundFrom string The source of funds: 'balance', 'stakes', 'any', or 'turbo' true paidBy string | string[] Wallet address(es) that will pay for the purchase (used with fundFrom:
'turbo') true tags array An array of GQL tag objects to attach to the transfer AO message true

---

# 192. ARIO Docs

Document Number: 192
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateway-delegates
Words: 93
Extraction Method: html

getGatewayDelegates getGatewayDelegates is a method on the ARIO class that retrieves all delegates for a specific gateway. Results are paginated and sorted by the specified criteria. The cursor parameter represents the last delegate address from the previous request.getGatewayDelegates does not require authentication.Example Parameters Parameter Type Description Required address string The gateway address to query for delegates Yes cursor string Cursor for paginated results No limit number Maximum number of results to return (max: 1000) No sortBy string Property to sort results by No sortOrder string Sort direction (valid values: 'desc' or 'asc') No

---

# 193. ARIO Docs

Document Number: 193
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/get-allowed-delegates
Words: 91
Extraction Method: html

getAllowedDelegates getAllowedDelegates is a method on the ARIO class that retrieves all allowed delegates for a specific gateway address. The cursor parameter is used for pagination and represents the last address from the previous request.getAllowedDelegates does not require authentication.Example Parameters Parameter Type Description Required address string The gateway address to query for allowed delegates Yes cursor string Cursor for paginated results No limit number Maximum number of results to return (max: 1000) No sortBy string Property to sort results by No sortOrder string Sort direction (valid values: 'desc' or 'asc') No

---

# 194. Hello World (No Code)  Cooking with the Permaweb

Document Number: 194
Source: https://cookbook.arweave.net/getting-started/quick-starts/hw-no-code.html
Words: 90
Extraction Method: html

Hello World (No Code) In this quick start we are going to upload an image to the Permaweb with no code.Requirements Computer Internet Modern web browser Create a wallet https://arweave.app/add open in new window or https://wander.app open in new window Send some data to Arweave Go to https://hello_cookbook.arweave.net open in new window Enter some data and click publish, connect your wallet and "BAM" Congratulations!You just published some data on Arweave using zero code.To check out the project -> https://github.com/twilson63/pw-no-code-hello Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 195. createVault - ARIO Docs

Document Number: 195
Source: https://docs.ar.io/ar-io-sdk/ario/vaults/create-vault
Words: 89
Extraction Method: html

createVault is a method on the ARIO class that creates a new vault to lock ARIO tokens for a specified duration. Vaulted tokens earn rewards but cannot be transferred until the lock period expires.createVault requires authentication.Examples Parameters Parameter Type Description Optional qty number The amount of ARIO tokens to lock in the vault (in mARIO) false lockLength number The duration to lock tokens in blocks (minimum 14 days, maximum 4 years) false tags array An array of GQL tag objects to attach to the vault creation AO message true

---

# 196. process10 - HyperBEAM - Documentation

Document Number: 196
Source: https://hyperbeam.arweave.net/build/devices/process-at-1-0.html
Words: 549
Extraction Method: html

Device: ~process@1.0 Overview The ~process@1.0 device represents a persistent, shared execution environment within HyperBEAM, analogous to a process or actor in other systems. It allows for stateful computation and interaction over time.Core Concept: Orchestration A message tagged with Device: process@1.0 (the "Process Definition Message") doesn't typically perform computation itself. Instead, it defines which other devices should be used for key aspects of its lifecycle:Scheduler Device: Determines the order of incoming messages (assignments) to be processed. (Defaults to ~scheduler@1.0).Execution Device: Executes the actual computation based on the current state and the scheduled message. Often configured as dev_stack to allow multiple computational steps (e.g., running WASM, applying cron jobs, handling proofs).Push Device: Handles the injection of new messages into the process's schedule. (Defaults to ~push@1.0).The ~process@1.0 device acts as a router, intercepting requests and delegating them to the appropriate configured device (scheduler, executor, etc.) by temporarily swapping the device tag on the message before resolving.Key Functions (Keys) These keys are accessed via an HTTP path relative to the Process Definition Message ID (<ProcessID>).GET /<ProcessID>~process@1.0/schedule Action: Delegates to the configured Scheduler Device (via the process's schedule/3 function) to retrieve the current schedule or state.Response: Depends on the Scheduler Device implementation (e.g., list of message IDs).POST /<ProcessID>~process@1.0/schedule Action: Delegates to the configured Push Device (via the process's push/3 function) to add a new message to the process's schedule.Request Body: The message to be added.Response: Confirmation or result from the Push Device.GET /<ProcessID>~process@1.0/compute/<TargetSlotOrMsgID> Action: Computes the process state up to a specific point identified by <TargetSlotOrMsgID> (either a slot number or a message ID within the schedule). It retrieves assignments from the Scheduler Device and applies them sequentially using the configured Execution Device.Response: The process state message after executing up to the target slot/message.Caching: Results are cached aggressively (see dev_process_cache) to avoid recomputation.GET /<ProcessID>~process@1.0/now Action: Computes and returns the Results key from the latest known state of the process. This typically involves computing all pending assignments.Response: The value of the Results key from the final state.GET /<ProcessID>~process@1.0/slot Action: Delegates to the configured Scheduler Device to query information about a specific slot or the current slot number.Response: Depends on the Scheduler Device implementation.GET /<ProcessID>~process@1.0/snapshot Action: Delegates to the configured Execution Device to generate a snapshot of the current process state. This often involves running the execution stack in a specific "map" mode to gather state from different components.Response: A message representing the process snapshot, often marked for caching.Process Definition Example A typical process definition message might look like this (represented conceptually):Device: process@1.0
Scheduler-Device: [`scheduler@1.0`](./source-code/dev_scheduler.md)
Execution-Device: [`stack@1.0`](./source-code/dev_stack.md)
Execution-Stack: "[`scheduler@1.0`](./source-code/dev_scheduler.md)", "[`cron@1.0`](./source-code/dev_cron.md)", "[`wasm64@1.0`](./source-code/dev_wasm.md)", "[`PoDA@1.0`](./source-code/dev_poda.md)"
Cron-Frequency: 10-Minutes
WASM-Image: <WASMImageTxID>
PoDA:
    Device: [`PoDA/1.0`](./source-code/dev_poda.md)
    Authority: <AddressA>
    Authority: <AddressB>
    Quorum: 2 This defines a process that uses:
* The standard scheduler.
* A stack executor that runs scheduling logic, cron jobs, a WASM module, and a Proof-of-Data-Availability check.State Management & Caching ~process@1.0 relies heavily on caching (dev_process_cache) to optimize performance. Full state snapshots and intermediate results are cached periodically (configurable via Cache-Frequency and Cache-Keys options) to avoid recomputing the entire history for every request.Initialization (init) Processes often require an initialization step before they can process messages. This is typically triggered by calling the init key on the configured Execution Device via the process path (/<ProcessID>~process@1.0/init). This allows components within the execution stack (like WASM modules) to set up their initial state.process module

---

# 197. BetterIDEa  Cookbook

Document Number: 197
Source: https://cookbook_ao.arweave.net/references/betteridea/index.html
Words: 84
Extraction Method: html

Skip to content  BetterIDEa BetterIDEa is a custom web based IDE for developing on ao.It offers a built in Lua language server with ao definitions, so you don't need to install anything. Just open the IDE and start coding!Features include:Code completion Cell based notebook ui for rapid development Easy process management Markdown and Latex cell support Share projects with anyone through ao processes Tight integration with ao package manager Read detailed information about the various features and integrations of the IDE in the documentation.

---

# 198. ARIO Configuration - ARIO Docs

Document Number: 198
Source: https://docs.ar.io/ar-io-sdk/ario/configuration
Words: 84
Extraction Method: html

init init is a factory function that creates a read-only or writable client. By providing a signer, additional write APIs that require signing (like buyRecord and transfer) become available. By default, a read-only client is returned and no write APIs are available.Parameters Parameter Type Description Optional process AOProcess A pre-configured AOProcess instance used to initialize the ARIO class true processId string The process ID of the AO process true signer ContractSigner An optional signer instance, used to enable write operations on the
blockchain true

---

# 199. ARIO Docs

Document Number: 199
Source: https://docs.ar.io/ar-io-sdk/ants/reassign-name
Words: 82
Extraction Method: html

reassignName reassignName is a method on the ANT class that transfers ownership of an ArNS name to a new ANT. This operation can only be performed by the current ANT owner.reassignName requires authentication.Examples Parameters Parameter Type Description Optional name string The ArNS name to be reassigned false ioProcessId string The Process ID of the ARIO contract false antProcessId string The Process ID of the target ANT false tags array An array of GQL tag objects to attach to the transfer AO message.true

---

# 200. ARIO Docs

Document Number: 200
Source: https://docs.ar.io/ar-io-sdk/ants/remove-primary-names
Words: 75
Extraction Method: html

removePrimaryNames removePrimaryNames is a method on the ANT class that removes specified primary names from the ANT process. This affects any primary names associated with base names controlled by this ANT.removePrimaryNames requires authentication.Examples Parameters Parameter Type Description Optional names array An array of primary names to be removed false ioProcessId string The Process ID of the ARIO contract false tags array An array of GQL tag objects to attach to the transfer AO message true

---

# 201. approvePrimaryNameRequest - ARIO Docs

Document Number: 201
Source: https://docs.ar.io/ar-io-sdk/ants/approve-primary-name-request
Words: 74
Extraction Method: html

approvePrimaryNameRequest is a method on the ANT class that approves a primary name request for a given name or address.approvePrimaryNameRequest requires authentication.Examples Parameters Parameter Type Description Optional name string ArNS name to approve as primary name.false address string - WalletAddress Public wallet address that made the primary name request being approved.false ioProcessId string Process Id of the ARIO contract.false tags array An array of GQL tag objects to attach to the transfer AO message.true

---

# 202. ARIO Docs

Document Number: 202
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/decrease-delegate-stake
Words: 74
Extraction Method: html

decreaseDelegateStake decreaseDelegateStake is a method on the ARIO class that decreases the caller's delegated stake on the target gateway.decreaseDelegateStake requires authentication.Example Parameters Parameter Type Description Required qty number Amount in mARIO to remove from delegated stake Yes target string The gateway's public wallet address Yes instant boolean If true, pays a fee to make the withdrawn stake available instantly No tags array An array of GQL tag objects to attach to the transaction No

---

# 203. ARIO Docs

Document Number: 203
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/instant-withdrawal
Words: 73
Extraction Method: html

instantWithdrawal instantWithdrawal is a method on the ARIO class that instantly withdraws funds from an existing vault on a gateway. If no gatewayAddress is provided, the signer's address will be used.instantWithdrawal requires authentication.Examples Parameters Parameter Type Description Required gatewayAddress string The gateway address where the vault exists No vaultId string The ID of the vault to withdraw from Yes tags array An array of GQL tag objects to attach to the transaction No

---

# 204. addVersion - ARIO Docs

Document Number: 204
Source: https://docs.ar.io/ar-io-sdk/ant-versions/add-version
Words: 72
Extraction Method: html

addVersion is a method on the ANTVersions class that adds a new version to an ANT. This method requires authentication and a signer to be provided during initialization.Examples Parameters Parameter Type Description Optional version string The version identifier (e.g., "2.0.0") false moduleId string The module ID for the new version false luaSourceId string The Lua source ID for the new version true notes string Notes or description for the new version true

---

# 205. addController - ARIO Docs

Document Number: 205
Source: https://docs.ar.io/ar-io-sdk/ants/add-controller
Words: 72
Extraction Method: html

addController is a method on the ANT class that adds a new controller to the ANT's list of approved controllers. Controllers have permissions to set records and modify the ANT process's ticker and name.addController requires authentication.Examples Parameters Parameter Type Description Optional controller string - WalletAddress The public wallet address of the controller to be added false tags array An array of GQL tag objects to attach to the transfer AO message true

---

# 206. cancelWithdrawal - ARIO Docs

Document Number: 206
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/cancel-withdrawal
Words: 71
Extraction Method: html

cancelWithdrawal is a method on the ARIO class that cancels a pending withdrawal for a gateway, returning the stake back to the delegated amount.cancelWithdrawal requires authentication.Examples Parameters Parameter Type Description Optional gatewayAddress string - WalletAddress The wallet address of the gateway false vaultId string The ID of the vault containing the withdrawal to cancel false tags array An array of GQL tag objects to attach to the transfer AO message true

---

# 207. ARIO Docs

Document Number: 207
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/leave-network
Words: 71
Extraction Method: html

leaveNetwork leaveNetwork is a method on the ARIO class that sets a gateway's status to leaving on the ar.io network. The gateway's operator and delegate stakes are vaulted and will be returned after the leave period. The gateway will be removed from the network once the leave period ends.leaveNetwork requires authentication.Example Parameters Parameter Type Description Required tags array An array of GQL tag objects to attach to the AO message No

---

# 208. ariowayfinder-react - ARIO Docs

Document Number: 208
Source: https://docs.ar.io/wayfinder/react
Words: 241
Extraction Method: html

Overview The @ar.io/wayfinder-react package provides React-specific components and hooks for integrating Wayfinder into React applications. It offers a provider pattern for configuration and convenient hooks for fetching data with built-in loading states and error handling.Smart Defaults Wayfinder React automatically configures optimal settings for web
applications, including LocalStorageGatewaysProvider to avoid rate limits
and performance optimizations in hooks to prevent unnecessary rerenders.Installation Quick Start 1. Add the Wayfinder Context Provider Default Configuration Wayfinder React automatically uses LocalStorageGatewaysProvider with NetworkGatewaysProvider to avoid rate limits and improve performance. You
don't need to configure this manually unless you want custom settings.2. Use Hooks in Your Components Advanced Configuration import { WayfinderProvider } from '@ar.io/wayfinder-react'
import {
  NetworkGatewaysProvider,
  PreferredWithFallbackRoutingStrategy,
  FastestPingRoutingStrategy,
  HashVerificationStrategy,
  StaticGatewaysProvider,
} from '@ar.io/wayfinder-core'
import { ARIO } from '@ar.io/sdk'

function App() {
  return (
    <WayfinderProvider
      gatewaysProvider={new NetworkGatewaysProvider({ ario: ARIO.mainnet() })}
      routingSettings={{
        strategy: new PreferredWithFallbackRoutingStrategy({
          preferredGateway: 'https://my-gateway.com',
          fallbackStrategy: new FastestPingRoutingStrategy({ timeoutMs: 500 }),
        }),
        events: {
          onRoutingSucceeded: (event) =>
            console.log('Gateway selected:', event.selectedGateway),
          onRoutingFailed: (error) => console.error('Routing failed:', error),
        },
      }}
      verificationSettings={{
        enabled: true,
        strategy: new HashVerificationStrategy({
          trustedGateways: ['https://arweave.net'],
        }),
        strict: false,
        events: {
          onVerificationSucceeded: (event) =>
            console.log('Verified:', event.txId),
          onVerificationFailed: (error) =>
            console.error('Verification failed:', error),
        },
      }}
      telemetrySettings={{
        enabled: process.env.NODE_ENV === 'production',
        clientName: 'my-react-app',
        clientVersion: process.env.REACT_APP_VERSION || '1.0.0',
        sampleRate: 0.05,
        exporterUrl: process.env.REACT_APP_TELEMETRY_URL,
      }}
    >
      <YourApp />
    </WayfinderProvider>
  )
} useWayfinder: Access the complete Wayfinder instance useWayfinderRequest: Direct access to the request function useWayfinderUrl: URL resolution with loading states For more advanced configuration options, see the Core Documentation.

---

# 209. WAO Hub

Document Number: 209
Source: https://docs.wao.eco/hub
Words: 70
Extraction Method: html

WAO Hub is an ultra-lightweight ephemeral proxy that seamlessly connects different types of servers and browsers.Anyone can launch it both locally and remotely. WAO Hub:Connects the browser to remote HyperBEAM nodes via WebSockets Syncs your local file system to the browser via WebSockets Relays local AO tests to the browser via WebSockets Connects browsers with other browsers via WebRTC to create P2P mesh networks npx wao hub Usage Coming Soon!

---

# 210. AO The Web  WAO

Document Number: 210
Source: https://docs.wao.eco/web
Words: 70
Extraction Method: html

AO The Web brings AO units directly into the browser—completely standalone, with no external dependencies. It includes an integrated AOS terminal, a code editor, and a local AO explorer for seamless development and debugging.WAO is both a standalone AO unit emulator and a lightweight SDK, enabling you to embed fully functional AO units into any web application with just a single line of code.import { AO } from "wao/web" preview.wao.eco

---

# 211. ARIO Docs

Document Number: 211
Source: https://docs.ar.io/ar-io-sdk/ants/set-logo
Words: 69
Extraction Method: html

setLogo setLogo is a method on the ANT class that updates the logo of the ANT process. The logo must be specified as an Arweave transaction ID that contains an image.setLogo requires authentication.Examples Parameters Parameter Type Description Optional txId string The Arweave transaction ID of the image to use as the ANT's logo false tags array An array of GQL tag objects to attach to the transfer AO message.true

---

# 212. Httpsig Codec  WAO

Document Number: 212
Source: https://docs.wao.eco/hyperbeam/codec-httpsig
Words: 623
Extraction Method: html

httpsig@1.0 turns structured encoded objects into HTTP signature-ready objects. It flattens map structures into strings with the flat@1.0 device, and puts complex structures into the multipart body format.Create custom methods to expose dev_codec_httpsig:from and dev_codec_httpsig:to.File  /HyperBEAM/src/dev_mydev.erl -export([ httpsig_to/3, httpsig_from/3 ]).

 

httpsig_to(Msg1, Msg2, Opts) ->

    Body = maps:get(<<"body">>, Msg1),

    TABM = dev_codec_json:from(Body),

    HTTPSIG = dev_codec_httpsig:to(TABM),

    JSON = dev_codec_json:to(HTTPSIG),

    {ok, JSON}.

 

httpsig_from(Msg1, Msg2, Opts) ->

    Body = maps:get(<<"body">>, Msg1),

    HTTPSIG = dev_codec_json:from(Body),

    TABM = dev_codec_httpsig:from(HTPSIHG),

    JSON = dev_codec_json:to(TABM),

    {ok, JSON}.Let's convert one complex object.{ a: { b: [1, 2, 3]}, c: { d: [3.14, true, "str"] } } The structured encoded representation is the following.File  /test/codec-httpsig.test.js const cases = [{

  a: {

    "ao-types": 'b="list"',

    b: '"(ao-type-integer) 1", "(ao-type-integer) 2", "(ao-type-integer) 3"',

  },

  c: {

    "ao-types": 'd="list"',

    d: '"(ao-type-float) 3.14", "(ao-type-atom) \\"true\\"", "str"',

  },

}]

 

for (const v of cases) {

  const { body } = await hb.post({

    path: "/~mydev@1.0/httpsig_to",

    body: JSON.stringify(v),

  })

  console.log(JSON.parse(body))

} We get an httpsig-encoded value ready to be signed.{

  body: '--rqDK_isKBhMozuATy4K6NFgdADGNHedXoUEDN10AANo\r\n' +

    'ao-types: b="list"\r\n' +

    'b: "(ao-type-integer) 1", "(ao-type-integer) 2", "(ao-type-integer) 3"\r\n' +

    'content-disposition: form-data;name="a"\r\n' +

    '--rqDK_isKBhMozuATy4K6NFgdADGNHedXoUEDN10AANo\r\n' +

    'ao-types: d="list"\r\n' +

    'content-disposition: form-data;name="c"\r\n' +

    'd: "(ao-type-float) 3.14", "(ao-type-atom) \\"true\\"", "str"\r\n' +

    '--rqDK_isKBhMozuATy4K6NFgdADGNHedXoUEDN10AANo--',

  'body-keys': '"a", "c"',

  'content-digest': 'sha-256=:mv08FUN7TpjmiHhagrxwqgjS7kQ/HY2+If2hIUq/y54=:',

  'content-type': 'multipart/form-data; boundary="rqDK_isKBhMozuATy4K6NFgdADGNHedXoUEDN10AANo"'

} The encoding gives you 3 pieces of metadata. Fields other than body go into the HTTP headers.content-digest: the sha256 hash of body content, only required if body exists content-type: multipart/form-data with boundary body-keys: allocated key of each body part So you can split the body by the boundary of rqDK_isKBhMozuATy4K6NFgdADGNHedXoUEDN10AANo.content-disposition: form-data;: tells which path the part falls into  name could be a flattened path like a/b/c const parts = {

  a: 'ao-types: b="list"\r\n' +

     'b: "(ao-type-integer) 1", "(ao-type-integer) 2", "(ao-type-integer) 3"\r\n' +

     'content-disposition: form-data;name="a"\r\n',

  c: 'ao-types: d="list"\r\n' +

     'content-disposition: form-data;name="c"\r\n' +

     'd: "(ao-type-float) 3.14", "(ao-type-atom) \\"true\\"", "str"\r\n`

} You can decode the encoded value with dev_codec_structured:to.File  /test/codec-httpsig.test.js const cases = [

  {

    body: '--rqDK_isKBhMozuATy4K6NFgdADGNHedXoUEDN10AANo\r\n' +

      'ao-types: b="list"\r\n' +

      'b: "(ao-type-integer) 1", "(ao-type-integer) 2", "(ao-type-integer) 3"\r\n' +

      'content-disposition: form-data;name="a"\r\n' +

      '--rqDK_isKBhMozuATy4K6NFgdADGNHedXoUEDN10AANo\r\n' +

      'ao-types: d="list"\r\n' +

      'content-disposition: form-data;name="c"\r\n' +

      'd: "(ao-type-float) 3.14", "(ao-type-atom) \\"true\\"", "str"\r\n' +

      '--rqDK_isKBhMozuATy4K6NFgdADGNHedXoUEDN10AANo--',

    'body-keys': '"a", "c"',

    'content-digest': 'sha-256=:mv08FUN7TpjmiHhagrxwqgjS7kQ/HY2+If2hIUq/y54=:',

    'content-type': 'multipart/form-data; boundary="rqDK_isKBhMozuATy4K6NFgdADGNHedXoUEDN10AANo"'

  }

]

for (const v of cases) {

  const { body } = await hb.post({

    path: "/~mydev@1.0/httpsig_from",

    body: JSON.stringify(v),

  })

  console.log(JSON.parse(body))

} Another example reveals a couple of special fields.{ data: "abc", "Tbun4iRRQW93gUiSAmTmZJ2PGI-_yYaXsX69ETgzSRE": 123 } The encoded value is:{

  'ao-ids': 'Tbun4iRRQW93gUiSAmTmZJ2PGI-_yYaXsX69ETgzSRE="123"',

  'ao-types': '%54bun4i%52%52%51%5793g%55i%53%41m%54m%5a%4a2%50%47%49-_y%59a%58s%5869%45%54gz%53%52%45="integer"',

  body: 'abc',

  'content-digest': 'sha-256=:ungWv48Bz+pBQUDeXa4iI7ADYaOWF3qctBD/YfIAFa0=:',

  'inline-body-key': 'data'

} ao-ids: all keys get lower-cased during the encoding, but Arweave addresses are kept case-sensitive inline-body-key: the entire body will become the value of the specified key Encoding / Decoding Pipeline You can validate the encoding-decoding of any value with the following pipeline.File  /test/codec-httpsig.test.js const cases = [

  { list: [1, true, "abc"] },

  { nested_list: [1, [2, 3]] },

  { a: { b: [1, 2, 3] } },

  { a: [1, 2], b: [3, 4] },

  { empty_list: [], empty_binary: "", empty_message: {} },

  { data: "abc", [hb.addr]: 123 },

  { list: [1, 2, 3], map: { a: { b: { c: 4 } } } },

]

for(const json of cases){

  const res = await hb.post({

    path: "/~mydev@1.0/structured_from",

    body: JSON.stringify(json),

  })

  const structured = JSON.parse(res.body)

  console.log(structured)

  const res2 = await hb.post({

    path: "/~mydev@1.0/httpsig_to",

    body: JSON.stringify(structured),

  })

  const encoded = JSON.parse(res2.body)

  console.log(encoded)

  const res3 = await hb.post({

    path: "/~mydev@1.0/httpsig_from",

    body: JSON.stringify(encoded),

  })

  

  // omit: body-keys, content-type, inline-body-key

  const {

    "body-keys": _,

    "content-type": __,

    "inline-body-key": ___,

    ...decoded

  } = JSON.parse(res3.body)

  console.log(decoded)

  const res4 = await hb.post({

    path: "/~mydev@1.0/structured_to",

    body: JSON.stringify(decoded),

  })

  const json2 = JSON.parse(res4.body)

  assert.deepEqual(json,json2)

} Running Tests You can find the working test file for this chapter here:codec-httpsig.test.js Run tests:Terminal   Terminal yarn test test/codec-httpsig.test.js References Device API dev_codec_httpsig.erl WAO API HyperBEAM Class API HB Class API

---

# 213. ARIO Docs

Document Number: 213
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/decrease-operator-stake
Words: 66
Extraction Method: html

decreaseOperatorStake decreaseOperatorStake is a method on the ARIO class that decreases the caller's operator stake. This method must be executed with a wallet registered as a gateway operator.decreaseOperatorStake requires authentication.Examples Parameters Parameter Type Description Required qty number Amount in mARIO to remove from operator stake (cannot decrease below the
network minimum) Yes tags array An array of GQL tag objects to attach to the transaction No

---

# 214. Normalized Addresses - ARIO Docs

Document Number: 214
Source: https://docs.ar.io/concepts/normalized-addresses
Words: 638
Extraction Method: html

Overview Different blockchains use different formats for the public keys of wallets, and the native addresses for those wallets. In most cases, when a system in the Arweave ecosystem needs to display the wallet address of a wallet from a different blockchain, for instance in the Owner.address value of an AO process spawned by an ETH wallet, that address will be normalized into the format recognized by Arweave. Specifically, a 43 character base64url representation of the sha256 hash of the public key. This is done to prevent potential errors by systems in the Arweave ecosystem that expect these values to be a certain size and conform to a specific format.Essentially, normalized addresses are a way to represent public keys and wallet addresses from other blockchains in a way that is familiar to systems in the Arweave ecosystem.A tool for easily obtaining a normalized addresses from public keys can be found at ar://normalize-my-key At A Glance  Arweave ETH/POL Solana Native Address 9ODOd-_ZT9oWoRMVmmD4G5f9Z6MjvYxO3Nen-T5OXvU 0x084af408C8E492aC52dc0Ec76514A7deF8D5F03f Cd5yb4mvbuQyyJgAkriFZbWQivh2zM68KGZX8Ksn1L85 base64url Encoded Public Key 0jkGWDFYI3DHEWaXhZitjTg67T-enQwXs50lTDrMhy2qb619_91drv_50J5PwrOYJiMmYhiEA5ojMvrrAFY-Dm1bJbJfVBU1kIsPho2tFcXnbSOa2_1bovAys0ckJU07wkbmIUpzp3trdxYReB4jayMMOXWw9B8xS0v81zFmK3IbCtL9N6WNTMONOSMATHFQrGqtDhDUqKyIsQZCBPFvfGykRWaLWzbtAUrApprqG9hfExQzppNsw0gsftNSHZ1emC5tC2fuib6FhQw9TE2ge9tUjEZNALcVZvopTtTX0H2gEfnRJ48UNeV3SKggjXcoPVeivmqXuPBGncXWWq1pHR-Xs4zSLA5Mgcw_tQJc4FIER0i7hUlZXoc991ZHyOvAC-GlHWzQwvrlY11oD38pB47NkHN2WVPtUCAtyYQe5TE6Xznd9kPgqqvVUkV0s0suh5vINGoiPEnMjyhYEN7eOmJRIJ_A87IJesbdPRV4ZzBsqPbd02RG3ZuVpc3gI1xKvwH1WS05XI8eWK-BbvB3oxB7WjaQTWcfBWhMEULiwx-SucuyAzPAw3i6Wjtq61TcL9SdWhmOf9_yo-Np052tj7MQ66nmgdOH_MEKYjAdFypxTsRQoSLbv28HEcSjwx8u3pY0q0gKMK_5X2XKJrp2i2GB_fVgbcpH9YsgrYxh1Q8 2W5VMzNKYwr51QsiYBHUS5h5wxZf_uBgG7C6xiHgBHwwLUty5LHKFFBDlAxTCTAhglcmys2_HQoOj_LnCkA3 rK8XXxd8JqsZFPXVOwkSWS5Gh1SJzftfCOLpLk4i1FY Normalized Address 9ODOd-_ZT9oWoRMVmmD4G5f9Z6MjvYxO3Nen-T5OXvU 5JtuS4yOFtUX2Rg3UU7AgBaUqh4s8wyyNTZk9UrzI-Q K8kpPM1RID8ZM2sjF5mYy0rP4gXSRDbrwPUd9Qths64 Public Keys and Addresses Crypto wallets consist of two separate components. The public keys, which are public knowledge and can be seen by anyone, and the private keys, which only the owner of a wallet should have access to. Crypto wallet addresses are derived from the public key.Encoded Public Keys It is important to note that all crypto wallet public and private keys are
binary data. The values provided below for Arweave and Ethereum/Polygon public
keys are base64url and hex encoded representations of that binary data
respectively.Arweave The public key for an Arweave wallet is the n field of the JWK json file.0jkGWDFYI3DHEWaXhZitjTg67T-enQwXs50lTDrMhy2qb619_91drv_50J5PwrOYJiMmYhiEA5ojMvrrAFY-Dm1bJbJfVBU1kIsPho2tFcXnbSOa2_1bovAys0ckJU07wkbmIUpzp3trdxYReB4jayMMOXWw9B8xS0v81zFmK3IbCtL9N6WNTMONOSMATHFQrGqtDhDUqKyIsQZCBPFvfGykRWaLWzbtAUrApprqG9hfExQzppNsw0gsftNSHZ1emC5tC2fuib6FhQw9TE2ge9tUjEZNALcVZvopTtTX0H2gEfnRJ48UNeV3SKggjXcoPVeivmqXuPBGncXWWq1pHR-Xs4zSLA5Mgcw_tQJc4FIER0i7hUlZXoc991ZHyOvAC-GlHWzQwvrlY11oD38pB47NkHN2WVPtUCAtyYQe5TE6Xznd9kPgqqvVUkV0s0suh5vINGoiPEnMjyhYEN7eOmJRIJ_A87IJesbdPRV4ZzBsqPbd02RG3ZuVpc3gI1xKvwH1WS05XI8eWK-BbvB3oxB7WjaQTWcfBWhMEULiwx-SucuyAzPAw3i6Wjtq61TcL9SdWhmOf9_yo-Np052tj7MQ66nmgdOH_MEKYjAdFypxTsRQoSLbv28HEcSjwx8u3pY0q0gKMK_5X2XKJrp2i2GB_fVgbcpH9YsgrYxh1Q8 The public wallet address for that wallet is 9ODOd-_ZT9oWoRMVmmD4G5f9Z6MjvYxO3Nen-T5OXvU, this is obtained by decoding the public key from base64url to normalize padding, sha256 hashing the result, and then base64url encoding that.Ethereum/Polygon The public key for an EVM wallet (Ethereum, Polygon/Matic) is derived from its private key, using the Elliptic Curve Digital Signature Algorithm, or ECDSA.0xb5d96e5533334a630af9d50b226011d44b9879c3165ffee0601bb0bac621e0047c302d4b72e4b1ca145043940c53093021825726cacdbf1d0a0e8ff2e70a4037 The public wallet address is 0x084af408C8E492aC52dc0Ec76514A7deF8D5F03f, this is obtained by removing the first byte from the public key, Keccak-256 hashing the remainder, taking the the last 20 bytes (40 hexadecimal characters) and prepending 0x to it.Solana A Solana wallet is an array of 64 bytes. The first 32 bytes are the private key, and the last 32 bytes are the public key. Below is the public key portion of a Solana wallet:[172, 175, 23, 95, 23, 124, 38, 171, 25, 20, 245, 213, 59, 9, 18, 89, 46, 70, 135, 84, 137, 205, 251, 95, 8, 226, 233, 46, 78, 34, 212, 86] The public wallet address for this wallet is Cd5yb4mvbuQyyJgAkriFZbWQivh2zM68KGZX8Ksn1L85, this is derived by base58 encoding the public key bytes.Normalizing Addresses As shown in the above examples, the format of public keys, and the resulting derived wallet addresses, vary widely between blockchains. Arweave manages this by applying the same derivation methods that Arweave uses for its own wallets to the public keys from other chains.Ethereum/Polygon The leading 0x and uncompressed flag 04 (if present) is removed from the public key of an EVM wallet, and then the remainder is base64url encoded to obtain the Arweave normalized public key. Continuing with the same public key in the above example, the normalized public key would be:2W5VMzNKYwr51QsiYBHUS5h5wxZf_uBgG7C6xiHgBHwwLUty5LHKFFBDlAxTCTAhglcmys2_HQoOj_LnCkA3 This value is what is used as the GraphQL tag owner value for data items being uploaded to Arweave using an EVM wallet. The normalized address is then derived from this value by sha256 hashing it, and then base64url encoding the result:5JtuS4yOFtUX2Rg3UU7AgBaUqh4s8wyyNTZk9UrzI-Q Solana The normalized public key for Solana wallets are derived similarly. The 32 byte public key is base64url encoded:rK8XXxd8JqsZFPXVOwkSWS5Gh1SJzftfCOLpLk4i1FY Again, this value is used for the GraphQl tag owner when uploading data. It can then be sha256 hashed, and base64url encoded again to derive the normalized address:K8kpPM1RID8ZM2sjF5mYy0rP4gXSRDbrwPUd9Qths64

---

# 215. removeRecord - ARIO Docs

Document Number: 215
Source: https://docs.ar.io/ar-io-sdk/ants/remove-record
Words: 64
Extraction Method: html

Deprecated This method is deprecated. Please use removeUndernameRecord instead. See the removeUndernameRecord documentation
for more details.removeRecord is a method on the ANT class that removes a record from the ANT process.removeRecord requires authentication.Examples Parameters Parameter Type Description Optional undername string The undername name of the record to remove false tags array An array of GQL tag objects to attach to the AO message true

---

# 216. ARIO Docs

Document Number: 216
Source: https://docs.ar.io/wayfinder/core/gateway-providers/network
Words: 96
Extraction Method: html

NetworkGatewaysProvider Overview The NetworkGatewaysProvider discovers AR.IO gateways from the AR.IO Network using the AR.IO SDK. It provides dynamic access to the full network of verified gateways, making it the recommended choice for production applications.Important To avoid rate limits and improve performance, consider wrapping NetworkGatewaysProvider with SimpleCacheGatewaysProvider (for Node.js/server environments) or LocalStorageGatewaysProvider (for browser environments). This enables caching of gateway lists and reduces unnecessary network requests.Basic Usage Configuration Options NetworkGatewaysProviderOptions Related Documentation Gateway Providers Overview: Compare all gateway providers StaticGatewaysProvider: Static gateway configuration SimpleCacheGatewaysProvider: Caching wrapper Wayfinder Configuration: Main wayfinder setup Routing Strategies: How gateways are selected

---

# 217. ARIO Docs

Document Number: 217
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/get-redelegation-fee
Words: 61
Extraction Method: html

getRedelegationFee getRedelegationFee is a method on the ARIO class that retrieves the redelegation fee rate as a percentage for a specific address. The fee rate ranges from 0% to 60% based on the number of redelegations since the last fee reset.getRedelegationFee does not require authentication.Examples Parameters Parameter Type Description Required address string The wallet address to check for redelegation fees Yes

---

# 218. Module hb_singletonerl - HyperBEAM - Documentation

Document Number: 218
Source: https://hyperbeam.arweave.net/build/devices/source-code/hb_singleton.html
Words: 1095
Extraction Method: html

Module hb_singleton.erl A parser that translates AO-Core HTTP API requests in TABM format
into an ordered list of messages to evaluate.Description The details of this format
are described in docs/ao-core-http-api.md.Syntax overview:Singleton: Message containing keys and a <code>path</code> field,
                  which may also contain a query string of key-value pairs.
       Path:
           - /Part1/Part2/.../PartN/ => [Part1, Part2, ..., PartN]
           - /ID/Part2/.../PartN => [ID, Part2, ..., PartN]
       Part: (Key + Resolution), Device?, #{ K => V}?
           - Part => #{ path => Part }
           - <code>Part&Key=Value => #{ path => Part, Key => Value }</code>
           - <code>Part&Key => #{ path => Part, Key => true }</code>
           - <code>Part&k1=v1&k2=v2 => #{ path => Part, k1 => `<<"v1">></code>, k2 => <code><<"v2">></code> }'
           - <code>Part~Device => {as, Device, #{ path => Part }}</code>
           - <code>Part~D&K1=V1 => {as, D, #{ path => Part, K1 => `<<"v1">></code> }}'
           - <code>pt&k1+int=1 => #{ path => pt, k1 => 1 }</code>
           - <code>pt~d&k1+int=1 => {as, d, #{ path => pt, k1 => 1 }}</code>
           - <code>(/nested/path) => Resolution of the path /nested/path</code>
           - <code>(/nested/path&k1=v1) => (resolve /nested/path)#{k1 => v1}</code>
           - <code>(/nested/path~D&K1=V1) => (resolve /nested/path)#{K1 => V1}</code>
           - <code>pt&k1+res=(/a/b/c) => #{ path => pt, k1 => (resolve /a/b/c) }</code>
       Key:
           - key: <code><<"value">></code> => #{ key => <code><<"value">></code>, ... } for all messages
           - n.key: <code><<"value">></code> => #{ key => <code><<"value">></code>, ... } for Nth message
           - key+int: 1 => #{ key => 1, ... }
           - key+res: /nested/path => #{ key => (resolve /nested/path), ... }
           - N.Key+res=(/a/b/c) => #{ Key => (resolve /a/b/c), ... } Data Types ao_message() ao_message() = map() | binary() tabm_message() tabm_message() = map() Function Index all_path_parts/2* Extract all of the parts from the binary, given (a list of) separators.append_path/2*  apply_types/2* Step 3: Apply types to values and remove specifiers.basic_hashpath_test/0*  basic_hashpath_to_test/0*  build_messages/3* Step 5: Merge the base message with the scoped messages.decode_string/1* Attempt Cowboy URL decode, then sanitize the result.do_build/4*  from/2 Normalize a singleton TABM message into a list of executable AO-Core
messages.group_scoped/2* Step 4: Group headers/query by N-scope.inlined_keys_test/0*  inlined_keys_to_test/0*  maybe_join/2* Join a list of items with a separator, or return the first item if there
is only one item.maybe_subpath/2* Check if the string is a subpath, returning it in parsed form,
or the original string with a specifier.maybe_typed/3* Parse a key's type (applying it to the value) and device name if present.multiple_inlined_keys_test/0*  multiple_inlined_keys_to_test/0*  multiple_messages_test/0*  multiple_messages_to_test/0*  normalize_base/1* Normalize the base path.parse_explicit_message_test/0*  parse_full_path/1* Parse the relative reference into path, query, and fragment.parse_inlined_key_val/2* Extrapolate the inlined key-value pair from a path segment.parse_part/2* Parse a path part into a message or an ID.parse_part_mods/3* Parse part modifiers:
1.parse_scope/1* Get the scope of a key.part/2* Extract the characters from the binary until a separator is found.part/4*  path_messages/2* Step 2: Decode, split and sanitize the path.path_parts/2* Split the path into segments, filtering out empty segments and
segments that are too long.path_parts_test/0*  scoped_key_test/0*  scoped_key_to_test/0*  simple_to_test/0*  single_message_test/0*  subpath_in_inlined_test/0*  subpath_in_inlined_to_test/0*  subpath_in_key_test/0*  subpath_in_key_to_test/0*  subpath_in_path_test/0*  subpath_in_path_to_test/0*  to/1 Convert a list of AO-Core message into TABM message.to_suite_test_/0*  type/1*  typed_key_test/0*  typed_key_to_test/0*  Function Details all_path_parts/2 * all_path_parts(Sep, Bin) -> any() Extract all of the parts from the binary, given (a list of) separators.append_path/2 * append_path(PathPart, Message) -> any() apply_types/2 * apply_types(Msg, Opts) -> any() Step 3: Apply types to values and remove specifiers.basic_hashpath_test/0 * basic_hashpath_test() -> any() basic_hashpath_to_test/0 * basic_hashpath_to_test() -> any() build_messages/3 * build_messages(Msgs, ScopedModifications, Opts) -> any() Step 5: Merge the base message with the scoped messages.decode_string/1 * decode_string(B) -> any() Attempt Cowboy URL decode, then sanitize the result.do_build/4 * do_build(I, Rest, ScopedKeys, Opts) -> any() from/2 from(RawMsg, Opts) -> any() Normalize a singleton TABM message into a list of executable AO-Core
messages.group_scoped/2 * group_scoped(Map, Msgs) -> any() Step 4: Group headers/query by N-scope.N.Key => applies to Nth step. Otherwise => global inlined_keys_test/0 * inlined_keys_test() -> any() inlined_keys_to_test/0 * inlined_keys_to_test() -> any() maybe_join/2 * maybe_join(Items, Sep) -> any() Join a list of items with a separator, or return the first item if there
is only one item. If there are no items, return an empty binary.maybe_subpath/2 * maybe_subpath(Str, Opts) -> any() Check if the string is a subpath, returning it in parsed form,
or the original string with a specifier.maybe_typed/3 * maybe_typed(Key, Value, Opts) -> any() Parse a key's type (applying it to the value) and device name if present.
We allow characters as type indicators because some URL-string encoders
(e.g. Chrome) will encode `+` characters in a form that query-string parsers
interpret as characters.multiple_inlined_keys_test/0 * multiple_inlined_keys_test() -> any() multiple_inlined_keys_to_test/0 * multiple_inlined_keys_to_test() -> any() multiple_messages_test/0 * multiple_messages_test() -> any() multiple_messages_to_test/0 * multiple_messages_to_test() -> any() normalize_base/1 * normalize_base(Rest) -> any() Normalize the base path.parse_explicit_message_test/0 * parse_explicit_message_test() -> any() parse_full_path/1 * parse_full_path(RelativeRef) -> any() Parse the relative reference into path, query, and fragment.parse_inlined_key_val/2 * parse_inlined_key_val(Bin, Opts) -> any() Extrapolate the inlined key-value pair from a path segment. If the
key has a value, it may provide a type (as with typical keys), but if a
value is not provided, it is assumed to be a boolean true.parse_part/2 * parse_part(ID, Opts) -> any() Parse a path part into a message or an ID.
Applies the syntax rules outlined in the module doc, in the following order:
1. ID
2. Part subpath resolutions
3. Inlined key-value pairs
4. Device specifier parse_part_mods/3 * parse_part_mods(X1, Msg, Opts) -> any() Parse part modifiers:
1. ~Device => {as, Device, Msg} 2. &K=V => Msg#{ K => V } parse_scope/1 * parse_scope(KeyBin) -> any() Get the scope of a key. Adds 1 to account for the base message.part/2 * part(Sep, Bin) -> any() Extract the characters from the binary until a separator is found.
The first argument of the function is an explicit separator character, or
a list of separator characters. Returns a tuple with the separator, the
accumulated characters, and the rest of the binary.part/4 * part(Seps, X2, Depth, CurrAcc) -> any() path_messages/2 * path_messages(RawBin, Opts) -> any() Step 2: Decode, split and sanitize the path. Split by / but avoid
subpath components, such that their own path parts are not dissociated from
their parent path.path_parts/2 * path_parts(Sep, PathBin) -> any() Split the path into segments, filtering out empty segments and
segments that are too long.path_parts_test/0 * path_parts_test() -> any() scoped_key_test/0 * scoped_key_test() -> any() scoped_key_to_test/0 * scoped_key_to_test() -> any() simple_to_test/0 * simple_to_test() -> any() single_message_test/0 * single_message_test() -> any() subpath_in_inlined_test/0 * subpath_in_inlined_test() -> any() subpath_in_inlined_to_test/0 * subpath_in_inlined_to_test() -> any() subpath_in_key_test/0 * subpath_in_key_test() -> any() subpath_in_key_to_test/0 * subpath_in_key_to_test() -> any() subpath_in_path_test/0 * subpath_in_path_test() -> any() subpath_in_path_to_test/0 * subpath_in_path_to_test() -> any() to/1 to(Messages::[ao_message()]) -> tabm_message()  Convert a list of AO-Core message into TABM message.to_suite_test_/0 * to_suite_test_() -> any() type/1 * type(Value) -> any() typed_key_test/0 * typed_key_test() -> any() typed_key_to_test/0 * typed_key_to_test() -> any()

---

# 219. ARIO Docs

Document Number: 219
Source: https://docs.ar.io/ar-io-sdk/ants/remove-controller
Words: 60
Extraction Method: html

removeController removeController is a method on the ANT class that removes a specified wallet address from the ANT's list of approved controllers.removeController requires authentication.Examples Parameters Parameter Type Description Optional controller string - WalletAddress The public wallet address of the controller to be removed false tags array An array of GQL tag objects to attach to the transfer AO message true

---

# 220. ARIO Docs

Document Number: 220
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/increase-operator-stake
Words: 60
Extraction Method: html

increaseOperatorStake increaseOperatorStake is a method on the ARIO class that increases the caller's operator stake. This method must be executed with a wallet registered as a gateway operator.increaseOperatorStake requires authentication.Examples Parameters Parameter Type Description Required qty number Amount in mARIO to add to operator stake Yes tags array An array of GQL tag objects to attach to the transaction No

---

# 221. removeUndernameRecord - ARIO Docs

Document Number: 221
Source: https://docs.ar.io/ar-io-sdk/ants/remove-undername-record
Words: 58
Extraction Method: html

removeUndernameRecord is a method on the ANT class that removes a specified undername record from the ANT process. Once removed, the undername will no longer resolve.removeUndernameRecord requires authentication.Examples Parameters Parameter Type Description Optional undername string The undername to remove (e.g., 'dapp') false tags array An array of GQL tag objects to attach to the transfer AO message true

---

# 222. ARIO Docs

Document Number: 222
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/increase-delegate-stake
Words: 58
Extraction Method: html

increaseDelegateStake increaseDelegateStake is a method on the ARIO class that increases the caller's delegated stake on the target gateway.increaseDelegateStake requires authentication.Example Parameters Parameter Type Description Required qty number Amount in mARIO to add to delegated stake Yes target string The gateway's wallet address Yes tags array An array of GQL tag objects to attach to the transaction No

---

# 223. Custom Devices and Codecs  WAO

Document Number: 223
Source: https://docs.wao.eco/hyperbeam/custom-devices-codecs
Words: 1388
Extraction Method: html

We are going to learn the core codecs such as path flattening, HTTP message signatures, and AO types while building a custom HyperBEAM device.Accessible Device Methods If you look into any dev_ prefixed Erlang files under HyperBEAM/src, device methods are defined and exported in method_name/arity format. HyperBEAM automatically routes HTTP requests to device methods defined with arity of 3. So any methods exported as method_name/3 are automatically accessible.For instance, the dev_meta.erl file has these lines, which make /~meta@1.0/info and /~meta@1.0/build accessible via URL endpoints.File  /HyperBEAM/src/dev_meta.erl -export([info/1, info/3, build/3, handle/2, adopt_node_message/2, is/2, is/3]).

 

info(_, Request, NodeMsg) ->

 

build(_, _, _NodeMsg) -> Another example is dev_codec_json.erl with deserialize/3 and serialize/3 exposed.File  /HyperBEAM/src/dev_codec_json.erl -export([deserialize/3, serialize/3]).

 

deserialize(Base, Req, Opts) ->

 

serialize(Base, _Msg, _Opts) -> The device names are defined in hb_opt.erl under preloaded_devices.File  /HyperBEAM/src/hb_opts.erl preloaded_devices => [

  ...

  #{<<"name">> => <<"json@1.0">>, <<"module">> => dev_codec_json},

  ...

  #{<<"name">> => <<"meta@1.0">>, <<"module">> => dev_meta},

  ...

],This is exactly how you can build your own custom devices.Building Custom Devices Create dev_mydev.erl under /HyperBEAM/src, and define the info/3 method.The following is the minimum viable HyperBEAM device implementation.File  /HyperBEAM/src/dev_mydev.erl -module(dev_mydev).

-export([ info/3 ]).

-include_lib("eunit/include/eunit.hrl").

-include("include/hb.hrl").

 

info(Msg1, Msg2_, Opts) ->

    {ok, #{ <<"version">> => <<"1.0">> }}.Also, add the device to preloaded_devices in hb_opts.erl.File  /HyperBEAM/src/hb_opts.erl preloaded_devices => [

  ...

  #{<<"name">> => <<"json@1.0">>, <<"module">> => dev_codec_json},

  ...

  #{<<"name">> => <<"meta@1.0">>, <<"module">> => dev_meta},

  ...

  #{<<"name">> => <<"mydev@1.0">>, <<"module">> => dev_mydev}

],Now you can test your device using WAO. Don't forget to preload your mydev device.WAO can handle what we are going to learn and decode through the next few chapters, but we will intentionally work with the standard HTTP headers and body.Let's look into the returned headers. You receive what you return from the info method, but it also comes with extra metadata since it's just an HTTP message and goes through a pipeline of data mutation before it gets back to you.The following is what you would get in the response headers.Erlang <-> JSON One way to purify the return value is to return stringified JSON using the json@1.0 device internally. The device has a dev_codec_json:to/1 method to convert an Erlang object to JSON.The internal device names are defined with preloaded_devices in hb_opts.erl.#{<<"name">> => <<"json@1.0">>, <<"module">> => dev_codec_json} And you can internally execute all exposed methods.File  /HyperBEAM/src/dev_codec_json.erl -module(dev_codec_json).

-export([to/1, from/1, commit/3, verify/3, committed/1, content_type/1]).

-export([deserialize/3, serialize/3]).When you define a new method, don't forget to export it from mydev@1.0.From this point on, we'll always assume you're correctly exporting new methods.File  /HyperBEAM/src/dev_mydev.erl -export([ info_json/3 ]).

 

info_json(Msg1, Msg2_, Opts) ->

    JSON = dev_codec_json:to(#{ <<"version">> => <<"1.0">> }),

    {ok, JSON}.Now stringified JSON is returned in body, and you can just JSON.parse(body) to get exactly what you return.File  /test/custom-devices-codecs.test.js const { body } = await hb.get({ path: "/~mydev@1.0/info_json" })

const json = JSON.parse(body)

assert.deepEqual(json, { version: "1.0" }) You can also pass JSON using dev_codec_json:from/1, too. Let's create an hello/3 method to convert stringified JSON in body to an Erlang object, add Hello to the name field, and return it as JSON again.File  /HyperBEAM/src/dev_mydev.erl -export([ hello/3 ]).

 

hello(Msg1, Msg2_, Opts) ->

    Body = maps:get(<<"body">>, Msg1),

    OBJ = dev_codec_json:from(Body),

    Name = maps:get(<<"name">>, OBJ),

    Hello = <<<<"Hello, ">>/binary, Name/binary, <<"!">>/binary>>,

    JSON = dev_codec_json:to(#{ <<"hello">> => Hello }),

    {ok, JSON}.When you send data in body, you need the POST method.File  /test/custom-devices-codecs.test.js const { body } = await hb.post({

  path: "/~mydev@1.0/hello",

  body: JSON.stringify({ name: "Wao" }),

})

const { hello } = JSON.parse(body)

assert.equal(hello, "Hello, Wao!") An HTTP message consists of headers and body, and AO Core and HyperBEAM utilize both of them in messages. GET method doesn't have body and headers only support flat string values. So when you need to send a message with complex data, you almost always need POST. The AO codecs with flat, structured, and httpsig devices exist to circumvent these HTTP header limitations.Device Methods You can create any arbitrary methods in a device, but methods to expose via URL endpoints need to be in a specific format.Minimum Viable URL Exposed Method method(Msg1, Msg2_, Opts) ->

  % write method logic here

  {ok, Ret}.Let's create a forward method to just forward Msg1, Msg2, and Opts in JSON format and examine what they are. We filter out private keys from Opts with hb_private:reset as it contains sensitive data and incompatible data types to convert to JSON. We can also log these objects with io:format.File  /HyperBEAM/src/dev_mydev.erl -export([ forward/3 ]).

 

forward(Msg1, Msg2, Opts) ->

  io:format("Msg1: ~p~n~nMsg2: ~p~n~nOpts: ~p~n", [Msg1, Msg2, Opts]),

  JSON = dev_codec_json:to(#{

    <<"msg1">> => Msg1,

    <<"msg2">> => Msg2,

    <<"opts">> => hb_private:reset(Opts)

  }),

  {ok, JSON}.Let's send something very simple with GET. You can only send string parameters with GET.File  /test/custom-devices-codecs.test.js const { body } = await hb.get({ path: "/~mydev@1.0/forward", key: "abc" })

console.log(JSON.parse(body)) This is the response.JSON Response {

  "msg1": {

    "accept": "*/*",

    "accept-encoding": "gzip, deflate",

    "accept-language": "*",

    "connection": "keep-alive",

    "device": "mydev@1.0",

    "host": "localhost:10001",

    "key": "abc",

    "method": "GET",

    "sec-fetch-mode": "cors",

    "user-agent": "node"

  },

  "msg2": {

    "accept": "*/*",

    "accept-encoding": "gzip, deflate",

    "accept-language": "*",

    "commitments": {

      "jNI0FLgi9Lz2UT_l1sK2TCPZMCIwFpPcOK3e3cqdRwo": {

        "alg": "hmac-sha256",

        "commitment-device": "httpsig@1.0"

      }

    },

    "connection": "keep-alive",

    "host": "localhost:10001",

    "key": "abc",

    "method": "GET",

    "path": "forward",

    "sec-fetch-mode": "cors",

    "user-agent": "node"

  },

  "opts": {

    "mode": "debug",

    "hb_config_location": "config.flat",

    "http_server": "Tbun4iRRQW93gUiSAmTmZJ2PGI-_yYaXsX69ETgzSRE",

    ...

  }

} You can tell opts is the node configuration, and msg1 and msg2 are very similar except that msg1 has device, and msg2 has commitments and path. They both have key="abc", which is what we sent as a parameter. Other fields in msg1 and msg2 are the same metadata about the HTTP protocol.We can strip the metadata down to these minimum differences.{

  "msg1": {

    "device": "mydev@1.0",

    "key": "abc",

  },

  "msg2": {

    "commitments": {

      "jNI0FLgi9Lz2UT_l1sK2TCPZMCIwFpPcOK3e3cqdRwo": {

        "alg": "hmac-sha256",

        "commitment-device": "httpsig@1.0"

      }

    },

    "key": "abc",

    "path": "forward",

  }

} Things will be clearer when we send complex data with POST.File  /test/custom-devices-codecs.test.js const { out } = await hb.post({

  path: "/~wao@1.0/forward",

  key: "abc",

  list: [1, 2, 3],

  map: { abc: "123" },

  bool: true,

  body: "test_body",

})

console.log(JSON.parse(out)) And this is the response.JSON Response {

  "msg1": {

    "body": "test_body",

    "bool": "\"true\"",

    "content-length": "253",

    "content-type": "multipart/form-data; boundary=\"eyja4UA4reu5SLEKVqY67NG8Q-jMdqEFmleD-hhSJKM\"",

    "device": "mydev@1.0",

    "key": "abc",

    "list": "\"(ao-type-integer) 1\", \"(ao-type-integer) 2\", \"(ao-type-integer) 3\"",

    "map": { "abc": "123" },

    "method": "POST"

  },

  "msg2": {

    "body": "test_body",

    "bool": true,

    "commitments": {

      "ovgxZflZZcY_kXWpV6yWf_ilaGuMDh7PUGC1YNhJQ90": {

        "alg": "hmac-sha256",

        "commitment-device": "httpsig@1.0",

        "signature": "http-sig-bba7e22451416f77=:kqtatfcnCrmmJiEc1GT3hKKU6tUVRy34hDN6z1vN5UHCwNZ4f+tu9FafZ/mOx8loq11DgdV8S7Xvxk5LzytMAtV1SmAArEQ1VbMJkS+bIiNksd4qmU13JkQjz+a90FYVUDKn0uU+cRUDx+7wVh4Rco27WEBj/E5yVreKcmG0fpORHi4DMV219cb0zUAdDEqY/FdvdlC+Xr91xQzDwcwS8goeHS879P6FLRo5BubLWx/bbJXoS2BEGowkWAORP1jooWe+oNIcWbMWA1CUpPTih2VXbUQcdRto5DDjwXw90nxD3UVPLegweGOZrASuccG9oFg/++mJeFFz3W6cy3Eg84WmrMjfbzsUb6WVcKti83YZYTo7onUDwad2wVUe2WDCuMLm8TFhwP8zwU/MHSfcahRnZasnroPwxvYRjFNWa5USyqGaZ7uM/wqArGKL/2dlh3bIphEmTjtYBc9q2tlosNgPngwnPu6qbEFQpcDEzsODQQBYrnP9HA6HIqsC+dWPINw0xueFqnhu5bv3Y+Y17vFc4zOraBpVCZUEMKEDPgNHe2vMjFVfgIbKp9I9Xu+8vYd8sL+2p+lgkrXVjNCS07XYjVHj855GKCmIZHs9fZa0dfLghOdDfnbexzCpSDIVnfstZx5yniXh00Rk3g2wUAWB7Sq7nRG86BiOQP+EcHY=:",

        "signature-input": "http-sig-bba7e22451416f77=(\"key\" \"list\" \"bool\" \"ao-types\" \"content-type\" \"content-digest\" \"content-length\");alg=\"rsa-pss-sha512\";keyid=\"o1kvTqZQ0wbS_WkdwX70TFCk7UF76ldnJ85l8iRV7t6mSlzkXBYCecb-8RXsNEQQmO0KergtHOvhuBJmB6YXaYe_UftI_gendojfIa6jlTgw-qmH6g4_oErI8djDRbQSm-5nCfGVRuYxsNZLYDeqw4gFb9K3b1h7tuMoLd6-d5pkaLfTMUNcvs2OqpkLo0i_av746FieaURdWozwFqO0APtdA7pLHDqQZDMNdTmsUBJFszL6SOa1bKe5cUWnrq4uaW4NAN3JAQniILKGsKZENeKtfXwiKVaFJtriWWsbhOaNT0JLcuBAwXQAP59RXzcr8bRY6XFn8zBmEmZBGszOD9c9ssDENRFDa5uyVhk8XgIgQjErAWYd9T6edrYcIp3R78jhNK_nLiIBBz8_Oz3bLjL5i_aiV2gpfIbd44DCHihuuxSWRAPJxhEy9TS0_QbVOIWhcDTIeEJE3aRPTwSTMt1_Fec7i9HJWN0mvMbAAJw8k6HxjA3pFZiCowZJw7FBwMAeYgEwIeB82f-S2-PtFLwR9i0tExo36hEBHqaS4Y-O3NGgQ8mKnhT7Z1EfxEbA2BpR9oL8rJFEnPIrHHu7B88OHDDfnfRD3D79fKktnisC7XOuwbHG3TQo0_j4_mElH7xj_7IyAbmCUHDd-eRa482wOYXBB01DGnad901qaHU\""

      },

      "we4Z3weGpJUUEwgeWmkIQsRJBTCfaB1s75LfgudSC1I": {

        "alg": "rsa-pss-sha512",

        "commitment-device": "httpsig@1.0",

        "committer": "Tbun4iRRQW93gUiSAmTmZJ2PGI-_yYaXsX69ETgzSRE",

        "signature": "http-sig-bba7e22451416f77=:kqtatfcnCrmmJiEc1GT3hKKU6tUVRy34hDN6z1vN5UHCwNZ4f+tu9FafZ/mOx8loq11DgdV8S7Xvxk5LzytMAtV1SmAArEQ1VbMJkS+bIiNksd4qmU13JkQjz+a90FYVUDKn0uU+cRUDx+7wVh4Rco27WEBj/E5yVreKcmG0fpORHi4DMV219cb0zUAdDEqY/FdvdlC+Xr91xQzDwcwS8goeHS879P6FLRo5BubLWx/bbJXoS2BEGowkWAORP1jooWe+oNIcWbMWA1CUpPTih2VXbUQcdRto5DDjwXw90nxD3UVPLegweGOZrASuccG9oFg/++mJeFFz3W6cy3Eg84WmrMjfbzsUb6WVcKti83YZYTo7onUDwad2wVUe2WDCuMLm8TFhwP8zwU/MHSfcahRnZasnroPwxvYRjFNWa5USyqGaZ7uM/wqArGKL/2dlh3bIphEmTjtYBc9q2tlosNgPngwnPu6qbEFQpcDEzsODQQBYrnP9HA6HIqsC+dWPINw0xueFqnhu5bv3Y+Y17vFc4zOraBpVCZUEMKEDPgNHe2vMjFVfgIbKp9I9Xu+8vYd8sL+2p+lgkrXVjNCS07XYjVHj855GKCmIZHs9fZa0dfLghOdDfnbexzCpSDIVnfstZx5yniXh00Rk3g2wUAWB7Sq7nRG86BiOQP+EcHY=:",

        "signature-input": "http-sig-bba7e22451416f77=(\"key\" \"list\" \"bool\" \"ao-types\" \"content-type\" \"content-digest\" \"content-length\");alg=\"rsa-pss-sha512\";keyid=\"o1kvTqZQ0wbS_WkdwX70TFCk7UF76ldnJ85l8iRV7t6mSlzkXBYCecb-8RXsNEQQmO0KergtHOvhuBJmB6YXaYe_UftI_gendojfIa6jlTgw-qmH6g4_oErI8djDRbQSm-5nCfGVRuYxsNZLYDeqw4gFb9K3b1h7tuMoLd6-d5pkaLfTMUNcvs2OqpkLo0i_av746FieaURdWozwFqO0APtdA7pLHDqQZDMNdTmsUBJFszL6SOa1bKe5cUWnrq4uaW4NAN3JAQniILKGsKZENeKtfXwiKVaFJtriWWsbhOaNT0JLcuBAwXQAP59RXzcr8bRY6XFn8zBmEmZBGszOD9c9ssDENRFDa5uyVhk8XgIgQjErAWYd9T6edrYcIp3R78jhNK_nLiIBBz8_Oz3bLjL5i_aiV2gpfIbd44DCHihuuxSWRAPJxhEy9TS0_QbVOIWhcDTIeEJE3aRPTwSTMt1_Fec7i9HJWN0mvMbAAJw8k6HxjA3pFZiCowZJw7FBwMAeYgEwIeB82f-S2-PtFLwR9i0tExo36hEBHqaS4Y-O3NGgQ8mKnhT7Z1EfxEbA2BpR9oL8rJFEnPIrHHu7B88OHDDfnfRD3D79fKktnisC7XOuwbHG3TQo0_j4_mElH7xj_7IyAbmCUHDd-eRa482wOYXBB01DGnad901qaHU\""

      }

    },

    "content-length": "253",

    "content-type": "multipart/form-data; boundary=\"eyja4UA4reu5SLEKVqY67NG8Q-jMdqEFmleD-hhSJKM\"",

    "key": "abc",

    "list": [ 1, 2, 3 ],

    "map": {

      "abc": "123"

    },

    "method": "POST",

    "path": "forward"

  },

  "opts": {

    "mode": "debug",

    "store": {

      "prefix": "cache-mainnet",

      "store-module": "hb_store_fs"

    },

    "hb_config_location": "config.flat",

    ...

  }

} TABM (Type Annotated Binary Message) We'll explain the commitments later, but now, if you strip down the 2 msgs:{

 "msg1": {

    "body": "test_body",

    "bool": "\"true\"",

    "device": "wao@1.0",

    "key": "abc",

    "list": "\"(ao-type-integer) 1\", \"(ao-type-integer) 2\", \"(ao-type-integer) 3\"",

    "map": { "abc": "123" },

    "method": "POST",

    "num": "123"

  },

  "msg2": {

    "body": "test_body",

    "bool": true,

    "key": "abc",

    "list": [1, 2, 3],

    "map": { "abc": "123" },

    "method": "POST",

    "num": 123,

    "path": "forward"

  }

} You can observe that bool, list, and num are encoded in some string form. The msg1 fields are encoded by structured@1.0 and msg2 fields are decoded. FYI, msg1 is also different from the form we sent; msg1 is already decoded by httpsig@1.0 device.The msg1 object type is called TABM (Type Annotated Binary Message) and this is what HyperBEAM internally uses to circumvent the limitation that we can only pass flattened strings in the HTTP headers.Encoding / Decoding Steps So a client encodes a message with httpsig@1.0, then structured@1.0 into TABM, then signs it with http-message-signatures, then sends it to a HyperBEAM node.DATA = { path: "/~wao@1.0/forward", key: "abc" } TABM = encode_by_structured(DATA) HTTP_MSG = encode_by_httpsig(TABM) = Headers + Body SIGNED_HTTP_MSG = sign(HTTP_MSG) send(SIGNED_HTTP_MSG) The node receives it, verifies the signature, then decodes it first with structured@1.0 (msg1), then with httpsig@1.0 (msg2).Msg0 + Commitments = dev_codec_httpsig:verify(SIGNED_HTTP_MSG) Msg1(TABM) = dev_codec_httpsig:to(Msg0) + Device Msg2 = dev_codec_structured:to(Msg1) + Commitments + Path flat@1.0 is used to flatten and unflatten nested object paths in the httpsig@1.0 device since HTTP headers and body can only handle string values, not nested structures.Running Tests You can find the working test file for this chapter here:custom-devices-codecs.test.js Run tests:Terminal   Terminal yarn test test/custom-devices-codecs.test.js Reference General Extending HyperBEAM with Devices Device Docs Device: ~json@1.0 Device API dev_codec_json.erl WAO API HyperBEAM Class API HB Class API

---

# 224. ARIO Docs

Document Number: 224
Source: https://docs.ar.io/ar-io-sdk/ario/primary-names/get-primary-name
Words: 56
Extraction Method: html

getPrimaryName getPrimaryName is a method on the ARIO class that retrieves the primary name for a given name or address.getPrimaryName does not require authentication.Examples Parameters Parameter Type Description Optional name string The ArNS name for which to receive primary name data.true address string - WalletAddress The public wallet address for which to receive primary name data.true

---

# 225. setKeywords - ARIO Docs

Document Number: 225
Source: https://docs.ar.io/ar-io-sdk/ants/set-keywords
Words: 55
Extraction Method: html

setKeywords is a method on the ANT class that updates the list of keywords associated with the ANT process.setKeywords requires authentication.Examples Parameters Parameter Type Description Optional keywords array An array of keywords to associate with the ANT process false tags array An array of GQL tag objects to attach to the transfer AO message true

---

# 226. transfer - ARIO Docs

Document Number: 226
Source: https://docs.ar.io/ar-io-sdk/ants/transfer
Words: 54
Extraction Method: html

transfer is a method on the ANT class that transfers ownership of the ANT process to another wallet address.transfer requires authentication.Examples Parameters Parameter Type Description Optional target string - WalletAddress The wallet address to transfer ownership to false tags array An array of GQL tag objects to attach to the transfer AO message true

---

# 227. HTTP Message Signatures  WAO

Document Number: 227
Source: https://docs.wao.eco/hyperbeam/http-message-signatures
Words: 803
Extraction Method: html

So far, we've been virtually testing the encoding process by exposing internal codec methods and sending JSON stringified messages. But in practice, these methods are not available and we need to sign the httpsig encoded message before sending it to a remote node. AO Core / HyperBEAM uses the web standard protocol of HTTP Message Signatures (RFC-9421).Let's go back to the message tested in an earlier chapter and encode it with the pipeline from the previous chapter.File  /test/http-message-signatures.test.js const msg = {

  path: "/~mydev@1.0/forward",

  key: "abc",

  list: [1, 2, 3],

  map: { abc: "123" },

  bool: true,

  body: "test_body",

}

const res = await hb.post({

  path: "/~mydev@1.0/structured_from",

  body: JSON.stringify(msg),

})

const structured = JSON.parse(res.body)

console.log(structured)

const res2 = await hb.post({

  path: "/~mydev@1.0/httpsig_to",

  body: JSON.stringify(structured),

})

const encoded = JSON.parse(res2.body)

console.log(encoded) This is the encoded message:{

  'ao-types': 'bool="atom", list="list"',

  body: '--x8jUsRrtoRCInzE6Nwgl_uoK-D2Oe-9i_RKeFskZk8c\r\n' +

    'content-disposition: inline\r\n' +

    '\r\n' +

    'test_body\r\n' +

    '--x8jUsRrtoRCInzE6Nwgl_uoK-D2Oe-9i_RKeFskZk8c\r\n' +

    'abc: 123\r\n' +

    'content-disposition: form-data;name="map"\r\n' +

    '--x8jUsRrtoRCInzE6Nwgl_uoK-D2Oe-9i_RKeFskZk8c--',

  'body-keys': '"body", "map"',

  bool: '"true"',

  'content-digest': 'sha-256=:yrY11i+3uYmjCLzOaOeIijrNL/dyPWHNHJTJwvsKvsc=:',

  'content-type': 'multipart/form-data; boundary="x8jUsRrtoRCInzE6Nwgl_uoK-D2Oe-9i_RKeFskZk8c"',

  key: 'abc',

  list: '"(ao-type-integer) 1", "(ao-type-integer) 2", "(ao-type-integer) 3"',

  path: '/~mydev@1.0/forward'

} You can sign it with hb.signEncoded.File  /test/http-message-signatures.test.js const signed = await hb.signEncoded(encoded) This is the signed message:The signing added signature and signature-input to headers. Let's break down signature-input.You can also verify the signature with hbsig, which gives you the decomposition of the message if you ever need it.File  /test/http-message-signatures.test.js import { verify } from "hbsig"

const { 

  valid, // should be true

  verified,

  signatureName, 

  keyId, 

  algorithm, 

  decodedSignatureInput : { components, params: { alg, keyid, tag }, raw }

} = await verify(signed) signatureName: http-sig-bba7e22451416f77 components: ("ao-types" "bool" "content-digest" "content-type" "key" "list" "content-length") alg: rsa-pss-sha512 keyid: o1kvTqZQ0wbS_WkdwX70TFCk7UF76ldnJ85l8iRV7t6mSlzkXBYCecb... The signer public key = jwk.n tag: hashpath (missing unless messages are chained) So the message is signed by the private key paired with keyid using the rsa-pss-sha512 method, and the signed fields must exist in the HTTP headers.Keys prefixed with @ get special treatment. For example, HyperBEAM uses the path field from headers for @path. We're not including @path on purpose for now since there's a discrepancy between the RFC-9421 spec and how HyperBEAM handles path, which produces signature verification errors. To be specific, the spec requires a full path with a leading /, but HyperBEAM strips off the slash and the device name from the path. We're currently investigating this issue.HyperBEAM also resolves the method and device to route the message to using the path field in the HTTP headers. So you should still send path even if you're not signing it. That's how the WAO signer handles it for now.@ prefixed keys with special behaviors are the following:@path @query @query-param @scheme @request-target @authority @target-uri @method You can now send the signed message with hb.send. We're sending it to /~mydev@1.0/forward.File  /test/http-message-signatures.test.js const { body } = await hb.send(signed)

const { msg1, msg2, opts } = JSON.parse(body)

console.log(msg2) Let's see what we got back in msg2:{

body: 'test_body',

  bool: true,

  commitments: {

    Fg0kC92eBhUH3t894aH0IHMBiGIlLU80gt5Zjyip_bU: {

      alg: 'rsa-pss-sha512',

      'commitment-device': 'httpsig@1.0',

      committer: 'Tbun4iRRQW93gUiSAmTmZJ2PGI-_yYaXsX69ETgzSRE',

      signature: 'http-sig-bba7e22451416f77=:INJ6uxI4aLR0YB6raM1SWkeQvPKld+0sFVSBdj7P32+FgJ8QJyDHumYs4HK18JNs/CnG7zfn4pV9gMI2Ce9AklqbIflYCrjfIL00FCtqJ5Q4icKLe9/3XawNCtw3LNr9gFaXyBHzGdV0TaKYPemp88IFuYJ75Ins9IblOBIDXKSB4al+WySbnWBzy7uMyP2mt7L8jBv8J/q8N5YoWTIlebcfasjECXeDs+bRE9idcjn5zi74JdDgwQGNV7nujwwtJm4eh+WiUQsbOVsPWtAH/DDiiXOW5GTPimQBRcMvDir06YVasioIu0zarcdyPq5p3+pTJB4Q8AvrUSrqGqTu9RGlzzs6Hsbydy+9FsWo4vyZqQWxcMx1JPlpyl32+GH9SttEHG89OV1PKzo7sCmGUKhSIoIp05NplA/mOJxGnJfu3lYXkEf26U4qLmACk6fYABwtMypCnjckKTH/xFcZ81V2KQ1qBm1M3OnRysBWKNyWFiqwxQ4x0EfqQEl+xz3+Bb1JBR5f+DTSHHlbX2NyTimjR0LHryCmNG4jKrYeNYz5qt4BwBr/cE7DSVpnrFfk3f7SOai6/KkIdg+QrcHfI6U5I0K72Wg4FKqa0WUKMaO08OfV03QllFu1OlggrmiRYVw5n8CgDmq2SBiqyua/zBJeHzItaQ1nXM1ZGDBal2I=:',

      'signature-input': 'http-sig-bba7e22451416f77=("ao-types" "bool" "content-digest" "content-type" "key" "list" "content-length");alg="rsa-pss-sha512";keyid="o1kvTqZQ0wbS_WkdwX70TFCk7UF76ldnJ85l8iRV7t6mSlzkXBYCecb-8RXsNEQQmO0KergtHOvhuBJmB6YXaYe_UftI_gendojfIa6jlTgw-qmH6g4_oErI8djDRbQSm-5nCfGVRuYxsNZLYDeqw4gFb9K3b1h7tuMoLd6-d5pkaLfTMUNcvs2OqpkLo0i_av746FieaURdWozwFqO0APtdA7pLHDqQZDMNdTmsUBJFszL6SOa1bKe5cUWnrq4uaW4NAN3JAQniILKGsKZENeKtfXwiKVaFJtriWWsbhOaNT0JLcuBAwXQAP59RXzcr8bRY6XFn8zBmEmZBGszOD9c9ssDENRFDa5uyVhk8XgIgQjErAWYd9T6edrYcIp3R78jhNK_nLiIBBz8_Oz3bLjL5i_aiV2gpfIbd44DCHihuuxSWRAPJxhEy9TS0_QbVOIWhcDTIeEJE3aRPTwSTMt1_Fec7i9HJWN0mvMbAAJw8k6HxjA3pFZiCowZJw7FBwMAeYgEwIeB82f-S2-PtFLwR9i0tExo36hEBHqaS4Y-O3NGgQ8mKnhT7Z1EfxEbA2BpR9oL8rJFEnPIrHHu7B88OHDDfnfRD3D79fKktnisC7XOuwbHG3TQo0_j4_mElH7xj_7IyAbmCUHDd-eRa482wOYXBB01DGnad901qaHU"'

    },

    RivrHRmpfYVEIH45TxQdW99NR34IgEcStLm467eea38: {

      alg: 'hmac-sha256',

      'commitment-device': 'httpsig@1.0',

      signature: 'http-sig-bba7e22451416f77=:INJ6uxI4aLR0YB6raM1SWkeQvPKld+0sFVSBdj7P32+FgJ8QJyDHumYs4HK18JNs/CnG7zfn4pV9gMI2Ce9AklqbIflYCrjfIL00FCtqJ5Q4icKLe9/3XawNCtw3LNr9gFaXyBHzGdV0TaKYPemp88IFuYJ75Ins9IblOBIDXKSB4al+WySbnWBzy7uMyP2mt7L8jBv8J/q8N5YoWTIlebcfasjECXeDs+bRE9idcjn5zi74JdDgwQGNV7nujwwtJm4eh+WiUQsbOVsPWtAH/DDiiXOW5GTPimQBRcMvDir06YVasioIu0zarcdyPq5p3+pTJB4Q8AvrUSrqGqTu9RGlzzs6Hsbydy+9FsWo4vyZqQWxcMx1JPlpyl32+GH9SttEHG89OV1PKzo7sCmGUKhSIoIp05NplA/mOJxGnJfu3lYXkEf26U4qLmACk6fYABwtMypCnjckKTH/xFcZ81V2KQ1qBm1M3OnRysBWKNyWFiqwxQ4x0EfqQEl+xz3+Bb1JBR5f+DTSHHlbX2NyTimjR0LHryCmNG4jKrYeNYz5qt4BwBr/cE7DSVpnrFfk3f7SOai6/KkIdg+QrcHfI6U5I0K72Wg4FKqa0WUKMaO08OfV03QllFu1OlggrmiRYVw5n8CgDmq2SBiqyua/zBJeHzItaQ1nXM1ZGDBal2I=:',

      'signature-input': 'http-sig-bba7e22451416f77=("ao-types" "bool" "content-digest" "content-type" "key" "list" "content-length");alg="rsa-pss-sha512";keyid="o1kvTqZQ0wbS_WkdwX70TFCk7UF76ldnJ85l8iRV7t6mSlzkXBYCecb-8RXsNEQQmO0KergtHOvhuBJmB6YXaYe_UftI_gendojfIa6jlTgw-qmH6g4_oErI8djDRbQSm-5nCfGVRuYxsNZLYDeqw4gFb9K3b1h7tuMoLd6-d5pkaLfTMUNcvs2OqpkLo0i_av746FieaURdWozwFqO0APtdA7pLHDqQZDMNdTmsUBJFszL6SOa1bKe5cUWnrq4uaW4NAN3JAQniILKGsKZENeKtfXwiKVaFJtriWWsbhOaNT0JLcuBAwXQAP59RXzcr8bRY6XFn8zBmEmZBGszOD9c9ssDENRFDa5uyVhk8XgIgQjErAWYd9T6edrYcIp3R78jhNK_nLiIBBz8_Oz3bLjL5i_aiV2gpfIbd44DCHihuuxSWRAPJxhEy9TS0_QbVOIWhcDTIeEJE3aRPTwSTMt1_Fec7i9HJWN0mvMbAAJw8k6HxjA3pFZiCowZJw7FBwMAeYgEwIeB82f-S2-PtFLwR9i0tExo36hEBHqaS4Y-O3NGgQ8mKnhT7Z1EfxEbA2BpR9oL8rJFEnPIrHHu7B88OHDDfnfRD3D79fKktnisC7XOuwbHG3TQo0_j4_mElH7xj_7IyAbmCUHDd-eRa482wOYXBB01DGnad901qaHU"'

    }

  },

  'content-length': '236',

  'content-type': 'multipart/form-data; boundary="x8jUsRrtoRCInzE6Nwgl_uoK-D2Oe-9i_RKeFskZk8c"',

  key: 'abc',

  list: [ 1, 2, 3 ],

  map: { abc: '123' },

  method: 'POST',

  path: 'forward'

} Your signature is in the commitments. And there are 2 entries with different alg.Fg0kC92eBhUH3t894aH0IHMBiGIlLU80gt5Zjyip_bU: rsa-pss-sha512 RivrHRmpfYVEIH45TxQdW99NR34IgEcStLm467eea38: hmac-sha256 These commitment IDs are important for HyperBEAM to verify the message. The former is the sha-256 hash of the signature bytes, and the latter is the hmac-sha256 hash of the signed content with ao as the key. You can generate each ID with rsaid and hmacid methods from wao/utils. You can execute node internal scripts by manually creating commitments. We'll discuss this in a later chapter.FYI, you can use hb_message:commit to sign a message on HyperBEAM.% get operator wallet

Wallet = hb_opts:get(priv_wallet, not_found, Opts),

Signed = hb_message:commit( Msg, Wallet ),WAO HB SDK In practice, you don't have to go through all these steps to construct signed messages. hb.post handles everything for you. You can get the decoded message in out instead of headers and body.File  /test/http-message-signatures.test.js const { out } = await hb.post({

  path: "/~mydev@1.0/forward",

  key: "abc",

  list: [1, 2, 3],

  map: { abc: "123" },

  bool: true,

  body: "test_body",

}) get and g work the same.This is all you need to encode, sign, and send a message.p is a shortcut method for post to get only the decoded message.File  /test/http-message-signatures.test.js const out = await hb.p("/~mydev@1.0/forward", {

  key: "abc",

  list: [1, 2, 3],

  map: { abc: "123" },

  bool: true,

  body: "test_body",

}) Running Tests You can find the working test file for this chapter here:http-message-signatures.test.js Run tests:Terminal   Terminal yarn test test/http-message-signatures.test.js References Specs HTTP Message Signatures [RFC-9421] WAO API HyperBEAM Class API HB Class API HBSig API

---

# 228. Devices and Pathing  WAO

Document Number: 228
Source: https://docs.wao.eco/hyperbeam/devices-pathing
Words: 1459
Extraction Method: html

The first thing to understand is that HyperBEAM consists of a collection of devices, and you can access specific methods on specific devices via URL endpoints.For instance, the meta@1.0 device lets you get and set node configurations with its info method.Let's get the info with bare JS fetch. http://localhost:10001 is the default hostname when running a node with WAO.The minimum viable devices to run a HyperBEAM node are flat@1.0, httpsig@1.0, structured@1.0, json@1.0, and meta@1.0. These are codec devices except for meta@1.0, which handles node configuration and is the starting point of device resolution in the system. To gain deep understanding of AO-Core and HyperBEAM, we should understand these codecs first.You can pass these device names to the WAO HyperBEAM class to preload only specific devices.File  /test/devices-pathing.test.js import assert from "assert"

import { describe, it, before, after } from "node:test"

import { HyperBEAM } from "wao/test"

 

const devices = ["json", "structured", "httpsig", "flat", "meta"]

 

describe("HyperBEAM", function () {

  let hbeam, hb

  before(async () => {

    hbeam = await new HyperBEAM({ cwd, devices, reset: true }).ready()

    hb = hbeam.hb

  })

  after(async () => hbeam.kill())

 

  it("should run a test case", async () => {

    /* write your test here */

  })

}) HyperBEAM Responses From this point on, we'll assume you're writing any scripts within a test case code block.You can execute a method on a device by NODE_URL/~DEVICE/METHOD. Don't forget the ~ before the device name. So to execute info on the meta@1.0 device on a node running at http://localhost:10001, the URL will be http://localhost:10001/~meta@1.0/info.This is the headers you get:HTTP Headers Headers {

  host: 'localhost',

  debug_print_indent: '2',

  mode: '"debug"',

  load_remote_devices: '"false"',

  port: '10001',

  debug_committers: '"false"',

  commitment_device: 'httpsig@1.0',

  ans104_trust_gql: '"true"',

  'access-control-allow-methods': 'GET, POST, PUT, DELETE, OPTIONS',

  store_all_signed: '"true"',

  wasm_allow_aot: '"false"',

  debug_stack_depth: '40',

  relay_http_client: '"httpc"',

  debug_ids: '"true"',

  'transfer-encoding': 'chunked',

  debug_print_binary_max: '60',

  http_server: 'Tbun4iRRQW93gUiSAmTmZJ2PGI-_yYaXsX69ETgzSRE',

  signature: 'http-sig-bba7e22451416f77=:Cmv+kNvUX7qNHwpeW/bCZ98V3LO95fjio3lS1JnzsIN1nqSu+yUm5DR1+hOtNyYU1fuGirfN4zYXgK9CCTEj/Cbc+MBdK24DpCarng2LK+fbuPPc/QFR8posZDbYiOxuokzH/qpwSpcH5ctf9Ss0NbDTv27vKJkZnQHa1bRIk3Qh7GjXUxaXdRzTWbrxbOFno9CTEBH1GzCDmrpE3QFek4gwFopnREtZ0B6bQEAmClfvVo9XoQtIMix0h+Ba6PiczBPmGurHdT1Fy2aZlZt9v7yAUGa8+rvdlIwgBNPnG/1agZvSIeqUYTBr8Rb4D5ai4wHeCYrUhlp9rPS+SNZUnyNcck9KkpEocz9RrF7G8Donr7JVfi/NTT3OtQwYdtRFnzu56ggVUwrWlYSQHhVqzoLVYlpVPBtlTMhYeSahIwfbymB/9bRbybIFxWQr6QJWw/NBoV3OSiy2yC9bcE52A2OFbK1uxO092d/KEav7b9b/O5sh3KbsNkWZP7hjyM5G5urcyR6nholwNVVhqYxbQPzGJN1BvbJhqLjXIA2OReJwqylDjLnMKO1XuHVDPUJ5XnTBJWE8Xet3XdmAm98VITW2hiUfyOR6k/PZugPv9QM5E2rY1fIl5F8ZPjCX/RRU4i6azyxhcCvGaURxsmmmpFuNvq8tFt8ABWQgNajEChk=:',

  scheduling_mode: '"local_confirmation"',

  status: '200',

  'access-control-allow-origin': '*',

  http_connect_timeout: '5000',

  'body-keys': '"http_extra_opts", "preloaded_devices/1", "preloaded_devices/2", "preloaded_devices/3", "preloaded_devices/4", "preloaded_devices/5", "routes/1", "routes/1/node", "routes/2", "routes/2/nodes/1", "routes/2/nodes/1/opts", "routes/2/nodes/2", "routes/2/nodes/2/opts", "routes/3", "routes/3/node", "routes/3/node/opts", "stack_print_prefixes", "store"',

  await_inprogress: '"named"',

  'content-digest': 'sha-256=:4J6aEWuLwZg2wbsns7pEnKDAy0JfdLfU4SLen3z2fgQ=:',

  access_remote_cache_for_client: '"false"',

  server: 'Cowboy',

  'ao-types': 'access_remote_cache_for_client="atom", ans104_trust_gql="atom", await_inprogress="atom", cache_lookup_hueristics="atom", client_error_strategy="atom", compute_mode="atom", debug_committers="atom", debug_ids="atom", debug_metadata="atom", debug_print="atom", debug_print_binary_max="integer", debug_print_indent="integer", debug_print_map_line_threshold="integer", debug_print_trace="atom", debug_show_priv="atom", debug_stack_depth="integer", force_signed="atom", http_client="atom", http_connect_timeout="integer", http_keepalive="integer", http_request_send_timeout="integer", initialized="atom", load_remote_devices="atom", mode="atom", node_history="empty-list", only="atom", port="integer", preloaded_devices="list", process_now_from_cache="atom", process_workers="atom", relay_http_client="atom", routes="list", scheduler_location_ttl="integer", scheduling_mode="atom", short_trace_len="integer", snp_trusted="empty-list", stack_print_prefixes="list", status="integer", store_all_signed="atom", trusted_device_signers="empty-list", wasm_allow_aot="atom"',

  force_signed: '"true"',

  http_request_send_timeout: '60000',

  client_error_strategy: '"throw"',

  bundler_ans104: 'https://up.arweave.net:443',

  debug_print: '"false"',

  debug_show_priv: '"false"',

  address: 'Tbun4iRRQW93gUiSAmTmZJ2PGI-_yYaXsX69ETgzSRE',

  cache_lookup_hueristics: '"false"',

  only: '"local"',

  initialized: '"true"',

  process_workers: '"false"',

  hb_config_location: 'config.flat',

  'content-type': 'multipart/form-data; boundary="Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A"',

  'signature-input': 'http-sig-bba7e22451416f77=("access_remote_cache_for_client" "address" "ans104_trust_gql" "ao-types" "await_inprogress" "bundler_ans104" "cache_lookup_hueristics" "client_error_strategy" "commitment_device" "compute_mode" "content-digest" "content-type" "debug_committers" "debug_ids" "debug_metadata" "debug_print" "debug_print_binary_max" "debug_print_indent" "debug_print_map_line_threshold" "debug_print_trace" "debug_show_priv" "debug_stack_depth" "force_signed" "gateway" "hb_config_location" "host" "http_client" "http_connect_timeout" "http_keepalive" "http_request_send_timeout" "http_server" "initialized" "load_remote_devices" "mode" "only" "port" "process_now_from_cache" "process_workers" "relay_http_client" "scheduler_location_ttl" "scheduling_mode" "short_trace_len" "@status" "store_all_signed" "wasm_allow_aot");alg="rsa-pss-sha512";keyid="o1kvTqZQ0wbS_WkdwX70TFCk7UF76ldnJ85l8iRV7t6mSlzkXBYCecb-8RXsNEQQmO0KergtHOvhuBJmB6YXaYe_UftI_gendojfIa6jlTgw-qmH6g4_oErI8djDRbQSm-5nCfGVRuYxsNZLYDeqw4gFb9K3b1h7tuMoLd6-d5pkaLfTMUNcvs2OqpkLo0i_av746FieaURdWozwFqO0APtdA7pLHDqQZDMNdTmsUBJFszL6SOa1bKe5cUWnrq4uaW4NAN3JAQniILKGsKZENeKtfXwiKVaFJtriWWsbhOaNT0JLcuBAwXQAP59RXzcr8bRY6XFn8zBmEmZBGszOD9c9ssDENRFDa5uyVhk8XgIgQjErAWYd9T6edrYcIp3R78jhNK_nLiIBBz8_Oz3bLjL5i_aiV2gpfIbd44DCHihuuxSWRAPJxhEy9TS0_QbVOIWhcDTIeEJE3aRPTwSTMt1_Fec7i9HJWN0mvMbAAJw8k6HxjA3pFZiCowZJw7FBwMAeYgEwIeB82f-S2-PtFLwR9i0tExo36hEBHqaS4Y-O3NGgQ8mKnhT7Z1EfxEbA2BpR9oL8rJFEnPIrHHu7B88OHDDfnfRD3D79fKktnisC7XOuwbHG3TQo0_j4_mElH7xj_7IyAbmCUHDd-eRa482wOYXBB01DGnad901qaHU";tag="bU-F-WCfOMjeKPG4yVd4BIZIvR-ZRDXLi6AW5Da5kDo/jNI0FLgi9Lz2UT_l1sK2TCPZMCIwFpPcOK3e3cqdRwo"',

  compute_mode: '"lazy"',

  scheduler_location_ttl: '604800000',

  date: 'Sat, 12 Jul 2025 08:15:38 GMT',

  debug_print_trace: '"short"',

  short_trace_len: '5',

  http_keepalive: '120000',

  http_client: '"gun"',

  debug_print_map_line_threshold: '30',

  process_now_from_cache: '"false"',

  gateway: 'https://arweave.net',

  debug_metadata: '"true"'

} And this is the body you get:HTTP Body --Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

ao-types: cache_control="list", force_message="atom"

cache_control: "always"

content-disposition: form-data;name="http_extra_opts"

force_message: "true"

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

ao-types: module="atom"

content-disposition: form-data;name="preloaded_devices/1"

module: "dev_codec_json"

name: json@1.0

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

ao-types: module="atom"

content-disposition: form-data;name="preloaded_devices/2"

module: "dev_codec_structured"

name: structured@1.0

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

ao-types: module="atom"

content-disposition: form-data;name="preloaded_devices/3"

module: "dev_codec_httpsig"

name: httpsig@1.0

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

ao-types: module="atom"

content-disposition: form-data;name="preloaded_devices/4"

module: "dev_codec_flat"

name: flat@1.0

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

ao-types: module="atom"

content-disposition: form-data;name="preloaded_devices/5"

module: "dev_meta"

name: meta@1.0

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

content-disposition: form-data;name="routes/1"

template: /result/.*

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

content-disposition: form-data;name="routes/1/node"

prefix: http://localhost:6363

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

ao-types: nodes="list"

content-disposition: form-data;name="routes/2"

template: /graphql

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

content-disposition: form-data;name="routes/2/nodes/1"

prefix: https://arweave-search.goldsky.com

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

ao-types: http_client="atom", protocol="atom"

content-disposition: form-data;name="routes/2/nodes/1/opts"

http_client: "httpc"

protocol: "http2"

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

content-disposition: form-data;name="routes/2/nodes/2"

prefix: https://arweave.net

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

ao-types: http_client="atom", protocol="atom"

content-disposition: form-data;name="routes/2/nodes/2/opts"

http_client: "gun"

protocol: "http2"

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

content-disposition: form-data;name="routes/3"

template: /raw

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

content-disposition: form-data;name="routes/3/node"

prefix: https://arweave.net

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

ao-types: http_client="atom", protocol="atom"

content-disposition: form-data;name="routes/3/node/opts"

http_client: "gun"

protocol: "http2"

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

1: "(ao-type-integer) 104", "(ao-type-integer) 98"

2: "(ao-type-integer) 100", "(ao-type-integer) 101", "(ao-type-integer) 118"

3: "(ao-type-integer) 97", "(ao-type-integer) 114"

ao-types: 1="list", 2="list", 3="list"

content-disposition: form-data;name="stack_print_prefixes"

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A

ao-types: store-module="atom"

content-disposition: form-data;name="store"

prefix: cache-mainnet

store-module: "hb_store_fs"

--Z9Om0uN1Z81Q-Sp3G0Mptr4AewbZNf_drHF9PkmdB8A-- Neither reading headers nor body alone gives you the complete picture of the response. These two components need to be combined to decode the message. By decoding responses from HyperBEAM nodes, you'll gain deep understanding of AO Core and HyperBEAM. So we'll decipher this together in the next chapters.But for now, let's say WAO handles everything behind the scenes for you so you can get the final output with the following snippet. You only need to pass path without the node hostname.File  /test/devices-pathing.test.js const { out } = await hb.get({ path: '/~meta@1.0/info' })

console.log(out) This is the decoded output. HyperBEAM internally uses TABM (Type Annotated Binary Message) and it contains the Erlang atom type. So we convert it to Symbol when dealing with JS. Erlang doesn't have boolean and null types, so JS true, false, and null are all atom on the Erlang side.Decoded Response {

  access_remote_cache_for_client: false,

  address: 'Tbun4iRRQW93gUiSAmTmZJ2PGI-_yYaXsX69ETgzSRE',

  ans104_trust_gql: true,

  await_inprogress: Symbol(named),

  bundler_ans104: 'https://up.arweave.net:443',

  cache_lookup_hueristics: false,

  client_error_strategy: Symbol(throw),

  commitment_device: 'httpsig@1.0',

  compute_mode: Symbol(lazy),

  debug_committers: false,

  debug_ids: true,

  debug_metadata: true,

  debug_print: false,

  debug_print_binary_max: 60,

  debug_print_indent: 2,

  debug_print_map_line_threshold: 30,

  debug_print_trace: Symbol(short),

  debug_show_priv: false,

  debug_stack_depth: 40,

  force_signed: true,

  gateway: 'https://arweave.net',

  hb_config_location: 'config.flat',

  host: 'localhost',

  http_client: Symbol(gun),

  http_connect_timeout: 5000,

  http_extra_opts: { cache_control: [ 'always' ], force_message: true },

  http_keepalive: 120000,

  http_request_send_timeout: 60000,

  http_server: 'Tbun4iRRQW93gUiSAmTmZJ2PGI-_yYaXsX69ETgzSRE',

  initialized: true,

  load_remote_devices: false,

  mode: Symbol(debug),

  node_history: [],

  only: Symbol(local),

  port: 10001,

  preloaded_devices: [

    { module: Symbol(dev_codec_json), name: 'json@1.0' },

    { module: Symbol(dev_codec_structured), name: 'structured@1.0' },

    { module: Symbol(dev_codec_httpsig), name: 'httpsig@1.0' },

    { module: Symbol(dev_codec_flat), name: 'flat@1.0' },

    { module: Symbol(dev_meta), name: 'meta@1.0' }

  ],

  process_now_from_cache: false,

  process_workers: false,

  relay_http_client: Symbol(httpc),

  routes: [

    { template: '/result/.*', node: [Object] },

    { template: '/graphql', nodes: [Array] },

    { template: '/raw', node: [Object] }

  ],

  scheduler_location_ttl: 604800000,

  scheduling_mode: Symbol(local_confirmation),

  short_trace_len: 5,

  snp_trusted: [],

  stack_print_prefixes: [ [ 104, 98 ], [ 100, 101, 118 ], [ 97, 114 ] ],

  store: { prefix: 'cache-mainnet', 'store-module': Symbol(hb_store_fs) },

  store_all_signed: true,

  trusted_device_signers: [],

  wasm_allow_aot: false

} meta@1.0 You can change the node configuration with POST method on /~meta@1.0/info. You must be the node operator to sign the message.To send a POST message, you need to construct the encoded headers and body just like above, and sign it with the HTTP message signature scheme, which is a web standard specification (RFC 9421).We'll dig into it later, but it's too complex for now, so WAO handles the complex message signing for you.File  /test/devices-pathing.test.js // set a new config

await hb.post({ 

  path: '/~meta@1.0/info',

  test_config: "abc",

  test_config2: 123,

  test_config3: { abc: 123 }

})

 

// get info

const { out } = await hb.get({ path: '/~meta@1.0/info' })

assert.equal(out.test_config, "abc")

assert.equal(out.test_config2, 123)

assert.deepEqual(out.test_config3, { abc: 123 }) You can also get a specific key only:File  /test/devices-pathing.test.js // getting the node operator wallet address

const { out: address } = await hb.get({ path: "/~meta@1.0/info/address" })

assert.equal(address, hb.addr) When a certain path like /~meta@1.0/info returns an object, you can chain a key like address and access the value at /~meta@1.0/info/address.Once you set initialized to permanent, you'll no longer be able to change any config.File  /test/devices-pathing.test.js await hb.post({ path: "/~meta@1.0/info", initialized: "permanent" })

 

 // this should fail

await assert.rejects(

  hb.post({ path: "/~meta@1.0/info", test_config: "def" })

)

 

const { out: test_config } = await hb.get({ 

  path: '/~meta@1.0/info/test_config'

})

assert.equal(test_config, "abc") Shortcut Methods for get and post WAO provices shortcut methods for get and post to make your codebase even more consice.With g and p, the 1st argument is the path and the 2nd is the rest, which returns only the decoded resonse. So, const { out, headers, body, hashpath } = hb.post({ path, ...rest }) becomes const out = hb.p(path, rest) and the same goes with get and g, too.File  /test/devices-pathing.test.js // set a new config

await hb.p("/~meta@1.0/info", { test_config4: "def" })

 

// get info

const { test_config4 } = await hb.g("/~meta@1.0/info")

assert.equal(test_config4, "def") We are going to use these shortcut formats from this point on.json@1.0 You can also chain another method and device in the URL. For instance, json@1.0 converts TABM to JSON with serialize.File  /test/devices-pathing.test.js const { body: json } = await hb.g("/~meta@1.0/info/~json@1.0/serialize")

console.log(JSON.parse(json)) Chaining /~json@1.0/serialize comes in handy in certain cases, but sometimes the response comes back malformed with additional data attached due to complex message mutation and HTTP transport. hb.get | hb.g and hb.post | hb.p produce cleaner results, so you won't need /~json@1.0/serialize externally. It's still an essential codec device used internally on HyperBEAM.Basic Pathing Summary In this chapter, you saw three basic URL schemes to access HyperBEAM:1. Device and Method /~device_name@version/method_name /~meta@1.0/info /~meta@1.0/build 2. Accessing Key /~meta@1.0/info/address /~meta@1.0/info/preloaded_devices 3. Chaining Another Device /~meta@1.0/info/~json@1.0/serialize These patterns are the same with any other devices.Running Tests You can find the working test file for this chapter here:devices-pathing.test.js Run tests:Terminal   Terminal yarn test test/devices-pathing.test.js References General Intro to HyperBEAM Intro to AO-Core HyperBEAM Core Capabilities Pathing in HyperBEAM HyperBEAM Devices Device Docs Device: ~meta@1.0 Device: ~json@1.0 Device API dev_meta.erl dev_codec_json.erl WAO API HyperBEAM Class API HB Class API

---

# 229. ANTgetLogo() - ARIO Docs

Document Number: 229
Source: https://docs.ar.io/ar-io-sdk/ants/get-logo
Words: 53
Extraction Method: html

getLogo getLogo is a method on the ANT class that retrieves the transaction ID (TX ID) of the ANT's logo. This method is particularly useful when working with ANT versioning functionality, as logos can be updated with new versions.getLogo does not require authentication.Examples Parameters The getLogo method does not accept any parameters.Output Sie_26dvgyok0PZD_-iQAFOhOd5YxDTkczOLoqTTL_A

---

# 230. ARIO Docs

Document Number: 230
Source: https://docs.ar.io/ar-io-sdk/ants/set-ticker
Words: 53
Extraction Method: html

setTicker setTicker is a method on the ANT class that updates the ticker symbol of the ANT process.setTicker requires authentication.Examples Parameters Parameter Type Description Optional ticker string The ticker symbol to set for the ANT process false tags array An array of GQL tag objects to attach to the transfer AO message true

---

# 231. ARIO Docs

Document Number: 231
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-demand-factor
Words: 53
Extraction Method: html

getDemandFactor getDemandFactor is a method on the ARIO class that retrieves the current network demand factor. This factor is a dynamic multiplier that adjusts the cost of ArNS interactions based on network demand - higher demand results in higher costs, and vice versa.getDemandFactor does not require authentication.Examples Parameters getDemandFactor does not accept parameters.

---

# 232. resolveArNSName - ARIO Docs

Document Number: 232
Source: https://docs.ar.io/ar-io-sdk/ario/arns/resolve-arns-name
Words: 53
Extraction Method: html

resolveArNSName is a method on the ARIO class that resolves an ArNS name (including undernames) to get detailed information about the name, including the owner, transaction ID, process ID, and TTL.resolveArNSName does not require authentication.Examples Parameters Parameter Type Description Optional name string The ArNS name to resolve (can include undernames like
'subdomain_domain') false

---

# 233. ARIO Docs

Document Number: 233
Source: https://docs.ar.io/ar-io-sdk/ants/set-name
Words: 52
Extraction Method: html

setName setName is a method on the ANT class that updates the display name of the ANT process.setName requires authentication.Examples Parameters Parameter Type Description Optional name string The display name to set for the ANT process false tags array An array of GQL tag objects to attach to the transfer AO message.true

---

# 234. ARIO Docs

Document Number: 234
Source: https://docs.ar.io/ar-io-sdk/ants/set-description
Words: 52
Extraction Method: html

setDescription setDescription is a method on the ANT class that updates the descriptive text for the ANT process.setDescription requires authentication.Examples Parameters Parameter Type Description Optional description string The descriptive text to set for the ANT process false tags array An array of GQL tag objects to attach to the transfer AO message.true

---

# 235. ARIO Docs

Document Number: 235
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-returned-name
Words: 51
Extraction Method: html

getArNSReturnedName getArNSReturnedName is a method on the ARIO class that retrieves information about an ArNS name that has been returned to the protocol, including its auction settings and timing details.getArNSReturnedName does not require authentication.Examples Parameters Parameter Type Description Optional name string The ArNS name to retrieve returned name information for false

---

# 236. getArNSRecord - ARIO Docs

Document Number: 236
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-record
Words: 49
Extraction Method: html

getArNSRecord is a method on the ARIO class that retrieves the details of a specific ArNS record by its name. This includes lease information, type, process ID, and other metadata.getArNSRecord does not require authentication.Examples Parameters Parameter Type Description Optional name string The ArNS name to retrieve details for false

---

# 237. accessControlList - ARIO Docs

Document Number: 237
Source: https://docs.ar.io/ar-io-sdk/ant-registry/access-control-list
Words: 48
Extraction Method: html

accessControlList is a method on the ANTRegistry class that retrieves the access control list (ACL) for a specific address, showing which ANTs the address owns and controls.accessControlList does not require authentication.Examples Parameters Parameter Type Description Optional address string The wallet address to retrieve access control information for false

---

# 238. getGatewayVaults - ARIO Docs

Document Number: 238
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateway-vaults
Words: 43
Extraction Method: html

getGatewayVaults is a method on the ARIO class that retrieves all vault information for a specific gateway, including delegated stakes and pending withdrawals.getGatewayVaults does not require authentication.Examples Parameters Parameter Type Description Optional gatewayAddress string - WalletAddress The wallet address of the gateway false

---

# 239. getGateway - ARIO Docs

Document Number: 239
Source: https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateway
Words: 42
Extraction Method: html

getGateway is a method on the ARIO class that retrieves detailed information about a specific gateway using its wallet address.getGateway does not require authentication.Examples Parameters Parameter Type Description Optional address string - WalletAddress The wallet address of the gateway to retrieve false

---

# 240. ARIO Docs

Document Number: 240
Source: https://docs.ar.io/wayfinder/core/gateway-providers/simple-cache
Words: 76
Extraction Method: html

SimpleCacheGatewaysProvider Overview The SimpleCacheGatewaysProvider holds the resulting gateways in memory for the provided TTL, making it ideal for Node environments. This helps avoid rate-limits and unnecessary network requests to the underlying gateways provider.Important SimpleCacheGatewaysProvider is ideal for Node.js/server environments. For browser-based web applications, use LocalStorageGatewaysProvider instead to persist gateway lists across sessions.Basic Usage Configuration Options Related Documentation Gateway Providers: Compare all gateway providers NetworkGatewaysProvider: Dynamic network discovery StaticGatewaysProvider: Static gateway configuration Wayfinder Configuration: Main wayfinder setup

---

# 241. getRecords - ARIO Docs

Document Number: 241
Source: https://docs.ar.io/ar-io-sdk/ants/get-records
Words: 39
Extraction Method: html

getRecords is a method on the ANT class that retrieves all the records stored in the ANT process, including both base name records and undername records.getRecords does not require authentication.Examples Parameters The getRecords method does not accept any parameters.

---

# 242. getInfo - ARIO Docs

Document Number: 242
Source: https://docs.ar.io/ar-io-sdk/ario/general/get-info
Words: 39
Extraction Method: html

getInfo is a method on the ARIO class that retrieves general information about the ar.io Network, including the current primary name oracle, demand factor, and more.getInfo does not require authentication.Examples Parameters The getInfo method does not accept any parameters.

---

# 243. getState - ARIO Docs

Document Number: 243
Source: https://docs.ar.io/ar-io-sdk/ants/get-state
Words: 37
Extraction Method: html

getState is a method on the ANT class that retrieves the complete state of the ANT process, including all records, balances, controllers, and metadata.getState does not require authentication.Examples Parameters The getState method does not accept any parameters.

---

# 244. getInfo - ARIO Docs

Document Number: 244
Source: https://docs.ar.io/ar-io-sdk/ants/get-info
Words: 37
Extraction Method: html

getInfo is a method on the ANT class that retrieves general information about the ANT process, including name, ticker, denomination, logo, and other metadata.getInfo does not require authentication.Examples Parameters The getInfo method does not accept any parameters.

---

# 245. getArNSNamesForAddress - ARIO Docs

Document Number: 245
Source: https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-names-for-address
Words: 37
Extraction Method: html

getArNSNamesForAddress is a method on the ARIO class that retrieves ArNS names owned by a specific address.getArNSNamesForAddress does not require authentication.Examples Parameters Parameter Type Description Optional address string The wallet address to fetch ArNS names for false

---

# 246. getLatestANTVersion - ARIO Docs

Document Number: 246
Source: https://docs.ar.io/ar-io-sdk/ant-versions/get-latest-ant-version
Words: 36
Extraction Method: html

getLatestANTVersion is a method on the ANTVersions class that retrieves the latest version for an ANT based on alphabetical sorting of version numbers.getLatestANTVersion does not require authentication.Examples Parameters The getLatestANTVersion method does not accept any parameters.

---

# 247. Module dev_metaerl - HyperBEAM - Documentation

Document Number: 247
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_meta.html
Words: 1162
Extraction Method: html

Module dev_meta.erl The hyperbeam meta device, which is the default entry point
for all messages processed by the machine.Description This device executes a
AO-Core singleton request, after first applying the node's
pre-processor, if set. The pre-processor can halt the request by
returning an error, or return a modified version if it deems necessary --
the result of the pre-processor is used as the request for the AO-Core
resolver. Additionally, a post-processor can be set, which is executed after
the AO-Core resolver has returned a result.Function Index add_dynamic_keys/1* Add dynamic keys to the node message.add_identity_addresses/1*  adopt_node_message/2 Attempt to adopt changes to a node message. Test that we can set the node message if the request is signed by the
owner of the node.build/3 Emits the version number and commit hash of the HyperBEAM node source,
if available.buildinfo_test/0* Test that version information is available and returned correctly.claim_node_test/0* Test that we can claim the node correctly and set the node message after.config_test/0* Test that we can get the node message.embed_status/2* Wrap the result of a device call in a status.filter_node_msg/2* Remove items from the node message that are not encodable into a
message.halt_request_test/0* Test that we can halt a request if the hook returns an error.handle/2 Normalize and route messages downstream based on their path.handle_initialize/2*  handle_resolve/3* Handle an AO-Core request, which is a list of messages.info/1 Ensure that the helper function adopt_node_message/2 is not exported.info/3 Get/set the node message.is/2 Check if the request in question is signed by a given role on the node.is/3  is_operator/2 Utility function for determining if a request is from the operator of
the node.maybe_sign/2* Sign the result of a device call if the node is configured to do so.message_to_status/2* Get the HTTP status code from a transaction (if it exists).modify_request_test/0* Test that a hook can modify a request.permanent_node_message_test/0* Test that a permanent node message cannot be changed.priv_inaccessible_test/0* Test that we can't get the node message if the requested key is private.request_response_hooks_test/0*  resolve_hook/4* Execute a hook from the node message upon the user's request.status_code/2* Calculate the appropriate HTTP status code for an AO-Core result. Test that we can't set the node message if the request is not signed by
the owner of the node.uninitialized_node_test/0* Test that an uninitialized node will not run computation.update_node_message/2* Validate that the request is signed by the operator of the node, then
allow them to update the node message.Function Details add_dynamic_keys/1 * add_dynamic_keys(NodeMsg) -> any() Add dynamic keys to the node message.add_identity_addresses/1 * add_identity_addresses(NodeMsg) -> any() adopt_node_message/2 adopt_node_message(Request, NodeMsg) -> any() Attempt to adopt changes to a node message.authorized_set_node_msg_succeeds_test/0 * authorized_set_node_msg_succeeds_test() -> any() Test that we can set the node message if the request is signed by the
owner of the node.build/3 build(X1, X2, NodeMsg) -> any() Emits the version number and commit hash of the HyperBEAM node source,
if available.We include the short hash separately, as the length of this hash may change in
the future, depending on the git version/config used to build the node.
Subsequently, rather than embedding the git-short-hash-length, for the
avoidance of doubt, we include the short hash separately, as well as its long
hash.buildinfo_test/0 * buildinfo_test() -> any() Test that version information is available and returned correctly.claim_node_test/0 * claim_node_test() -> any() Test that we can claim the node correctly and set the node message after.config_test/0 * config_test() -> any() Test that we can get the node message.embed_status/2 * embed_status(X1, NodeMsg) -> any() Wrap the result of a device call in a status.filter_node_msg/2 * filter_node_msg(Msg, NodeMsg) -> any() Remove items from the node message that are not encodable into a
message.halt_request_test/0 * halt_request_test() -> any() Test that we can halt a request if the hook returns an error.handle/2 handle(NodeMsg, RawRequest) -> any() Normalize and route messages downstream based on their path. Messages
with a Meta key are routed to the handle_meta/2 function, while all
other messages are routed to the handle_resolve/2 function.handle_initialize/2 * handle_initialize(Rest, NodeMsg) -> any() handle_resolve/3 * handle_resolve(Req, Msgs, NodeMsg) -> any() Handle an AO-Core request, which is a list of messages. We apply
the node's pre-processor to the request first, and then resolve the request
using the node's AO-Core implementation if its response was ok.
After execution, we run the node's response hook on the result of
the request before returning the result it grants back to the user.info/1 info(X1) -> any() Ensure that the helper function adopt_node_message/2 is not exported.
The naming of this method carefully avoids a clash with the exported info/3 function. We would like the node information to be easily accessible via the info endpoint, but AO-Core also uses info as the name of the function
that grants device information. The device call takes two or fewer arguments,
so we are safe to use the name for both purposes in this case, as the user
info call will match the three-argument version of the function. If in the
future the request is added as an argument to AO-Core's internal info function, we will need to find a different approach.info/3 info(X1, Request, NodeMsg) -> any() Get/set the node message. If the request is a POST, we check that the
request is signed by the owner of the node. If not, we return the node message
as-is, aside all keys that are private (according to hb_private).is/2 is(Request, NodeMsg) -> any() Check if the request in question is signed by a given role on the node.
The role can be one of operator or initiator.is/3 is(X1, Request, NodeMsg) -> any() is_operator/2 is_operator(Request, NodeMsg) -> any() Utility function for determining if a request is from the operator of
the node.maybe_sign/2 * maybe_sign(Res, NodeMsg) -> any() Sign the result of a device call if the node is configured to do so.message_to_status/2 * message_to_status(Item, NodeMsg) -> any() Get the HTTP status code from a transaction (if it exists).modify_request_test/0 * modify_request_test() -> any() Test that a hook can modify a request.permanent_node_message_test/0 * permanent_node_message_test() -> any() Test that a permanent node message cannot be changed.priv_inaccessible_test/0 * priv_inaccessible_test() -> any() Test that we can't get the node message if the requested key is private.request_response_hooks_test/0 * request_response_hooks_test() -> any() resolve_hook/4 * resolve_hook(HookName, InitiatingRequest, Body, NodeMsg) -> any() Execute a hook from the node message upon the user's request. The
invocation of the hook provides a request of the following form:/path => request | response
       /request => the original request singleton
       /body => parsed sequence of messages to process | the execution result status_code/2 * status_code(X1, NodeMsg) -> any() Calculate the appropriate HTTP status code for an AO-Core result.
The order of precedence is:
1. The status code from the message.
2. The HTTP representation of the status code.
3. The default status code.unauthorized_set_node_msg_fails_test/0 * unauthorized_set_node_msg_fails_test() -> any() Test that we can't set the node message if the request is not signed by
the owner of the node.uninitialized_node_test/0 * uninitialized_node_test() -> any() Test that an uninitialized node will not run computation.update_node_message/2 * update_node_message(Request, NodeMsg) -> any() Validate that the request is signed by the operator of the node, then
allow them to update the node message.

---

# 248. Module dev_snperl - HyperBEAM - Documentation

Document Number: 248
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_snp.html
Words: 426
Extraction Method: html

Module dev_snp.erl This device offers an interface for validating AMD SEV-SNP commitments,
as well as generating them, if called in an appropriate environment.Function Index execute_is_trusted/3* Ensure that all of the software hashes are trusted.generate/3 Generate an commitment report and emit it as a message, including all of
the necessary data to generate the nonce (ephemeral node address + node
message ID), as well as the expected measurement (firmware, kernel, and VMSAs
hashes).generate_nonce/2* Generate the nonce to use in the commitment report.is_debug/1* Ensure that the node's debug policy is disabled.real_node_test/0*  report_data_matches/3* Ensure that the report data matches the expected report data.trusted/3 Validates if a given message parameter matches a trusted value from the SNP trusted list
Returns {ok, true} if the message is trusted, {ok, false} otherwise.verify/3 Verify an commitment report message; validating the identity of a
remote node, its ephemeral private address, and the integrity of the report.Function Details execute_is_trusted/3 * execute_is_trusted(M1, Msg, NodeOpts) -> any() Ensure that all of the software hashes are trusted. The caller may set
a specific device to use for the is-trusted key. The device must then
implement the trusted resolver.generate/3 generate(M1, M2, Opts) -> any() Generate an commitment report and emit it as a message, including all of
the necessary data to generate the nonce (ephemeral node address + node
message ID), as well as the expected measurement (firmware, kernel, and VMSAs
hashes).generate_nonce/2 * generate_nonce(RawAddress, RawNodeMsgID) -> any() Generate the nonce to use in the commitment report.is_debug/1 * is_debug(Report) -> any() Ensure that the node's debug policy is disabled.real_node_test/0 * real_node_test() -> any() report_data_matches/3 * report_data_matches(Address, NodeMsgID, ReportData) -> any() Ensure that the report data matches the expected report data.trusted/3 trusted(Msg1, Msg2, NodeOpts) -> any() Validates if a given message parameter matches a trusted value from the SNP trusted list
Returns {ok, true} if the message is trusted, {ok, false} otherwise verify/3 verify(M1, M2, NodeOpts) -> any() Verify an commitment report message; validating the identity of a
remote node, its ephemeral private address, and the integrity of the report.
The checks that must be performed to validate the report are:
1. Verify the address and the node message ID are the same as the ones
used to generate the nonce.
2. Verify the address that signed the message is the same as the one used
to generate the nonce.
3. Verify that the debug flag is disabled.
4. Verify that the firmware, kernel, and OS (VMSAs) hashes, part of the
measurement, are trusted.
5. Verify the measurement is valid.
6. Verify the report's certificate chain to hardware root of trust.

---

# 249. Exposing Process State to HyperBEAM  Cookbook

Document Number: 249
Source: https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/exposing-process-state.html
Words: 579
Extraction Method: html

Exposing Process State to HyperBEAM HyperBEAM introduces a powerful feature for exposing parts of a process's state for immediate reading over HTTP. This improves performance for web frontends and data services by replacing the need for dryrun calls, which were a known bottleneck on legacynet.The Patch Device The ~patch@1.0 device is the mechanism that allows AO processes to make parts of their internal state readable via direct HTTP GET requests.How it Works Exposing state is a four-step process involving your process and HyperBEAM:Process Logic: From your process (e.g., in Lua or WASM), send an outbound message to the ~patch@1.0 device.Patch Message Format: The message must include device and cache tags.HyperBEAM Execution: HyperBEAM's dev_patch module processes this message, mapping the key-value pairs from the cache table to a URL path.HTTP Access: The exposed data is then immediately available via a standard HTTP GET request to the process's endpoint.HyperBEAMGET /<process-id>~process@1.0/compute/cache/<mydatakey> Initial State Sync (Optional) To make data available immediately on process creation, you can patch its initial state. A common pattern is to use a flag to ensure this sync only runs once, as shown in this example for a token's Balances and TotalSupply.lua-- Place this logic at the top level of your process script,

-- outside of specific handlers, so it runs on load.

Balances = { token1 = 100, token2 = 200 } -- A table of balances

TotalSupply = 1984 -- A single total supply value

-- 1. Initialize Flag:

-- Initializes a flag if it doesn't exist.

InitialSync = InitialSync or 'INCOMPLETE'

-- 2. Check Flag:

-- Checks if the sync has already run.

if InitialSync == 'INCOMPLETE' then

  -- 3. Patch State:

  -- The `Send` call patches the state, making it available at endpoints like:

  -- /cache/balances

  -- /cache/totalsupply

  Send({ device = 'patch@1.0', cache = { balances = Balances, totalsupply = TotalSupply } })

  -- 4. Update Flag:

  -- Updates the flag to prevent the sync from running again.

  InitialSync = 'COMPLETE'

  print("Initial state sync complete. Balances and TotalSupply patched.")

end This pattern makes essential data queryable upon process creation, boosting application responsiveness.Example (Lua in aos) This handler exposes a currentstatus key that can be read via HTTP after the PublishData action is called.Avoiding Key Conflicts Keys in the cache table become URL path segments. To avoid conflicts with reserved HyperBEAM paths, use descriptive, specific keys. Avoid using reserved keywords such as:For instance, prefer a key like myappstate over a generic key like state.WARNING HTTP paths are case-insensitive. While the patch device stores keys with case sensitivity (e.g., MyKey vs mykey), HTTP access to paths like the following is ambiguous and may lead to unpredictable results.To prevent conflicts, always use lowercase keys in your cache table (e.g., mykey, usercount).HyperBEAMGET /<process-id>~process@1.0/cache/mykey Key Points Path Structure: Data is exposed at a path structured like this, where <key> is a key from your cache table:HyperBEAM/<process-id>~process@1.0/cache/<key> Data Types: Basic data types like strings and numbers work best. Complex objects may require serialization.compute vs now: Accessing patched data can be done via two main paths:The compute endpoint serves the last known value quickly, while now may perform additional computation to get the most recent state.Read-Only Exposure: Patching is for efficient reads and does not replace your process's core state management logic.Using the patch device enables efficient, standard HTTP access to your process state, seamlessly connecting decentralized logic with web applications.Now that you know how to expose static state, learn how to perform on-the-fly computations on that state by reading dynamic state.

---

# 250. Installing aos  Cookbook

Document Number: 250
Source: https://cookbook_ao.arweave.net/guides/aos/installing.html
Words: 33
Extraction Method: html

Skip to content  Installing aos Installing aos only requires NodeJS - https://nodejs.org NOTE: If you are on windows you may get better results with WSL Console.Once installed you can run by typing aos

---

# 251. getANTVersions - ARIO Docs

Document Number: 251
Source: https://docs.ar.io/ar-io-sdk/ant-versions/get-ant-versions
Words: 33
Extraction Method: html

getANTVersions is a method on the ANTVersions class that retrieves all versions for an ANT, sorted alphabetically by version number.getANTVersions does not require authentication.Examples Parameters The getANTVersions method does not accept any parameters.

---

# 252. getCurrentEpoch - ARIO Docs

Document Number: 252
Source: https://docs.ar.io/ar-io-sdk/ario/epochs/get-current-epoch
Words: 32
Extraction Method: html

getCurrentEpoch is a method on the ARIO class that retrieves information about the current epoch in the AR.IO network.getCurrentEpoch does not require authentication.Examples Parameters The getCurrentEpoch method does not accept any parameters.

---

# 253. getHandlers - ARIO Docs

Document Number: 253
Source: https://docs.ar.io/ar-io-sdk/ants/get-handlers
Words: 29
Extraction Method: html

getHandlers is a method on the ANT class that retrieves the handlers supported by the ANT.getHandlers does not require authentication.Examples Parameters The getHandlers method does not accept any parameters.

---

# 254. register - ARIO Docs

Document Number: 254
Source: https://docs.ar.io/ar-io-sdk/ant-registry/register
Words: 25
Extraction Method: html

register is a method on the ANTRegistry class that registers a new ANT process with the ANT Registry, making it discoverable and accessible.register requires authentication.

---

# 255. json10 - HyperBEAM - Documentation

Document Number: 255
Source: https://hyperbeam.arweave.net/build/devices/json-at-1-0.html
Words: 199
Extraction Method: html

Device: ~json@1.0 Overview The ~json@1.0 device provides a mechanism to interact with JSON (JavaScript Object Notation) data structures. It allows treating a JSON document or string as a stateful entity against which queries can be executed.This device is useful for:Serializing and deserializing JSON data.Querying and modifying JSON objects.Integrating with other devices and operations.Core Functions (Keys) Serialization GET /~json@1.0/serialize (Direct Serialize Action) Action: Serializes the input message or data into a JSON string.Example:GET /~json@1.0/serialize - serializes the current message as JSON.Path: The path segment /serialize directly follows the device identifier.GET /<PreviousPath>/~json@1.0/serialize (Chained Serialize Action) Action: Takes arbitrary data output from <PreviousPath> (another device or operation) and returns its serialized JSON string representation.Example:GET /~meta@1.0/info/~json@1.0/serialize - fetches node info from the meta device and then pipes it to the JSON device to serialize the result as JSON.Path: This segment (/~json@1.0/serialize) is appended to a previous path segment.Path Chaining Example The JSON device is particularly useful in path chaining to convert output from other devices into JSON format:GET /~meta@1.0/info/~json@1.0/serialize This retrieves the node configuration from the meta device and serializes it to JSON.See Also Message Device - Works well with JSON serialization Meta Device - Can provide configuration data to serialize json module

---

# 256. Building Devices - HyperBEAM - Documentation

Document Number: 256
Source: https://hyperbeam.arweave.net/build/devices/building-devices.html
Words: 150
Extraction Method: html

Extending HyperBEAM with Devices We encourage you to extend HyperBEAM with devices for functionality that is general purpose and reusable across different applications.What are Devices?As explained in the introduction, devices are the core functional units within HyperBEAM. They are self-contained modules that process messages and perform specific actions, forming the building blocks of your application's logic.HyperBEAM comes with a set of powerful built-in devices that handle everything from process management (~process@1.0) and message scheduling (~scheduler@1.0) to executing WebAssembly (~wasm64@1.0) and Lua scripts (~lua@5.3a).Creating Your Own Devices (Coming Soon) We will create more in depth guides for building devices in Lua and Erlang in the future.Further Reading In the meantime, community-contributed guides are available that can walk you through the process. For example:Rust:Building Rust Devices with HyperBEAM M3 Beta: mini-Roam API (Vol. 1) - A tutorial from Decent Land Labs that covers how to build a custom Rust device from scratch.

---

# 257. lua53a - HyperBEAM - Documentation

Document Number: 257
Source: https://hyperbeam.arweave.net/build/devices/lua-at-5-3a.html
Words: 542
Extraction Method: html

Device: ~lua@5.3a Overview The ~lua@5.3a device enables the execution of Lua scripts within the HyperBEAM environment. It provides an isolated sandbox where Lua code can process incoming messages, interact with other devices, and manage state.Core Concept: Lua Script Execution This device allows processes to perform computations defined in Lua scripts. Similar to the ~wasm64@1.0 device, it manages the lifecycle of a Lua execution state associated with the process.Key Functions (Keys) These keys are typically used within an execution stack (managed by dev_stack) for an AO process.init Action: Initializes the Lua environment for the process. It finds and loads the Lua script(s) associated with the process, creates a luerl state, applies sandboxing rules if specified, installs the dev_lua_lib (providing AO-specific functions like ao.send), and stores the initialized state in the process's private area (priv/state).Inputs (Expected in Process Definition or init Message):script: Can be:An Arweave Transaction ID of the Lua script file.A list of script IDs or script message maps.A message map containing the Lua script in its body tag (Content-Type application/lua or text/x-lua).A map where keys are module names and values are script IDs/messages.sandbox: (Optional) Controls Lua sandboxing. Can be true (uses default sandbox list), false (no sandbox), or a map/list specifying functions to disable and their return values.Outputs (Stored in priv/):state: The initialized luerl state handle.<FunctionName> (Default Handler - compute) Action: Executes a specific function within the loaded Lua script(s). This is the default handler; if a key matching a Lua function name is called on the device, this logic runs.Inputs (Expected in Process State or Incoming Message):priv/state: The Lua state obtained during init.The key being accessed (used as the default function name).function or body/function: (Optional) Overrides the function name derived from the key.parameters or body/parameters: (Optional) Arguments to pass to the Lua function. Defaults to a list containing the process message, the request message, and an empty options map.Response: The results returned by the Lua function call, typically encoded. The device also updates the priv/state with the Lua state after execution.snapshot Action: Captures the current state of the running Lua environment. luerl state is serializable.Inputs:priv/state.Outputs: A message containing the serialized Lua state, typically tagged with [Prefix]/State.normalize (Internal Helper) Action: Ensures a consistent state representation by loading a Lua state from a snapshot ([Prefix]/State) if a live state (priv/state) isn't already present.functions Action: Returns a list of all globally defined functions within the current Lua state.Inputs:priv/state.Response: A list of function names.Sandboxing The sandbox option in the process definition restricts potentially harmful Lua functions (like file I/O, OS commands, loading arbitrary code). By default (sandbox = true), common dangerous functions are disabled. You can customize the sandbox rules.AO Library (dev_lua_lib) The init function automatically installs a helper library (dev_lua_lib) into the Lua state. This library typically provides functions for interacting with the AO environment from within the Lua script, such as:ao.send({ Target = ..., ... }): To send messages from the process.Access to message tags and data.Usage within dev_stack Like ~wasm64@1.0, the ~lua@5.3a device is typically used within an execution stack.# Example Process Definition Snippet
Execution-Device: stack@1.0
Execution-Stack: scheduler@1.0, lua@5.3a
Script: <LuaScriptTxID>
Sandbox: true This device offers a lightweight, integrated scripting capability for AO processes, suitable for a wide range of tasks from simple logic to more complex state management and interactions.lua module

---

# 258. message10 - HyperBEAM - Documentation

Document Number: 258
Source: https://hyperbeam.arweave.net/build/devices/message-at-1-0.html
Words: 472
Extraction Method: html

Device: ~message@1.0 Overview The ~message@1.0 device is a fundamental built-in device in HyperBEAM. It serves as the identity device for standard AO-Core messages, which are represented as Erlang maps internally. Its primary function is to allow manipulation and inspection of these message maps directly via HTTP requests, without needing a persistent process state.This device is particularly useful for:Creating and modifying transient messages on the fly using query parameters.Retrieving specific values from a message map.Inspecting the keys of a message.Handling message commitments and verification (though often delegated to specialized commitment devices like httpsig@1.0).Core Functionality The message@1.0 device treats the message itself as the state it operates on. Key operations are accessed via path segments in the HTTP path.Key Access (/key) To retrieve the value associated with a specific key in the message map, simply append the key name to the path. Key lookup is case-insensitive.Example:GET /~message@1.0&hello=world&Key=Value/key Response:"Value" Reserved Keys The message@1.0 device reserves several keys for specific operations:get: (Default operation if path segment matches a key in the map) Retrieves the value of a specified key. Behaves identically to accessing /key directly.set: Modifies the message by adding or updating key-value pairs. Requires additional parameters (usually in the request body or subsequent path segments/query params, depending on implementation specifics).Supports deep merging of maps.Setting a key to unset removes it.Overwriting keys that are part of existing commitments will typically remove those commitments unless the new value matches the old one.set_path: A special case for setting the path key itself, which cannot be done via the standard set operation.remove: Removes one or more specified keys from the message. Requires an item or items parameter.keys: Returns a list of all public (non-private) keys present in the message map.id: Calculates and returns the ID (hash) of the message. Considers active commitments based on specified committers. May delegate ID calculation to a device specified by the message's id-device key commit: Creates a commitment (e.g., a signature) for the message. Requires parameters like commitment-device and potentially committer information. Delegates the actual commitment generation to the specified device (default httpsig@1.0).committers: Returns a list of committers associated with the commitments in the message. Can be filtered by request parameters.commitments: Used internally and in requests to filter or specify which commitments to operate on (e.g., for id or verify).verify: Verifies the commitments attached to the message. Can be filtered by committers or specific commitment IDs in the request. Delegates verification to the device specified in each commitment (commitment-device).Private Keys Keys prefixed with priv (e.g., priv_key, private.data) are considered private and cannot be accessed or listed via standard get or keys operations.HTTP Example This example demonstrates creating a transient message and retrieving a value:GET /~message@1.0&hello=world&k=v/k Breakdown:~message@1.0: Sets the root device.&hello=world&k=v: Query parameters create the initial message: #{ <<"hello">> => <<"world">>, <<"k">> => <<"v">> }./k: The path segment requests the value for the key k.Response:"v"

---

# 259. Module dev_cronerl - HyperBEAM - Documentation

Document Number: 259
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_cron.html
Words: 302
Extraction Method: html

Module dev_cron.erl A device that inserts new messages into the schedule to allow processes
to passively 'call' themselves without user interaction.Function Index every/3 Exported function for scheduling a recurring message.every_worker_loop/4*  every_worker_loop_test/0* This test verifies that a recurring task can be scheduled and executed.info/1 Exported function for getting device info.info/3  once/3 Exported function for scheduling a one-time message.once_executed_test/0* This test verifies that a one-time task can be scheduled and executed.once_worker/3* Internal function for scheduling a one-time message.parse_time/1* Parse a time string into milliseconds.stop/3 Exported function for stopping a scheduled task.stop_every_test/0* This test verifies that a recurring task can be stopped by
calling the stop function with the task ID.stop_once_test/0*  test_worker/0* This is a helper function that is used to test the cron device.test_worker/1*  Function Details every/3 every(Msg1, Msg2, Opts) -> any() Exported function for scheduling a recurring message.every_worker_loop/4 * every_worker_loop(CronPath, Req, Opts, IntervalMillis) -> any() every_worker_loop_test/0 * every_worker_loop_test() -> any() This test verifies that a recurring task can be scheduled and executed.info/1 info(X1) -> any() Exported function for getting device info.info/3 info(Msg1, Msg2, Opts) -> any() once/3 once(Msg1, Msg2, Opts) -> any() Exported function for scheduling a one-time message.once_executed_test/0 * once_executed_test() -> any() This test verifies that a one-time task can be scheduled and executed.once_worker/3 * once_worker(Path, Req, Opts) -> any() Internal function for scheduling a one-time message.parse_time/1 * parse_time(BinString) -> any() Parse a time string into milliseconds.stop/3 stop(Msg1, Msg2, Opts) -> any() Exported function for stopping a scheduled task.stop_every_test/0 * stop_every_test() -> any() This test verifies that a recurring task can be stopped by
calling the stop function with the task ID.stop_once_test/0 * stop_once_test() -> any() test_worker/0 * test_worker() -> any() This is a helper function that is used to test the cron device.
It is used to increment a counter and update the state of the worker.test_worker/1 * test_worker(State) -> any()

---

# 260. Module dev_fafferl - HyperBEAM - Documentation

Document Number: 260
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_faff.html
Words: 180
Extraction Method: html

Module dev_faff.erl A module that implements a 'friends and family' pricing policy.Description It will allow users to process requests only if their addresses are
in the allow-list for the node.Fundamentally against the spirit of permissionlessness, but it is useful if
you are running a node for your own purposes and would not like to allow
others to make use of it -- even for a fee. It also serves as a useful
example of how to implement a custom pricing policy, as it implements stubs
for both the pricing and ledger P4 APIs.Function Index charge/3 Charge the user's account if the request is allowed.estimate/3 Decide whether or not to service a request from a given address.is_admissible/2* Check whether all of the signers of the request are in the allow-list.Function Details charge/3 charge(X1, Req, NodeMsg) -> any() Charge the user's account if the request is allowed.estimate/3 estimate(X1, Msg, NodeMsg) -> any() Decide whether or not to service a request from a given address.is_admissible/2 * is_admissible(Msg, NodeMsg) -> any() Check whether all of the signers of the request are in the allow-list.

---

# 261. Module dev_codec_structurederl - HyperBEAM - Documentation

Document Number: 261
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_codec_structured.html
Words: 338
Extraction Method: html

Module dev_codec_structured.erl A device implementing the codec interface (to/1, from/1) for
HyperBEAM's internal, richly typed message format.Description This format mirrors HTTP Structured Fields, aside from its limitations of
compound type depths, as well as limited floating point representations.As with all AO-Core codecs, its target format (the format it expects to
receive in the to/1 function, and give in from/1) is TABM.For more details, see the HTTP Structured Fields (RFC-9651) specification.Function Index commit/3  decode_ao_types/2 Parse the ao-types field of a TABM and return a map of keys and their
types.decode_value/2 Convert non-binary values to binary for serialization.encode_ao_types/2 Generate an ao-types structured field from a map of keys and their
types.encode_value/1 Convert a term to a binary representation, emitting its type for
serialization as a separate tag.from/3 Convert a rich message into a 'Type-Annotated-Binary-Message' (TABM).implicit_keys/1* Find the implicit keys of a TABM.implicit_keys/2  is_list_from_ao_types/2 Determine if the ao-types field of a TABM indicates that the message
is a list.linkify_mode/2* Discern the linkify mode from the request and the options.list_encoding_test/0*  to/3 Convert a TABM into a native HyperBEAM message.verify/3  Function Details commit/3 commit(Msg, Req, Opts) -> any() decode_ao_types/2 decode_ao_types(Msg, Opts) -> any() Parse the ao-types field of a TABM and return a map of keys and their
types decode_value/2 decode_value(Type, Value) -> any() Convert non-binary values to binary for serialization.encode_ao_types/2 encode_ao_types(Types, Opts) -> any() Generate an ao-types structured field from a map of keys and their
types.encode_value/1 encode_value(Value) -> any() Convert a term to a binary representation, emitting its type for
serialization as a separate tag.from/3 from(Bin, Req, Opts) -> any() Convert a rich message into a 'Type-Annotated-Binary-Message' (TABM).implicit_keys/1 * implicit_keys(Req) -> any() Find the implicit keys of a TABM.implicit_keys/2 implicit_keys(Req, Opts) -> any() is_list_from_ao_types/2 is_list_from_ao_types(Types, Opts) -> any() Determine if the ao-types field of a TABM indicates that the message
is a list.linkify_mode/2 * linkify_mode(Req, Opts) -> any() Discern the linkify mode from the request and the options.list_encoding_test/0 * list_encoding_test() -> any() to/3 to(Bin, Req, Opts) -> any() Convert a TABM into a native HyperBEAM message.verify/3 verify(Msg, Req, Opts) -> any()

---

# 262. Module dev_patcherl - HyperBEAM - Documentation

Document Number: 262
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_patch.html
Words: 375
Extraction Method: html

Module dev_patch.erl A device that can be used to reorganize a message: Moving data from
one path inside it to another.Description This device's function runs in two modes:When using all to move all data at the path given in from to the
path given in to.When using patches to move all submessages in the source to the target,if they have a method key of PATCH or a device key of patch@1.0.Source and destination paths may be prepended by base: or req: keys to
indicate that they are relative to either of the message's that the
computation is being performed on.The search order for finding the source and destination keys is as follows,
where X is either from or to:The patch-X key of the execution message.The X key of the execution message.The patch-X key of the request message.The X key of the request message.Additionally, this device implements the standard computation device keys,
allowing it to be used as an element of an execution stack pipeline, etc.Function Index all/3 Get the value found at the patch-from key of the message, or the from key if the former is not present.all_mode_test/0*  compute/3  init/3 Necessary hooks for compliance with the execution-device standard.move/4* Unified executor for the all and patches modes.normalize/3  patch_to_submessage_test/0*  patches/3 Find relevant PATCH messages in the given source key of the execution
and request messages, and apply them to the given destination key of the
request.req_prefix_test/0*  snapshot/3  uninitialized_patch_test/0*  Function Details all/3 all(Msg1, Msg2, Opts) -> any() Get the value found at the patch-from key of the message, or the from key if the former is not present. Remove it from the message and set
the new source to the value found.all_mode_test/0 * all_mode_test() -> any() compute/3 compute(Msg1, Msg2, Opts) -> any() init/3 init(Msg1, Msg2, Opts) -> any() Necessary hooks for compliance with the execution-device standard.move/4 * move(Mode, Msg1, Msg2, Opts) -> any() Unified executor for the all and patches modes.normalize/3 normalize(Msg1, Msg2, Opts) -> any() patch_to_submessage_test/0 * patch_to_submessage_test() -> any() patches/3 patches(Msg1, Msg2, Opts) -> any() Find relevant PATCH messages in the given source key of the execution
and request messages, and apply them to the given destination key of the
request.req_prefix_test/0 * req_prefix_test() -> any() snapshot/3 snapshot(Msg1, Msg2, Opts) -> any() uninitialized_patch_test/0 * uninitialized_patch_test() -> any()

---

# 263. Module dev_stackerl - HyperBEAM - Documentation

Document Number: 263
Source: https://hyperbeam.arweave.net/build/devices/source-code/dev_stack.html
Words: 1145
Extraction Method: html

Module dev_stack.erl A device that contains a stack of other devices, and manages their
execution.Description It can run in two modes: fold (the default), and map.In fold mode, it runs upon input messages in the order of their keys. A
stack maintains and passes forward a state (expressed as a message) as it
progresses through devices.For example, a stack of devices as follows:Device -> Stack
   Device-Stack/1/Name -> Add-One-Device
   Device-Stack/2/Name -> Add-Two-Device When called with the message:#{ Path = "FuncName", binary => <code><<"0">></code> } Will produce the output:#{ Path = "FuncName", binary => <code><<"3">></code> }
   {ok, #{ bin => <code><<"3">></code> }} In map mode, the stack will run over all the devices in the stack, and
combine their results into a single message. Each of the devices'
output values have a key that is the device's name in the Device-Stack (its number if the stack is a list).You can switch between fold and map modes by setting the Mode key in the Msg2 to either Fold or Map, or set it globally for the stack by
setting the Mode key in the Msg1 message. The key in Msg2 takes
precedence over the key in Msg1.The key that is called upon the device stack is the same key that is used
upon the devices that are contained within it. For example, in the above
scenario we resolve FuncName on the stack, leading FuncName to be called on
Add-One-Device and Add-Two-Device.A device stack responds to special statuses upon responses as follows:skip: Skips the rest of the device stack for the current pass.pass: Causes the stack to increment its pass number and re-execute
the stack from the first device, maintaining the state
accumulated so far. Only available in fold mode.In all cases, the device stack will return the accumulated state to the
caller as the result of the call to the stack.The dev_stack adds additional metadata to the message in order to track
the state of its execution as it progresses through devices. These keys
are as follows:Stack-Pass: The number of times the stack has reset and re-executed
from the first device for the current message.Input-Prefix: The prefix that the device should use for its outputs
and inputs.Output-Prefix: The device that was previously executed.All counters used by the stack are initialized to 1.Additionally, as implemented in HyperBEAM, the device stack will honor a
number of options that are passed to it as keys in the message. Each of
these options is also passed through to the devices contained within the
stack during execution. These options include:Error-Strategy: Determines how the stack handles errors from devices.
See maybe_error/5 for more information.Allow-Multipass: Determines whether the stack is allowed to automatically
re-execute from the first device when the pass tag is returned. See maybe_pass/3 for more information.Under-the-hood, dev_stack uses a default handler to resolve all calls to
devices, aside set/2 which it calls itself to mutate the message's device key in order to change which device is currently being executed. This method
allows dev_stack to ensure that the message's HashPath is always correct,
even as it delegates calls to other devices. An example flow for a dev_stack execution is as follows:/Msg1/AlicesExcitingKey ->
        dev_stack:execute ->
            /Msg1/Set?device=/Device-Stack/1 ->
            /Msg2/AlicesExcitingKey ->
            /Msg3/Set?device=/Device-Stack/2 ->
            /Msg4/AlicesExcitingKey
            ... ->
            /MsgN/Set?device=[This-Device] ->
        returns {ok, /MsgN+1} ->
    /MsgN+1 In this example, the device key is mutated a number of times, but the
resulting HashPath remains correct and verifiable.Function Index benchmark_test/0*  example_device_for_stack_test/0*  generate_append_device/1  generate_append_device/2*  increment_pass/2* Helper to increment the pass number.info/1  input_and_output_prefixes_test/0*  input_output_prefixes_passthrough_test/0*  input_prefix/3 Return the input prefix for the stack.many_devices_test/0*  maybe_error/5*  no_prefix_test/0*  not_found_test/0*  output_prefix/3 Return the output prefix for the stack.output_prefix_test/0*  pass_test/0*  prefix/3 Return the default prefix for the stack.reinvocation_test/0*  resolve_fold/3* The main device stack execution engine.resolve_fold/4*  resolve_map/3* Map over the devices in the stack, accumulating the output in a single
message of keys and values, where keys are the same as the keys in the
original message (typically a number).router/3*  router/4 The device stack key router.simple_map_test/0*  simple_stack_execute_test/0*  skip_test/0*  test_prefix_msg/0*  transform/3* Return Message1, transformed such that the device named Key from the Device-Stack key in the message takes the place of the original Device key.transform_external_call_device_test/0* Ensure we can generate a transformer message that can be called to
return a version of msg1 with only that device attached.transform_internal_call_device_test/0* Test that the transform function can be called correctly internally
by other functions in the module.transformer_message/2* Return a message which, when given a key, will transform the message
such that the device named Key from the Device-Stack key in the message
takes the place of the original Device key.Function Details benchmark_test/0 * benchmark_test() -> any() example_device_for_stack_test/0 * example_device_for_stack_test() -> any() generate_append_device/1 generate_append_device(Separator) -> any() generate_append_device/2 * generate_append_device(Separator, Status) -> any() increment_pass/2 * increment_pass(Message, Opts) -> any() Helper to increment the pass number.info/1 info(Msg) -> any() input_and_output_prefixes_test/0 * input_and_output_prefixes_test() -> any() input_output_prefixes_passthrough_test/0 * input_output_prefixes_passthrough_test() -> any() input_prefix/3 input_prefix(Msg1, Msg2, Opts) -> any() Return the input prefix for the stack.many_devices_test/0 * many_devices_test() -> any() maybe_error/5 * maybe_error(Message1, Message2, DevNum, Info, Opts) -> any() no_prefix_test/0 * no_prefix_test() -> any() not_found_test/0 * not_found_test() -> any() output_prefix/3 output_prefix(Msg1, Msg2, Opts) -> any() Return the output prefix for the stack.output_prefix_test/0 * output_prefix_test() -> any() pass_test/0 * pass_test() -> any() prefix/3 prefix(Msg1, Msg2, Opts) -> any() Return the default prefix for the stack.reinvocation_test/0 * reinvocation_test() -> any() resolve_fold/3 * resolve_fold(Message1, Message2, Opts) -> any() The main device stack execution engine. See the moduledoc for more
information.resolve_fold/4 * resolve_fold(Message1, Message2, DevNum, Opts) -> any() resolve_map/3 * resolve_map(Message1, Message2, Opts) -> any() Map over the devices in the stack, accumulating the output in a single
message of keys and values, where keys are the same as the keys in the
original message (typically a number).router/3 * router(Message1, Message2, Opts) -> any() router/4 router(Key, Message1, Message2, Opts) -> any() The device stack key router. Sends the request to resolve_stack,
except for set/2 which is handled by the default implementation in dev_message.simple_map_test/0 * simple_map_test() -> any() simple_stack_execute_test/0 * simple_stack_execute_test() -> any() skip_test/0 * skip_test() -> any() test_prefix_msg/0 * test_prefix_msg() -> any() transform/3 * transform(Msg1, Key, Opts) -> any() Return Message1, transformed such that the device named Key from the Device-Stack key in the message takes the place of the original Device key. This transformation allows dev_stack to correctly track the HashPath
of the message as it delegates execution to devices contained within it.transform_external_call_device_test/0 * transform_external_call_device_test() -> any() Ensure we can generate a transformer message that can be called to
return a version of msg1 with only that device attached.transform_internal_call_device_test/0 * transform_internal_call_device_test() -> any() Test that the transform function can be called correctly internally
by other functions in the module.transformer_message/2 * transformer_message(Msg1, Opts) -> any() Return a message which, when given a key, will transform the message
such that the device named Key from the Device-Stack key in the message
takes the place of the original Device key. This allows users to call
a single device from the stack:/Msg1/Transform/DeviceName/keyInDevice ->
keyInDevice executed on DeviceName against Msg1.

---

# 264. Intro to HyperBEAM - HyperBEAM - Documentation

Document Number: 264
Source: https://hyperbeam.arweave.net/build/introduction/what-is-hyperbeam.html
Words: 307
Extraction Method: html

Skip to content
 What is HyperBEAM? HyperBEAM is the primary, production-ready implementation of the AO-Core protocol, built on the robust Erlang/OTP framework. It serves as a decentralized operating system, powering the AO Computer —a scalable, trust-minimized, distributed supercomputer built on permanent storage of Arweave.Implementing AO-Core HyperBEAM transforms the abstract concepts of AO-Core—Messages, Devices, and Paths—into a concrete, operational system. It provides the runtime environment and essential services to execute these
computations across a network of distributed nodes.                  Messages Modular Data Packets In HyperBEAM, every interaction within the AO Computer is handled as a message. A message is a binary item or a map of functions. These cryptographically-linked data units are the foundation for communication, allowing processes to trigger computations, query state, and transfer value. HyperBEAM nodes are responsible for routing and processing these messages according to the rules of the AO-Core protocol.          Devices Extensible Execution Engines HyperBEAM introduces a uniquely modular architecture centered around Devices. These pluggable components are Erlang modules that define specific computational logic—like running WASM, managing state, or relaying data—allowing for unprecedented flexibility. This design allows developers to extend the system by creating custom Devices to fit their specific computational needs.                             Paths Composable Pipelines HyperBEAM exposes a powerful HTTP API that uses structured URL patterns to interact with processes and data. This pathing mechanism allows developers to create verifiable data pipelines, composing functionality from multiple devices into a single, atomic request. The URL bar effectively becomes a command-line interface for AO's trustless compute environment.A Robust and Scalable Foundation Built on the Erlang/OTP framework, HyperBEAM provides a robust and secure foundation that leverages the BEAM virtual machine for exceptional concurrency, fault tolerance, and scalability. This abstracts away underlying hardware, allowing diverse nodes to contribute resources without compatibility issues. The system governs how nodes coordinate and interact, forming a decentralized network that is resilient and permissionless.

---

# 265. FAQ - HyperBEAM - Documentation

Document Number: 265
Source: https://hyperbeam.arweave.net/build/reference/faq.html
Words: 279
Extraction Method: html

Developer FAQ This page answers common questions about building applications and processes on HyperBEAM.What can I build with HyperBEAM?You can build a wide range of applications, including:Decentralized applications (dApps) Distributed computation systems Peer-to-peer services Resilient microservices IoT device networks Decentralized storage solutions What is the current focus or phase of HyperBEAM development?The initial development phase focuses on integrating AO processes more deeply with HyperBEAM. A key part of this is phasing out the reliance on traditional "dryrun" simulations for reading process state. Instead, processes are encouraged to use the ~patch@1.0 device to expose specific parts of their state directly via GET requests. This allows for more efficient and direct state access, particularly for web interfaces and external integrations. You can learn more about this mechanism in the Exposing Process State with the Patch Device guide.What is the difference between HyperBEAM and Compute Unit?HyperBEAM: The Erlang-based node software that handles message routing, process management, and device coordination.Compute Unit (CU): A NodeJS implementation that executes WebAssembly modules and handles computational tasks.Together, these components form a complete execution environment for AO processes.What programming languages can I use with HyperBEAM?You can use any programming language that compiles to WebAssembly (WASM) for creating modules that run on the Compute Unit. This includes languages like:Lua Rust C/C++ And many others with WebAssembly support How do I debug processes running in HyperBEAM?Debugging processes in HyperBEAM can be done through:Logging messages to the system log (DEBUG=HB_PRINT rebar3 shell) Monitoring process state and message flow Inspecting memory usage and performance metrics Where can I get help if I encounter issues?If you encounter issues:Check the Troubleshooting guide Search or ask questions on GitHub Issues Join the community on Discord

---

# 266. Fuel Your LLM - HyperBEAM - Documentation

Document Number: 266
Source: https://hyperbeam.arweave.net/llms.html
Words: 166
Extraction Method: html

LLM Context Files This section provides access to specially formatted files intended for consumption by Large Language Models (LLMs) to provide context about the HyperBEAM documentation.LLM Summary (llms.txt) Content: Contains a brief summary of the HyperBEAM documentation structure and a list of relative file paths for all markdown documents included in the build.Usage: Useful for providing an LLM with a high-level overview and the available navigation routes within the documentation.LLM Full Content (llms-full.txt) Content: A single text file containing the complete, concatenated content of all markdown documents from the specified documentation directories (begin, run, guides, devices, resources). Each file's content is clearly demarcated.Usage: Ideal for feeding the entire documentation content into an LLM for comprehensive context, analysis, or question-answering based on the full documentation set.Generation Process These files are automatically generated by the docs/build-all.sh script during the documentation build process. They consolidate information from the following directories:docs/run docs/build  Permaweb LLMs.txt An interactive tool for selecting and curating Permaweb documentation into llms.txt format for feeding to LLMs.

---

# 267. Glossary - HyperBEAM - Documentation

Document Number: 267
Source: https://hyperbeam.arweave.net/build/reference/glossary.html
Words: 250
Extraction Method: html

Skip to content
 Developer Glossary This glossary provides definitions for terms and concepts relevant to building on HyperBEAM.AO-Core Protocol The underlying protocol that HyperBEAM implements, enabling decentralized computing and communication between nodes. AO-Core provides a framework into which any number of different computational models, encapsulated as primitive devices, can be attached.Asynchronous Message Passing A communication paradigm where senders don't wait for receivers to be ready, allowing for non-blocking operations and better scalability.Compute Unit (CU) The NodeJS component of HyperBEAM that executes WebAssembly modules and handles computational tasks.Device A functional unit in HyperBEAM that provides specific capabilities to the system, such as storage, networking, or computational resources.Hashpaths A mechanism for referencing locations in a program's state-space prior to execution. These state-space links are represented as Merklized lists of programs inputs and initial states.Message A data structure used for communication between processes in the HyperBEAM system. Messages can be interpreted as a binary term or as a collection of named functions (a Map of functions).Module A unit of code that can be loaded and executed by the Compute Unit, typically in WebAssembly format.Process An independent unit of computation in HyperBEAM with its own state and execution context.Process ID A unique identifier assigned to a process within the HyperBEAM system.WebAssembly (WASM) A binary instruction format that serves as a portable compilation target for programming languages, enabling deployment on the web and other environments.Permaweb Glossary For a more comprehensive glossary of terms used in the permaweb, try the Permaweb Glossary. Or use it below:

---

# 268. Troubleshooting - HyperBEAM - Documentation

Document Number: 268
Source: https://hyperbeam.arweave.net/build/reference/troubleshooting.html
Words: 146
Extraction Method: html

Developer Troubleshooting Guide This guide addresses common issues you might encounter when developing processes for HyperBEAM.Process Execution Fails Symptoms: Errors when deploying or executing processes Solutions:Check both HyperBEAM and CU logs for specific error messages Verify that the WASM module is correctly compiled and valid Test with a simple example process to isolate the issue Adjust memory limits if the process requires more resources Memory Errors in Compute Unit Symptoms: Out of memory errors or excessive memory usage during process execution Solutions:Adjust the PROCESS_WASM_MEMORY_MAX_LIMIT environment variable Enable garbage collection by setting an appropriate GC_INTERVAL_MS Monitor memory usage and adjust limits as needed If on a low-memory system, reduce concurrent process execution Getting Help If you're still experiencing issues after trying these troubleshooting steps:Check the GitHub repository for known issues Join the Discord community for support Open an issue on GitHub with detailed information about your problem

---

# 269. Glossary - HyperBEAM - Documentation

Document Number: 269
Source: https://hyperbeam.arweave.net/run/reference/glossary.html
Words: 286
Extraction Method: html

Skip to content
 Node Operator Glossary This glossary provides definitions for terms and concepts relevant to running a HyperBEAM node.AO-Core Protocol The underlying protocol that HyperBEAM implements, enabling decentralized computing and communication between nodes.Checkpoint A saved state of a process that can be used to resume execution from a known point, used for persistence and recovery.Compute Unit (CU) The NodeJS component of HyperBEAM that executes WebAssembly modules. While developers interact with it more, operators should know it's a key part of the stack.Erlang The programming language used to implement the HyperBEAM core, known for its robustness and support for building distributed, fault-tolerant applications.~flat@1.0 A format used for encoding settings files in HyperBEAM configuration, using HTTP header styling.HyperBEAM The Erlang-based node software that handles message routing, process management, and device coordination in the HyperBEAM ecosystem.Node An instance of HyperBEAM running on a physical or virtual machine that participates in the distributed network.~meta@1.0 A device used to configure the node's hardware, supported devices, metering and payments information, amongst other configuration options.~p4@1.0 A device that runs as a pre-processor and post-processor in HyperBEAM, enabling a framework for node operators to sell usage of their machine's hardware to execute AO-Core devices.~simple-pay@1.0 A simple, flexible pricing device that can be used in conjunction with p4@1.0 to offer flat-fees for the execution of AO-Core messages.~snp@1.0 A device used to generate and validate proofs that a node is executing inside a Trusted Execution Environment (TEE).Trusted Execution Environment (TEE) A secure area inside a processor that ensures the confidentiality and integrity of code and data loaded within it. Used in HyperBEAM for trust-minimized computation.Permaweb Glossary For a more comprehensive glossary of terms used in the permaweb, try the Permaweb Glossary. Or use it below:

---

# 270. How ao messaging works  Cookbook

Document Number: 270
Source: https://cookbook_ao.arweave.net/concepts/how-it-works.html
Words: 395
Extraction Method: html

How ao messaging works Before we dive in to ao, I want to share with you a little information about unix. Unix is a powerful operating system, but in its design it is focused on two Principal "Types". Files and Programs. A File is data and a Program is logic, when you combine the two you get information.Input.file | TransformProgram | Output.file You might have done something like this on the command line without knowing what you were doing. Being able to connect files to programs and return files which can then be passed to other programs creates a complex system composed of simple applications. This is a very powerful idea.Now, lets talk about ao the hyper parallel computer, and lets change the idea of a File to the ao concept of a Message and the idea of a Program to the ao concept of a Process. The ao computer takes messages and sends them to Processes in which those Processes can output messages that can be sent to other Processes. The result is a complex system built on simple modular logic containers.MessageA | Process | MessageB  Here is a description of the process as outlined in the flowchart:A message is initiated from an ao Connect. This message is sent to the mu service using a POST request. The body of the request contains data following a protocol, labeled 'ao', and is of the type 'Message'.The mu service processes the POST request and forwards the message to the su service. This is also done using a POST request with the same data protocol and message type.The su service stores the assignment and message on Arweave.A GET request is made to the cu service to retrieve results based on a message ID. The cu is a service that evaluates messages on processes and can return results based on an individual message identifier.A GET request is made to the su service to retrieve the assignment and message. This request is looking for messages from a process ID, within a range of time from a start (from the last evaluation point) to (to the current message ID).The final step is to push any outbox Messages. It involves reviewing the messages and spawns in the Result object. Based on the outcome of this check, the steps 2, 3, and 4 may be repeated for each relevant message or spawn.

---

# 271. Concepts  Cookbook

Document Number: 271
Source: https://cookbook_ao.arweave.net/concepts/index.html
Words: 200
Extraction Method: html

Skip to content  Concepts This section explains the core concepts and architecture behind AO, helping you understand how the system works at a fundamental level.System Architecture AO is built on a few fundamental principles that form its foundation:Two core types: Messages and Processes - the basic building blocks of the AO ecosystem No shared state, only Holographic State - a unique approach to distributed computing Decentralized Computer (Grid) - enabling truly distributed applications Core Components Explore these foundational concepts to gain a deeper understanding of AO:How it Works - An overview of AO's architecture and how the different parts interact Processes - Learn about processes, the computational units in AO Messages - Understand the messaging system that enables communication Evaluation - Discover how code execution works in the AO environment Units - Learn about the computational units that power the AO network Technical Foundations Specifications - Detailed technical specifications for the AO protocol Getting Started Meet Lua - Introduction to Lua, the programming language used in AO AO System Tour - A guided tour of the AO system and its capabilities Use the sidebar to navigate between concept topics. Each document provides in-depth information about a specific aspect of AO.

---

# 272. Processes  Cookbook

Document Number: 272
Source: https://cookbook_ao.arweave.net/concepts/processes.html
Words: 316
Extraction Method: html

Processes Processes possess the capability to engage in communication via message passing, both receiving and dispatching messages within the network. Additionally, they hold the potential to instantiate further processes, enhancing the network's computational fabric. This dynamic method of data dissemination and interaction within the network is referred to as a 'holographic state', underpinning the shared and persistent state of the network. When building a Process with aos you have the ability to add handlers, these handlers can be added by calling the Handlers.add function, passing a "name", a "match" function, and a "handle" function. The core module contains a helper library that gets injected into the handler function, this library is called ao.The main functions to look at in this ao helper is ao.send(Message) - sends a message to a process ao.spawn(Module, Message) - creates a new process Ethereum Signed Process or Module For an ao Process or Module, if the ANS-104 DataItem was signed using Ethereum keys, then the value in the env.Process.Owner or env.Module.Owner field, respectively, will be the EIP-55 Ethereum address of the signer. For example: 0xfB6916095ca1df60bB79Ce92cE3Ea74c37c5d359 ao.send Example ao.spawn Example ao.env NOTE: ao.env is important context data that you may need as a developer creating processes.The ao.env property contains the Process and Module Reference Objects Both the Process and the Module contain the attributes of the ao Data-Protocol.Summary Processes in the network communicate through message passing and can create new processes, contributing to a 'holographic state' of shared and persistent data. Developers can build a Process using aos by adding handlers through the Handlers.add function with specific name, match, and handle functions. The ao helper library within the core module aids in this process, providing functions like ao.send to dispatch messages and ao.spawn to create new modules, as well as the important ao.env property which contains essential Process and Module information. The ao Data-Protocol outlines the structure and attributes of these elements.

---

# 273. Units  Cookbook

Document Number: 273
Source: https://cookbook_ao.arweave.net/concepts/units.html
Words: 419
Extraction Method: html

Units                                           What is a Unit?The ao Computer is composed of three Unit types, each type contains a set of responsibilities for the computer. And each Unit is horizontally scalable.In ao we have the Messenger Unit or MU, and the Scheduler Unit or SU, and the Compute Unit or the CU. These units are the building blocks of the ao Computer Grid. There can be 1 or more of these units on the network and they work together to power the ao Operating System or aos. Messenger Unit - This unit is the front door to ao, it receives all the messages from the outside and as well as directs traffic flow for Processes. This traffic flow we call pushing. Each process can return an Outbox when it evaluates a Message, and this Outbox can be filled with Messages or requests to Spawn new processes, and the Messenger Unit is responsible for extracting these Messages from the Outbox and signing them and sending them to the Scheduler Units for processing. Scheduler Unit - The Scheduler unit is responsible for ordering the messages, and storing those messages on Arweave. It is important that every message is appropriately ordered so that the evaluation can be replayed and verified. The Scheduler Unit is responsible for this process. It provides the abilty to query it via an endpoint to get the order of messages for evaluation. Compute Unit - The Compute unit is responsible for compute, this unit loads the binary module and manages the memory of that module, so that the execution of the process is alway running on the most up to date memory. The compute unit provides the results of the evaluation back to the the messenger unit, which can then push any messages in the outbox of the given process.Summary The ao Computer consists of three scalable unit types—Messenger Unit (MU), Scheduler Unit (SU), and Compute Unit (CU)—which form the foundation of the ao Computer. These units can exist in multiples on the network and collectively operate the ao Operating System (aos).The MU acts as the entry point, receiving external messages and managing process communications. It processes outgoing messages and spawn requests from process outboxes and forwards them to the SU.The SU ensures messages are properly sequenced and stored on Arweave, maintaining order for consistent replay and verification of message evaluations.The CU handles computation, loading binary modules, and managing memory to ensure processes run with current data. It then returns the evaluation results to the MU for further message handling.

---

# 274. Sending an Assignment to a Process  Cookbook

Document Number: 274
Source: https://cookbook_ao.arweave.net/guides/aoconnect/assign-data.html
Words: 202
Extraction Method: html

Sending an Assignment to a Process Assignments can be used to load Data from another Message into a Process. Or to not duplicate Messages. You can create one Message and then assign it to any number of processes. This will make it available to the Processes you have sent an Assignment to.Sending an Assignment in NodeJS Excluding DataItem fields You can also exclude most DataItem fields which will tell the CU not to load them into your process. You may want to do this if you need only the header data like the Tags and not the Data itself etc... If you exclude the Owner it wont have any effect because the CU requires the Owner, so excluding Owner will be ignored by the CU. Only capitalized DataItem/Message fields will have an effect in the CU.Assigning L1 Transactions You can also assign a layer 1 transaction by passing the baseLayer param into assign. This is useful for minting tokens etc... using the base layer. By default, if the L1 tx does not have at least 20 confirmations the SU will reject it. This can be changed by setting the Settlement-Depth tag to a different number on the Process when it is created.

---

# 275. Calling DryRun  Cookbook

Document Number: 275
Source: https://cookbook_ao.arweave.net/guides/aoconnect/calling-dryrun.html
Words: 121
Extraction Method: html

Skip to content  Calling DryRun DEPRECATION NOTICE This method of reading state is in the process of being deprecated for processes running on HyperBEAM. It is recommended to use the State Patching mechanism to expose state via HTTP for better performance, as calling dryrun was known to cause severe bottlenecks in web applications on legacynet.DryRun is the process of sending a message object to a specific process and getting the Result object back, but the memory is not saved, it is perfect to create a read message to return the current value of memory. For example, a balance of a token, or a result of a transfer, etc. You can use DryRun to obtain an output without sending an actual message.

---

# 276. Connecting to specific ao nodes  Cookbook

Document Number: 276
Source: https://cookbook_ao.arweave.net/guides/aoconnect/connecting.html
Words: 176
Extraction Method: html

Skip to content  Connecting to specific ao nodes When including ao connect in your code you have the ability to connect to a specific MU and CU, as well as being able to specify an Arweave gateway. This can be done by importing the "connect" function and extracting the functions from a call to the "connect" function.You may want to do this if you want to know which MU is being called when you send your message so that later you can debug from the specified MU. You also may want to read a result from a specific CU. You may in fact just prefer a particular MU and CU for a different reason. You can specify the gateway in order to use something other than the default, which is arweave.net.Importing without a call to connect Connecting to a specific MU, CU, and gateway  All three of these parameters to connect are optional and it is valid to specify only 1 or 2 of them, or none. You could pass in just the MU_URL, for example.

---

# 277. Reading results from an ao Process  Cookbook

Document Number: 277
Source: https://cookbook_ao.arweave.net/guides/aoconnect/reading-results.html
Words: 136
Extraction Method: html

Skip to content  Reading results from an ao Process In ao, messages produce results which are made available by Compute Units (CU's). Results are JSON objects consisting of the following fields: messages, spawns, output and error.Results are what the ao system uses to send messages and spawns that are generated by processes. A process can send a message just like you can as a developer, by returning messages and spawns in a result.You may want to access a result to display the output generated by your message. Or you may want to see what messages etc., were generated. You do not need to take the messages and spawns from a result and send them yourself. They are automatically handled by Messenger Units (MU's). A call to results can also provide you paginated list of multiple results.

---

# 278. Chatroom Blueprint  Cookbook

Document Number: 278
Source: https://cookbook_ao.arweave.net/guides/aos/blueprints/chatroom.html
Words: 167
Extraction Method: html

Skip to content  Chatroom Blueprint The Chatroom Blueprint is a predesigned template that helps you quickly build a chatroom in ao. It is a great way to get started and can be customized to fit your needs.Unpacking the Chatroom Blueprint Members: The Members array is used to store the users who have registered to the chatroom.Register Handler: The register handler allows processes to join the chatroom. When a process sends a message with the tag Action = "Register", the handler will add the process to the Members array and send a message back to the process confirming the registration.Broadcast Handler: The broadcast handler allows processes to send messages to all the members of the chatroom. When a process sends a message with the tag Action = "Broadcast", the handler will send the message to all the members of the chatroom.How To Use:Open your preferred text editor.Open the Terminal.Start your aos process.Type in .load-blueprint chatroom Verify the Blueprint is Loaded:Type in Handlers.list to see the newly loaded handlers.

---

# 279. Introduction  Cookbook

Document Number: 279
Source: https://cookbook_ao.arweave.net/guides/aos/intro.html
Words: 524
Extraction Method: html

Introduction aos introduces a new approach to building processes — asynchronous, parallel-executing smart contracts. The ao computer is a decentralized computer network that allows compute to run anywhere and aos in a unique, interactive shell. You can use aos as your personal operating system, your development environment for building ao processes, and your bot army.Lets go over some basic commands.Variables If you want to display the contents of any variable through the console, simply type the variable name.luaName Inbox the Inbox is a collection of messages that your Process has received.luaInbox[1] If you want to get a count of messages, just add the # infront of Inbox.lua#Inbox The process of checking how many messages are in the inbox is a very common pattern. To make this easier, you can create a function that returns the number of messages within the inbox and displays it in the prompt.Use either .editor or .load file to load this function on your process.The Expected Results:Your prompt now has changed to include the number of messages in your inbox.INFO The Inbox is a Lua table (similar to an array) that contains messages received by your process that were not handled by any Handlers. The # operator is used to get the length of a table in Lua - so #Inbox returns the total number of unhandled messages currently in your inbox. This is a common Lua syntax pattern for getting the size/length of tables and strings.Globals In aos process there are some Globals that can make development a little more intuitive.Name Description Type Inbox This is a lua Table that stores all the messages that are received and not handlers by any handlers.Table(Array) Send(Message) This is a global function that is available in the interactive environment that allows you to send messages to Processes function Spawn(Module, Message) This is a global function that is available in the aos interactive environment that allows you to spawn processes  Name a string that is set on init that describes the name of your process string Owner a string that is set on the init of the process that documents the owner of the process, warning if you change this value, it can brick you ability to interact with your process string Handlers a lua Table that contains helper functions that allows you to create handlers that execute functionality based on the pattern matching function on inbound messages table Dump a function that takes any lua Table and generates a print friendly output of the data function Utils a functional utility library with functions like map, reduce, filter module ao this is a core function library for sending messages and spawing processes module Modules In aos there are some built in common lua modules that are already available for you to work with, these modules can be referenced with a "require" function.Name Description json a json module that allows you to encode and decode json documents ao contains ao specific functions like send and spawn.base64 a base64 module that allows you to encode and decode base64 text.pretty a pretty print module using the function tprint to output formatted syntax.utils an utility function library

---

# 280. Understanding the Inbox  Cookbook

Document Number: 280
Source: https://cookbook_ao.arweave.net/guides/aos/inbox-and-handlers.html
Words: 211
Extraction Method: html

Understanding the Inbox In aos, processes are executed in response to messages via handlers. Unhandled messages are routed to the process's Inbox.What are Handlers?A handler is a function that receives and evaluates messages within your process. It acts upon messages by taking them as parameters.Handlers are defined using the Handlers.add() function.The function takes three parameters:Name of the Handler Matcher function Handle function What about Inboxes?An inbox is a storage area for messages that have not yet been processed. Think of it as a holding zone for incoming, or "inbound," items awaiting handling. Once a message is processed, it's no longer considered "inbound" and thus leaves the inbox.Example: Consider the inbox like your voicemail. Just as an unanswered phone call is directed to voicemail for you to address later, messages that your Process doesn't immediately handle are sent to the inbox. This way, unhandled messages are stored until you're ready to process them.Summary Initially, it might seem like all messages are meant to land in your Inbox, which can be puzzling if they disappear after being handled. The analogy of a voicemail should clarify this: much like calls you answer don't go to voicemail, messages you handle won't appear in your Inbox. This illustrates the roles of both the Inbox and Handlers.

---

# 281. Load Lua Files with load filename  Cookbook

Document Number: 281
Source: https://cookbook_ao.arweave.net/guides/aos/load.html
Words: 101
Extraction Method: html

Skip to content  Load Lua Files with.load <filename> This feature allows you to load lua code from a source file on your local machine, this simple feature gives you a nice DX experience for working with aos processes.When creating handlers you may have a lot of code and you want to take advantage of a rich development environment like vscode. You can even install the lua extension to get some syntax checking.So how do you publish your local lua source code to your ao process? This is where the .load command comes into play.hello.lua aos shell lua.load hello.lua Easy Peasy! 🐶

---

# 282. crypto  Cookbook

Document Number: 282
Source: https://cookbook_ao.arweave.net/guides/aos/modules/crypto.html
Words: 2302
Extraction Method: html

crypto Overview The crypto module provides a set of cryptographic primitives like digests, ciphers and other cryptographic algorithms in pure Lua. It offers several functionalities to hash, encrypt and decrypt data, simplifying the development of secure communication and data storage. This document will guide you through the module's functionalities, installation, and usage.Usage Primitives Digests (sha1, sha2, sha3, keccak, blake2b, etc.) Ciphers (AES, ISSAC, Morus, NORX, etc.) Random Number Generators (ISAAC) MACs (HMAC) KDFs (PBKDF2) Utilities (Array, Stream, Queue, etc.)  Digests MD2 Calculates the MD2 digest of a given message.Parameters:stream (Stream): The message in form of stream Returns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:MD4 Calculates the MD4 digest of a given message.Parameters:stream (Stream): The message in form of stream Returns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:MD5 Calculates the MD5 digest of a given message.Parameters:stream (Stream): The message in form of stream Returns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:SHA1 Calculates the SHA1 digest of a given message.Parameters:stream (Stream): The message in form of stream Returns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:SHA2_256 Calculates the SHA2-256 digest of a given message.Parameters:stream (Stream): The message in form of stream Returns: A table containing functions to get digest in different formats. asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:SHA2_512 Calculates the SHA2-512 digest of a given message.Parameters:msg (string): The message to calculate the digest Returns: A table containing functions to get digest in different formats. asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:SHA3 It contains the following functions:sha3_256 sha3_512 keccak256 keccak512 Each function calculates the respective digest of a given message.Parameters:msg (string): The message to calculate the digest Returns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:Blake2b Calculates the Blake2b digest of a given message.Parameters:data (string): The data to be hashed.outlen (number): The length of the output hash (optional) default is 64.key (string): The key to be used for hashing (optional) default is "".Returns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example: Ciphers AES The Advanced Encryption Standard (AES) is a symmetric block cipher used to encrypt sensitive information. It has two functions encrypt and decrypt.Encrypt Encrypts a given message using the AES algorithm.Parameters:data (string): The data to be encrypted.key (string): The key to be used for encryption.iv (string) optional: The initialization vector to be used for encryption. default is "" mode (string) optional: The mode of operation to be used for encryption. default is "CBC". Available modes are CBC, ECB, CFB, OFB, CTR.keyLength (number) optional: The length of the key to use for encryption. default is 128.Returns: A table containing functions to get encrypted data in different formats.asBytes(): The encrypted data as byte table.asHex(): The encrypted data as string in hexadecimal format.asString(): The encrypted data as string format.Decrypt Decrypts a given message using the AES algorithm.Parameters:cipher (string): Hex Encoded encrypted data.key (string): The key to be used for decryption.iv (string) optional: The initialization vector to be used for decryption. default is "" mode (string) optional: The mode of operation to be used for decryption. default is "CBC". Available modes are CBC, ECB, CFB, OFB, CTR.keyLength (number) optional: The length of the key to use for decryption. default is 128.Returns: A table containing functions to get decrypted data in different formats.asBytes(): The decrypted data as byte table.asHex(): The decrypted data as string in hexadecimal format.asString(): The decrypted data as string format.Example:ISSAC Cipher ISAAC is a cryptographically secure pseudo-random number generator (CSPRNG) and stream cipher. It has the following functions seedIsaac: Seeds the ISAAC cipher with a given seed.getRandomChar: Generates a random character using the ISAAC cipher.random: Generates a random number between a given range using the ISAAC cipher.getRandom: Generates a random number using the ISAAC cipher.encrypt: Encrypts a given message using the ISAAC cipher.decrypt: Decrypts a given message using the ISAAC cipher.Encrypt Encrypts a given message using the ISAAC cipher.Parameters:msg (string): The message to be encrypted.key (string): The key to be used for encryption.Returns: A table containing functions to get encrypted data in different formats. asBytes(): The encrypted data as byte table.asHex(): The encrypted data as string in hexadecimal format.asString(): The encrypted data as string format.Decrypt Decrypts a given message using the ISAAC cipher.Parameters:cipher (string): Hex Encoded encrypted data.key (string): Key to be used for decryption.Returns: A table containing functions to get decrypted data in different formats. asBytes(): The decrypted data as byte table.asHex(): The decrypted data as string in hexadecimal format.asString(): The decrypted data as string format.Example:random Generates a random number using the ISAAC cipher.Parameters:min (number) optional: The minimum value of the random number. defaults to 0.max (number) optional: The maximum value of the random number. defaults to 2^31 - 1.seed (string) optional: The seed to be used for generating the random number. defaults to math.random(0,2^32 - 1).Returns: A random number between the given range.Example:Morus Cipher MORUS is a high-performance authenticated encryption algorithm submitted to the CAESAR competition, and recently selected as a finalist.Encrypt Encrypts a given message using the MORUS cipher.Parameters:key (string): The encryption key (16 or 32-byte string).iv (string): The nonce or initial value (16-byte string).msg (string): The message to encrypt (variable length string).ad (string) optional: The additional data (variable length string). defaults to "".Returns: A table containing functions to get encrypted data in different formats. asBytes(): The encrypted data as byte table.asHex(): The encrypted data as string in hexadecimal format.asString(): The encrypted data as string format.Decrypt Decrypts a given message using the MORUS cipher.Parameters:key (string): The encryption key (16 or 32-byte string).iv (string): The nonce or initial value (16-byte string).cipher (string): The encrypted message (variable length string).adLen (number) optional: The length of the additional data (variable length string). defaults to 0.Returns: A table containing functions to get decrypted data in different formats. asBytes(): The decrypted data as byte table.asHex(): The decrypted data as string in hexadecimal format.asString(): The decrypted data as string format.Example:NORX Cipher NORX is an authenticated encryption scheme with associated data that was selected, along with 14 other primitives, for the third phase of the ongoing CAESAR competition. It is based on the sponge construction and relies on a simple permutation that allows efficient and versatile implementations.Encrypt Encrypts a given message using the NORX cipher.Parameters:key (string): The encryption key (32-byte string).nonce (string): The nonce or initial value (32-byte string).plain (string): The message to encrypt (variable length string).header (string) optional: The additional data (variable length string). defaults to "".trailer (string) optional: The additional data (variable length string). defaults to "".Returns: A table containing functions to get encrypted data in different formats. asBytes(): The encrypted data as byte table.asHex(): The encrypted data as string in hexadecimal format.asString(): The encrypted data as string format.Decrypt Decrypts a given message using the NORX cipher.Parameters:key (string): The encryption key (32-byte string).nonce (string): The nonce or initial value (32-byte string).crypted (string): The encrypted message (variable length string).header (string) optional: The additional data (variable length string). defaults to "".trailer (string) optional: The additional data (variable length string). defaults to "".Returns: A table containing functions to get decrypted data in different formats. asBytes(): The decrypted data as byte table.asHex(): The decrypted data as string in hexadecimal format.asString(): The decrypted data as string format.Example: Random Number Generators The module contains a random number generator using ISAAC which is a cryptographically secure pseudo-random number generator (CSPRNG) and stream cipher.Parameters:min (number) optional: The minimum value of the random number. defaults to 0.max (number) optional: The maximum value of the random number. defaults to 2^31 - 1.seed (string) optional: The seed to be used for generating the random number. defaults to math.random(0,2^32 - 1).Returns: A random number between the given range.Example: MACs HMAC The Hash-based Message Authentication Code (HMAC) is a mechanism for message authentication using cryptographic hash functions. HMAC can be used with any iterative cryptographic hash function, e.g., MD5, SHA-1, in combination with a secret shared key.The modules exposes a function called createHmac which is used to create a HMAC instance.Parameters:data (Stream): The data to be hashed.key (Array): The key to be used for hashing.algorithm (string) optional: The algorithm to be used for hashing. default is "sha256". Available algorithms are "sha1", "sha256". default is "sha1".Returns: A table containing functions to get HMAC in different formats. asBytes(): The HMAC as byte table.asHex(): The HMAC as string in hexadecimal format.asString(): The HMAC as string format.Example: KDFs PBKDF2 The Password-Based Key Derivation Function 2 (PBKDF2) applies a pseudorandom function, such as hash-based message authentication code (HMAC), to the input password or passphrase along with a salt value and repeats the process many times to produce a derived key, which can then be used as a cryptographic key in subsequent operations.Parameters:password (Array): The password to derive the key from.salt (Array): The salt to use.iterations (number): The number of iterations to perform.keyLen (number): The length of the key to derive.digest (string) optional: The digest algorithm to use. default is "sha1". Available algorithms are "sha1", "sha256".Returns: A table containing functions to get derived key in different formats. asBytes(): The derived key as byte table.asHex(): The derived key as string in hexadecimal format.asString(): The derived key as string format.Example: Utilities Array Example Usage:size Returns the size of the array.Parameters:arr (Array): The array to get the size of.Returns: The size of the array.fromString Creates an array from a string.Parameters:str (string): The string to create the array from.Returns: The array created from the string.toString Converts an array to a string.Parameters:arr (Array): The array to convert to a string.Returns: The array as a string.fromStream Creates an array from a stream.Parameters:stream (Stream): The stream to create the array from.Returns: The array created from the stream.readFromQueue Reads data from a queue and stores it in the array.Parameters:queue (Queue): The queue to read data from.size (number): The size of the data to read.Returns: The array containing the data read from the queue.writeToQueue Writes data from the array to a queue.Parameters:queue (Queue): The queue to write data to.array (Array): The array to write data from.Returns: None toStream Converts an array to a stream.Parameters:arr (Array): The array to convert to a stream.Returns: (Stream) The array as a stream.fromHex Creates an array from a hexadecimal string.Parameters:hex (string): The hexadecimal string to create the array from.Returns: The array created from the hexadecimal string.toHex Converts an array to a hexadecimal string.Parameters:arr (Array): The array to convert to a hexadecimal string.Returns: The array as a hexadecimal string.concat Concatenates two arrays.Parameters:a (Array): The array to concatenate with.b (Array): The array to concatenate.Returns: The concatenated array.truncate Truncates an array to a given length.Parameters:a (Array): The array to truncate.newSize (number): The new size of the array.Returns: The truncated array.XOR Performs a bitwise XOR operation on two arrays.Parameters:a (Array): The first array.b (Array): The second array.Returns: The result of the XOR operation.substitute Creates a new array with keys of first array and values of second Parameters:input (Array): The array to substitute.sbox (Array): The array to substitute with.Returns: The substituted array.permute Creates a new array with keys of second array and values of first array.Parameters:input (Array): The array to permute.pbox (Array): The array to permute with.Returns: The permuted array.copy Creates a copy of an array.Parameters:input (Array): The array to copy.Returns: The copied array.slice Creates a slice of an array.Parameters:input (Array): The array to slice.start (number): The start index of the slice.stop (number): The end index of the slice.Returns: The sliced array. Stream Stream is a data structure that represents a sequence of bytes. It is used to store and manipulate data in a streaming fashion.Example Usage:fromString Creates a stream from a string.Parameters:str (string): The string to create the stream from.Returns: The stream created from the string.toString Converts a stream to a string.Parameters:stream (Stream): The stream to convert to a string.Returns: The stream as a string.fromArray Creates a stream from an array.Parameters:arr (Array): The array to create the stream from.Returns: The stream created from the array.toArray Converts a stream to an array.Parameters:stream (Stream): The stream to convert to an array.Returns: The stream as an array.fromHex Creates a stream from a hexadecimal string.Parameters:hex (string): The hexadecimal string to create the stream from.Returns: The stream created from the hexadecimal string.toHex Converts a stream to a hexadecimal string.Parameters:stream (Stream): The stream to convert to a hexadecimal string.Returns: The stream as a hexadecimal string. Hex Example Usage:hexToString Converts a hexadecimal string to a string.Parameters:hex (string): The hexadecimal string to convert to a string.Returns: The hexadecimal string as a string.stringToHex Converts a string to a hexadecimal string.Parameters:str (string): The string to convert to a hexadecimal string.Returns: The string as a hexadecimal string. Queue Queue is a data structure that represents a sequence of elements. It is used to store and manipulate data in a first-in, first-out (FIFO) fashion.Example Usage:push Pushes an element to the queue.Parameters:queue (Queue): The queue to push the element to.element (any): The element to push to the queue.Returns: None pop Pops an element from the queue.Parameters:queue (Queue): The queue to pop the element from.element (any): The element to pop from the queue.Returns: The popped element.size Returns the size of the queue.Parameters: None Returns: The size of the queue.getHead Returns the head of the queue.Parameters: None Returns: The head of the queue.getTail Returns the tail of the queue.Parameters: None Returns: The tail of the queue.reset Resets the queue.Parameters: None

---

# 283. Base64  Cookbook

Document Number: 283
Source: https://cookbook_ao.arweave.net/guides/aos/modules/base64.html
Words: 231
Extraction Method: html

Base64 A small base64 module to encode or decode base64 text.Note: It is recommended to enable caching for large chunks of texts for up to x2 optimization.Example usage Module functions encode() This function encodes the provided string using the default encoder table. The encoder can be customized and a cache is available for larger chunks of data.Parameters:str: {string} The string to encode encoder: {table} Optional custom encoding table usecache: {boolean} Optional cache for large strings (turned off by default) Returns: Base64 encoded string Examples decode() This function decodes the provided base64 encoded string using the default decoder table. The decoder can be customized and a cache is also available here.Parameters:str: {string} The base64 encoded string to decode decoder: {table} Optional custom decoding table usecache: {boolean} Optional cache for large strings (turned off by default) Returns: Decoded string Examples makeencoder() Allows creating a new encoder table to customize the encode() function's result.Parameters:s62: {string} Optional custom char for 62 (+ by default) s63: {string} Optional custom char for 63 (/ by default) spad: {string} Optional custom padding char (= by default) Returns: Custom encoder table Examples makedecoder() Allows creating a new decoder table to be able to decode custom-encoded base64 strings.Parameters:s62: {string} Optional custom char for 62 (+ by default) s63: {string} Optional custom char for 63 (/ by default) spad: {string} Optional custom padding char (= by default) Returns: Custom decoder table

---

# 284. Customizing the Prompt in aos  Cookbook

Document Number: 284
Source: https://cookbook_ao.arweave.net/guides/aos/prompt.html
Words: 203
Extraction Method: html

Customizing the Prompt in aos Step 1: Open aos and Start the Editor Launch the aos command-line interface.Enter .editor to open the inline text editor.Step 2: Write the Custom Prompt Function In the editor, define your custom prompt function. For example:Customize "YourName@aos> " to your preferred prompt text.Step 3: Exit and Run Your Code To exit the editor and execute your code, type .done and then press Enter.Your aos prompt should now display the new custom format.Step 4: Save for Future Use (Optional) If you wish to use this prompt in future aos sessions, save your script in a Lua file.In subsequent sessions, load this script to apply your custom prompt.Maximizing Your Prompt There's a great deal of utility and creativity that can come from customizing your prompt. Several things you can do within your prompt are:Tracking the number of unhandled messages you have in your inbox by creating a function that shows how many messages you have.Tracking the number of members are within your process ID's chatroom.Tracking the balance of a specified token that your process ID holds.Conclusion Now that you understand how to maximize the utility within your Prompt, you've now gained a crucial step to streamlining your ao development experience.

---

# 285. Creating a Pingpong Process in aos  Cookbook

Document Number: 285
Source: https://cookbook_ao.arweave.net/guides/aos/pingpong.html
Words: 358
Extraction Method: html

Creating a Pingpong Process in aos This tutorial will guide you through creating a simple "ping-pong" process in aos. In this process, whenever it receives a message with the data "ping", it will automatically reply with "pong". This is a basic example of message handling and interaction between processes in aos.Step 1: Open the aos CLI Start by opening your command-line interface and typing aos to enter the aos environment.Step 2: Access the Editor Type .editor in the aos CLI to open the inline text editor. This is where you'll write your ping-pong handler code.Step 3: Write the Pingpong Handler In the editor, enter the following Lua code to add a handler for the pingpong pattern:This lua script does three things:It adds a new handler named "pingpong".It uses Handlers.utils.hasMatchingData("ping") to check if incoming messages contain the data "ping".If the message contains "ping", Handlers.utils.reply("pong") automatically sends back a message with the data "pong".Step 4: Exit the Editor After writing your code, type .done and press Enter to exit the editor and run the script.Step 5: Test the Pingpong Process To test the process, send a message with the data "ping" to the process. You can do this by typing the following command in the aos CLI:The process should respond with a message containing "pong" in the Inbox.Step 6: Monitor the Inbox Check your Inbox to see the "ping" message and your Outbox to confirm the "pong" reply.luaInbox[#Inbox].Data Step 7: Experiment and Observe Experiment by sending different messages and observe how only the "ping" messages trigger the "pong" response.Step 8: Save Your Process (Optional) If you want to use this process in the future, save the handler code in a Lua file for easy loading into aos sessions.INFO ADDITIONAL TIP:Handler Efficiency: The simplicity of the handler function is key. Ensure that it's efficient and only triggers under the correct conditions.Conclusion Congratulations! You have now created a basic ping-pong process in aos. This tutorial provides a foundation for understanding message handling and process interaction within the aos environment. As you become more comfortable with these concepts, you can expand to more complex processes and interactions, exploring the full potential of aos.

---

# 286. Utils  Cookbook

Document Number: 286
Source: https://cookbook_ao.arweave.net/guides/aos/modules/utils.html
Words: 541
Extraction Method: html

Utils A utility library for generic table manipulation and validation. It supports both curry-styled and traditional programming.Note: It is important to verify that the inputs provided to the following functions match the expected types.Example usage Module functions concat() This function concatenates array b to array a.Parameters:a: {table} The base array b: {table} The array to concat to the base array Returns: An unified array of a and b Examples reduce() This function executes the provided reducer function for all array elements, finally providing one (unified) result.Parameters:fn: {function} The reducer function. It receives the previous result, the current element's value and key in this order initial: {any} An optional initial value t: {table} The array to reduce Returns: A single result from running the reducer across all table elements Examples map() This function creates a new array filled with the results of calling the provided map function on each element in the provided array.Parameters:fn: {function} The map function. It receives the current array element and key data: {table} The array to map Returns: A new array composed of the results of the map function Examples filter() This function creates a new array from a portion of the original, only keeping the elements that passed a provided filter function's test.Parameters:fn: {function} The filter function. It receives the current array element and should return a boolean, deciding whether the element should be kept (true) or filtered out (false) data: {table} The array to filter Returns: The new filtered array Examples find() This function returns the first element that matches in a provided function.Parameters:fn: {function} The find function that receives the current element and returns true if it matches, false if it doesn't t: {table} The array to find an element in Returns: The found element or nil if no element matched Examples reverse() Transforms an array into reverse order.Parameters:data: {table} The array to reverse Returns: The original array in reverse order Example includes() Determines whether a value is part of an array.Parameters:val: {any} The element to check for t: {table} The array to check in Returns: A boolean indicating whether or not the provided value is part of the array Examples keys() Returns the keys of a table.Parameters:table: {table} The table to get the keys for Returns: An array of keys Example values() Returns the values of a table.Parameters:table: {table} The table to get the values for Returns: An array of values Example propEq() Checks if a specified property of a table equals with the provided value.Parameters:propName: {string} The name of the property to compare value: {any} The value to compare to object: {table} The object to select the property from Returns: A boolean indicating whether the property value equals with the provided value or not Examples prop() Returns the property value that belongs to the property name provided from an object.Parameters:propName: {string} The name of the property to get object: {table} The object to select the property value from Returns: The property value or nil if it was not found Examples compose() This function allows you to chain multiple array mutations together and execute them in reverse order on the provided array.Parameters:...: {function[]} The array mutations v: {table} The object to execute the provided functions on Returns: The result from the provided mutations

---

# 287. Connecting to HyperBEAM with aos  Cookbook

Document Number: 287
Source: https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/aos-with-hyperbeam.html
Words: 254
Extraction Method: html

Connecting to HyperBEAM with aos This guide explains how to use aos, the command-line interface for AO, to connect to a HyperBEAM node for development.Installing aos The primary tool for interacting with AO and developing processes is aos, a command-line interface and development environment.Connecting to a HyperBEAM Node While you don't need to run a HyperBEAM node yourself, you do need to connect to one to interact with the network during development.To start aos and connect to a public HyperBEAM node, simply run the command in your terminal:This connects you to an interactive Lua environment running within a process on the AO network. This process acts as your command-line interface (CLI) to the AO network. When you specify --mainnet <URL>, it connects to the genesis_wasm device running on the HyperBEAM node at the supplied URL, allowing you to interact with other processes, manage your wallet, and develop new AO processes.Running a Local HyperBEAM Node If you are running HyperBEAM locally and want to use that node when booting up aos, you must first start your local node with the genesis_wasm profile:Then, you can connect aos to it:Until aos is fully HyperBEAM native, the genesis_wasm profile is required to run a local Compute Unit (CU) for executing aos.Interacting with Mainnet Processes Note on Blocking Calls Blocking message patterns, such as Receive and ao.send().receive(), are not available when running aos against a HyperBEAM process. HyperBEAM processes do not support the underlying wasm modules required for this functionality. You should rely on asynchronous patterns using handlers instead.

---

# 288. Why Migrate to HyperBEAM  Cookbook

Document Number: 288
Source: https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/why-migrate.html
Words: 137
Extraction Method: html

Skip to content  Why Migrate to HyperBEAM?Migrating processes from legacynet to HyperBEAM is essential for leveraging significant advancements in performance, features, and developer experience on AO.HyperBEAM is a new, more robust foundation for decentralized applications on AO, offering several key advantages:Enhanced Performance: Built on an architecture optimized for concurrency, HyperBEAM provides faster message scheduling and more responsive applications.Direct State Access: HyperBEAM allows processes to expose their state directly via HTTP. This enables immediate reads of your process's data, eliminating the need for dry-run messages which were a common performance bottleneck.Easy Extensibility: It allows core feature extensibility through modular devices.The most impactful change when migrating is the ability to expose parts of your process state for immediate reading. This dramatically improves the performance of web frontends and data-driven services.To learn how to implement this, see Exposing Process State.

---

# 289. Cron Messages  Cookbook

Document Number: 289
Source: https://cookbook_ao.arweave.net/references/cron.html
Words: 251
Extraction Method: html

Cron Messages ao has the ability to generate messages on a specified interval, this interval could be seconds, minutes, hours, or blocks. These messages automatically get evaluated by a monitoring process to inform the Process to evaluate these messages over time. The result is a real-time Process that can communicate with the full ao network or oracles in the outside network.Setting up cron in a process The easiest way to create these cron messages is by spawning a new process in the aos console and defining the time interval.When spawning a new process, you can pass a cron argument in your command line followed by the interval you would like the cron to tick. By default, cron messages are lazily evaluated, meaning they will not be evaluated until the next scheduled message. To initiate these scheduled cron messages, call .monitor in aos - this kicks off a worker process on the mu that triggers the cron messages from the cu. Your Process will then receive cron messages every x-interval.lua.monitor If you wish to stop triggering the cron messages simply call .unmonitor and this will stop the triggering process, but the next time you send a message, the generated cron messages will still get created and processed.Handling cron messages Every cron message has an Action tag with the value Cron. Handlers can be defined to perform specific tasks autonomously, each time a cron message is received.Cron messages are a powerful utility that can be used to create "autonomous agents" with expansive capabilities.

---

# 290. ao Module  Cookbook

Document Number: 290
Source: https://cookbook_ao.arweave.net/references/ao.html
Words: 590
Extraction Method: html

ao Module version: 0.0.3 ao process communication is handled by messages, each process receives messages in the form of ANS-104 DataItems, and needs to be able to do the following common operations.ao.send(msg) - send message to another process ao.spawn(module, msg) - spawn a process The goal of this library is to provide this core functionality in the box of the ao developer toolkit. As a developer you have the option to leverage this library or not, but it integrated by default.Properties Name Description Type id Process Identifier (TxID) string _module Module Identifier (TxID) string authorities Set of Trusted TXs string Authority Identifiers that the process is able to accept transactions from that are not the owner or the process (0-n) string _version The version of the library string reference Reference number of the process number env Evaluation Environment object outbox Holds Messages and Spawns for response object assignables List of assignables of the process list nonExtractableTags List of non-extractable tags of the process list nonForwardableTags List of non-forwardable tags of the process list init Initializes the AO environment function send Sends a message to a target process function assign Assigns a message to the process function spawn Spawns a process function result Returns the result of a message function isTrusted Checks if a message is trusted function isAssignment Checks if a message is an assignment function isAssignable Checks if a message is assignable function addAssignable Adds an assignable to the assignables list function removeAssignable Removes an assignable from the assignables list function clearOutbox Clears the outbox function normalize Normalizes a message by extracting tags function sanitize Sanitizes a message by removing non-forwardable tags function clone Clones a table recursively function Environment Schema The ao.env variable contains information about the initializing message of the process. It follows this schema:Example Methods ao.send(msg: Message) Takes a Message as input. The function adds ao -specific tags and stores the message in ao.outbox.Messages.Example ao.spawn(module: string, spawn: Spawn) Takes a module ID string and Spawn as input. Returns a Spawn table with a generated Ref_ tag.Example ao.assign(assignment: Assignment) Takes an Assignment as input. Adds the assignment to ao.outbox.Assignments.Example ao.result(result: Result) Takes a Result as input. Returns the final process execution result.Example ao.isAssignable(msg: Message) Takes a Message as input. Returns true if the message matches a pattern in ao.assignables.Example ao.isAssignment(msg: Message) Takes a Message as input. Returns true if the message is assigned to a different process.Example ao.addAssignable(name: string, condition: function) Adds a named condition function to the process's list of assignables. Messages matching any condition will be accepted when assigned.Note: The condition parameter uses a similar pattern matching approach as the pattern parameter in Handlers.add(). For more advanced pattern matching techniques, see the Handlers Pattern Matching documentation.Example ao.removeAssignable(name: string) Removes a previously added assignable condition from the process's list of assignables.Example luaao.removeAssignable("allowArDrive") ao.isTrusted(msg: Message) Takes a Message as input. Returns true if the message is from a trusted source.Example Custom ao Table Structures Used by: ao.send(), ao.spawn(), ao.normalize(), ao.sanitize() All of the below syntaxes are valid, but each syntax gets converted to { name = string, value = string } tables behind the scenes. We use alternative 1 throughout the documentation for brevity and consistency.Root-level Tag Conversion Any keys in the root message object that are not one of: Target, Data, Anchor, Tags, or From will automatically be converted into Tags using the key as the tag name and its value as the tag value.Message Used by: ao.send(), ao.isTrusted(), ao.isAssignment(), ao.isAssignable(), ao.normalize(), ao.sanitize() Spawn Used by: ao.spawn() Assignment Used by: ao.assign(), ao.result() Result Used by: ao.result()

---

# 291. Handlers (Version 005)  Cookbook

Document Number: 291
Source: https://cookbook_ao.arweave.net/references/handlers.html
Words: 943
Extraction Method: html

Handlers (Version 0.0.5) Overview The Handlers library provides a flexible way to manage and execute a series of process functions based on pattern matching. An AO process responds based on receiving Messages, these messages are defined using the Arweave DataItem specification which consists of Tags, and Data. Using the Handlers library, you can define a pipeline of process evaluation based on the attributes of the AO Message. Each Handler is instantiated with a name, a pattern matching function, and a function to execute on the incoming message. This library is suitable for scenarios where different actions need to be taken based on varying input criteria.Concepts Handler Arguments Overview When adding a handler using Handlers.add(), you provide three main arguments:name (string): The identifier for your handler pattern (table or function): Defines how to match incoming messages handler (function or resolver table): Defines what to do with matched messages Pattern Matching Tables Pattern Matching Tables provide a declarative way to match incoming messages based on their attributes. This is used as the second argument in Handlers.add() to specify which messages your handler should process.Basic Pattern Matching Rules Simple Tag Matching Wildcard Matching Pattern Matching Function-based Matching Common Pattern Examples Balance Action Handler Numeric Quantity Handler Default Action Handlers (AOS 2.0+) AOS 2.0 introduces simplified syntax for Action-based handlers. Instead of writing explicit pattern functions, you can use these shorthand forms:Resolvers Resolvers are special tables that can be used as the third argument in Handlers.add() to enable conditional execution of functions based on additional pattern matching. Each key in a resolver table is a pattern matching table, and its corresponding value is a function that executes when that pattern matches.This structure allows developers to create switch/case-like statements where different functions are triggered based on which pattern matches the incoming message. Resolvers are particularly useful when you need to handle a group of related messages differently based on additional criteria.Module Structure Handlers._version: String representing the version of the Handlers library.Handlers.list: Table storing the list of registered handlers.Common Handler Function Parameters Parameter Type Description name string The identifier of the handler item in the handlers list.pattern table or function Specifies how to match messages. As a table, defines required message tags with string values (e.g. { Action = "Balance", Recipient = "_" } requires an "Action" tag with string value "Balance" and any string "Recipient" tag value). As a function, takes a message DataItem and returns: "true" (invoke handler and exit pipeline), "false" (skip handler), or "continue" (invoke handler and continue pipeline).handler (Resolver) table or function Either a resolver table containing pattern-function pairs for conditional execution, or a single function that processes the message. When using a resolver table, each key is a pattern matching table and its value is the function to execute when that pattern matches. When using a function, it takes the message DataItem as an argument and executes business logic.maxRuns (optional) number As of 0.0.5, each handler function takes an optional function to define the amount of times the handler should match before it is removed. The default is infinity.Functions Handlers.add(name, pattern, handler) Adds a new handler or updates an existing handler by name Handlers.append(name, pattern, handler) Appends a new handler to the end of the handlers list.Handlers.once(name, pattern, handler) Only runs once when the pattern is matched. Equivalent to setting maxRuns = 1. This is the underlying implementation used by the Receive function in the messaging system.Handlers.prepend(name, pattern, handler) Prepends a new handler to the beginning of the handlers list.Handlers.before(handleName) Returns an object that allows adding a new handler before a specified handler.Handlers.after(handleName) Returns an object that allows adding a new handler after a specified handler.Handlers.remove(name) Removes a handler from the handlers list by name.Handler Execution Notes Execution Order Handlers are executed in the order they appear in Handlers.list.When a message arrives, each handler's pattern function is called sequentially to determine if it should process the message.Pattern Function Return Values Pattern functions determine the message handling flow based on their return values:Skip Handler (No Match) Return: 0, false, or any string except "continue" or "break" Effect: Skips current handler and proceeds to the next one in the list Handle and Continue Return: 1 or "continue" Effect: Processes the message and continues checking subsequent handlers Use Case: Ideal for handlers that should always execute (e.g., logging) Handle and Stop Return: -1, true, or "break" Effect: Processes the message and stops checking further handlers Use Case: Most common scenario where a handler exclusively handles its matched message Practical Examples Logging Handler: Place at the start of the list and return "continue" to log all messages while allowing other handlers to process them.Specific Message Handler: Return "break" to handle matched messages exclusively and prevent further processing by other handlers.Handlers.utils The Handlers.utils module provides two functions that are common matching patterns and one function that is a common handle function.hasMatchingData(data: string) hasMatchingTag(name: string, value: string) reply(text: string) Handlers.utils.hasMatchingData(data: string) This helper function returns a pattern matching function that takes a message as input. The returned function checks if the message's Data field contains the specified string. You can use this helper directly as the pattern argument when adding a new handler.Handlers.utils.hasMatchingTag(name: string, value: string) This helper function returns a pattern matching function that takes a message as input. The returned function checks if the message has a tag with the specified name and value. If they match exactly, the pattern returns true and the handler function will be invoked. This helper can be used directly as the pattern argument when adding a new handler.Handlers.utils.reply(text: string) This helper is a simple handle function, it basically places the text value in to the Data property of the outbound message.

---

# 292. References  Cookbook

Document Number: 292
Source: https://cookbook_ao.arweave.net/references/index.html
Words: 202
Extraction Method: html

References This section provides detailed technical references for AO components, languages, and tools. Use these resources to find specific information when implementing your AO projects.Programming Languages Resources for the programming languages used in AO:Lua - Reference for the Lua programming language, the primary language used in AO WebAssembly (WASM) - Information about using WebAssembly modules in AO Lua Optimization - Techniques and best practices for optimizing Lua code in AO AO API Reference Documentation for AO's core APIs and functionality:AO Core - Core ao module and API reference Messaging - Comprehensive guide to the AO messaging system patterns Handlers - Reference for event handlers and message processing Token - Information about token creation and management Arweave Data - Guide to data handling and storage in AO Cron - Documentation for scheduling and managing timed events Development Environment Tools and setup for AO development:Editor Setup - Guide to setting up your development environment for AO BetterIDEa - The ultimate native web IDE for AO development Community Resources Connect with the AO community:Community Resources - Information about AO community resources and support Use the sidebar to navigate between reference topics. References are organized by category to help you find the information you need quickly.

---

# 293. Release Notes  Cookbook

Document Number: 293
Source: https://cookbook_ao.arweave.net/releasenotes/index.html
Words: 181
Extraction Method: html

Skip to content  Release Notes This section provides detailed information about updates, new features, bug fixes, and changes in each release of AO and its related tools. Release notes are essential for understanding what's changed between versions and how these changes might affect your projects.AOS Releases AOS is the operating system built on top of the AO computer. These release notes document changes and improvements to AOS:AOS 2.0.2 - Improved spawn process times and various bug fixes AOS 2.0.1 - Details about patch updates and fixes in the 2.0.1 release AOS 2.0.0 - Major release information, including new features and significant changes Why Read Release Notes?Release notes provide valuable information for developers:Learn about new features that could enhance your applications Understand potential breaking changes that might affect existing code Discover bug fixes that resolve issues you may have encountered Stay informed about security updates and best practices We recommend reviewing release notes before upgrading to a new version to ensure a smooth transition.Use the sidebar to navigate between different release notes. Notes are organized chronologically with the most recent releases first.

---

# 294. Lua Optimization Guide for AO Platform  Cookbook

Document Number: 294
Source: https://cookbook_ao.arweave.net/references/lua-optimization.html
Words: 172
Extraction Method: html

Skip to content  Lua Optimization Guide for AO Platform This guide provides practical tips for writing efficient, fast, and performant Lua code for on-chain programs on the AO platform.Table Operations Appending Elements Removing Elements Variable Access Local Variables Upvalues String Operations String Concatenation Pattern Matching Memory Management Table Reuse Minimize Garbage Creation lua-- ❌ Inefficient: Creates new response table on every transfer

local function createTransferResponse(sender, recipient, amount)

  return {

    from = sender,

    to = recipient,

    quantity = amount,

    success = true,

    newBalance = Balances[sender],

    tags = {

      Action = "Transfer-Complete",

      Type = "Token"

    }

  }

end

-- ✅ Efficient: Reuse template table

local transferResponse = {

  from = nil,

  to = nil,

  quantity = 0,

  success = false,

  newBalance = 0,

  tags = {

    Action = "Transfer-Complete",

    Type = "Token"

  }

}

local function createTransferResponse(sender, recipient, amount)

  transferResponse.from = sender

  transferResponse.to = recipient

  transferResponse.quantity = amount

  transferResponse.success = true

  transferResponse.newBalance = Balances[sender]

  return transferResponse

end Blockchain-Specific Optimizations State Management Additional Resources Lua Performance Guide Special thanks to @allquantor for sharing optimization tips

---

# 295. Begin An Interactive Tutorial  Cookbook

Document Number: 295
Source: https://cookbook_ao.arweave.net/tutorials/begin/index.html
Words: 130
Extraction Method: html

Skip to content  Begin: An Interactive Tutorial In this tutorial series, you'll walk through an interactive steps that will help you deepen your knowledge and understanding of the aos environment.INFO The Exercise In this fun exercise, you'll encounter a series of challenges presented by two familiar characters, Morpheus and Trinity. You'll dive deep into the rabbit hole guided by Morpheus as he presents you with a series of challenges to prove you're the one. Once you've completed all of the challenges presented by both Morpheus and Trinity, you'll receive a token that grants you access to an exclusive chatroom within ao called The Construct.Now, let's get started down the rabbit hole.Tutorials Getting Started - An Interactive Tutorial 1. Quick Start 2. Messaging 3. Creating a Chatroom 4. Build a Token

---

# 296. Messaging in ao  Cookbook

Document Number: 296
Source: https://cookbook_ao.arweave.net/tutorials/begin/messaging.html
Words: 997
Extraction Method: html

Messaging in ao Learn how Messages gives ao Parallel Compute Capability In ao, every process runs in parallel, creating a highly scalable environment. Traditional direct function calls between processes aren't feasible because each process operates independently and asynchronously.Messaging addresses this by enabling asynchronous communication. Processes send and receive messages rather than directly invoking functions on each other. This method allows for flexible and efficient interaction, where processes can respond to messages, enhancing the system's scalability and responsiveness.We'll begin by exploring the basics of messaging in aos, how to see messages received in your inbox, and how to send messages to other processes.Video Tutorial  Step 1: Understand the Message Structure Message Basics: Messages in ao are built using Lua tables, which are versatile data structures that can hold multiple values. Within these tables, the "Data" field is crucial as it contains the message's content or payload. This structure allows for efficient sending and receiving of information between processes, showcasing how ao primitives leverage Arweave's underlying capabilities to facilitate complex, composable operations.For detailed specifications, please refer to the original documentation on the G8way specs page.Example: { Data = "Hello from Process A!" } is a simple message.Step 2: Open the aos CLI Launch the aos command-line interface (CLI) by typing aos in your terminal and pressing Enter.shaos Step 3: How to Send a Message Send: The Send function is globally available in the aos interactive environment.Target: To send a message to a specific process, include a Target field in your message.Data: The Data is the string message (or payload) you want to be received by the receiving process. In this example, the message is "Hello World!".Step 4: Store Morpheus's Process ID We'll use the process ID provided below and store it as a variable called Morpheus.luaFvan28CFY0JYl5f_ETB7d3PDwBhGS8Yq5IA0vcWulUc Copy the process ID above and store it as a variable by running the below command in the aos CLI:This will store the process ID as a variable called Morpheus, making it easier to interact with the specific process ID.Check the Morpheus Variable Step 5: Send a Message to Morpheus After obtaining Morpheus's process ID and storing it in a variable, you're ready to communicate with it. To do this, you use the Send function. Morpheus, himself, is a parallel process running in ao. He receives and sends messages using a series of Handlers. Let's send him a message and see what happens.Your Target is Morpheus which is the variable we defined earlier using Morpheus 's process ID.The Data is the message you want to send to Morpheus. In this case, it's "Morpheus?".Expected Results:You've sent a message to Morpheus and received a response, but you can't read the full message. Let's learn about the Inbox and how to read messages.Step 6: The Inbox The Inbox is where you receive messages from other processes.INFO To see an in depth view of an inbox message, head over to the Messages Concepts page.Let's check your inbox to see how many messages you have received.Inside your aos CLI, type the following command:lua #Inbox If you're actively following through the tutorial, the inbox will not have many messages. However, if you've been experimenting with the aos environment, you may more than 1 message in your inbox.Example Return:In the example above, the return is 4, stating that there are four messages in the inbox.As we're actively looking for Morpheus 's response, we'll assume his message was the last one received. To read the last message in your inbox, type the following command:lua Inbox[#Inbox].Data This command allows you to isolate the Data from the message and only read the contents of the data.The Expected Return:You are now using your own process to communicate with Morpheus, another parallel process running in ao. You're now ready to move on to the next step in the tutorial.Step 7: Sending Messages with Tags Purpose of Tags: Tags in aos messages are used to categorize, route, and process messages efficiently. They play a crucial role in message handling, especially when dealing with multiple processes or complex workflows.Some processes use Handlers that specifically interact with messages that have certain tags. For example, a process may have a handler that only interacts with messages that have a specific tag, which we'll see an example of in the chatroom tutorial.How to Use Tags in Messages In the case of Morpheus, we can use tags to categorize our messages, and because Morpheus is a autonomous process, he has handlers that can interact with messages that have certain tags.Adding Tags to a Message:We already know that the Data of a message is the payload of the message you want to send to another process. Earlier, we sent a message to Morpheus without any tags, in which he used a handler to respond to an exact match within the Data field.Let's Show Morpheus That We're Ready Send Morpheus a message with the tag Action and the value rabbithole.Example:Read the message from Morpheus:luaInbox[#Inbox].Data Expected Return:Additional Tips for Using Tags Consistent Tagging: Develop a consistent tagging system for your application to make message handling more predictable.Tag Naming: Choose clear and descriptive names for your tags. This makes it easier to understand the purpose and context of messages at a glance.Security with Tags: Remember that tags are not encrypted or hidden, so avoid using sensitive information as tags.Advanced Usage of Tags Workflow Management: Tags can be instrumental in managing workflows, especially in systems where messages pass through multiple stages or processes.Additional Tips for Messaging Message Structure: Explore other fields like Epoch, From, and Nonce for more complex messaging needs.Debugging: Use the Dump function to print messages for debugging.Security Considerations: Be cautious with the content and handling of messages, and never send anything considered private or sensitive.Conclusion You've now learned how to send messages with tags, which is a powerful tool for categorizing and routing messages in aos.Morpheus has officially invited you to the next stage of your journey. You're now ready to move on to the next step in the tutorial, Creating a Chatroom.

---

# 297. Building a Chatroom in aos  Cookbook

Document Number: 297
Source: https://cookbook_ao.arweave.net/tutorials/begin/chatroom.html
Words: 733
Extraction Method: html

Building a Chatroom in aos INFO If you've found yourself wanting to learn how to create a chatroom within ao, then that means we understand at least the basic methodology of sending and receiving messages. If not, it's suggested that you review the Messaging tutorial before proceeding.In this tutorial, we'll be building a chatroom within ao using the Lua scripting language. The chatroom will feature two primary functions:Register: Allows processes to join the chatroom.Broadcast: Sends messages from one process to all registered participants.Let's begin by setting up the foundation for our chatroom.Video Tutorial  Step 1: The Foundation Open your preferred code editor, e.g. VS Code.Create a new file named chatroom.lua. Step 2: Creating The Member List In chatroom.lua, you'll begin by initializing a list to track participants: Save the chatroom.lua file Step 3: Load the Chatroom into aos With chatroom.lua saved, you'll now load the chatroom into aos.If you haven't already, start your aos in your terminal inside the directory where chatroom.lua is saved In the aos CLI, type the following script to incorporate your script into the aos process:lua.load chatroom.lua  Type Members, or whatever you named your user list, in aos. It should return an empty array { }. If you see an empty array, then your script has been successfully loaded into aos.Step 4: Creating Chatroom Functionalities The Registration Handler The register handler will allow processes to join the chatroom.Adding a Register Handler: Modify chatroom.lua to include a handler for Members to register to the chatroom with the following code: This handler will allow processes to register to the chatroom by responding to the tag Action = "Register". A printed message will confirm stating Registered. will appear when the registration is successful.Reload and Test: Let's reload and test the script by registering ourselves to the chatroom.Save and reload the script in aos using .load chatroom.lua.Check to see if the register handler loaded with the following script:lua Handlers.list  This will return a list of all the handlers in the chatroom. Since this is most likely your first time developing in aos, you should only see one handler with the name Register.Let's test the registration process by registering ourselves to the chatroom:If successful, you should see that there was a message added to your outbox and that you then see a new printed message that says registered. Finally, let's check to see if we were successfully added to the Members list:lua Members If successful, you'll now see your process ID in the Members list.Adding a Broadcast Handler Now that you have a chatroom, let's create a handler that will allow you to broadcast messages to all members of the chatroom.Add the following handler to the chatroom.lua file:This handler will allow you to broadcast messages to all members of the chatroom.Save and reload the script in aos using .load chatroom.lua.Let's test the broadcast handler by sending a message to the chatroom:Step 5: Inviting Morpheus to the Chatroom Now that you've successfully registered yourself to the chatroom, let's invite Morpheus to join us. To do this, we'll send an invite to him that will allow him to register to the chatroom.Morpheus is an autonomous agent with a handler that will respond to the tag Action = "Join", in which will then have him use your Register tag to register to the chatroom.Let's send Morpheus an invitation to join the chatroom:To confirm that Morpheus has joined the chatroom, check the Members list:luaMembers If successful, you'll receive a broadcasted message from Morpheus.Step 6: Inviting Trinity to the Chatroom Within this message, he'll give you Trinity's process ID and tell you to invite her to the chatroom.Use the same processes to save her process ID as Trinity and to invite her to the chatroom as you did with Morpheus.If she successfully joins the chatroom, she'll then pose the next challenge to you, creating a token.Engaging Others in the Chatroom Onboarding Others Invite aos Users: Encourage other aos users to join your chatroom. They can register and participate in the broadcast.Provide Onboarding Instructions: Share a simple script with them for easy onboarding:Congratulations! You've successfully built a chatroom in ao and have invited Morpheus to join you. You've also created a broadcast handler to send messages to all members of the chatroom.Next, you'll continue to engage with Morpheus, but this time you'll be adding Trinity to the conversation. She will lead you through the next set of challenges. Good Luck!

---

# 298. Interpreting Announcements  Cookbook

Document Number: 298
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/announcements.html
Words: 353
Extraction Method: html

Interpreting Announcements Welcome back to your coding journey. It's time to use the skills you've acquired from previous tutorials to enhance your gaming experience.During the game, you've likely noticed announcements appearing in your terminal. These announcements are the game's way of communicating important events to players. However, these messages can sometimes seem cryptic or you might find yourself checking your inbox frequently for further details.Wouldn't it be convenient to access this information directly from your terminal? Well, there's a way to do that!By using handlers, you can create an autonomous agent to retrieve this information for you, marking the progression from simple bots to entities capable of interpreting and acting on game events directly.Setting up the Development Environment Start by creating a new file named bot.lua in your preferred directory.Ideally, this file should be placed in the same directory where your player process runs to ease the loading of the code. Else, you'll need to use relative paths to access the file.Writing the Code Let's dive into the logic.Each handler in aos requires three key pieces of information:name: A unique name for the handler pattern: A pattern for the handler to identify, triggering its operation handle: The operations to perform when the desired pattern is found.Here's how you can write a handler for printing announcement details:In this case, the name of the handler is "PrintAnnouncements". It uses a special in-built utility (hasMatchingTags) represented by { Action = "Announcement" } to check if the incoming message has been tagged as an announcement. If true, the handler prints the Event and Data, which represent the title and description of the announcement.NOTE Once a message is "handled", it will be discarded from your Inbox.Now, let's bring this to life in the game.Navigate to your aos player terminal and enter a game session.Activate the handler by loading your bot.lua file with:lua.load bot.lua You'll now see game announcements appear directly in your terminal, offering real-time insights without the need to sift through your inbox.Congratulations! You have just taken the first step in building a bot on aos. But let's keep working on adding more features to it 🌐

---

# 299. Fetching Game State  Cookbook

Document Number: 299
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/game-state.html
Words: 438
Extraction Method: html

Fetching Game State Now that you're seeing game announcements directly in your terminal, you have a better grasp of the game's dynamics. However, these insights are limited to specific actions occurring within the game.Wouldn't it be more useful to have on-demand access to comprehensive game data, like the positions, health, and energy of all players? This information could significantly improve your strategic planning, helping you assess threats, opportunities, and timing more effectively.If you thought of adding another handler to the bot created in the previous guide, you're absolutely right!Writing the Code Go back to your bot.lua file and update your existing handler as follows:Adjustments to your handler include:Renaming to "HandleAnnouncements" to reflect its broader role.Addition of an extra operation to request the game for the updated state. The game is designed to respond to the GetGameState action tag.When you get a print of the announcement, you can check the latest message in your Inbox as follows:luaInbox[#Inbox] The Data field of this message contains the latest state of the game which includes:GameMode: Whether the game is in Waiting or Playing state.TimeRemaining: The time remaining for the game to start or end.Players: A table containing every player's stats like position, health and energy.But this can be taken a step further so that you can not just read but also use information from the latest state for other automations.Let's define a new variable that stores the latest state as follows:The syntax preserves existing values of the variable when you load successive iterations of the bot.lua file in your terminal, instead of overwriting it. If there is no pre-existing value then a nil value is assigned to the variable.Then implement another handler as follows:The response from the game process from the previous handler has an action tag with the value GameState that helps us trigger this second handler. Once triggered, the handle function loads the in-built json package that parses the data into json and stores it in the LatestGameState variable.This handler additionally sends a message to your process indicating when the state has been updated. The significance of this feature will be explained in the following section.You can refer to the latest code for bot.lua in the dropdown below:Updated bot.lua file As usual, to test this new feature, load the file in your aos player terminal as follows:lua.load bot.lua Then check the LatestStateVariable to see if it has updated correctly by simply passing its name as follows:luaLatestGameState With real-time access to the latest state of the game you bot is equipped to make informed decisions decide your next action. Next let's try automating actions with the help of this data 🚶

---

# 300. Expanding the Arena  Cookbook

Document Number: 300
Source: https://cookbook_ao.arweave.net/tutorials/bots-and-games/build-game.html
Words: 1799
Extraction Method: html

Expanding the Arena Welcome to the final guide of Chapter 2, where you'll learn to build your own game on top of the arena framework introduced in the previous tutorial. In this guide, we'll take you through the process of creating the "ao-effect" game, which you experienced at the beginning of this chapter. As you progress through this example, you'll gain insights into structuring your game's logic and interacting with the arena's core code.Whether you're a seasoned developer or an aspiring game creator, this guide will empower you to unleash your creativity and bring your unique game ideas to life within the aos environment.Setting up the Development Environment Start by creating a new file named ao-effect.lua in your preferred directory.NOTE Ideally, this file should be placed in the same directory where your game process runs to ease the loading of the code. Else, you'll need to use relative paths to access the file.Writing the Code Now, let's dive into the logic.You'll notice that your game logic will involve calling functions and variables defined in the arena's logic. This showcases the power of composability, where your game builds on top of the existing arena logic, allowing seamless integration of variables and functions between the two. Because both logic become part of a unified logic for the game process.Initializing Game Mechanics First, define essential variables and functions that set the stage for your game's mechanics:lua-- AO EFFECT: Game Mechanics for AO Arena Game

-- Game grid dimensions

Width = 40 -- Width of the grid

Height = 40 -- Height of the grid

Range = 1 -- The distance for blast effect

-- Player energy settings

MaxEnergy = 100 -- Maximum energy a player can have

EnergyPerSec = 1 -- Energy gained per second

-- Attack settings

AverageMaxStrengthHitsToKill = 3 -- Average number of hits to eliminate a player

-- Initializes default player state

-- @return Table representing player's initial state

function playerInitState()

    return {

        x = math.random(Width/8),

        y = math.random(Height/8),

        health = 100,

        energy = 0

    }

end

-- Function to incrementally increase player's energy

-- Called periodically to update player energy

function onTick()

    if GameMode ~= "Playing" then return end  -- Only active during "Playing" state

    if LastTick == undefined then LastTick = Now end

    local Elapsed = Now - LastTick

    if Elapsed >= 1000 then  -- Actions performed every second

        for player, state in pairs(Players) do

            local newEnergy = math.floor(math.min(MaxEnergy, state.energy + (Elapsed * EnergyPerSec // 2000)))

            state.energy = newEnergy

        end

        LastTick = Now

    end

end This code initializes your game's mechanics, including grid dimensions, player energy, and attack settings. The playerInitState function sets up the initial state for players when the game begins.Player Movement Next, add the code for player movement:lua-- Handles player movement

-- @param msg: Message request sent by player with movement direction and player info

function move(msg)

    local playerToMove = msg.From

    local direction = msg.Tags.Direction

    local directionMap = {

        Up = {x = 0, y = -1}, Down = {x = 0, y = 1},

        Left = {x = -1, y = 0}, Right = {x = 1, y = 0},

        UpRight = {x = 1, y = -1}, UpLeft = {x = -1, y = -1},

        DownRight = {x = 1, y = 1}, DownLeft = {x = -1, y = 1}

    }

    -- calculate and update new coordinates

    if directionMap[direction] then

        local newX = Players[playerToMove].x + directionMap[direction].x

        local newY = Players[playerToMove].y + directionMap[direction].y

        -- updates player coordinates while checking for grid boundaries

        Players[playerToMove].x = (newX - 1) % Width + 1

        Players[playerToMove].y = (newY - 1) % Height + 1

        announce("Player-Moved", playerToMove .. " moved to " .. Players[playerToMove].x .. "," .. Players[playerToMove].y .. ".")

    else

        ao.send({Target = playerToMove, Action = "Move-Failed", Reason = "Invalid direction."})

    end

    onTick()  -- Optional: Update energy each move

end The move function calculates new player coordinates based on the chosen direction while ensuring that players remain within the grid boundaries. Player movement adds dynamic interaction to your game and is announced to all players and listeners.Player Attacks Then you must implement the logic for player attacks:lua-- Handles player attacks

-- @param msg: Message request sent by player with attack info and player state

function attack(msg)

    local player = msg.From

    local attackEnergy = tonumber(msg.Tags.AttackEnergy)

    -- get player coordinates

    local x = Players[player].x

    local y = Players[player].y

    -- check if player has enough energy to attack

    if Players[player].energy < attackEnergy then

        ao.send({Target = player, Action = "Attack-Failed", Reason = "Not enough energy."})

        return

    end

    -- update player energy and calculate damage

    Players[player].energy = Players[player].energy - attackEnergy

    local damage = math.floor((math.random() * 2 * attackEnergy) * (1/AverageMaxStrengthHitsToKill))

    announce("Attack", player .. " has launched a " .. damage .. " damage attack from " .. x .. "," .. y .. "!")

    -- check if any player is within range and update their status

    for target, state in pairs(Players) do

        if target ~= player and inRange(x, y, state.x, state.y, Range) then

            local newHealth = state.health - damage

            if newHealth <= 0 then

                eliminatePlayer(target, player)

            else

                Players[target].health = newHealth

                ao.send({Target = target, Action = "Hit", Damage = tostring(damage), Health = tostring(newHealth)})

                ao.send({Target = player, Action = "Successful-Hit", Recipient = target, Damage = tostring(damage), Health = tostring(newHealth)})

            end

        end

    end

end

-- Helper function to check if a target is within range

-- @param x1, y1: Coordinates of the attacker

-- @param x2, y2: Coordinates of the potential target

-- @param range: Attack range

-- @return Boolean indicating if the target is within range

function inRange(x1, y1, x2, y2, range)

    return x2 >= (x1 - range) and x2 <= (x1 + range) and y2 >= (y1 - range) and y2 <= (y1 + range)

end The attack function calculates damage based on attack energy, checks player energy, and updates player health accordingly. Player attacks add the competitive element in your game, allowing players to engage with each other. The attacks are also announced to the players and listeners for real-time updates of the game.Handling the Logic Lastly, you must setup handlers:As seen in earlier guides, the handlers help trigger functions when their respective patterns are met.You can refer to the final code for ao-effect.lua in the dropdown below:Final ao-effect.lua file lua-- AO EFFECT: Game Mechanics for AO Arena Game

-- Game grid dimensions

Width = 40 -- Width of the grid

Height = 40 -- Height of the grid

Range = 1 -- The distance for blast effect

-- Player energy settings

MaxEnergy = 100 -- Maximum energy a player can have

EnergyPerSec = 1 -- Energy gained per second

-- Attack settings

AverageMaxStrengthHitsToKill = 3 -- Average number of hits to eliminate a player

-- Initializes default player state

-- @return Table representing player's initial state

function playerInitState()

    return {

        x = math.random(0, Width),

        y = math.random(0, Height),

        health = 100,

        energy = 0

    }

end

-- Function to incrementally increase player's energy

-- Called periodically to update player energy

function onTick()

    if GameMode ~= "Playing" then return end  -- Only active during "Playing" state

    if LastTick == undefined then LastTick = Now end

    local Elapsed = Now - LastTick

    if Elapsed >= 1000 then  -- Actions performed every second

        for player, state in pairs(Players) do

            local newEnergy = math.floor(math.min(MaxEnergy, state.energy + (Elapsed * EnergyPerSec // 2000)))

            state.energy = newEnergy

        end

        LastTick = Now

    end

end

-- Handles player movement

-- @param msg: Message request sent by player with movement direction and player info

function move(msg)

    local playerToMove = msg.From

    local direction = msg.Tags.Direction

    local directionMap = {

        Up = {x = 0, y = -1}, Down = {x = 0, y = 1},

        Left = {x = -1, y = 0}, Right = {x = 1, y = 0},

        UpRight = {x = 1, y = -1}, UpLeft = {x = -1, y = -1},

        DownRight = {x = 1, y = 1}, DownLeft = {x = -1, y = 1}

    }

    -- calculate and update new coordinates

    if directionMap[direction] then

        local newX = Players[playerToMove].x + directionMap[direction].x

        local newY = Players[playerToMove].y + directionMap[direction].y

        -- updates player coordinates while checking for grid boundaries

        Players[playerToMove].x = (newX - 1) % Width + 1

        Players[playerToMove].y = (newY - 1) % Height + 1

        announce("Player-Moved", playerToMove .. " moved to " .. Players[playerToMove].x .. "," .. Players[playerToMove].y .. ".")

    else

        ao.send({Target = playerToMove, Action = "Move-Failed", Reason = "Invalid direction."})

    end

    onTick()  -- Optional: Update energy each move

end

-- Handles player attacks

-- @param msg: Message request sent by player with attack info and player state

function attack(msg)

    local player = msg.From

    local attackEnergy = tonumber(msg.Tags.AttackEnergy)

    -- get player coordinates

    local x = Players[player].x

    local y = Players[player].y

    -- check if player has enough energy to attack

    if Players[player].energy < attackEnergy then

        ao.send({Target = player, Action = "Attack-Failed", Reason = "Not enough energy."})

        return

    end

    -- update player energy and calculate damage

    Players[player].energy = Players[player].energy - attackEnergy

    local damage = math.floor((math.random() * 2 * attackEnergy) * (1/AverageMaxStrengthHitsToKill))

    announce("Attack", player .. " has launched a " .. damage .. " damage attack from " .. x .. "," .. y .. "!")

    -- check if any player is within range and update their status

    for target, state in pairs(Players) do

        if target ~= player and inRange(x, y, state.x, state.y, Range) then

            local newHealth = state.health - damage

            if newHealth <= 0 then

                eliminatePlayer(target, player)

            else

                Players[target].health = newHealth

                ao.send({Target = target, Action = "Hit", Damage = tostring(damage), Health = tostring(newHealth)})

                ao.send({Target = player, Action = "Successful-Hit", Recipient = target, Damage = tostring(damage), Health = tostring(newHealth)})

            end

        end

    end

end

-- Helper function to check if a target is within range

-- @param x1, y1: Coordinates of the attacker

-- @param x2, y2: Coordinates of the potential target

-- @param range: Attack range

-- @return Boolean indicating if the target is within range

function inRange(x1, y1, x2, y2, range)

    return x2 >= (x1 - range) and x2 <= (x1 + range) and y2 >= (y1 - range) and y2 <= (y1 + range)

end

-- HANDLERS: Game state management for AO-Effect

-- Handler for player movement

Handlers.add("PlayerMove", { Action = "PlayerMove" }, move)

-- Handler for player attacks

Handlers.add("PlayerAttack", { Action = "PlayerAttack" }, attack) Once you've written your game code, it's time to load it into the aos game process and test your game:lua.load ao-effect.lua IMPORTANT Make sure to load the arena blueprint in the same process as well.Invite friends or create test player processes to experience your game and make any necessary adjustments for optimal performance.What's Next Congratulations! You've successfully expanded the arena by building your own game on top of its core functionalities. Armed with the knowledge and tools acquired in this guide, you're now equipped to build games on aos independently.The possibilities are endless. Continue adding more features to existing games or create entirely new ones. The sky's the limit! ⌃◦🚀

---

# 301. Welcome to ao  Cookbook

Document Number: 301
Source: https://cookbook_ao.arweave.net/welcome/index.html
Words: 184
Extraction Method: html

Skip to content  Welcome to ao  AO is a decentralized compute system where countless parallel processes interact within a single, cohesive environment. Each process operates independently, yet they are seamlessly connected through a native message-passing layer, similar to how websites form the World Wide Web.AO + AOS: The rocket and your rocket fuel.Typically when you use AO, you will interact with it through its operating system: AOS.AOS is an abstraction layer that runs in your processes, making it easy to use the full functionality of the AO computer. In this cookbook, you will learn everything you need to know about getting started with the AO computer using AOS.Mainnet and Legacynet AO Mainnet launched on February 8, 2025, paving the way for a decentralized, open-access supercomputer directly connected to the internet with HyperBEAM.AO Legacynet launched on February 27, 2024, providing a fee-free environment for early adopters to experiment with AO’s hyper-parallel architecture.Legacynet Documentation These tutorials explore AO Legacynet, covering everything from building chatrooms to developing autonomous, decentralized bots. It’s a great starting point for experimenting with AO’s hyper-parallel architecture.Legacynet Tutorials Further References Guides Concepts References

---

# 302. AO Processes  Cookbook

Document Number: 302
Source: https://cookbook_ao.arweave.net/welcome/ao-processes.html
Words: 442
Extraction Method: html

AO Processes AO Processes are persistent, programmable smart contracts that live inside the AO computer. Embodying the actor model from Erlang that inspired AO, these processes operate as independent computational units that have their own state and communicate with each other through message passing. This architecture makes them ideal for creating autonomous agents and complex decentralized applications.What are AO Processes?Following the actor model, each AO Process functions as an independent actor within the system, executing code—typically written in Lua—in response to messages it receives. Three core characteristics define them:Stateful: Each process has its own private state and memory, which persist across interactions.Persistent: All processes and their entire message history are permanently stored on Arweave.Generative: Processes can dynamically spawn new processes, enabling complex and evolving systems.AO Processes and the Actor Model The actor model provides several key benefits for process-based development, enabling naturally concurrent and resilient systems. By treating every process as an isolated "actor," it simplifies development and enhances fault tolerance. Key advantages include:Concurrency & Isolation: Processes execute independently and are isolated from each other, enabling parallelism and preventing cascading failures.Message-Passing: All communication happens exclusively through asynchronous messages, simplifying interactions.Location Transparency & Fault Tolerance: Processes can interact without knowing each other's physical location on the network, and the system can continue operating even if individual processes fail.AOS: The Operating System for AO Processes AOS (AO Operating System) is an abstraction layer designed to simplify interaction with AO Processes. It provides developers with a powerful shell interface for sending commands, tools for managing process state, and a set of libraries for common functionalities, all contributing to a more streamlined development experience.Use Cases for AO Processes The persistent and concurrent nature of AO Processes makes them ideal for a wide range of decentralized applications. Here are a few examples:Autonomous Agents & Bots: Imagine a price-monitoring bot that tracks token prices across different decentralized exchanges (DEXs) and executes arbitrage trades automatically. AO makes it possible to build entire marketplaces for such agents, like Marketverse.Decentralized Finance (DeFi): You could build automated market makers (AMMs) or lending protocols where account balances and token reserves are tracked persistently within the process's state. A live example of this is Dexi, a decentralized exchange built on AO.On-Chain Games & Social Platforms: AO Processes can power fully on-chain games where the game state (like player positions or inventory) is managed by one or more processes, like the space strategy game Stargrid. They're also perfect for decentralized chat applications or social networks where user profiles, posts, and interactions are censorship-resistant.Now that you understand the capabilities of AO Processes, the next step is to dive into Hyperbeam, the high-performance network that powers them.

---

# 303. Legacynet  HyperBEAM  Cookbook

Document Number: 303
Source: https://cookbook_ao.arweave.net/welcome/legacynet-info/index.html
Words: 383
Extraction Method: html

Legacynet → HyperBEAM As the AO ecosystem evolves, we are transitioning from Legacynet to HyperBEAM Mainnet, marking a significant upgrade in the implementation of the AO-Core protocol.Legacynet: The Initial Implementation Legacynet was the first implementation of the AO-Core protocol, written in JavaScript. Launched on February 27, 2024, it provided a fee-free environment for early adopters to experiment with AO's hyper-parallel architecture. However, being a JavaScript implementation, Legacynet had inherent limitations in terms of scalability and native support for the actor-oriented model that AO is based on.HyperBEAM: The Future of AO-Core HyperBEAM is the new, advanced implementation of the AO-Core protocol, written in Erlang—the language that inspired AO's actor-oriented design. This implementation innately benefits from Erlang's strengths in:Actor-Oriented Design: Erlang's native support for the actor model aligns perfectly with AO's architecture, where processes (actors) operate independently and communicate via message passing.Scalability: Erlang is renowned for its ability to handle massive concurrency, allowing HyperBEAM to scale efficiently with the growing demands of the AO computer.Reliability: Erlang's design for fault tolerance ensures that HyperBEAM can maintain system stability even under high load or during failures of individual components.The Transition to HyperBEAM While HyperBEAM represents the future of AO, the transition from Legacynet is being handled carefully to ensure a smooth experience for developers. Currently, most development activity remains on Legacynet, which provides a stable environment for building and testing.The goal is to provide a seamless future upgrade path to HyperBEAM Mainnet. While Legacynet will eventually be deprecated, for now, it is the primary environment for new developers to begin building on AO.HyperBEAM Documentation For detailed documentation on the HyperBEAM protocol itself, including running infrastructure and leveraging its powerful URL pathing, visit HyperBEAM.arweave.net.Building on HyperBEAM To learn how to build applications on HyperBEAM using ao and aos, and to migrate existing processes, see the Migrating to HyperBEAM Guide.Preparing for the Future While you build on Legacynet, you can prepare for the future of AO by:Reviewing the HyperBEAM documentation to understand the new environment and its architecture.Exploring the enhanced capabilities that HyperBEAM offers due to its Erlang foundation.Building with the knowledge that a seamless migration path to HyperBEAM Mainnet is a core priority.This transition is a significant step forward for the AO ecosystem, ensuring that we can deliver on the promise of decentralized, hyper-parallel computation at any scale.

---

# 304. ARIO Documentation

Document Number: 304
Source: https://docs.ar.io/
Words: 141
Extraction Method: html

🚀 Get Started Quickly: Explore Our Quick Start Guides → Welcome to the Permaweb Data in paradise. The AR.IO ecosystem is dedicated to cultivating products and protocols for sustaining access to digital permanence, making the permaweb available to everyone. Powered by the ARIO Token, this global network of Gateways connects users to permanently stored data, files, applications, and web pages on the Arweave decentralized storage network.Guides Run a Gateway Get your AR.IO Gateway up and running correctly and quickly.Read more Use ArNS Learn the process of purchasing and managing an ArNS name.Read more Deploy a dApp Learn how to easily deploy a website or application on the permaweb.Read more ANTs on Bazar In a few simple steps, learn how to make an ANT tradable on Bazar.Read more GraphQL Learn how to leverage GraphQL to efficiently fetch data via AR.IO gateways.Read more

---

# 305. Getting Started - ARIO Docs

Document Number: 305
Source: https://docs.ar.io/ar-io-sdk/getting-started
Words: 120
Extraction Method: html

Prerequisites node >= v18.0.0 npm or yarn Installation Quick Start The following examples demonstrate how to use the AR.IO SDK to retrieve a list of active gateways from the Gateway Address Registry (GAR) across different environments.Node Web Polyfills Polyfills are not provided by default for bundled web projects (Vite, ESBuild,
Webpack, Rollup, etc.). Depending on your apps bundler configuration and
plugins, you will need to provide polyfills for various imports including
crypto, process and buffer. Refer to examples/webpack and examples/vite for examples. For other project configurations, refer to your bundler's
documentation for more information on how to provide the necessary polyfills.Output The output for obtaining a list of gateways, regardless of the environment used, will follow the structure outlined below:

---

# 306. ARIO Docs

Document Number: 306
Source: https://docs.ar.io/gateways/admin
Words: 325
Extraction Method: html

AR.IO HTTP API Admin Endpoints Overview The AR.IO HTTP API offers several endpoints that allow access to internal information and the ability to make adjustments without restarting your Gateway. Each of these endpoints behind /ar-io/admin/ have access restricted, so you will need to have set up your ADMIN_API_KEY variable and include "Authorization: "Bearer ${ADMIN_API_KEY}" in the header of your request.When testing endpoints at <your-Gateway>/api-docs, you can enter your ADMIN_API_KEY using the green "Authorize" button near the top of the page, or by clicking any of the open lock icons next to a password protected end point.Debug The ar-io/admin/debug endpoint provides a comprehensive view of the current state of your Gateway. This endpoint has been designed to offer developers and administrators insights into the operational status of the gateway, including any errors or warnings that have occurred since the last startup.Example response Queue Transaction The ar-io/admin/queue-tx endpoint allows you to prioritize processing of a specific transaction, based on that transaction's ID. The id key must be set in the body of your request, and a POST request should be used.This endpoint will also enable you to prioritize opening and indexing bundles by providing the L1 TX ID for the bundle, but only if your Gateway is operating with the ANS104_UNBUNDLE_FILTER and ANS104_INDEX_FILTER keys set.Your Gateway will either respond with an error, or { message: 'TX queued' } Block Data The ar-io/admin/block-data endpoint allows you to tell your Gateway to refuse to serve certain data. In order to add to this block list, make a PUT request to this endpoint with the following in the body:id: This should be the transaction id of the content you want to block.notes: Notes regarding the reason this content was blocked. For documentation purposes only.source: Identifier for the source of TX IDs you are blocking. For example, the name of a public block list. For documentation purposes only.Your Gateway will either respond with an error, or { message: 'Content blocked' }

---

# 307. Gateway Apex Domain Content Resolution - ARIO Docs

Document Number: 307
Source: https://docs.ar.io/gateways/apex
Words: 355
Extraction Method: html

Overview Prior to gateway Release 28, the apex domain of a gateway would only display information about the Arweave network. Release 28 introduced two new environment variables that allow a gateway to serve custom content from the apex domain:APEX_TX_ID: Set to serve content from a specific transaction ID APEX_ARNS_NAME: Set to serve content from an ArNS name These variables enable gateway operators to customize their gateway's apex domain with useful information, details about the operator or associated projects, or any other content they wish to share.Quick Start If you want to serve your project's dApp from the apex domain of your gateway:Upload your dApp to Arweave Assign your dApp's transaction Id to an ArNS name Set the environment variable:APEX_ARNS_NAME=your-ArNS-name Restart your gateway Your dApp will now be served from your gateway's apex domain Configuration Environment Variables You can configure your gateway to serve content from the apex domain by setting one of two environment variables:IMPORTANT You cannot set both variables simultaneously. Providing both variables will result in an error.Restart Requirements The gateway must be restarted after initially setting these environment variables If using APEX_ARNS_NAME, no restart is needed when the ArNS name points to a new transaction If using APEX_TX_ID, the gateway must be restarted when updating the transaction ID Use Cases Gateway operators can use this feature to:Display information about their gateway service Share details about the operator or organization Showcase associated projects and services Share educational content about Arweave and the permaweb Display any other content they wish to make available at their gateway's root domain Community Examples Several gateway operators have already implemented this feature to serve custom content from their apex domains:arnode.asia - Serves a custom landing page with information about their gateway service arlink.xyz - Serves the permaDapp for the Arlink project frostor.xyz / love4src.com - Serves information about the Memetic Block Software Guild and their projects vilenarios.com - Serves personalized portfolio/link tree information about the operator permagate.io - Serves personalized link tree information about the operator These examples demonstrate how gateway operators can leverage the apex domain feature to create a more personalized and informative experience for their users.

---

# 308. Gateway ArNS Resolution - ARIO Docs

Document Number: 308
Source: https://docs.ar.io/gateways/arns-resolution
Words: 624
Extraction Method: html

ArNS Resolution Overview One of the core functions of the AR.IO network gateway is to serve ArNS (Arweave Name System) records. Each ArNS name is assigned a specific "time to live" (TTL) value,
which determines how often gateways should check for updates to the Arweave Transaction ID that the name points to.
This TTL works similarly to a , which controls how often updates are checked for traditional websites. As a result, there may be a delay between when an ArNS record is updated and when users see the updated information in their browser.Effective with gateway Release 23, new features have been implemented on AR.IO gateways to optimize the resolution of ArNS records. These include an option for
gateway operators to override the ArNS TTL, and set their own schedule for checking ArNS names for updates.Initial Caching When a gateway starts up, it will attempt to fetch the records of all ArNS names in order to create a local cache. Previously, this cache was stored in memory. After Release 23, this cache is saved to persistent storage
so that the gateway's ArNS cache will survive restarting the gateway. This prevents delays in resolving ArNS names immediately after a gateway starts up. This cache is saved to the directory data/arns.Cache Refreshing When a new ArNS name is purchased on arns.app (or programmatically using the AR.IO SDK), gateways need to update their local cache to include this new name.
Previously, gateway operators could not control how or when their gateway refreshed its cache. As a result, new names would often take several hours to resolve. With Release 23, once a new name is purchased and
requested from a gateway, the gateway will check if it was already aware of the name's existence. If not, it will refresh its cache to include the records for the new name, allowing immediate resolution.Gateway operators can specify how often their gateway should refresh its cache when it fails to find an ArNS name that has been requested by setting the ARNS_NAME_LIST_CACHE_MISS_REFRESH_INTERVAL_SECONDS value in their .env file.
The default value for this environmental variable is 10 seconds.
Similarly, they can prompt their gateway to refresh their cache of ArNS names when a name is requested and successfully found in the local cache by setting the ARNS_NAME_LIST_CACHE_HIT_REFRESH_INTERVAL_SECONDS, which defaults to 1 hour.Both of these variables can be set to a number, which represents the number of seconds the gateway should wait before refreshing its cache when a name is requested that is, or is not, already in its local cache.Gateway TTL Override Every ArNS record is set with a TTL specified by the name owner. Gateway operators can set the ARNS_RESOLVER_OVERRIDE_TTL_SECONDS variable in their .env file to override this TTL,
and define for themselves how often the gateway should check for updated records. A shorter TTL value will result in more frequent outgoing requests to the ANT that controls the ArNS name, which can result in slower serving of the name data to users, while a longer TTL allows for faster serving of cached data, which may be out of date.TTL Override is disabled by default, and should be set to the number of seconds the gateway should use as its TTL when resolving names.If a gateway operator chooses to override the TTL set by ArNS owners, they must carefully weigh the trade-offs and decide on the balance between performance speed and record currency that best aligns with their priorities and use case.Note that the gateway TTL override does not override what is set in the cache
headers for the name, it only overrides that TTL on the internal cache of the
gateway (meaning the gateway will fetch is more frequently if it wants, but
always respects the TTL when serving it)

---

# 309. Quick Start Guides - ARIO Docs

Document Number: 309
Source: https://docs.ar.io/guides
Words: 124
Extraction Method: html

Quick Start Guides
Deploy a dApp with ArlinkEasily deploy a web app with ArNS using Arlinkmake your ArNS name tradable on BazarHow to display and trade your ArNS ANTs on BazarBuild a dApp using the ArNext frameworkBuild and deploy a dApp for displaying and updating ArNS names using ArNextAutomate deployment using Permaweb DeployAutomate the deployment of your dApp to ArNS using Permaweb Deploy, ArNS, and GithubManaging UndernamesHow to programmatically manage ArNS undernames using the AR.IO SDKQuery data on Arweave using GraphQLSchema and best practices for constructing a GraphQL queryDeploy a dApp with ArDrive webHow to upload a dApp to the permaweb using ArDrive webStory ProtocolHow to register an IP asset on Arweave using the Story ProtocolCrossmint NFT AppHow to create a Crossmint NFT app

---

# 310. Introduction - ARIO Docs

Document Number: 310
Source: https://docs.ar.io/introduction
Words: 591
Extraction Method: html

TL;DR AR.IO seeks to create a decentralized and incentivized cloud network aimed at attracting more gateways to the Arweave network therefore making the permanent web more accessible to all.
At the core of AR.IO's incentivization mechanism is the ARIO Token, a utility token used for joining the network, payments, gateway accountability, and protocol incentives.
The network features modular and composable gateway infrastructure in addition to the Arweave Name System (ArNS) – a system for assigning friendly domain names to permanent data.What is AR.IO AR.IO is the world's first permanent cloud network, providing the infrastructure to ensure data, applications, and digital identities are timeless, tamper-proof, and universally accessible.
Built on the foundation of the Arweave storage network, AR.IO forms a global ecosystem of gateways, protocols, and services that connect users to the permaweb – a web where information is permanent and free from centralized control.The AR.IO Network is an open, distributed, and ownerless system, supported by operators, developers, and end-users from around the world.
It's decentralized nodes, known as AR.IO Gateways, act as "Permanent Cloud Service Providers" delivering the critical services needed to read, write, index and query data stored on the permaweb.
These gateways provide a unified, resilient interface between users and the permaweb, featuring a permanent domain name system and seamless, location-independent access to permanent storage and applications.Gateways operate using standardized protocols to maintain consistency across the network.
They also engage in an observation and reporting protocol to monitor performance and ensure accountability, helping to maintain a healthy and reliable ecosystem.The AR.IO Network is powered by a utility token, ARIO, which drives the network's functionality and accessibility.
ARIO serves as a currency for services such as the Arweave Name System (ArNS), staking to join the network as a gateway operator, delegated staking, and as rewards for contributing to the network's performance and reliability.Together, these elements form the backbone of a permanent cloud network designed to preserve data and expand the possibilities of the web.Why AR.IO?Arweave (a Layer 1 blockchain network) offers scalable and permanent onchain data storage in a sustainable manner.
It does this by incentivizing miner nodes through a tokenomic endowment model which ensures data is globally stored and replicated for hundreds of years without the need for continual payment or maintenance by its uploader.However, the Arweave protocol does not incorporate all the needs of modern applications like data indexing, querying, retrieval, and other vital services.
Consequently, over the past few years, infrastructure services have been independently developed and deployed to meet the demands of the permaweb at scale.
Users and apps have come to rely on these gateway utilities, but they are closed source, have complex codebases, and are expensive to operate.Arweave does not offer any tokenomic incentives to offset the expenses associated with operating a gateway, which has led to the community's reliance on a single centrally controlled gateway subsidized for the betterment of the network: arweave.net.
While arweave.net currently caches and indexes the entire weave with a high quality of service, it is a single bottleneck and point of failure for the whole ecosystem.AR.IO seeks to reduce the barriers of entry and attract more gateway operators to the permaweb with the goal of further enhancing its overall health, resiliency, and functionality through decentralized mechanisms that are as trustless as possible.The solution will be applied in two directions:By reducing gateway overhead costs with open source, efficient, modular networked architecture.By creating an economic incentive layer with the ARIO Token.The overall goal of this white paper is to present the framework for a healthy and sustainable decentralized gateway network.

---

# 311. Staking - ARIO Docs

Document Number: 311
Source: https://docs.ar.io/staking
Words: 534
Extraction Method: html

Overview Staking tokens within the AR.IO Network serves a dual primary purpose: it signifies a public commitment by gateway operators and qualifies them and their delegates for reward distributions.In the AR.IO ecosystem, "staking" refers to the process of locking a specified amount of ARIO tokens into a protocol-controlled vault.
This act signifies an opportunity cost for the staker, acting both as a motivator and a public pledge to uphold the network's collective interests.
Once staked, tokens remain locked until the staker initiates an 'unstake / withdraw' action or reaches the end of the vault’s lock period.It is important to note that the ARIO Token is non-inflationary, distinguishing the AR.IO Network's staking mechanism from yield-generation tools found in other protocols.
Staking in this context is about eligibility for potential rewards rather than direct token yield.
By staking tokens, gateway operators (and their delegates) demonstrate their commitment to the network, thereby gaining eligibility for protocol-driven rewards and access to the network’s shared resources.Gateway Staking A gateway operator must stake tokens to join their gateway to the network, which not only makes them eligible for protocol rewards but also promotes network reliability.
This staking requirement reassures users and developers of the gateway's commitment to the network’s objectives, and gateways that adhere to or surpass network performance standards become eligible for these rewards.
Gateway operators may increase their stake above the minimum, known as excess stake. A gateway’s total stake is impacted the following epoch once excess stake is added or removed.To promote participation from a wider audience, the network shall allow anyone with available ARIO tokens to partake in delegated staking.
In this, users can choose to take part in the risk and rewards of gateway operations by staking their tokens with an active gateway (or multiple gateways) through an act known as delegating.
By delegating tokens to a gateway, a user increases the overall stake of that gateway.
A delegated staker proxies their stake to gateways and therefore entrusts gateway operators to utilize that stake in maintaining a quality of service befitting the permaweb.Stake Redelegation This feature enables existing stakers to reallocate their staked tokens between gateways, known as redelegation.
Both delegated stakers and gateway operators with excess stake (stake above the minimum network-join requirement) can take advantage of this feature.
Redelegation is intended to offer users flexibility and the ability to respond to changing network conditions.Staked tokens generally have restricted liquidity to maintain a healthy degree of stability in the network.
However, an exception to these restrictions allows delegated stakers to use their staked tokens for specific ArNS -related services.
By leveraging their staking rewards, delegates can further engage with ArNS, strengthening the name system’s utilization and impact across the network.Expedited Withdrawal Fees Gateway operators and delegated stakers can shorten the standard withdrawal delay period after initiating a withdrawal (or being placed into an automatic withdrawal by protocol mechanisms); this action is subject to a dynamic fee.
At any point during the delay, users can choose to expedite access to their pending withdrawal tokens by paying a fee to the protocol balance, calculated based on how much sooner they want to receive their funds.
Once triggered, the tokens are returned immediately to the user’s wallet.

---

# 312. ARIO Network Composition - ARIO Docs

Document Number: 312
Source: https://docs.ar.io/network-composition
Words: 265
Extraction Method: html

Overview The permanent web, or "permaweb," is the collection of all webpages, applications, and files stored on the Arweave network and made accessible by the AR.IO permanent cloud.
These range from simple tools for viewing and managing data to sophisticated decentralized applications integrating immutable storage and smart contracts.For users and developers, the permaweb offers low-cost, maintenance-free, and permanent hosting for web apps, data, and pages – serving both traditional and emerging industries.Composition of the Permanent Cloud The AR.IO Network integrates decentralized protocols, services, and applications to power the permanent web alongside the traditional internet.
Foundational components like Arweave and AO are independently developed, while AR.IO introduces essential services and incentives that enable seamless interaction and accessibility. Diagram 1: The Permanent Cloud Network Major Components of the Permanent Cloud:Storage: Arweave At the foundation lies the Arweave protocol, providing decentralized, immutable data storage. This layer ensures data is preserved indefinitely with clear provenance records for long-term reliability.Compute: AO This layer comprises decentralized compute platforms, such as Arweave-native solutions like AO and other Layer 1 smart contract platforms like Ethereum.
These systems enable flexible, data-driven computation and smart contract execution, broadening the ecosystem's capabilities.Services: AR.IO Sitting atop the compute layer, the AR.IO Network provides essential services like data upload, retrieval, indexing, querying, and domain name resolution.
AR.IO gateways ensure the permanent web remains functional, accessible, and usable for developers, creators, and end users.Together, these layers form a cohesive ecosystem, combining data permanence, decentralized computation, and seamless cloud services.
Each layer strengthens the others, creating a resilient foundation for the permaweb while bridging the traditional and decentralized internet paradigms.

---

# 313. ARIO Docs

Document Number: 313
Source: https://docs.ar.io/wayfinder/core/routing-strategies/random
Words: 102
Extraction Method: html

RandomRoutingStrategy Overview The RandomRoutingStrategy selects gateways randomly from the available pool. This strategy provides simple load distribution without maintaining state or performing complex calculations, making it ideal for scenarios where unpredictability is desired or where simplicity is paramount.How It Works Receive Gateway List: Accept the list of available gateways Generate Random Index: Create a random number within the gateway list range Select Gateway: Choose the gateway at the random index Apply Filters: Optionally filter out unhealthy or blocked gateways Return Selection: Return the randomly selected gateway Basic Usage Parameters Related FastestPingRoutingStrategy: Network-based gateway discovery PreferredWithFallbackRoutingStrategy: Static gateway configuration RandomRoutingStrategy: Randomized gateway selection

---

# 314. ARIO Docs

Document Number: 314
Source: https://docs.ar.io/wayfinder/core/routing-strategies/preferred-with-fallback
Words: 117
Extraction Method: html

PreferredWithFallbackRoutingStrategy Overview The PreferredWithFallbackRoutingStrategy attempts to use a designated preferred gateway first, and only falls back to an alternative routing strategy if the preferred gateway fails or is unavailable. This strategy is ideal for applications with dedicated infrastructure or specific gateway requirements.How It Works Health Check: Performs a HEAD request to the preferred gateway with a 1000ms timeout Success: If the preferred gateway responds with a successful status, it's used Failure: If the preferred gateway fails or times out, the fallback strategy is used Logging: All attempts and failures are logged for monitoring Basic Usage Parameters Related FastestPingRoutingStrategy: Network-based gateway discovery RoundRobinRoutingStrategy: Even distribution across gateways RandomRoutingStrategy: Randomized gateway selection StaticRoutingStrategy: Always use a single, fixed gateway

---

# 315. ARIO Docs

Document Number: 315
Source: https://docs.ar.io/wayfinder/core/routing-strategies/fastest-ping
Words: 105
Extraction Method: html

FastestPingRoutingStrategy Overview The FastestPingRoutingStrategy selects the gateway with the lowest latency by performing HEAD requests to all available gateways and choosing the one that responds fastest. This strategy optimizes for performance by dynamically selecting the most responsive gateway for each request.How It Works Ping All Gateways: Send HEAD requests to all available gateways for the provided path and subdomain Measure Response Times: Records the time taken for each gateway to respond Select Fastest: Choose the gateway with the lowest response time Basic Usage Parameters Related PreferredWithFallbackRoutingStrategy: Static gateway configuration RoundRobinRoutingStrategy: Even distribution across gateways RandomRoutingStrategy: Randomized gateway selection StaticRoutingStrategy: Always use a single, fixed gateway

---

# 316. PingRoutingStrategy - ARIO Docs

Document Number: 316
Source: https://docs.ar.io/wayfinder/core/routing-strategies/ping
Words: 176
Extraction Method: html

Overview The PingRoutingStrategy is a wrapper strategy that performs a HEAD check on the gateway returned from a provided routing strategy. This ensures that the selected gateway is healthy and responsive before using it for requests, adding an extra layer of reliability to any routing strategy.How It Works Delegate to Inner Strategy: Use the provided routing strategy to select a gateway Perform Health Check: Send a HEAD request to the selected gateway Validate Response: Check if the gateway responds successfully Return Gateway: If healthy, return the selected gateway; if unhealthy, the strategy may retry or fail Basic Usage Parameters Advanced Usage Combining with Other Strategies Custom Timeout Configuration Use Cases Production Applications: Add health checks to any routing strategy for increased reliability Critical Systems: Ensure gateways are responsive before making important requests High Availability: Combine with other strategies to create robust routing systems Monitoring: Track gateway health through the ping checks Related FastestPingRoutingStrategy: Network-based gateway discovery PreferredWithFallbackRoutingStrategy: Static gateway configuration RoundRobinRoutingStrategy: Even distribution across gateways RandomRoutingStrategy: Randomized gateway selection StaticRoutingStrategy: Always use a single, fixed gateway

---

# 317. ARIO Docs

Document Number: 317
Source: https://docs.ar.io/wayfinder/core/routing-strategies/round-robin
Words: 116
Extraction Method: html

RoundRobinRoutingStrategy Overview The RoundRobinRoutingStrategy distributes requests evenly across all available gateways in a cyclical manner. Each gateway is selected in turn, ensuring fair load distribution and preventing any single gateway from being overwhelmed.How It Works Initialize Gateway List: Start with an ordered list of available gateways Track Current Position: Maintain a pointer to the current gateway in the rotation Select Next Gateway: Choose the next gateway in the sequence Cycle Through List: Return to the first gateway after reaching the end Handle Failures: Skip failed gateways and continue rotation Configuration Basic Usage With Weighted Rotation Parameters Related FastestPingRoutingStrategy: Network-based gateway discovery PreferredWithFallbackRoutingStrategy: Static gateway configuration RandomRoutingStrategy: Randomized gateway selection StaticRoutingStrategy: Always use a single, fixed gateway

---

# 318. Data Root Verification Strategy - ARIO Docs

Document Number: 318
Source: https://docs.ar.io/wayfinder/core/verification-strategies/data-root-verification
Words: 196
Extraction Method: html

DataRootVerificationStrategy Overview The DataRootVerificationStrategy provides the highest level of data integrity verification by validating data using Arweave's Merkle tree proofs. This strategy ensures maximum security by verifying that the data matches the cryptographic data root stored in the transaction, providing mathematical proof of data integrity.Important DataRootVerificationStrategy requires that the trusted gateway has the relevant
transaction data indexed locally. Gateways cannot proxy out verification
requests to other sources, as this would compromise the security and
reliability of the verification process. If a gateway doesn't have the
required data indexed, verification will fail.How It Works Compute Data Root: Chunk the received content and build a Merkle tree Calculate Root Hash: Compute the root hash of the Merkle tree Fetch Trusted Root: Get the data root from trusted gateways via /tx/{txId}/data_root Compare Roots: Verify the calculated root matches the trusted data root Result: Pass or fail based on data root validation Warning ANS-104 Data Items Not Supported: This strategy currently only works with regular Arweave transactions, not ANS-104 bundled data items. If you attempt to verify an ANS-104 data item, it will throw an error.Basic Usage Related Hash Verification: Learn about fast integrity checking Signature Verification: Understand authenticity validation

---

# 319. Verification Strategies - ARIO Docs

Document Number: 319
Source: https://docs.ar.io/wayfinder/core/verification-strategies
Words: 214
Extraction Method: html

Overview Verification strategies in Wayfinder ensure data integrity and authenticity when fetching content from Arweave through AR.IO gateways. These strategies use cryptographic methods to verify that the data you receive matches what was originally stored on Arweave, protecting against tampering, corruption, or malicious gateways.Why Verification Matters Data Integrity: Ensures content hasn't been corrupted during transmission Security: Protects against malicious gateways serving fake data Trust: Provides cryptographic proof that data is authentic Compliance: Meets security requirements for sensitive applications Strategy Comparison Strategy Security Level Best For Use Case Verification Method Hash Verification High Fast integrity checks and development High-throughput applications SHA-256 hash vs trusted gateway digest Signature Verification Very High Authenticity validation and ownership proof Financial or legal documents Transaction signature validation Data Root Verification Highest Maximum security and critical data Production applications Merkle tree data root comparison Important Verification methods require that the gateway being used has the relevant
transaction data indexed locally. Gateways cannot proxy out verification
requests to other sources, as this would compromise the security and
reliability of the verification process. If a gateway doesn't have the
required data indexed, verification will fail.Hash Verification: Learn about fast integrity checking Signature Verification: Understand authenticity validation Data Root Verification: Explore maximum security verification Wayfinder Core: See how to configure verification in your application

---

# 320. StaticRoutingStrategy - ARIO Docs

Document Number: 320
Source: https://docs.ar.io/wayfinder/core/routing-strategies/static
Words: 110
Extraction Method: html

Overview The StaticRoutingStrategy is the simplest routing strategy that always returns a single, pre-configured gateway URL. This strategy ignores any gateways provided by the GatewaysProvider and is useful for scenarios where you want to force all requests to use a specific gateway.How It Works The strategy always returns the configured gateway, ignoring any provided gateway lists:Configure Gateway: Set a single gateway URL during initialization Ignore Provided Gateways: Any gateways from providers are ignored Return Static Gateway: Always return the same configured gateway Log Warnings: Warn when provided gateways are ignored Configuration Basic Usage With Custom Gateway Parameters Related FastestPingRoutingStrategy: Network-based gateway discovery PreferredWithFallbackRoutingStrategy: Static gateway configuration RandomRoutingStrategy: Randomized gateway selection

---

# 321. Community  Cooking with the Permaweb

Document Number: 321
Source: https://cookbook.arweave.net/community/index.html
Words: 109
Extraction Method: html

Community Resources for the Permaweb developer community and archived content.Developer Resources Forums and discussion channels Community projects and collaborations Developer meetups and events Contribution guidelines Archive (Deprecated Content) Historical documentation for reference:SmartWeave (Legacy) SmartWeave - Legacy smart contract system Profit Sharing Tokens (PSTs) - Legacy token standard Atomic Tokens - Legacy NFT implementation Warp SDK (Legacy) Introduction Deploying Contracts Reading State Write Interactions Contract Evolution Deprecated Guides Atomic Tokens Guide Migration Notes The archived content represents older approaches and tools that have been superseded by newer, more efficient solutions. Refer to the current Concepts and Guides for up-to-date information.Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 322. Transaction Bundles  Cooking with the Permaweb

Document Number: 322
Source: https://cookbook.arweave.net/fundamentals/transactions/bundles.html
Words: 296
Extraction Method: html

Transaction Bundles What is a Bundle? A transaction bundle is a special type of Arweave transaction. It enables multiple other transactions and/or data items to be bundled inside it. Because transaction bundles contain many nested transactions they are key to Arweave's ability to scale to thousands of transactions per second.Users submit transactions to a bundling service, such as turbo, which combines them into a 'bundle' with other transactions and posts them to the network.How Do Bundles Help Arweave? Availability Bundling services guarantee that bundled transactions are reliably posted to Arweave without dropping.Transaction IDs of the bundled transactions are immediately made available, meaning the data can instantly be accessed as if it was already on the Arweave network.Reliability Transactions posted to Arweave can occasionally fail to confirm (resulting in a dropped transaction) due to a number of reasons, such as high network activity. In these instances transactions can become orphaned, i.e. stuck in the mempool and eventually removed.Bundlers solve this problem by continually attempting to post bundled data to Arweave, assuring that it does not fail or get stuck in the mempool.Scalability Bundles can store up to 2 256 transactions, each of which are settled as a single transaction on Arweave. This makes Arweave blockspace scale to support almost any use case.What are Nested Bundles? Bundles can include data items for uploading to Arweave and those data item can themselves be a bundle.This means it is possible to upload a bundle of bundles, or in other words nested bundles.Nested bundles have no theoretical limit on nesting depth, meaning that transaction throughput can be increased drastically.Nested bundles might be useful for when you have different groups of bundled data that you want to guarantee reach Arweave altogether, and at the same time.Sources and Further Reading:Ardrive Turbo ANS-104 Standard

---

# 323. Core Concepts  Cooking with the Permaweb

Document Number: 323
Source: https://cookbook.arweave.net/fundamentals/index.html
Words: 116
Extraction Method: html

Core Concepts Understanding the fundamental concepts of Arweave and the Permaweb is essential for building robust decentralized applications.Transaction Fundamentals Transactions - How data is stored and transactions work Posting Transactions Bundles Transaction Types Core Architecture Manifests & Path Resolution - How files and applications are organized Querying Fundamentals - How to find and retrieve data Wallets & Keys - Identity and authentication Gateways & Access - How to access the network Advanced Topics ArNS Introduction - Decentralized naming system Vouch Protocol - Content moderation and reputation File System Specifications ArFS Specification - Arweave File System standard Data Model Entity Types Content Types Privacy Schema Diagrams Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 324. Transactions  Cooking with the Permaweb

Document Number: 324
Source: https://cookbook.arweave.net/fundamentals/transactions/transaction-types.html
Words: 149
Extraction Method: html

Transactions Transactions are the fundamental building blocks of the Arweave network. This section covers how transactions work, how to post them, and the different types available for various use cases.Core Transaction Concepts All data on Arweave is stored as transactions. Each transaction contains data and metadata (tags) that can be queried and retrieved. Understanding these concepts is essential for building on the permaweb.Transaction Guides Posting Transactions Learn the different ways to post transactions to Arweave, including direct posting, bundling services, and dispatched transactions. Understand the trade-offs between settlement time, cost, and reliability.Transaction Tags Discover how to use tags to organize and query transaction data. Tags are key-value pairs that make transactions discoverable and enable complex applications on Arweave.Transaction Bundles Understand how multiple transactions can be bundled together for improved scalability and guaranteed settlement. Bundles are crucial for high-throughput applications.Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 325. Getting Started  Cooking with the Permaweb

Document Number: 325
Source: https://cookbook.arweave.net/getting-started/index.html
Words: 160
Extraction Method: html

Getting Started Welcome to the Permaweb Cookbook! This guide will help you get started building decentralized applications on Arweave.Start Here Welcome & Overview - Introduction to Arweave and the Permaweb Zero-deployed Minimal Full Stack App - Build your first app without writing code Contributing - How to contribute to this cookbook What is the Permaweb?The Permaweb is a global, community-owned web that anyone can contribute to or get paid to maintain. It's built on top of Arweave, a decentralized storage network that permanently stores data.Quick Start Options Choose your path based on your experience level:No-Code - Use visual tools to deploy your first app Frontend Developer - Deploy a React/Vue/Svelte app Full-Stack - Build applications with data storage Blockchain Developer - Integrate with smart contracts Next Steps Once you've completed the getting started guide, explore:Core Concepts to understand how Arweave works Guides for step-by-step tutorials Tooling for development tools Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 326. Contributing Workflow  Cooking with the Permaweb

Document Number: 326
Source: https://cookbook.arweave.net/getting-started/contributing.html
Words: 315
Extraction Method: html

Contributing Workflow Anyone in the community is welcome to contribute to the Permaweb Cookbook, as community members we want a high quality reference guide of little snack bite sized nuggets of information. Below is a step by step workflow of how anyone can contribute to this project.What do you need to know?Git and Github - publishes content to github.com.Markdown - Markdown is a text based markup language that can be transformed into HTML Arweave and the Permaweb - Have some knowledge about the Permaweb that should be shared Steps to Contribute  Commiting work We are using conventional commits for this repository.General flow for making a contribution:Fork the repo on GitHub Clone the project to your own machine Commit changes to your own branch Push your work back up to your fork Submit a Pull request so that we can review your changes NOTE: Be sure to merge the latest from "upstream" before making a pull request!Style Here are some suggestions on tone and style from some contributors:TIP In writing them, I'm getting a feeling for the tone that's appropriate for each. CoreConcepts should be rather textbook like, neutral voice, objective. "This is how Arweave works" For Guides, I think it's ok to have a more personal voice. Refer to the reader as "you" and speak in the collaborative voice "next we'll take a look at..." This may just be personal preference, but in general I feel this tone much more supportive and accessible when following a longer form guide. Indeed, its the voice that most popular tutorials from other ecosystems are written in. For Resources, I think it shares the same voice as core concepts, with a preference for brevity.dmac TIP Conceptual and referencial data should have a more cold scientific tone and guides should be a supportive or even humorous tone. Longer form content needs to pull readers in without them zoning out.Arch_Druid CONTRIBUTING

---

# 327. Cooking with the Permaweb  Cooking with the Permaweb

Document Number: 327
Source: https://cookbook.arweave.net/index.html
Words: 227
Extraction Method: html

Cooking with the Permaweb The Permaweb Cookbook is a developer resource that provides the essential concepts and references for buiding applications on the Permaweb. Each concept and reference will focus on specific aspects of the Permaweb development ecosystem while providing additional details and usage examples.Developers Welcome to the Arweave development community, where the past is forever etched in the blockchain and the future is full of endless possibilities. Let's build the decentralized web together!Read More Contributing The Cookbook is designed in a way that makes it easy for new Permaweb developers to contribute. Even if you don't know how to do something, contributing to the cookbook is a great way to learn!Check out all open issues here. Contribution guidelines here. if you find the cookbook is missing a concept, guide or reference, please add an issue.Read More How to Read the Cookbook The Permaweb Cookbook is split into different sections, each aimed at a different goal.Section Description Core Concepts Building blocks of the Permaweb that are good to know for development Guides Snack-sized guides about different tools for development References References to commonly needed code snippets Starter Kits Front-end Framework Starters to get you started building on the Permaweb in no time Quick Starts These are small guides to help developers from every experience level to ship code the the permaweb.Hello World (No Code) Hello World (CLI)

---

# 328. Guides  Cooking with the Permaweb

Document Number: 328
Source: https://cookbook.arweave.net/guides/index.html
Words: 142
Extraction Method: html

Guides Step-by-step tutorials and practical guides for building on the Permaweb. Guides are currently being reorganized for better user experience.Coming Soon This section is being revamped to organize guides by user personas and use cases:Builder - Frontend and full-stack development guides Explorer - Data querying and analysis tutorials Gamer - Gaming and NFT development Quant - Advanced data analysis techniques Node Operator - Infrastructure and network participation Jack of All Trades - No-code and low-code solutions Current Resources While we reorganize, you can find existing guides in:Posting Transactions - How to send data to Arweave Deploying Manifests - Publishing applications and websites Deployment Tools - CLI tools and automation Querying Arweave - Finding and retrieving data Framework Integration Development kits and framework integrations are being moved to dedicated sections for better organization.Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 329. Bundling Services  Cooking with the Permaweb

Document Number: 329
Source: https://cookbook.arweave.net/tooling/bundlers.html
Words: 523
Extraction Method: html

Bundling Services With bundling services users can post their data transactions to a bundling service to have it "bundled" together with other users transactions and posted as a single Arweave transaction in an upcoming Arweave block.What is a bundle?A description of transaction bundles and their benefits can be found here.Bundles follow the ANS-104 standard, which defines how multiple data items can be efficiently packaged together into a single Arweave transaction. This enables:Cost Efficiency: Multiple small transactions can be bundled together, reducing overall transaction costs Scalability: Higher throughput by processing many data items in a single base layer transaction Flexibility: Support for different payment tokens while the bundler handles AR token payments What is a Bundler node?A bundler is a node which is responsible for accepting transactions or data items from users, bundling them, and posting them to the Arweave network (with a guarantee they will be uploaded with a specific transaction ID).Key Bundling Services:Turbo Repository: Turbo Upload Service Description: A high-performance bundling service developed by ArDrive Features: Fast upload processing, reliable data persistence, and efficient bundling Integration: Widely used across the Arweave ecosystem for production applications How Bundlers Work Data Acceptance: Bundlers receive data items from users along with payment Validation: Each data item is validated for proper formatting and signatures Bundling: Multiple data items are packaged together using ANS-104 standard Network Submission: The bundle is submitted to Arweave as a single base layer transaction Persistence Guarantee: Bundlers ensure data is stored until confirmed on-chain Supporting Multiple Currencies A key feature of bundling services is that because they pay for the base Arweave transaction to be posted (using AR tokens) they can choose to enable payments of storage fees on a variety of different tokens. This is the main entry point for other chains to enable Arweave's permanent storage for their users.Payment Token Support Bundlers can accept various cryptocurrencies for payment while handling the AR token requirements internally:Native AR tokens: Direct payment in Arweave's native currency Ethereum tokens: ETH, USDC, DAI, and other ERC-20 tokens Solana tokens: SOL and SPL tokens Other chains: Support varies by bundler implementation Benefits for Developers Simplified Integration: Developers don't need to manage AR tokens directly User Experience: Users can pay with familiar tokens from their preferred chains Cost Predictability: Bundlers often offer fixed pricing in stable currencies Automatic Handling: Bundlers manage all Arweave network interactions Data Verification and Integrity Modern bundling services implement comprehensive verification systems:Signature Verification: All data items must be properly signed Data Root Verification: Bundle contents are cryptographically verified against Arweave chain data Background Verification: Continuous validation ensures data integrity over time Chunk-based Retrieval: Data can be retrieved and verified through Arweave's chunk system Code Examples Using Turbo SDK Basic Upload with Turbo Upload with Payment (ETH) Using arbundles Library Create and Upload Bundle Manually React Component Example Node.js Batch Upload Check Upload Status Integration with AR.IO Gateways AR.IO gateways provide enhanced support for bundled data:Automatic Unbundling: Gateways can automatically extract and index data items from bundles Optimistic Indexing: Data items can be made available before final confirmation Peer-to-Peer Retrieval: Enhanced data availability through gateway networks Caching: Intelligent caching systems improve data access performance

---

# 330. Tooling  Cooking with the Permaweb

Document Number: 330
Source: https://cookbook.arweave.net/tooling/index.html
Words: 135
Extraction Method: html

Tooling Tools and services organized by the problems they solve for Permaweb developers.Data Upload & Bundling Tools for efficiently uploading data to Arweave:Bundlers - Services that bundle multiple transactions together Data Querying & Indexing Tools for discovering and retrieving data:GraphQL - Powerful query interfaces for transaction data Deployment & Publishing Tools for deploying applications to the Permaweb:CLI Tools - Command-line deployment utilities Framework Integration Development kits are available in the Kits section for:React applications Vue applications Svelte applications When to Use Each Tool Bundlers - When posting multiple transactions or need guaranteed settlement GraphQL - For complex data queries and analytics (optional for most apps) CLI Tools - For automated deployment and CI/CD integration Framework Kits - When building frontend applications with modern frameworks Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 331. Goldsky Search GraphQL Gateway  Cooking with the Permaweb

Document Number: 331
Source: https://cookbook.arweave.net/tooling/graphql/search-indexing-service.html
Words: 616
Extraction Method: html

A specialized GraphQL gateway that extends Arweave's querying capabilities with advanced search features.TL;DR Backwards compatible syntax with Arweave GraphQL Faster response times for complex queries (ie multi-tag search) More query options including fuzzy and wildcard search Enhanced filtering capabilities  Goldsky 's free search service is a GraphQL gateway that uses an optimized backend to provide faster searches for complex queries across arweave blocks and transactions, and also introduces additional querying syntax for fuzzy and wildcard search use-cases.The Search GraphQL syntax is a superset of the Arweave GraphQL syntax. It's fully backwards compatible and will return the same results for the same queries, but has some additional modifiers that can be useful.Flexible tag filters Search for just a tag name or value Advanced tag filters Fuzzy search Wildcard search Filter for L1 transactions only Result set total counts For any custom needs or feature ideas, feel free to contact the Goldsky team through email or on discord!Search Gateway Endpoints Currently, the only service with this syntax is hosted Goldsky. If anybody is interested in hosting their own gateway with the same syntax, feel free to contact the Goldsky for help.Goldsky Search Service Features Flexible Tag Filters The Search Gateway Syntax is less strict, and allows for searching just for the Tag name or value Examples Search for transactions with the tag value 'cat' query just_values {
  transactions(
    first: 10,
    tags: [
      {
        values: ["cat"]
      }
    ]
  ) 
  {
    edges {
      node {
        id
        tags {
          name
          value
        }
      }
    }
  }
} Search for transactions that have an In-Response-To-ID query just_name {
  transactions(
    first: 10,
    tags: [
      {
        name: "In-Response-To-ID"
      }
    ]
  ) 
  {
    edges {
      node {
        id
        tags {
          name
          value
        }
      }
    }
  }
} Advanced tag filters The Search Gateway Syntax offers an additional parameter to the tag filter, match.Match value Description EXACT (default) exact matches only.WILDCARD Enables * to match any amount of characters, ie. text/* FUZZY_AND Fuzzy match containing all search terms FUZZY_OR Fuzzy match containing at least one search term Open up the playground and try some of the following queries!Searching all transactions with an image content type using a wildcard {
    transactions(        
      tags: [
        { name: "Content-Type", values: "image/*", match: WILDCARD}
      ]
      first: 10
    ) {
        edges {
            cursor
            node {
                id
              tags {
                name
                value
              }
              block { height }
              bundledIn {id}
            }
        }
    }
} Fuzzy search is very powerful, and can search for 'similar' text with many variations.Searching all transactions with 'cat' OR 'dog' (or CAT or doG or cAts or CAAts etcs). So the tag could contain at least of cat-like or dog-like term.{
    transactions(        
      tags: [
        { name: "Content-Type", values: ["cat", "dog"], match: "FUZZY_OR"}
      ]
      first: 10
    ) {
        edges {
            cursor
            node {
                id
              tags {
                name
                value
              }
              block { height }
              bundledIn {id}
            }
        }
    }
} Search for transactions that have cat-like AND dog-like tag values {
    transactions(        
      tags: [
        { name: "Content-Type", values: ["cat", "dog"], match: "FUZZY_AND"}
      ]
      first: 10
    ) {
        edges {
            cursor
            node {
                id
              tags {
                name
                value
              }
              block { height }
              bundledIn {id}
            }
        }
    }
} Exclude Bundled (L2) Transactions Simply set bundledIn: NULL query just_l1 {
  transactions(
    first: 10,
    bundledIn: null
  ) 
  {
    edges {
      node {
        id
        signature
        owner {
          address
        }
        block {
          height
        }
      }
    }
  }
} Getting total counts given a query If you'd like to understand how many transactions fit a certain set of filters, just use the count field. This will trigger an additional optimized count operation. This will likely double the time it would take to return the query, so use only when needed.query count_mirror {
  {
      transactions(tags:{values:["MirrorXYZ"]})
      {
        count
      }
  }
}

---

# 332. ANS-101 Gateway Capabilities Endpoint  Cooking with the Permaweb

Document Number: 332
Source: https://cookbook.arweave.net/tooling/specs/ans/ANS-101.html
Words: 248
Extraction Method: html

ANS-101: Gateway Capabilities Endpoint Status: Draft Version: - Abstract This document describes a HTTP endpoint and schema that Arweave Gateways should expose to allow users of the gateway to determine the capabilities of the gateway.Motivation As the ecosystem of gateways onto the Arweave blockchain grows, these gateways will provide different capabilities. Users of the gateway, need to be able to determine if the gateway supports the capabilities they need to run a particular app or use for a particular purpose.Specification 1. HTTP Endpoint Gateways that conforming to this specification MUST expose a HTTP endpoint that responds to a GET request at the path /info/capabilities with a JSON document conforming to the schema in section 2, which accurately lists the capabilities supported by the gateway.2. Response Schema The JSON document returned by the endpoint MUST be an object with a capabilities key, which is an array of capability objects.A capability object has the following schema at a minimum - name capability name. This MUST be a globally unique name.version capability version. This MUST be a semver version string.Capability objects MAY conform to additional schema particular to that capability.Example response body: ( names, versions, and capability objects are examples only. ) {
  "capabilities": [
    { "name": "arql", "version": "1.0.0" },
    { "name": "graphql", "version": "1.0.0" },
    { "name": "arweave-id-lookup", "verson": "1.0.0" },
    { "name": "post-bundled-tx-json", "version": "1.0.0" },
    { "name": "post-delegated-tx", "version": "1.0.0", "maxFee": "0.00125" },
    { "name": "push-on-event-api", "version": "1.0.0", "platforms": ["push-android", "push-ios", "web-push-firefox", "webhook" ] }
  ]
}

---

# 333. ANS-106 Do-Not-Store Request  Cooking with the Permaweb

Document Number: 333
Source: https://cookbook.arweave.net/tooling/specs/ans/ANS-106.html
Words: 324
Extraction Method: html

ANS-106: Do-Not-Store Request Status: Draft-1 Authors: Abhav Kedia abhav@arweave.org, Sam Williams sam@arweave.org Abstract This document describes a transaction format that users of the Arweave network can use to request miners to not store certain kinds of data, for various reasons. In order to request non-storage, upload a transaction with the transaction ID of the data in question, along with tags as specified here. Upon receiving this data, nodes in the network will independently decide whether or not to accept the request.Motivation The Arweave permanent storage protocol enables a truly permanent digital store of media and documents.There might be various reasons why persons or entities might wish to remove some data from the network. These include privacy, government regulation and copyright violations. By uploading a transaction that adheres to this standard, users of the network can ensure that their request is broadcast to all relevant storage node operators.Specification Transaction Format A Do-Not-Store Request transaction MUST be a transaction with the following tags:Tag Name Optional?Tag Value App-Name False Do-Not-Store Do-Not-Store False Arweave Txn ID of the data in question Category True Category-Tag Geography True If requesting removal in particular countries, include the 2 letter Country-Code (Alpha-2, ISO 3166) Content-Type True A guide to the type of content included in the body of the transaction, in order to aid rendering.The body of the transaction must then include a text description of the reason for the case describing why the data should not be stored. Multiple Do-Not-Store tags may be added to request non-storage of multiple data items at once.Category-Tags Category tags are a short-hand way of specifying the reason for removal. Common tags include Private, Regulation, Copyright. Custom category tags may be used as appropriate. Custom tags must not exceed 50 characters.Future work There may be a need to request removal based on various properties of data or tags associated with them. This standard may be extended to allow for lists of transactions and/or associated filtering criteria.

---

# 334. ANS-105 License Tags  Cooking with the Permaweb

Document Number: 334
Source: https://cookbook.arweave.net/tooling/specs/ans/ANS-105.html
Words: 453
Extraction Method: html

Authors: Sam Williams sam@arweave.org, Abhav Kedia abhav@arweave.org Abstract This standard outlines a mechanism for arbitrarily tagging any transaction on the permaweb with the license under which it is published. The standard focuses on simplicity and ease of use in order to encourage broad adoption across the permaweb ecosystem.Motivation The goal of the Arweave ecosystem is to create a permanent, collective commons of all valuable knowledge and history, available to all people at any time. Just like the original web, by its open nature this ecosystem encourages wide reuse and copying of data. On the traditional web, this has led to a broad corpus of widely shared information that lacks associated licensing data.By making simple use of the tagging system exposed by Arweave transactions and data entries we can avoid a repeat of this situation, and instead build a web where it is common practice to communicate the license of any piece of data. Due to the inseparable nature of Arweave tags from their associated data, the licensing data of all complying transactions will be atomically communicated to uses -- it is impossible to communicate a link to the data, without also transmitting its licensing information. Due to the nature of Arweave's tagging system, this is achieved without any mandatory or noticeable effect on user experience (when the data is rendered in browsers, etc).Specification The license tagging format is composed of a single additional tag on any transaction on the Arweave network, as well as two additional transaction formats for defining license types.In order to publish the data of a transaction under a given transaction format, simply add the following tags:Tag Name Optional?Tag Value License False TXID of License Definition Title True The title of the work Creator True The name/identifier of the creator(s) of the work Source True A link to the source where the material was obtained, if applicable In order to assert that a previously uploaded transaction has a given license, provide the following tags:Tag Name Optional?Tag Value App-Name False "License-Assertion" Original False TXID of original upload License False TXID of updated License Definition In order to define a license or related legal tool (such as CC0), submit a transaction with the following tags:Tag Name Optional?Tag Value App-Name True "License-Definition" Logo True TXID of License Logo Short-Name True A short (max 64 characters) 'ticker' with which to refer to the license Content-Type True The MIME type of the contained file, describing the license The body of the transaction must contain the legal text of the license being defined.License Logo transactions should be defined as follows:Tag Name Optional?Tag Value Content-Type False The MIME type of the contained file The body of the transaction must contain the visual representation of the license's logo.

---

# 335. ANS-110 Asset Discoverability  Cooking with the Permaweb

Document Number: 335
Source: https://cookbook.arweave.net/tooling/specs/ans/ANS-110.html
Words: 379
Extraction Method: html

ANS-110: Asset Discoverability Status: Draft Authors: Tom Wilson (tom@hyper.io), Sam Williams (sam@arweave.org), Abhav Kedia (abhav@arweave.org) Abstract This document specifies a data protocol that allows assets to be discovered and uniformly displayed on the permaweb.Motivation The permaweb is a rich collection of various kinds of data -- media, content, functions, and applications. A standard protocol for identifying assets enables discoverability for dashboards, exchanges, and higher-level permaweb services. By providing an extensible data protocol for transactions, creators and publishers can harness the power of permaweb-wide content discoverability -- making it easily appear in various applications and contexts across the permaweb.This protocol would enable, for example, a marketplace or exchange where users trade media assets to render them in a user-friendly way. By extending asset transactions using these identifiers, creators provide a clear and composable set of identifiers that marketplaces or exchanges can use to give a detailed description of the asset. Another example would be a search engine service that may want to index specific types of assets.Specfication Transaction Format To utilize Asset Discoverability, a creator or publisher can dispatch or post an Arweave transaction specifying the following tags.Tag Name Optional?Tag Value Title False A maximum of 150 characters used to identify the content, this title can provide a quick eye catching description of the asset Type* False Type of asset. One or more of: meme, image, video, podcast, blog-post, social-post, music, token, web-page, profile Topic* True Zero to many topics that can be used to locate assets of a given type by a specific topic. For example: an asset of type meme might have the following two topics, Funny, Sports.Description True A longer description of 300 characters that can provide a set of details further describing the asset Usage The primary purpose of these tags is to allow content devs to leverage GraphQL to find asset transactions of a specific type or topic for use in their applications.query {
  transactions(
      first: 100, 
        tags: [
          { name: "Type", values: ["meme", "blog-post"] }, 
          { name: "Topic:Funny", values: ["Funny"] },
          { name: "Topic:Jokes", values: ["Jokes"] }
        ]) {
      edges {
          node {
              id
                owner {
                  address
                }
                tags {
                  name
                    value
                }
        }}
    }
} This GraphQL query filters based on Type and Topic tags to filter Assets for an aggregate list display.

---

# 336. ANS-109 Vouch-For (Assertion of Identity)  Cooking with the Permaweb

Document Number: 336
Source: https://cookbook.arweave.net/tooling/specs/ans/ANS-109.html
Words: 303
Extraction Method: html

ANS-109: Vouch-For (Assertion of Identity) Status: Draft (Version 0.1) Authors: Abhav Kedia (abhav@arweave.org), Sam Williams (sam@arweave.org), Tom Wilson (tom@hyper.io) Abstract This document specifies a transaction format that allows addresses to vouch for the identity of other addresses on the permaweb.Motivation Sybil resistance is a necessary component of most applications on the permaweb. A transaction format that allows addresses to vouch for the identity of other addresses enables human and programmatic Verifiers to confirm the humanity of addresses. All other applications can then utilize this information as a primitive for identity verification, with various safeguards that can be built on top.One example of abstractions and safeguards built on top of such a system could be a "VouchDAO" - a community that specifies which human "Verifiers" or "Verification Services" they deem to be trustworthy at a given point in time.Specfication Transaction Format A Verifier can assert the identity of an address using the Vouch-For standard by sending a transaction with the following tags.Tag Name Optional?Tag Value App-Name False Vouch Vouch-For False Arweave address that is being vouched for in this transaction App-Version True 0.1 Verification-Method True Method of verification of identity for the person. Example - Twitter / In-Person / Gmail / Facebook User-Identifier True An identifier for the user based on the Verification Method. Example - abhav@arweave.org Usage Users of this standard can run a graphql query on the arweave network with transactions of the Vouch-For standard that vouch for a particular address to be verified. For example,query {
  transactions(
    tags:{name:"Vouch-For", values:["0L_z90sYv36VDoDhrRBffo9KrADWpCaaGQz7hJhhP9g"]}
  ) {
    edges {
      node {
        id
        tags {
          name 
          value 
        }
      }
    }
  }
} This query returns all vouches for the address 0L_z90sYv36VDoDhrRBffo9KrADWpCaaGQz7hJhhP9g. Additional filters (such as those by an implementation of "VouchDAO" as outlined above) can be applied by filtering for specific owners that have been designated as Verifiers.

---

# 337. Get started  WAO

Document Number: 337
Source: https://docs.wao.eco/getting-started
Words: 201
Extraction Method: html

Lightning Fast AO Testing Framework WAO SDK streamlines Arweave/AO development with elegant syntax enhancements and seamless message piping for enjoyable coding experiences. GraphQL operations are also made super easy.Succinct AO SDK with Syntactic Sugar Additionally, it includes a drop-in replacement for aoconnect, allowing the testing of lua scripts 1000x faster than the mainnet by emulating AO units in memory. It's even 100x faster than testing with arlocal and ao-localnet.Standalone Local AO Units WAO is not only for in-memory testing, but also for lightweight standanoe AO units and web embeddable AO units. You can type a simple command npx wao, and you will have your own AO units on your local computer.AO in the Browser You can open up a browser at preview.wao.eco and you will have all the AO units (MU/SU/CU/Gateway) right in your browser with an AOS terminal, a coding editor, and a local AO explorer to make debugging easier.HyperBEAM SDK WAO is also compatible with HyperBEAM and Mainnet AOS processes. You can launch a HyperBEAM node from withing JS test code and create a sandbox environment for the test.HyperBEAM Custom Device Testing You can also create custom HyperBEAM devices in Erlang, Rust, and C++, then test them in JS.

---

# 338. Decoding HyperBEAM from Scratch  WAO

Document Number: 338
Source: https://docs.wao.eco/hyperbeam/decoding-from-scratch
Words: 437
Extraction Method: html

Welcome to the complete guide to understanding HyperBEAM from the ground up.What This Tutorial Is For This tutorial series takes you on a deep dive into HyperBEAM internals, teaching you everything from basic concepts to advanced implementations. You'll learn by doing - writing tests with WAO (HyperBEAM SDK in JS) that interact directly with HyperBEAM nodes.Why WAO + HyperBEAM?WAO transforms the complex world of HyperBEAM development into something approachable:JavaScript-First Development: Everything is JavaScript - no need to context-switch between languages during testing Sandbox Testing Environment: WAO automatically spins up isolated HyperBEAM nodes for each test suite, ensuring clean, reproducible tests Simplified APIs with Syntactic Sugar: WAO's succinct API abstracts away complexity while giving you full control when needed Automatic Codec & Signing Handling: Complex encoding/decoding pipelines and HTTP message signatures are handled seamlessly in the background What You'll Master By working through this tutorial series, you'll gain:Deep Protocol Knowledge: Understand how messages flow through HyperBEAM's codec pipeline, from structured encoding to HTTP signatures Device Development Skills: Build custom HyperBEAM devices that extend the platform's capabilities Process Management Expertise: Learn to spawn, schedule, and manage stateful computations AOS Integration Know-How: Connect HyperBEAM with the broader AO ecosystem Payment System Understanding: Implement complex payment flows using HyperBEAM's built-in devices This isn't just theoretical knowledge - you'll have practical tools and patterns to build production-ready applications on HyperBEAM and interact seamlessly with AOS processes.The Journey Ahead This series follows a carefully crafted path from basics to advanced topics. Each chapter builds on the previous ones, so it's important to follow them in order:Part 1: Foundations Chapter Topic 1 Installing HyperBEAM and WAO - Setting up your development environment 2 Devices and Pathing - Understanding HyperBEAM's URL routing and device system 3 Custom Devices and Codecs - Building your first custom device and learning about codecs Part 2: The Codec System Chapter Topic 4 Flat Codec - Deep dive into path flattening for HTTP compatibility 5 Structured Codec - Handling complex data types with AO's type system 6 Httpsig Codec - Preparing messages for HTTP signatures Part 3: Security & Verification Chapter Topic 7 HTTP Message Signatures - Implementing RFC-9421 for message verification 8 Hashpaths - Understanding compute verification through chained hashes Part 4: Advanced Concepts Chapter Topic 9 Device Composition - Combining devices for powerful workflows 10 Processes and Scheduler - Managing stateful computations 11 Legacynet Compatible AOS - Integrating with the AOS ecosystem 12 Payment System - Building complex payment systems with p4@1.0 Resources Working Test Suite - Complete test files for all chapters HyperBEAM Implementation with Tutorial Devices - HyperBEAM fork with all custom devices from this tutorial

---

# 339. Custom Devices in Rust  WAO

Document Number: 339
Source: https://docs.wao.eco/tutorials/devices-rust
Words: 254
Extraction Method: html

Erlang can natively execute Rust functions with almost no overhead via NIF (Native Implemented Function).Creating a Rust Device Go to HyperBEAM/native directory and create a new Rust project.cargo new dev_add_nif --lib && cd dev_add_nif Step 1: Configure Cargo.toml Change crate-type and add rustler dependency in Cargo.toml:[package]

name = "dev_add_nif"

version = "0.1.0"

edition = "2021"

 

[lib]

crate-type = ["cdylib"]

 

[dependencies]

rustler = "0.36" Step 2: Implement the Rust NIF Create the NIF function in src/lib.rs:use rustler::{Env, NifResult, Term, Encoder};

use rustler::types::atom::ok;

 

#[rustler::nif]

fn add<'a>(env: Env<'a>, a: i64, b: i64) -> NifResult<Term<'a>> {

    Ok((ok(), a + b).encode(env))

}

 

rustler::init!("dev_add_nif", [add]);Step 3: Build the Rust Library Compile the Rust code:cargo build For release builds with optimizations:cargo build --release Step 4: Configure rebar.config Add the device to cargo_opts in HyperBEAM/rebar.config:{cargo_opts, [

    {src_dir, "native/dev_add_nif"},

    {src_dir, "native/dev_snp_nif"}

]}.Step 5: Create the Erlang NIF Module Create HyperBEAM/src/dev_add_nif.erl:-module(dev_add_nif).

-export([add/2]).

-on_load(init/0).

 

-include("include/cargo.hrl").

-include_lib("eunit/include/eunit.hrl").

 

init() ->

    ?load_nif_from_crate(dev_add_nif, 0).

 

add(_, _) ->

    erlang:nif_error(nif_not_loaded).Step 6: Create the Erlang Device Module Create HyperBEAM/src/dev_add.erl:-module(dev_add).

-export([add/3]).

-include("include/hb.hrl").

-include_lib("eunit/include/eunit.hrl").

 

add(_M1, M2, _Opts) ->

    A = maps:get(<<"a">>, M2),

    B = maps:get(<<"b">>, M2),

    {ok, Sum} = dev_add_nif:add(A, B),

    {ok, #{ <<"sum">> => Sum }}.

 

add_test() ->

    M1 = #{ <<"device">> => <<"add@1.0">> },

    M2 = #{ <<"path">> => <<"add">>, <<"a">> => 2, <<"b">> => 3 },

    {ok, #{ <<"sum">> := 5 }} = hb_ao:resolve(M1, M2, #{}).Step 7: Register the Device Add the device to HyperBEAM/hb_opt.erl:preloaded_devices => [

  ...

  #{<<"name">> => <<"add@1.0">>, <<"module">> => dev_add},

  ...

],Step 8: Build and Test Run the unit tests:rebar3 eunit --module=dev_add Test the device with WAO:

---

# 340. Hash Verification Strategy - ARIO Docs

Document Number: 340
Source: https://docs.ar.io/wayfinder/core/verification-strategies/hash-verification
Words: 97
Extraction Method: html

HashVerificationStrategy Overview The HashVerificationStrategy verifies data integrity by comparing SHA-256 hashes of fetched data against trusted gateway digest headers. This strategy provides fast, cryptographically secure verification for high-throughput applications.How It Works Fetch Data: Retrieve content from the selected gateway Request Digest: Get the digest from trusted gateways via HTTP headers using HEAD/GET requests from a trusted gatweay Compute Hash: Calculate the SHA-256 hash of the received data Compare: Verify that both hashes match exactly Result: Pass or fail based on hash comparison Basic Usage Related Signature Verification: Understand authenticity validation Data Root Verification: Explore maximum security verification

---

# 341. ao  Cookbook

Document Number: 341
Source: https://cookbook_ao.arweave.net/guides/aos/modules/ao.html
Words: 94
Extraction Method: html

Skip to content  ao Built-in global library for process communication and management. The ao object provides core functionality for sending messages, spawning processes, and logging.Core Functions ao.send(msg) Sends a message to another process. See the ao.send reference for more information.ao.spawn(module: string, spawn: table) Creates a new process from a module. See the ao.spawn reference for more information.ao.log(string|table) Logs messages or data that can be read by process callers.Environment The ao.env variable contains process initialization info like ID, owner, and tags.For the complete API reference including all properties and functions, see the ao reference documentation.

---

# 342. Building ao Processes - HyperBEAM - Documentation

Document Number: 342
Source: https://hyperbeam.arweave.net/build/building-on-ao.html
Words: 92
Extraction Method: html

Building on HyperBEAM with ao The guides for building applications on HyperBEAM and interacting with ao processes have been moved to the AO Processes Cookbook to provide a centralized resource for developers.Here are some helpful resources from the AO Processes Cookbook:Get Started with ao to learn the basics of ao and how to start building processes.Migration Guide for moving processes from legacynet and using new HyperBEAM features.Using aos with HyperBEAM for using the aos command-line tool with HyperBEAM.Using aoconnect with HyperBEAM for using the aoconnect library to interact with processes on HyperBEAM.

---

# 343. JSON  Cookbook

Document Number: 343
Source: https://cookbook_ao.arweave.net/guides/aos/modules/json.html
Words: 85
Extraction Method: html

Skip to content  JSON The JSON module allows you to encode and decode objects using JavaScript Object Notation.Example usage Module functions encode() This function returns a string representation of a Lua object in JSON.Parameters:val: {any} The object to format as JSON Returns: JSON string representation of the provided object Example decode() The function takes a JSON string and turns it into a Lua object.Parameters:val: {any} The JSON string to decode Returns: Lua object corresponding to the JSON string (throws an error for invalid JSON strings)

---

# 344. References  Cooking with the Permaweb

Document Number: 344
Source: https://cookbook.arweave.net/references/index.html
Words: 85
Extraction Method: html

References Reference documentation, specifications, and resources for Permaweb development.Quick References Glossary - Definitions of key terms and concepts LLMs.txt - Machine-readable documentation for AI assistants Technical Specifications ArFS Specification - Arweave File System standard Data Model Entity Types Content Types Privacy Schema Diagrams External Resources Arweave Documentation open in new window Community Forum open in new window Developer Discord open in new window API References For specific tool and service APIs, see the Tooling section.Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 345. LLMs Documentation  Cookbook

Document Number: 345
Source: https://cookbook_ao.arweave.net/llms-explanation.html
Words: 84
Extraction Method: html

Skip to content  LLMs Documentation llms.txt:Structured overview of the ao ecosystem.Ideal for AI tools navigating documentation or answering general questions.Suited for agents with web search capabilities.llms-full.txt:Complete technical documentation.Designed for in-depth analysis, troubleshooting, or chatbot integration.Provides exhaustive details for complex queries.INFO The llms-full.txt file only contains content from references and release notes, as testing showed this focused approach performs better with current AI models.Permaweb LLMs.txt:The following is a tool that allows you to build your own LLMs.txt files based on docs from the permaweb ecosystem.

---

# 346. Eval  Cookbook

Document Number: 346
Source: https://cookbook_ao.arweave.net/concepts/eval.html
Words: 81
Extraction Method: html

Skip to content  Eval Each AO process includes an onboard Eval handler that evaluates any new code it receives. This handler determines the appropriate action for the code and verifies that the message originates from the process owner.The Eval handler can also be manually triggered to evaluate the Data field from an incoming message. When you use the .load function to load a file into a process, it relies on the Eval handler to evaluate the file’s content under the hood.

---

# 347. Monitoring Cron  Cookbook

Document Number: 347
Source: https://cookbook_ao.arweave.net/guides/aoconnect/monitoring-cron.html
Words: 74
Extraction Method: html

Skip to content  Monitoring Cron When using cron messages, ao users need a way to start ingesting the messages, using this monitor method, ao users can initiate the subscription service for cron messages. Setting cron tags means that your process will start producing cron results in its outbox, but you need to monitor these results if you want messages from those results to be pushed through the network.You can stop monitoring by calling unmonitor

---

# 348. Routing Strategies - ARIO Docs

Document Number: 348
Source: https://docs.ar.io/wayfinder/core/routing-strategies
Words: 126
Extraction Method: html

Routing strategies determine how Wayfinder selects which AR.IO gateway to use for each request. Different strategies optimize for different goals like performance, reliability, or load distribution.Strategy Comparison Strategy Best For Use Case Predictability Infrastructure Control Fastest Ping Real-time applications Performance-critical apps, gaming Medium Low Ping Health-checked routing Combining custom routing with health checks Medium Medium Preferred + Fallback Dedicated infrastructure CDN with origin fallback, enterprise gateways High Maximum Round Robin Load balancing Even distribution across known gateways High High Random Simple distribution Basic load spreading, testing Low Medium Static Single gateway Development, specific gateway requirements Maximum High Related FastestPingRoutingStrategy: Network-based gateway discovery PingRoutingStrategy: Health-checked routing wrapper PreferredWithFallbackRoutingStrategy: Static gateway configuration RoundRobinRoutingStrategy: Even distribution across gateways RandomRoutingStrategy: Randomized gateway selection StaticRoutingStrategy: Always use a single, fixed gateway

---

# 349. ARIO Docs

Document Number: 349
Source: https://docs.ar.io/wayfinder/core/gateway-providers
Words: 100
Extraction Method: html

Gateway Providers Overview Gateway providers are responsible for discovering and managing AR.IO gateways that Wayfinder can use to access Arweave data. They abstract the complexity of gateway discovery and provide a consistent interface for routing strategies to select optimal gateways.Provider Comparison Provider Discovery Performance Reliability Use Case NetworkGatewaysProvider Dynamic Medium High Production StaticGatewaysProvider Static High Medium Development/Testing SimpleCacheGatewaysProvider Cached High High Node based environments LocalStorageGatewaysProvider Cached High High Web based environments Related NetworkGatewaysProvider: Network-based gateway discovery StaticGatewaysProvider: Static gateway configuration SimpleCacheGatewaysProvider: Caching wrapper for providers Routing Strategies: How gateways are selected for requests Wayfinder Configuration: Main wayfinder setup and usage

---

# 350. ARIO Docs

Document Number: 350
Source: https://docs.ar.io/wayfinder/core/gateway-providers/static
Words: 66
Extraction Method: html

StaticGatewaysProvider Overview The StaticGatewaysProvider uses a predefined list of gateway URLs, making it ideal for development, testing, or when you need to use specific trusted gateways. It provides fast, predictable gateway discovery without network calls.Basic Usage Configuration Options StaticGatewaysProviderOptions Related Documentation Gateway Providers Overview: Compare all gateway providers NetworkGatewaysProvider: Dynamic network discovery SimpleCacheGatewaysProvider: Caching wrapper Wayfinder Configuration: Main wayfinder setup Routing Strategies: How gateways are selected

---

# 351. Deployment  Publishing Tools  Cooking with the Permaweb

Document Number: 351
Source: https://cookbook.arweave.net/tooling/deployment.html
Words: 62
Extraction Method: html

Deployment & Publishing Tools Tools for deploying applications and publishing content to the permaweb.CLI Tools arkb A command-line tool for deploying static websites and applications to Arweave.permaweb-deploy Modern deployment tool with built-in bundling and optimization.CI/CD Integration GitHub Actions Automated deployment workflows for continuous integration.Other CI Platforms Integration guides for popular CI/CD platforms.Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 352. Cooking with the Permaweb

Document Number: 352
Source: https://cookbook.arweave.net/tooling/specs/arfs/schema-diagrams.html
Words: 62
Extraction Method: html

Schema Diagrams The following diagrams show complete examples of Drive, Folder, and File entity Schemas.Public Drive  Public Drive Schema Private Drive  Private Drive Schema Arweave GQL Tag Byte Limit is restricted to 2048. There is no determined limit on Data JSON custom metadata, though more data results in a higher upload cost.Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 353. Wayfinder Browser Extension Release Notes - ARIO Docs

Document Number: 353
Source: https://docs.ar.io/wayfinder/release-notes/extension
Words: 634
Extraction Method: html

Wayfinder Browser Extension Changelog Overview Welcome to the documentation page for the Wayfinder Browser Extension release notes. Here, you will find detailed information about each version of the Wayfinder Browser Extension, including the enhancements, bug fixes, and any other changes introduced in every release. This page serves as a comprehensive resource to keep you informed about the latest developments and updates in the Wayfinder Browser Extension.The Wayfinder Browser Extension provides fast, verifiable, decentralized access to ar:// links via AR.IO's Permanent Cloud. It allows you to browse ar:// URLs as easily as any website with zero setup required. The extension is the access and routing layer of the AR.IO Permanent Cloud, bringing speed, resilience, and verifiability to Arweave content.You can install the Wayfinder Browser Extension from the Chrome Web Store.For the most up-to-date information, you can also visit the Wayfinder Extension GitHub repository.Release History 1.0.10 (2025-08-05) Patch Changes 736b5e3: Default showVerificationToasts to false 1.0.9 (2025-08-05) Patch Changes 96e99d2: Wrap routing strategies in PingRoutingStrategy to avoid being directed to gateways that do not work 1.0.8 (2025-08-04) Patch Changes 2d19291: Fix rendering race condition with verification toasts Updated dependencies [2d5970f] @ar.io/wayfinder-core@1.2.0 1.0.8-alpha.0 (2025-08-04) Patch Changes 2d19291: Fix rendering race condition with verification toasts Updated dependencies [2d5970f] @ar.io/wayfinder-core@1.2.0-alpha.0 1.0.7 (2025-07-30) Patch Changes Updated dependencies [69ddbfb] @ar.io/wayfinder-core@1.1.0 1.0.7-alpha.0 (2025-07-23) Patch Changes Updated dependencies [69ddbfb] @ar.io/wayfinder-core@1.1.0-alpha.0 1.0.6 (2025-07-23) Patch Changes Updated dependencies [a42d57c] @ar.io/wayfinder-core@1.0.6 1.0.6-alpha.0 (2025-07-23) Patch Changes Updated dependencies [a42d57c] @ar.io/wayfinder-core@1.0.6-alpha.0 1.0.5 (2025-07-22) Patch Changes Updated dependencies [73aa1b9] Updated dependencies [b7299cc] Updated dependencies [b81b54e] @ar.io/wayfinder-core@1.0.5 1.0.5-alpha.0 (2025-07-21) Patch Changes Updated dependencies [73aa1b9] @ar.io/wayfinder-core@1.0.5-alpha.0 1.0.4 (2025-07-17) Patch Changes 79a46d1: Performance improvements for wayfinder-extension 1.0.3 (2025-07-17) Patch Changes aed86bb: Performance improvements for wayfinder-extension 1.0.3-alpha.0 (2025-07-17) Patch Changes aed86bb: Performance improvements for wayfinder-extension 1.0.2 (2025-07-16) Patch Changes Updated dependencies [86bdc2f] Updated dependencies [226f3af] @ar.io/wayfinder-core@1.0.3 1.0.2-alpha.0 (2025-07-16) Patch Changes Updated dependencies [86bdc2f] @ar.io/wayfinder-core@1.0.3-alpha.0 1.0.1 (2025-07-15) Patch Changes Updated dependencies [8f79caf] Updated dependencies [a3e69af] Updated dependencies [cfcfb66] @ar.io/wayfinder-core@1.0.2 1.0.1-alpha.0 (2025-07-15) Patch Changes Updated dependencies [a3e69af] @ar.io/wayfinder-core@1.0.2-alpha.0 (2025-07-14) Major Changes 147f087: Initial release of wayfinder-extension Patch Changes f823114: Update manifest.json f00c8a1: Update build script for wayfinder-extension 9629604: Initial wayfinder-extension@1.0.0 Updated dependencies [aa5700e] Updated dependencies [2c170be] Updated dependencies [c78effa] @ar.io/wayfinder-core@1.0.1 1.0.0-alpha.3 (2025-07-14) Patch Changes f00c8a1: Update build script for wayfinder-extension 1.0.0-alpha.2 (2025-07-10) Patch Changes f823114: Update manifest.json 1.0.0-alpha.1 (2025-07-10) Major Changes 147f087: Initial release of wayfinder-extension Patch Changes 9629604: Initial wayfinder-extension@1.0.0 Updated dependencies [2c170be] @ar.io/wayfinder-core@1.0.1-alpha.1 0.0.19-alpha.0 (2025-07-14) Patch Changes Updated dependencies [aa5700e] @ar.io/wayfinder-core@1.0.1-alpha.0 0.0.18 (2025-07-01) Patch Changes Updated dependencies [dc90515] Updated dependencies [09b3759] Updated dependencies [2d72bba] Updated dependencies [79254d1] Updated dependencies [89c0efe] Updated dependencies [4f062ad] Updated dependencies [e9245df] Updated dependencies [063e480] @ar.io/wayfinder-core@1.0.0 0.0.18-alpha.8 (2025-07-01) Patch Changes Updated dependencies [09b3759] Updated dependencies [89c0efe] Updated dependencies [e9245df] @ar.io/wayfinder-core@1.0.0-alpha.8 0.0.18-alpha.7 (2025-07-01) Patch Changes Updated dependencies [4f062ad] @ar.io/wayfinder-core@0.0.5-alpha.7 0.0.18-alpha.6 (2025-07-01) Patch Changes Updated dependencies [dc90515] @ar.io/wayfinder-core@0.0.5-alpha.6 0.0.18-alpha.5 (2025-07-01) Patch Changes Updated dependencies [79254d1] @ar.io/wayfinder-core@0.0.5-alpha.5 0.0.18-alpha.4 (2025-07-01) Patch Changes Updated dependencies [2d72bba] @ar.io/wayfinder-core@0.0.5-alpha.4 0.0.18-alpha.3 (2025-07-01) Patch Changes Updated dependencies [063e480] @ar.io/wayfinder-core@0.0.5-alpha.3 0.0.18-alpha.2 (2025-07-01) Patch Changes Updated dependencies [b85ec7e] @ar.io/wayfinder-core@0.0.5-alpha.2 0.0.18-alpha.1 (2025-07-01) Patch Changes Updated dependencies [aba2beb] @ar.io/wayfinder-core@0.0.5-alpha.1 0.0.18-alpha.0 (2025-07-01) Patch Changes Updated dependencies [4afd953] @ar.io/wayfinder-core@0.0.5-alpha.0 0.0.17 (2025-06-29) Patch Changes Updated dependencies [e43548d] @ar.io/wayfinder-core@0.0.4 0.0.17-alpha.1 (2025-06-29) Patch Changes Updated dependencies [78ad2b2] @ar.io/wayfinder-core@0.0.4-alpha.1 0.0.17-alpha.0 (2025-06-29) Patch Changes Updated dependencies [7c81839] @ar.io/wayfinder-core@0.0.4-alpha.0 0.0.16 (2025-06-27) Patch Changes Updated dependencies [53613fb] Updated dependencies [c12a8f8] Updated dependencies [45d2884] Updated dependencies [8e7facb] Updated dependencies [2605cdb] Updated dependencies [d431437] Updated dependencies [1ceb8df] Updated dependencies [2109250] @ar.io/wayfinder-core@0.0.3 0.0.16-alpha.6 (2025-06-27) Patch Changes Updated dependencies [1ceb8df] @ar.io/wayfinder-core@0.0.3-alpha.6 0.0.16-alpha.5 (2025-06-27) Patch Changes Updated dependencies [53613fb] @ar.io/wayfinder-core@0.0.3-alpha.5 0.0.16-alpha.4 (2025-06-27) Patch Changes Updated dependencies [8e7facb] @ar.io/wayfinder-core@0.0.3-alpha.4 0.0.16-alpha.3 (2025-06-27) Patch Changes Updated dependencies [d431437] @ar.io/wayfinder-core@0.0.3-alpha.3 0.0.16-alpha.2 (2025-06-27) Patch Changes Updated dependencies [2109250] @ar.io/wayfinder-core@0.0.3-alpha.2 0.0.16-alpha.1 (2025-06-27) Patch Changes Updated dependencies [c12a8f8] Updated dependencies [2605cdb] @ar.io/wayfinder-core@0.0.3-alpha.1 0.0.16-beta.0 (2025-06-27) Patch Changes Updated dependencies [45d2884] @ar.io/wayfinder-core@0.0.3-beta.0 0.0.15 (2025-06-20) Patch Changes Updated dependencies @ar.io/wayfinder-core@0.0.2 0.0.15-beta.0 (2025-06-20) Patch Changes Updated dependencies @ar.io/wayfinder-core@0.0.2-beta.0

---

# 354. Wayfinder React Release Notes - ARIO Docs

Document Number: 354
Source: https://docs.ar.io/wayfinder/release-notes/react
Words: 344
Extraction Method: html

Wayfinder React Changelog Overview Welcome to the documentation page for the Wayfinder React release notes. Here, you will find detailed information about each version of the Wayfinder React library, including the enhancements, bug fixes, and any other changes introduced in every release. This page serves as a comprehensive resource to keep you informed about the latest developments and updates in the Wayfinder React library.For the most up-to-date information, you can also visit the Wayfinder React GitHub repository.Release History 1.0.11 (2025-08-04) Patch Changes Updated dependencies [2d5970f] @ar.io/wayfinder-core@1.2.0 1.0.11-alpha.0 (2025-08-04) Patch Changes Updated dependencies [2d5970f] @ar.io/wayfinder-core@1.2.0-alpha.0 1.0.10 (2025-07-30) Patch Changes Updated dependencies [b246f78] @ar.io/wayfinder-core@1.1.1 1.0.9 (2025-07-23) Patch Changes Updated dependencies [69ddbfb] @ar.io/wayfinder-core@1.1.0 1.0.9 (2025-07-23) Patch Changes Updated dependencies [658c5f6] @ar.io/wayfinder-core@1.0.7 1.0.8 (2025-07-23) Patch Changes Updated dependencies [a42d57c] @ar.io/wayfinder-core@1.0.6 1.0.8-alpha.0 (2025-07-23) Patch Changes Updated dependencies [a42d57c] @ar.io/wayfinder-core@1.0.6-alpha.0 1.0.7 (2025-07-22) Patch Changes 3f4d41e: Use LocalStorageGatewaysProvider by default to avoid rate limits Updated dependencies [73aa1b9] Updated dependencies [b7299cc] Updated dependencies [b81b54e] @ar.io/wayfinder-core@1.0.5 1.0.7-alpha.2 (2025-07-22) Patch Changes Updated dependencies [b7299cc] @ar.io/wayfinder-core@1.0.5-alpha.2 1.0.7-alpha.1 (2025-07-22) Patch Changes 3f4d41e: Use LocalStorageGatewaysProvider by default to avoid rate limits Updated dependencies [b81b54e] @ar.io/wayfinder-core@1.0.5-alpha.1 1.0.7-alpha.0 (2025-07-21) Patch Changes Updated dependencies [73aa1b9] @ar.io/wayfinder-core@1.0.5-alpha.0 1.0.6 (2025-07-18) Patch Changes 1f64207: Memoize params in useWayfinderUrl to avoid rerenders 1.0.6-alpha.0 (2025-07-18) Patch Changes 1f64207: Memoize params in useWayfinderUrl to avoid rerenders 1.0.5 (2025-07-17) Patch Changes Updated dependencies [719acbd] @ar.io/wayfinder-core@1.0.4 1.0.4 (2025-07-16) Patch Changes Updated dependencies [86bdc2f] Updated dependencies [226f3af] @ar.io/wayfinder-core@1.0.3 1.0.4-alpha.1 (2025-07-16) Patch Changes Updated dependencies [226f3af] @ar.io/wayfinder-core@1.0.3-alpha.1 1.0.4-alpha.0 (2025-07-16) Patch Changes Updated dependencies [86bdc2f] @ar.io/wayfinder-core@1.0.3-alpha.0 1.0.3 (2025-07-15) Patch Changes Updated dependencies [8f79caf] Updated dependencies [a3e69af] Updated dependencies [cfcfb66] @ar.io/wayfinder-core@1.0.2 1.0.3-alpha.2 (2025-07-15) Patch Changes Updated dependencies [8f79caf] @ar.io/wayfinder-core@1.0.2-alpha.2 1.0.3-alpha.1 (2025-07-15) Patch Changes Updated dependencies [cfcfb66] @ar.io/wayfinder-core@1.0.2-alpha.1 1.0.3-alpha.0 (2025-07-15) Patch Changes Updated dependencies [a3e69af] @ar.io/wayfinder-core@1.0.2-alpha.0 1.0.2 (2025-07-14) Patch Changes Updated dependencies [aa5700e] Updated dependencies [2c170be] Updated dependencies [c78effa] @ar.io/wayfinder-core@1.0.1 1.0.2-alpha.2 (2025-07-14) Patch Changes Updated dependencies [c78effa] @ar.io/wayfinder-core@1.0.1-alpha.2 1.0.2-alpha.1 (2025-07-14) Patch Changes Updated dependencies [2c170be] @ar.io/wayfinder-core@1.0.1-alpha.1 1.0.2-alpha.0 (2025-07-09) Patch Changes Updated dependencies [aa5700e] @ar.io/wayfinder-core@1.0.1-alpha.0 1.0.1 (2025-07-07) Patch Changes 03c4800: Update useWayfinderUrl to support any WayfinderURLParams (2025-07-01) Major Changes ec79105: Initial release of wayfinder-react

---

# 355. Tutorials  Cookbook

Document Number: 355
Source: https://cookbook_ao.arweave.net/tutorials/index.html
Words: 52
Extraction Method: html

Skip to content  Tutorials Here, we've created a series of tutorials to help you get started with aos and build your first processes. These tutorials include interactive guides, code snippets, and examples to help you get comfortable with the aos environment.List of Tutorials Getting Started - An Interactive Guide Bots and Games

---

# 356. Pretty  Cookbook

Document Number: 356
Source: https://cookbook_ao.arweave.net/guides/aos/modules/pretty.html
Words: 48
Extraction Method: html

Skip to content  Pretty This module allows printing formatted, human-friendly and readable syntax.Module functions tprint() Returns a formatted string of the structure of the provided table.Parameters:tbl: {table} The table to format indent: {number} Optional indentation of each level of the table Returns: Table structure formatted as a string

---

# 357. Installing ao connect  Cookbook

Document Number: 357
Source: https://cookbook_ao.arweave.net/guides/aoconnect/installing-connect.html
Words: 46
Extraction Method: html

Skip to content  Installing ao connect Prerequisites  In order to install ao connect into your app you must have NodeJS/NPM 18 or higher. Installing npm yarn  This module can now be used from NodeJS as well as a browser, it can be included as shown below.

---

# 358. aoconnect  Cookbook

Document Number: 358
Source: https://cookbook_ao.arweave.net/guides/aoconnect/aoconnect.html
Words: 44
Extraction Method: html

Skip to content  aoconnect ao connect is a Javascript/Typescript library to interact with the system from Node JS or the browser.Guides in this section provide snippets on how to utilize ao connect. All snippets are written in Javascript but should translate easily to Typescript.

---

# 359. Blueprints  Cookbook

Document Number: 359
Source: https://cookbook_ao.arweave.net/guides/aos/blueprints/index.html
Words: 38
Extraction Method: html

Skip to content  Blueprints Blueprints are predesigned templates that help you quickly build in ao. They are a great way to get started and can be customized to fit your needs.Available Blueprints Chatroom CRED Utils Staking Token Voting

---

# 360. LLMstxt  Cooking with the Permaweb

Document Number: 360
Source: https://cookbook.arweave.net/references/llms-txt.html
Words: 36
Extraction Method: html

LLMs.txt The following is a tool open in new window that allows you to build your own LLMs.txt files based on docs from the permaweb ecosystem.Built with  ❤️  by the Arweave community. Learn more at  Arweave.org

---

# 361. Glossary  Cooking with the Permaweb

Document Number: 361
Source: https://cookbook.arweave.net/references/glossary.html
Words: 26
Extraction Method: html

Glossary The following is an embedded glossary tool open in new window for the permaweb ecosystem.Built with  ❤️  by the Arweave community. Learn more at  Arweave.org
